[
    {
        "commit": "aa0274d261cc50af416883effdab505fad400485",
        "message": "Pull io_uring fix from Jens Axboe:\n \"A single fix for a parameter type which affects 32-bit\"\n\n* tag 'io_uring-6.13-20241207' of git://git.kernel.dk/linux:\n  io_uring: Change res2 parameter type in io_uring_cmd_done",
        "kernel_version": "v6.13-rc2",
        "release_date": "2024-12-07 10:01:13 -0800 Merge tag 'io_uring-6.13-20241207' of git://git.kernel.dk/linux"
    },
    {
        "commit": "feffde684ac29a3b7aec82d2df850fbdbdee55e4",
        "message": "Pull btrfs fixes from David Sterba:\n\n - add lockdep annotations for io_uring/encoded read integration, inode\n   lock is held when returning to userspace\n\n - properly reflect experimental config option to sysfs\n\n - handle NULL root in case the rescue mode accepts invalid/damaged tree\n   roots (rescue=ibadroot)\n\n - regression fix of a deadlock between transaction and extent locks\n\n - fix pending bio accounting bug in encoded read ioctl\n\n - fix NOWAIT mode when checking references for NOCOW files\n\n - fix use-after-free in a rb-tree cleanup in ref-verify debugging tool\n\n* tag 'for-6.13-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux:\n  btrfs: fix lockdep warnings on io_uring encoded reads\n  btrfs: ref-verify: fix use-after-free after invalid ref action\n  btrfs: add a sanity check for btrfs root in btrfs_search_slot()\n  btrfs: don't loop for nowait writes when checking for cross references\n  btrfs: sysfs: advertise experimental features only if CONFIG_BTRFS_EXPERIMENTAL=y\n  btrfs: fix deadlock between transaction commits and extent locks\n  btrfs: fix use-after-free in btrfs_encoded_read_endio()",
        "kernel_version": "v6.13-rc2",
        "release_date": "2024-12-03 11:02:17 -0800 Merge tag 'for-6.13-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux"
    },
    {
        "commit": "a07d2d7930c75e6bf88683b376d09ab1f3fed2aa",
        "message": "Change the type of the res2 parameter in io_uring_cmd_done from ssize_t\nto u64. This aligns the parameter type with io_req_set_cqe32_extra,\nwhich expects u64 arguments.\nThe change eliminates potential issues on 32-bit architectures where\nssize_t might be 32-bit.\n\nOnly user of passing res2 is drivers/nvme/host/ioctl.c and it actually\npasses u64.\n\nFixes: ee692a21e9bf (\"fs,io_uring: add infrastructure for uring-cmd\")\nCc: stable@vger.kernel.org\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nTested-by: Li Zetao <lizetao1@huawei.com>\nReviewed-by: Li Zetao <lizetao1@huawei.com>\nSigned-off-by: Bernd Schubert <bschubert@ddn.com>\nLink: https://lore.kernel.org/r/20241203-io_uring_cmd_done-res2-as-u64-v2-1-5e59ae617151@ddn.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc2",
        "release_date": "2024-12-03 06:33:13 -0700 io_uring: Change res2 parameter type in io_uring_cmd_done"
    },
    {
        "commit": "dd54fcced81d479d77acbeb4eea74b9ab9276bff",
        "message": "Pull more io_uring updates from Jens Axboe:\n\n - Remove a leftover struct from when the cqwait registered waiting was\n   transitioned to regions.\n\n - Fix for an issue introduced in this merge window, where nop->fd might\n   be used uninitialized. Ensure it's always set.\n\n - Add capping of the task_work run in local task_work mode, to prevent\n   bursty and long chains from adding too much latency.\n\n - Work around xa_store() leaving ->head non-NULL if it encounters an\n   allocation error during storing. Just a debug trigger, and can go\n   away once xa_store() behaves in a more expected way for this\n   condition. Not a major thing as it basically requires fault injection\n   to trigger it.\n\n - Fix a few mapping corner cases\n\n - Fix KCSAN complaint on reading the table size post unlock. Again not\n   a \"real\" issue, but it's easy to silence by just keeping the reading\n   inside the lock that protects it.\n\n* tag 'io_uring-6.13-20242901' of git://git.kernel.dk/linux:\n  io_uring/tctx: work around xa_store() allocation error issue\n  io_uring: fix corner case forgetting to vunmap\n  io_uring: fix task_work cap overshooting\n  io_uring: check for overflows in io_pin_pages\n  io_uring/nop: ensure nop->fd is always initialized\n  io_uring: limit local tw done\n  io_uring: add io_local_work_pending()\n  io_uring/region: return negative -E2BIG in io_create_region()\n  io_uring: protect register tracing\n  io_uring: remove io_uring_cqwait_reg_arg",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-30 15:43:02 -0800 Merge tag 'io_uring-6.13-20242901' of git://git.kernel.dk/linux"
    },
    {
        "commit": "22d2e48e318564f8c9b09faf03ecb4f03fb44dd5",
        "message": "Lockdep doesn't like the fact that btrfs_uring_read_extent() returns to\nuserspace still holding the inode lock, even though we release it once\nthe I/O finishes. Add calls to rwsem_release() and rwsem_acquire_read() to\nwork round this.\n\nReported-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>\n34310c442e17 (\"btrfs: add io_uring command for encoded reads (ENCODED_READ ioctl)\")\nSigned-off-by: Mark Harmstone <maharmstone@fb.com>\nReviewed-by: David Sterba <dsterba@suse.com>\nSigned-off-by: David Sterba <dsterba@suse.com>",
        "kernel_version": "v6.13-rc2",
        "release_date": "2024-11-29 16:56:38 +0100 btrfs: fix lockdep warnings on io_uring encoded reads"
    },
    {
        "commit": "e8b8344de3980709080d86c157d24e7de07d70ad",
        "message": "Set new allocated bfqq to bic or remove freed bfqq from bic are both\nprotected by bfqd->lock, however bfq_limit_depth() is deferencing bfqq\nfrom bic without the lock, this can lead to UAF if the io_context is\nshared by multiple tasks.\n\nFor example, test bfq with io_uring can trigger following UAF in v6.6:\n\n==================================================================\nBUG: KASAN: slab-use-after-free in bfqq_group+0x15/0x50\n\nCall Trace:\n <TASK>\n dump_stack_lvl+0x47/0x80\n print_address_description.constprop.0+0x66/0x300\n print_report+0x3e/0x70\n kasan_report+0xb4/0xf0\n bfqq_group+0x15/0x50\n bfqq_request_over_limit+0x130/0x9a0\n bfq_limit_depth+0x1b5/0x480\n __blk_mq_alloc_requests+0x2b5/0xa00\n blk_mq_get_new_requests+0x11d/0x1d0\n blk_mq_submit_bio+0x286/0xb00\n submit_bio_noacct_nocheck+0x331/0x400\n __block_write_full_folio+0x3d0/0x640\n writepage_cb+0x3b/0xc0\n write_cache_pages+0x254/0x6c0\n write_cache_pages+0x254/0x6c0\n do_writepages+0x192/0x310\n filemap_fdatawrite_wbc+0x95/0xc0\n __filemap_fdatawrite_range+0x99/0xd0\n filemap_write_and_wait_range.part.0+0x4d/0xa0\n blkdev_read_iter+0xef/0x1e0\n io_read+0x1b6/0x8a0\n io_issue_sqe+0x87/0x300\n io_wq_submit_work+0xeb/0x390\n io_worker_handle_work+0x24d/0x550\n io_wq_worker+0x27f/0x6c0\n ret_from_fork_asm+0x1b/0x30\n </TASK>\n\nAllocated by task 808602:\n kasan_save_stack+0x1e/0x40\n kasan_set_track+0x21/0x30\n __kasan_slab_alloc+0x83/0x90\n kmem_cache_alloc_node+0x1b1/0x6d0\n bfq_get_queue+0x138/0xfa0\n bfq_get_bfqq_handle_split+0xe3/0x2c0\n bfq_init_rq+0x196/0xbb0\n bfq_insert_request.isra.0+0xb5/0x480\n bfq_insert_requests+0x156/0x180\n blk_mq_insert_request+0x15d/0x440\n blk_mq_submit_bio+0x8a4/0xb00\n submit_bio_noacct_nocheck+0x331/0x400\n __blkdev_direct_IO_async+0x2dd/0x330\n blkdev_write_iter+0x39a/0x450\n io_write+0x22a/0x840\n io_issue_sqe+0x87/0x300\n io_wq_submit_work+0xeb/0x390\n io_worker_handle_work+0x24d/0x550\n io_wq_worker+0x27f/0x6c0\n ret_from_fork+0x2d/0x50\n ret_from_fork_asm+0x1b/0x30\n\nFreed by task 808589:\n kasan_save_stack+0x1e/0x40\n kasan_set_track+0x21/0x30\n kasan_save_free_info+0x27/0x40\n __kasan_slab_free+0x126/0x1b0\n kmem_cache_free+0x10c/0x750\n bfq_put_queue+0x2dd/0x770\n __bfq_insert_request.isra.0+0x155/0x7a0\n bfq_insert_request.isra.0+0x122/0x480\n bfq_insert_requests+0x156/0x180\n blk_mq_dispatch_plug_list+0x528/0x7e0\n blk_mq_flush_plug_list.part.0+0xe5/0x590\n __blk_flush_plug+0x3b/0x90\n blk_finish_plug+0x40/0x60\n do_writepages+0x19d/0x310\n filemap_fdatawrite_wbc+0x95/0xc0\n __filemap_fdatawrite_range+0x99/0xd0\n filemap_write_and_wait_range.part.0+0x4d/0xa0\n blkdev_read_iter+0xef/0x1e0\n io_read+0x1b6/0x8a0\n io_issue_sqe+0x87/0x300\n io_wq_submit_work+0xeb/0x390\n io_worker_handle_work+0x24d/0x550\n io_wq_worker+0x27f/0x6c0\n ret_from_fork+0x2d/0x50\n ret_from_fork_asm+0x1b/0x30\n\nFix the problem by protecting bic_to_bfqq() with bfqd->lock.\n\nCC: Jan Kara <jack@suse.cz>\nFixes: 76f1df88bbc2 (\"bfq: Limit number of requests consumed by each cgroup\")\nSigned-off-by: Yu Kuai <yukuai3@huawei.com>\nLink: https://lore.kernel.org/r/20241129091509.2227136-1-yukuai1@huaweicloud.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-29 08:42:14 -0700 block, bfq: fix bfqq uaf in bfq_limit_depth()"
    },
    {
        "commit": "7eb75ce7527129d7f1fee6951566af409a37a1c4",
        "message": "syzbot triggered the following WARN_ON:\n\nWARNING: CPU: 0 PID: 16 at io_uring/tctx.c:51 __io_uring_free+0xfa/0x140 io_uring/tctx.c:51\n\nwhich is the\n\nWARN_ON_ONCE(!xa_empty(&tctx->xa));\n\nsanity check in __io_uring_free() when a io_uring_task is going through\nits final put. The syzbot test case includes injecting memory allocation\nfailures, and it very much looks like xa_store() can fail one of its\nmemory allocations and end up with ->head being non-NULL even though no\nentries exist in the xarray.\n\nUntil this issue gets sorted out, work around it by attempting to\niterate entries in our xarray, and WARN_ON_ONCE() if one is found.\n\nReported-by: syzbot+cc36d44ec9f368e443d3@syzkaller.appspotmail.com\nLink: https://lore.kernel.org/io-uring/673c1643.050a0220.87769.0066.GAE@google.com/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-29 07:20:28 -0700 io_uring/tctx: work around xa_store() allocation error issue"
    },
    {
        "commit": "43eef70e7e2ac74e7767731dd806720c7fb5e010",
        "message": "io_pages_unmap() is a bit tricky in trying to figure whether the pages\nwere previously vmap'ed or not. In particular If there is juts one page\nit belives there is no need to vunmap. Paired io_pages_map(), however,\ncould've failed io_mem_alloc_compound() and attempted to\nio_mem_alloc_single(), which does vmap, and that leads to unpaired vmap.\n\nThe solution is to fail if io_mem_alloc_compound() can't allocate a\nsingle page. That's the easiest way to deal with it, and those two\nfunctions are getting removed soon, so no need to overcomplicate it.\n\nCc: stable@vger.kernel.org\nFixes: 3ab1db3c6039e (\"io_uring: get rid of remap_pfn_range() for mapping rings/sqes\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/477e75a3907a2fe83249e49c0a92cd480b2c60e0.1732569842.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-27 15:00:57 -0700 io_uring: fix corner case forgetting to vunmap"
    },
    {
        "commit": "49c5c63d48eb5b110580e4c4b937f0006fcc9b10",
        "message": "A previous commit fixed task_work overrunning by a lot more than what\nthe user asked for, by adding a retry list. However, it didn't cap the\noverall count, hence for multiple task_work runs inside the same wait\nloop, it'd still overshoot the target by potentially a large amount.\n\nCap it generally inside the wait path. Note that this will still\novershoot the default limit of 20, but should overshoot by no more than\nlimit-1 in addition to the limit. That still provides a ceiling over how\nmuch task_work will be run, rather than still having gaps where it was\nuncapped essentially.\n\nFixes: f46b9cdb22f7 (\"io_uring: limit local tw done\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-26 13:42:27 -0700 io_uring: fix task_work cap overshooting"
    },
    {
        "commit": "0c0a4eae26ac78379d0c1db053de168a8febc6c9",
        "message": "WARNING: CPU: 0 PID: 5834 at io_uring/memmap.c:144 io_pin_pages+0x149/0x180 io_uring/memmap.c:144\nCPU: 0 UID: 0 PID: 5834 Comm: syz-executor825 Not tainted 6.12.0-next-20241118-syzkaller #0\nCall Trace:\n <TASK>\n __io_uaddr_map+0xfb/0x2d0 io_uring/memmap.c:183\n io_rings_map io_uring/io_uring.c:2611 [inline]\n io_allocate_scq_urings+0x1c0/0x650 io_uring/io_uring.c:3470\n io_uring_create+0x5b5/0xc00 io_uring/io_uring.c:3692\n io_uring_setup io_uring/io_uring.c:3781 [inline]\n ...\n </TASK>\n\nio_pin_pages()'s uaddr parameter came directly from the user and can be\ngarbage. Don't just add size to it as it can overflow.\n\nCc: stable@vger.kernel.org\nReported-by: syzbot+2159cbb522b02847c053@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1b7520ddb168e1d537d64be47414a0629d0d8f8f.1732581026.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-26 07:59:17 -0700 io_uring: check for overflows in io_pin_pages"
    },
    {
        "commit": "ee116574de8415b0673c466e6cd28ba5f70c41a2",
        "message": "A previous commit added file support for nop, but it only initializes\nnop->fd if IORING_NOP_FIXED_FILE is set. That check should be\nIORING_NOP_FILE. Fix up the condition in nop preparation, and initialize\nit to a sane value even if we're not going to be directly using it.\n\nWhile in there, do the same thing for the nop->buffer field.\n\nReported-by: syzbot+9a8500a45c2cabdf9577@syzkaller.appspotmail.com\nFixes: a85f31052bce (\"io_uring/nop: add support for testing registered files and buffers\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-21 07:15:30 -0700 io_uring/nop: ensure nop->fd is always initialized"
    },
    {
        "commit": "f46b9cdb22f7a167c36b6bcddaef7e8aee2598fa",
        "message": "Instead of eagerly running all available local tw, limit the amount of\nlocal tw done to the max of IO_LOCAL_TW_DEFAULT_MAX (20) or wait_nr. The\nvalue of 20 is chosen as a reasonable heuristic to allow enough work\nbatching but also keep latency down.\n\nAdd a retry_llist that maintains a list of local tw that couldn't be\ndone in time. No synchronisation is needed since it is only modified\nwithin the task context.\n\nSigned-off-by: David Wei <dw@davidwei.uk>\nLink: https://lore.kernel.org/r/20241120221452.3762588-3-dw@davidwei.uk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-21 07:11:00 -0700 io_uring: limit local tw done"
    },
    {
        "commit": "40cfe553240b32333b42652370ef5232e6ac59e1",
        "message": "In preparation for adding a new llist of tw to retry due to hitting the\ntw limit, add a helper io_local_work_pending(). This function returns\ntrue if there is any local tw pending. For now it only checks\nctx->work_llist.\n\nSigned-off-by: David Wei <dw@davidwei.uk>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20241120221452.3762588-2-dw@davidwei.uk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-21 07:11:00 -0700 io_uring: add io_local_work_pending()"
    },
    {
        "commit": "9008fe8fad8255edfdbecea32d7eb0485d939d0d",
        "message": "On m68k, where the minimum alignment of unsigned long is 2 bytes:\n\n    Kernel panic - not syncing: __kmem_cache_create_args: Failed to create slab 'io_kiocb'. Error -22\n    CPU: 0 UID: 0 PID: 1 Comm: swapper Not tainted 6.12.0-atari-03776-g7eaa1f99261a #1783\n    Stack from 0102fe5c:\n\t    0102fe5c 00514a2b 00514a2b ffffff00 00000001 0051f5ed 00425e78 00514a2b\n\t    0041eb74 ffffffea 00000310 0051f5ed ffffffea ffffffea 00601f60 00000044\n\t    0102ff20 000e7a68 0051ab8e 004383b8 0051f5ed ffffffea 000000b8 00000007\n\t    01020c00 00000000 000e77f0 0041e5f0 005f67c0 0051f5ed 000000b6 0102fef4\n\t    00000310 0102fef4 00000000 00000016 005f676c 0060a34c 00000010 00000004\n\t    00000038 0000009a 01000000 000000b8 005f668e 0102e000 00001372 0102ff88\n    Call Trace: [<00425e78>] dump_stack+0xc/0x10\n     [<0041eb74>] panic+0xd8/0x26c\n     [<000e7a68>] __kmem_cache_create_args+0x278/0x2e8\n     [<000e77f0>] __kmem_cache_create_args+0x0/0x2e8\n     [<0041e5f0>] memset+0x0/0x8c\n     [<005f67c0>] io_uring_init+0x54/0xd2\n\nThe minimal alignment of an integral type may differ from its size,\nhence is not safe to assume that an arbitrary freeptr_t (which is\nbasically an unsigned long) is always aligned to 4 or 8 bytes.\n\nAs nothing seems to require the additional alignment, it is safe to fix\nthis by relaxing the check to the actual minimum alignment of freeptr_t.\n\nFixes: aaa736b186239b7d (\"io_uring: specify freeptr usage for SLAB_TYPESAFE_BY_RCU io_kiocb cache\")\nFixes: d345bd2e9834e2da (\"mm: add kmem_cache_create_rcu()\")\nReported-by: Guenter Roeck <linux@roeck-us.net>\nCloses: https://lore.kernel.org/37c588d4-2c32-4aad-a19e-642961f200d7@roeck-us.net\nCc: <stable@vger.kernel.org>\nSigned-off-by: Geert Uytterhoeven <geert@linux-m68k.org>\nTested-by: Guenter Roeck <linux@roeck-us.net>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Vlastimil Babka <vbabka@suse.cz>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-20 19:15:47 +0100 slab: Fix too strict alignment check in create_cache()"
    },
    {
        "commit": "2ae6bdb1e145af0a47253953132decbd2d52f4b2",
        "message": "This code accidentally returns positivie E2BIG instead of negative\n-E2BIG.  The callers treat negatives and positives the same so this\ndoesn't affect the kernel.  The error code is returned to userspace via\nthe system call.\n\nFixes: dfbbfbf19187 (\"io_uring: introduce concept of memory regions\")\nSigned-off-by: Dan Carpenter <dan.carpenter@linaro.org>\nLink: https://lore.kernel.org/r/d8ea3bef-74d8-4f77-8223-6d36464dd4dc@stanley.mountain\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-20 08:01:59 -0700 io_uring/region: return negative -E2BIG in io_create_region()"
    },
    {
        "commit": "bf9aa14fc523d2763fc9a10672a709224e8fcaf4",
        "message": "Pull timer updates from Thomas Gleixner:\n \"A rather large update for timekeeping and timers:\n\n   - The final step to get rid of auto-rearming posix-timers\n\n     posix-timers are currently auto-rearmed by the kernel when the\n     signal of the timer is ignored so that the timer signal can be\n     delivered once the corresponding signal is unignored.\n\n     This requires to throttle the timer to prevent a DoS by small\n     intervals and keeps the system pointlessly out of low power states\n     for no value. This is a long standing non-trivial problem due to\n     the lock order of posix-timer lock and the sighand lock along with\n     life time issues as the timer and the sigqueue have different life\n     time rules.\n\n     Cure this by:\n\n       - Embedding the sigqueue into the timer struct to have the same\n         life time rules. Aside of that this also avoids the lookup of\n         the timer in the signal delivery and rearm path as it's just a\n         always valid container_of() now.\n\n       - Queuing ignored timer signals onto a seperate ignored list.\n\n       - Moving queued timer signals onto the ignored list when the\n         signal is switched to SIG_IGN before it could be delivered.\n\n       - Walking the ignored list when SIG_IGN is lifted and requeue the\n         signals to the actual signal lists. This allows the signal\n         delivery code to rearm the timer.\n\n     This also required to consolidate the signal delivery rules so they\n     are consistent across all situations. With that all self test\n     scenarios finally succeed.\n\n   - Core infrastructure for VFS multigrain timestamping\n\n     This is required to allow the kernel to use coarse grained time\n     stamps by default and switch to fine grained time stamps when inode\n     attributes are actively observed via getattr().\n\n     These changes have been provided to the VFS tree as well, so that\n     the VFS specific infrastructure could be built on top.\n\n   - Cleanup and consolidation of the sleep() infrastructure\n\n       - Move all sleep and timeout functions into one file\n\n       - Rework udelay() and ndelay() into proper documented inline\n         functions and replace the hardcoded magic numbers by proper\n         defines.\n\n       - Rework the fsleep() implementation to take the reality of the\n         timer wheel granularity on different HZ values into account.\n         Right now the boundaries are hard coded time ranges which fail\n         to provide the requested accuracy on different HZ settings.\n\n       - Update documentation for all sleep/timeout related functions\n         and fix up stale documentation links all over the place\n\n       - Fixup a few usage sites\n\n   - Rework of timekeeping and adjtimex(2) to prepare for multiple PTP\n     clocks\n\n     A system can have multiple PTP clocks which are participating in\n     seperate and independent PTP clock domains. So far the kernel only\n     considers the PTP clock which is based on CLOCK TAI relevant as\n     that's the clock which drives the timekeeping adjustments via the\n     various user space daemons through adjtimex(2).\n\n     The non TAI based clock domains are accessible via the file\n     descriptor based posix clocks, but their usability is very limited.\n     They can't be accessed fast as they always go all the way out to\n     the hardware and they cannot be utilized in the kernel itself.\n\n     As Time Sensitive Networking (TSN) gains traction it is required to\n     provide fast user and kernel space access to these clocks.\n\n     The approach taken is to utilize the timekeeping and adjtimex(2)\n     infrastructure to provide this access in a similar way how the\n     kernel provides access to clock MONOTONIC, REALTIME etc.\n\n     Instead of creating a duplicated infrastructure this rework\n     converts timekeeping and adjtimex(2) into generic functionality\n     which operates on pointers to data structures instead of using\n     static variables.\n\n     This allows to provide time accessors and adjtimex(2) functionality\n     for the independent PTP clocks in a subsequent step.\n\n   - Consolidate hrtimer initialization\n\n     hrtimers are set up by initializing the data structure and then\n     seperately setting the callback function for historical reasons.\n\n     That's an extra unnecessary step and makes Rust support less\n     straight forward than it should be.\n\n     Provide a new set of hrtimer_setup*() functions and convert the\n     core code and a few usage sites of the less frequently used\n     interfaces over.\n\n     The bulk of the htimer_init() to hrtimer_setup() conversion is\n     already prepared and scheduled for the next merge window.\n\n   - Drivers:\n\n       - Ensure that the global timekeeping clocksource is utilizing the\n         cluster 0 timer on MIPS multi-cluster systems.\n\n         Otherwise CPUs on different clusters use their cluster specific\n         clocksource which is not guaranteed to be synchronized with\n         other clusters.\n\n       - Mostly boring cleanups, fixes, improvements and code movement\"\n\n* tag 'timers-core-2024-11-18' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (140 commits)\n  posix-timers: Fix spurious warning on double enqueue versus do_exit()\n  clocksource/drivers/arm_arch_timer: Use of_property_present() for non-boolean properties\n  clocksource/drivers/gpx: Remove redundant casts\n  clocksource/drivers/timer-ti-dm: Fix child node refcount handling\n  dt-bindings: timer: actions,owl-timer: convert to YAML\n  clocksource/drivers/ralink: Add Ralink System Tick Counter driver\n  clocksource/drivers/mips-gic-timer: Always use cluster 0 counter as clocksource\n  clocksource/drivers/timer-ti-dm: Don't fail probe if int not found\n  clocksource/drivers:sp804: Make user selectable\n  clocksource/drivers/dw_apb: Remove unused dw_apb_clockevent functions\n  hrtimers: Delete hrtimer_init_on_stack()\n  alarmtimer: Switch to use hrtimer_setup() and hrtimer_setup_on_stack()\n  io_uring: Switch to use hrtimer_setup_on_stack()\n  sched/idle: Switch to use hrtimer_setup_on_stack()\n  hrtimers: Delete hrtimer_init_sleeper_on_stack()\n  wait: Switch to use hrtimer_setup_sleeper_on_stack()\n  timers: Switch to use hrtimer_setup_sleeper_on_stack()\n  net: pktgen: Switch to use hrtimer_setup_sleeper_on_stack()\n  futex: Switch to use hrtimer_setup_sleeper_on_stack()\n  fs/aio: Switch to use hrtimer_setup_sleeper_on_stack()\n  ...",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-19 16:35:06 -0800 Merge tag 'timers-core-2024-11-18' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip"
    },
    {
        "commit": "8350142a4b4cedebfa76cd4cc6e5a7ba6a330629",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - Cleanups of the eventfd handling code, making it fully private.\n\n - Support for sending a sync message to another ring, without having a\n   ring available to send a normal async message.\n\n - Get rid of the separate unlocked hash table, unify everything around\n   the single locked one.\n\n - Add support for ring resizing. It can be hard to appropriately size\n   the CQ ring upfront, if the application doesn't know how busy it will\n   be. This results in applications sizing rings for the most busy case,\n   which can be wasteful. With ring resizing, they can start small and\n   grow the ring, if needed.\n\n - Add support for fixed wait regions, rather than needing to copy the\n   same wait data tons of times for each wait operation.\n\n - Rewrite the resource node handling, which before was serialized per\n   ring. This caused issues with particularly fixed files, where one\n   file waiting on IO could hold up putting and freeing of other\n   unrelated files. Now each node is handled separately. New code is\n   much simpler too, and was a net 250 line reduction in code.\n\n - Add support for just doing partial buffer clones, rather than always\n   cloning the entire buffer table.\n\n - Series adding static NAPI support, where a specific NAPI instance is\n   used rather than having a list of them available that need lookup.\n\n - Add support for mapped regions, and also convert the fixed wait\n   support mentioned above to that concept. This avoids doing special\n   mappings for various planned features, and folds the existing\n   registered wait into that too.\n\n - Add support for hybrid IO polling, which is a variant of strict\n   IOPOLL but with an initial sleep delay to avoid spinning too early\n   and wasting resources on devices that aren't necessarily in the < 5\n   usec category wrt latencies.\n\n - Various cleanups and little fixes.\n\n* tag 'for-6.13/io_uring-20241118' of git://git.kernel.dk/linux: (79 commits)\n  io_uring/region: fix error codes after failed vmap\n  io_uring: restore back registered wait arguments\n  io_uring: add memory region registration\n  io_uring: introduce concept of memory regions\n  io_uring: temporarily disable registered waits\n  io_uring: disable ENTER_EXT_ARG_REG for IOPOLL\n  io_uring: fortify io_pin_pages with a warning\n  switch io_msg_ring() to CLASS(fd)\n  io_uring: fix invalid hybrid polling ctx leaks\n  io_uring/uring_cmd: fix buffer index retrieval\n  io_uring/rsrc: add & apply io_req_assign_buf_node()\n  io_uring/rsrc: remove '->ctx_ptr' of 'struct io_rsrc_node'\n  io_uring/rsrc: pass 'struct io_ring_ctx' reference to rsrc helpers\n  io_uring: avoid normal tw intermediate fallback\n  io_uring/napi: add static napi tracking strategy\n  io_uring/napi: clean up __io_napi_do_busy_loop\n  io_uring/napi: Use lock guards\n  io_uring/napi: improve __io_napi_add\n  io_uring/napi: fix io_napi_entry RCU accesses\n  io_uring/napi: protect concurrent io_napi_entry timeout accesses\n  ...",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-18 17:02:57 -0800 Merge tag 'for-6.13/io_uring-20241118' of git://git.kernel.dk/linux"
    },
    {
        "commit": "c14a8a4c04c5859322eb5801db662b56b2294f67",
        "message": "Pull btrfs updates from David Sterba:\n \"Changes outside of btrfs: add io_uring command flag to track a dying\n  task (the rest will go via the block git tree).\n\n  User visible changes:\n\n   - wire encoded read (ioctl) to io_uring commands, this can be used on\n     itself, in the future this will allow 'send' to be asynchronous. As\n     a consequence, the encoded read ioctl can also work in non-blocking\n     mode\n\n   - new ioctl to wait for cleaned subvolumes, no need to use the\n     generic and root-only SEARCH_TREE ioctl, will be used by \"btrfs\n     subvol sync\"\n\n   - recognize different paths/symlinks for the same devices and don't\n     report them during rescanning, this can be observed with LVM or DM\n\n   - seeding device use case change, the sprout device (the one\n     capturing new writes) will not clear the read-only status of the\n     super block; this prevents accumulating space from deleted\n     snapshots\n\n  Performance improvements:\n\n   - reduce lock contention when traversing extent buffers\n\n   - reduce extent tree lock contention when searching for inline\n     backref\n\n   - switch from rb-trees to xarray for delayed ref tracking,\n     improvements due to better cache locality, branching factors and\n     more compact data structures\n\n   - enable extent map shrinker again (prevent memory exhaustion under\n     some types of IO load), reworked to run in a single worker thread\n     (there used to be problems causing long stalls under memory\n     pressure)\n\n  Core changes:\n\n   - raid-stripe-tree feature updates:\n       - make device replace and scrub work\n       - implement partial deletion of stripe extents\n       - new selftests\n\n   - split the config option BTRFS_DEBUG and add EXPERIMENTAL for\n     features that are experimental or with known problems so we don't\n     misuse debugging config for that\n\n   - subpage mode updates (sector < page):\n       - update compression implementations\n       - update writepage, writeback\n\n   - continued folio API conversions:\n       - buffered writes\n\n   - make buffered write copy one page at a time, preparatory work for\n     future integration with large folios, may cause performance drop\n\n   - proper locking of root item regarding starting send\n\n   - error handling improvements\n\n   - code cleanups and refactoring:\n       - dead code removal\n       - unused parameter reduction\n       - lockdep assertions\"\n\n* tag 'for-6.13-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux: (119 commits)\n  btrfs: send: check for read-only send root under critical section\n  btrfs: send: check for dead send root under critical section\n  btrfs: remove check for NULL fs_info at btrfs_folio_end_lock_bitmap()\n  btrfs: fix warning on PTR_ERR() against NULL device at btrfs_control_ioctl()\n  btrfs: fix a typo in btrfs_use_zone_append\n  btrfs: avoid superfluous calls to free_extent_map() in btrfs_encoded_read()\n  btrfs: simplify logic to decrement snapshot counter at btrfs_mksnapshot()\n  btrfs: remove hole from struct btrfs_delayed_node\n  btrfs: update stale comment for struct btrfs_delayed_ref_node::add_list\n  btrfs: add new ioctl to wait for cleaned subvolumes\n  btrfs: simplify range tracking in cow_file_range()\n  btrfs: remove conditional path allocation in btrfs_read_locked_inode()\n  btrfs: push cleanup into btrfs_read_locked_inode()\n  io_uring/cmd: let cmds to know about dying task\n  btrfs: add struct io_btrfs_cmd as type for io_uring_cmd_to_pdu()\n  btrfs: add io_uring command for encoded reads (ENCODED_READ ioctl)\n  btrfs: move priv off stack in btrfs_encoded_read_regular_fill_pages()\n  btrfs: don't sleep in btrfs_encoded_read() if IOCB_NOWAIT is set\n  btrfs: change btrfs_encoded_read() so that reading of extent is done by caller\n  btrfs: remove pointless iocb::ki_pos addition in btrfs_encoded_read()\n  ...",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-18 16:37:41 -0800 Merge tag 'for-6.13-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux"
    },
    {
        "commit": "82339c49119f5e38ca3c81d698b84134c342373f",
        "message": "Pull xattr updates from Al Viro:\n \"Sanitize xattr and io_uring interactions with it, add *xattrat()\n  syscalls, sanitize struct filename handling in there\"\n\n* tag 'pull-xattr' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:\n  xattr: remove redundant check on variable err\n  fs/xattr: add *at family syscalls\n  new helpers: file_removexattr(), filename_removexattr()\n  new helpers: file_listxattr(), filename_listxattr()\n  replace do_getxattr() with saner helpers.\n  replace do_setxattr() with saner helpers.\n  new helper: import_xattr_name()\n  fs: rename struct xattr_ctx to kernel_xattr_ctx\n  xattr: switch to CLASS(fd)\n  io_[gs]etxattr_prep(): just use getname()\n  io_uring: IORING_OP_F[GS]ETXATTR is fine with REQ_F_FIXED_FILE\n  getname_maybe_null() - the third variant of pathname copy-in\n  teach filename_lookup() to treat NULL filename as \"\"",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-18 12:44:25 -0800 Merge tag 'pull-xattr' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs"
    },
    {
        "commit": "e358e09a894dbcd51fdbbcf62bec1df249915834",
        "message": "Syz reports:\n\nBUG: KCSAN: data-race in __se_sys_io_uring_register / io_sqe_files_register\n\nread-write to 0xffff8881021940b8 of 4 bytes by task 5923 on cpu 1:\n io_sqe_files_register+0x2c4/0x3b0 io_uring/rsrc.c:713\n __io_uring_register io_uring/register.c:403 [inline]\n __do_sys_io_uring_register io_uring/register.c:611 [inline]\n __se_sys_io_uring_register+0x8d0/0x1280 io_uring/register.c:591\n __x64_sys_io_uring_register+0x55/0x70 io_uring/register.c:591\n x64_sys_call+0x202/0x2d60 arch/x86/include/generated/asm/syscalls_64.h:428\n do_syscall_x64 arch/x86/entry/common.c:52 [inline]\n do_syscall_64+0xc9/0x1c0 arch/x86/entry/common.c:83\n entry_SYSCALL_64_after_hwframe+0x77/0x7f\n\nread to 0xffff8881021940b8 of 4 bytes by task 5924 on cpu 0:\n __do_sys_io_uring_register io_uring/register.c:613 [inline]\n __se_sys_io_uring_register+0xe4a/0x1280 io_uring/register.c:591\n __x64_sys_io_uring_register+0x55/0x70 io_uring/register.c:591\n x64_sys_call+0x202/0x2d60 arch/x86/include/generated/asm/syscalls_64.h:428\n do_syscall_x64 arch/x86/entry/common.c:52 [inline]\n do_syscall_64+0xc9/0x1c0 arch/x86/entry/common.c:83\n entry_SYSCALL_64_after_hwframe+0x77/0x7f\n\nWhich should be due to reading the table size after unlock. We don't\ncare much as it's just to print it in trace, but we might as well do it\nunder the lock.\n\nReported-by: syzbot+5a486fef3de40e0d8c76@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8233af2886a37b57f79e444e3db88fcfda1817ac.1731942203.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-18 09:10:56 -0700 io_uring: protect register tracing"
    },
    {
        "commit": "c750629caeca01979da3403f4bebecda88713233",
        "message": "A separate wait argument registration API was removed, also delete\nleftover uapi definitions.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/143b6a53591badac23632d3e6fa3e5db4b342ee2.1731942445.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-18 09:10:49 -0700 io_uring: remove io_uring_cqwait_reg_arg"
    },
    {
        "commit": "a652958888fb1ada3e4f6b548576c2d2c1b60d66",
        "message": "io_create_region() jumps after a vmap failure without setting the return\ncode, it could be 0 or just uninitialised.\n\nFixes: dfbbfbf191878 (\"io_uring: introduce concept of memory regions\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0abac19dbf81c061cffaa9534a2471ed5460ad3e.1731803848.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-17 09:01:35 -0700 io_uring/region: fix error codes after failed vmap"
    },
    {
        "commit": "d617b3147d54c42351eac63b5398d4ddf4f4011b",
        "message": "Now we've got a more generic region registration API, place\nIORING_ENTER_EXT_ARG_REG and re-enable it.\n\nFirst, the user has to register a region with the\nIORING_MEM_REGION_REG_WAIT_ARG flag set. It can only be done for a\nring in a disabled state, aka IORING_SETUP_R_DISABLED, to avoid races\nwith already running waiters. With that we should have stable constant\nvalues for ctx->cq_wait_{size,arg} in io_get_ext_arg_reg() and hence no\nREAD_ONCE required.\n\nThe other API difference is that we're now passing byte offsets instead\nof indexes. The user _must_ align all offsets / pointers to the native\nword size, failing to do so might but not necessarily has to lead to a\nfailure usually returned as -EFAULT. liburing will be hiding this\ndetails from users.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/81822c1b4ffbe8ad391b4f9ad1564def0d26d990.1731689588.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-15 12:28:38 -0700 io_uring: restore back registered wait arguments"
    },
    {
        "commit": "93238e66185524aad925acefb2312203b9e26d63",
        "message": "Regions will serve multiple purposes. First, with it we can decouple\nring/etc. object creation from registration / mapping of the memory they\nwill be placed in. We already have hacks that allow to put both SQ and\nCQ into the same huge page, in the future we should be able to:\n\nregion = create_region(io_ring);\ncreate_pbuf_ring(io_uring, region, offset=0);\ncreate_pbuf_ring(io_uring, region, offset=N);\n\nThe second use case is efficiently passing parameters. The following\npatch enables back on top of regions IORING_ENTER_EXT_ARG_REG, which\noptimises wait arguments. It'll also be useful for request arguments\nreplacing iovecs, msghdr, etc. pointers. Eventually it would also be\nhandy for BPF as well if it comes to fruition.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0798cf3a14fad19cfc96fc9feca5f3e11481691d.1731689588.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-15 09:58:34 -0700 io_uring: add memory region registration"
    },
    {
        "commit": "dfbbfbf191878e8dd422768ce009858d8b5b761e",
        "message": "We've got a good number of mappings we share with the userspace, that\nincludes the main rings, provided buffer rings, upcoming rings for\nzerocopy rx and more. All of them duplicate user argument parsing and\nsome internal details as well (page pinnning, huge page optimisations,\nmmap'ing, etc.)\n\nIntroduce a notion of regions. For userspace for now it's just a new\nstructure called struct io_uring_region_desc which is supposed to\nparameterise all such mapping / queue creations. A region either\nrepresents a user provided chunk of memory, in which case the user_addr\nfield should point to it, or a request for the kernel to allocate the\nmemory, in which case the user would need to mmap it after using the\noffset returned in the mmap_offset field. With a uniform userspace API\nwe can avoid additional boiler plate code and apply future optimisation\nto all of them at once.\n\nInternally, there is a new structure struct io_mapped_region holding all\nrelevant runtime information and some helpers to work with it. This\npatch limits it to user provided regions.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0e6fe25818dfbaebd1bd90b870a6cac503fe1a24.1731689588.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-15 09:58:34 -0700 io_uring: introduce concept of memory regions"
    },
    {
        "commit": "83e041522eb9c45479f4490b212687cf1e7e9999",
        "message": "Disable wait argument registration as it'll be replaced with a more\ngeneric feature. We'll still need IORING_ENTER_EXT_ARG_REG parsing\nin a few commits so leave it be.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/70b1d1d218c41ba77a76d1789c8641dab0b0563e.1731689588.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-15 09:58:34 -0700 io_uring: temporarily disable registered waits"
    },
    {
        "commit": "3730aebbdac8770f64ab66eb5e7129bc8dae731d",
        "message": "IOPOLL doesn't use the extended arguments, no need for it to support\nIORING_ENTER_EXT_ARG_REG. Let's disable it for IOPOLL, if anything it\nleaves more space for future extensions.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a35ecd919dbdc17bd5b7932273e317832c531b45.1731689588.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-15 09:58:34 -0700 io_uring: disable ENTER_EXT_ARG_REG for IOPOLL"
    },
    {
        "commit": "68685fa20edc5307fc893a06473c19661c236f29",
        "message": "We're a bit too frivolous with types of nr_pages arguments, converting\nit to long and back to int, passing an unsigned int pointer as an int\npointer and so on. Shouldn't cause any problem but should be carefully\nreviewed, but until then let's add a WARN_ON_ONCE check to be more\nconfident callers don't pass poorely checked arguents.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d48e0c097cbd90fb47acaddb6c247596510d8cfc.1731689588.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-15 09:58:34 -0700 io_uring: fortify io_pin_pages with a warning"
    },
    {
        "commit": "b9d69371e8fa90fa3ab100f4fcb4815b13b3673a",
        "message": "It has already allocated the ctx by the point where it checks the hybrid\npoll configuration, plain return leaks the memory.\n\nFixes: 01ee194d1aba1 (\"io_uring: add support for hybrid IOPOLL\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Anuj Gupta <anuj20.g@samsung.com>\nLink: https://lore.kernel.org/r/b57f2608088020501d352fcdeebdb949e281d65b.1731468230.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-13 07:38:04 -0700 io_uring: fix invalid hybrid polling ctx leaks"
    },
    {
        "commit": "adc77b19f62d7e80f98400b2fca9d700d2afdd6f",
        "message": "Syzbot has reported the following KMSAN splat:\n\nBUG: KMSAN: uninit-value in ocfs2_file_read_iter+0x9a4/0xf80\n ocfs2_file_read_iter+0x9a4/0xf80\n __io_read+0x8d4/0x20f0\n io_read+0x3e/0xf0\n io_issue_sqe+0x42b/0x22c0\n io_wq_submit_work+0xaf9/0xdc0\n io_worker_handle_work+0xd13/0x2110\n io_wq_worker+0x447/0x1410\n ret_from_fork+0x6f/0x90\n ret_from_fork_asm+0x1a/0x30\n\nUninit was created at:\n __alloc_pages_noprof+0x9a7/0xe00\n alloc_pages_mpol_noprof+0x299/0x990\n alloc_pages_noprof+0x1bf/0x1e0\n allocate_slab+0x33a/0x1250\n ___slab_alloc+0x12ef/0x35e0\n kmem_cache_alloc_bulk_noprof+0x486/0x1330\n __io_alloc_req_refill+0x84/0x560\n io_submit_sqes+0x172f/0x2f30\n __se_sys_io_uring_enter+0x406/0x41c0\n __x64_sys_io_uring_enter+0x11f/0x1a0\n x64_sys_call+0x2b54/0x3ba0\n do_syscall_64+0xcd/0x1e0\n entry_SYSCALL_64_after_hwframe+0x77/0x7f\n\nSince an instance of 'struct kiocb' may be passed from the block layer\nwith 'private' field uninitialized, introduce 'ocfs2_iocb_init_rw_locked()'\nand use it from where 'ocfs2_dio_end_io()' might take care, i.e. in\n'ocfs2_file_read_iter()' and 'ocfs2_file_write_iter()'.\n\nLink: https://lkml.kernel.org/r/20241029091736.1501946-1-dmantipov@yandex.ru\nFixes: 7cdfc3a1c397 (\"ocfs2: Remember rw lock level during direct io\")\nSigned-off-by: Dmitry Antipov <dmantipov@yandex.ru>\nReported-by: syzbot+a73e253cca4f0230a5a5@syzkaller.appspotmail.com\nCloses: https://syzkaller.appspot.com/bug?extid=a73e253cca4f0230a5a5\nCc: Mark Fasheh <mark@fasheh.com>\nCc: Joel Becker <jlbec@evilplan.org>\nCc: Junxiao Bi <junxiao.bi@oracle.com>\nCc: Joseph Qi <jiangqi903@gmail.com>\nCc: Changwei Ge <gechangwei@live.cn>\nCc: Jun Piao <piaojun@huawei.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-11 17:17:04 -0800 ocfs2: fix uninitialized value in ocfs2_file_read_iter()"
    },
    {
        "commit": "d369735e02ef122d19d4c3d093028da0eb400636",
        "message": "In ublk_ch_mmap(), queue id is calculated in the following way:\n\n\t(vma->vm_pgoff << PAGE_SHIFT) / `max_cmd_buf_size`\n\n'max_cmd_buf_size' is equal to\n\n\t`UBLK_MAX_QUEUE_DEPTH * sizeof(struct ublksrv_io_desc)`\n\nand UBLK_MAX_QUEUE_DEPTH is 4096 and part of UAPI, so 'max_cmd_buf_size'\nis always page aligned in 4K page size kernel. However, it isn't true in\n64K page size kernel.\n\nFixes the issue by always rounding up 'max_cmd_buf_size' with PAGE_SIZE.\n\nCc: stable@vger.kernel.org\nFixes: 71f28f3136af (\"ublk_drv: add io_uring based userspace block driver\")\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20241111110718.1394001-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-11 08:16:17 -0700 ublk: fix ublk_ch_mmap() for 64K page size"
    },
    {
        "commit": "a43e236fb9aef4528f2bd24095d1f348030f5d9d",
        "message": "Add back buffer index retrieval for IORING_URING_CMD_FIXED.\n\nReported-by: Guangwu Zhang <guazhang@redhat.com>\nCc: Jeff Moyer <jmoyer@redhat.com>\nFixes: b54a14041ee6 (\"io_uring/rsrc: add io_rsrc_node_lookup() helper\")\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nReviewed-by: Anuj Gupta <anuj20.g@samsung.com>\nTested-by: Guangwu Zhang <guazhang@redhat.com>\nLink: https://lore.kernel.org/r/20241111101318.1387557-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-11 08:11:37 -0700 io_uring/uring_cmd: fix buffer index retrieval"
    },
    {
        "commit": "df3b8ca604f224eb4cd51669416ad4d607682273",
        "message": "When the taks that submitted a request is dying, a task work for that\nrequest might get run by a kernel thread or even worse by a half\ndismantled task. We can't just cancel the task work without running the\ncallback as the cmd might need to do some clean up, so pass a flag\ninstead. If set, it's not safe to access any task resources and the\ncallback is expected to cancel the cmd ASAP.\n\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: David Sterba <dsterba@suse.com>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-11 14:34:21 +0100 io_uring/cmd: let cmds to know about dying task"
    },
    {
        "commit": "1cc86aeadafd667c381c588a84fdef2d5e8093a5",
        "message": "Add struct io_btrfs_cmd as a wrapper type for io_uring_cmd_to_pdu(),\nrather than using a raw pointer.\n\nSuggested-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Mark Harmstone <maharmstone@fb.com>\nReviewed-by: David Sterba <dsterba@suse.com>\nSigned-off-by: David Sterba <dsterba@suse.com>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-11 14:34:21 +0100 btrfs: add struct io_btrfs_cmd as type for io_uring_cmd_to_pdu()"
    },
    {
        "commit": "34310c442e175f286b4c06ab5caa4e0b267ea31c",
        "message": "Add an io_uring command for encoded reads, using the same interface as\nthe existing BTRFS_IOC_ENCODED_READ ioctl.\n\nbtrfs_uring_encoded_read() is an io_uring version of\nbtrfs_ioctl_encoded_read(), which validates the user input and calls\nbtrfs_encoded_read() to read the appropriate metadata. If we determine\nthat we need to read an extent from disk, we call\nbtrfs_encoded_read_regular_fill_pages() through\nbtrfs_uring_read_extent() to prepare the bio.\n\nThe existing btrfs_encoded_read_regular_fill_pages() is changed so that\nif it is passed a valid uring_ctx, rather than waking up any waiting\nthreads it calls btrfs_uring_read_extent_endio(). This in turn copies\nthe read data back to userspace, and calls io_uring_cmd_done() to\ncomplete the io_uring command.\n\nBecause we're potentially doing a non-blocking read,\nbtrfs_uring_read_extent() doesn't clean up after itself if it returns\n-EIOCBQUEUED. Instead, it allocates a priv struct, populates the fields\nthere that we will need to unlock the inode and free our allocations,\nand defers this to the btrfs_uring_read_finished() that gets called when\nthe bio completes.\n\nSigned-off-by: Mark Harmstone <maharmstone@fb.com>\nReviewed-by: David Sterba <dsterba@suse.com>\nSigned-off-by: David Sterba <dsterba@suse.com>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-11 14:34:21 +0100 btrfs: add io_uring command for encoded reads (ENCODED_READ ioctl)"
    },
    {
        "commit": "039c878db7add23c1c9ea18424c442cce76670f9",
        "message": "The following pattern becomes more and more:\n\n+       io_req_assign_rsrc_node(&req->buf_node, node);\n+       req->flags |= REQ_F_BUF_NODE;\n\nso make it a helper, which is less fragile to use than above code, for\nexample, the BUF_NODE flag is even missed in current io_uring_cmd_prep().\n\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20241107110149.890530-4-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-07 15:24:33 -0700 io_uring/rsrc: add & apply io_req_assign_buf_node()"
    },
    {
        "commit": "4f219fcce5e4366cc121fc98270beb1fbbb3df2b",
        "message": "Remove '->ctx_ptr' of 'struct io_rsrc_node', and add 'type' field,\nmeantime remove io_rsrc_node_type().\n\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20241107110149.890530-3-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-07 15:24:33 -0700 io_uring/rsrc: remove '->ctx_ptr' of 'struct io_rsrc_node'"
    },
    {
        "commit": "0d98c509086837a8cf5a32f82f2a58f39a539192",
        "message": "`io_rsrc_node` instance won't be shared among different io_uring ctxs,\nand its allocation 'ctx' is always same with the user's 'ctx', so it is\nsafe to pass user 'ctx' reference to rsrc helpers. Even in io_clone_buffers(),\n`io_rsrc_node` instance is allocated actually for destination io_uring_ctx.\n\nThen io_rsrc_node_ctx() can be removed, and the 8 bytes `ctx` pointer will be\nremoved from `io_rsrc_node` in the following patch.\n\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20241107110149.890530-2-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-07 15:24:33 -0700 io_uring/rsrc: pass 'struct io_ring_ctx' reference to rsrc helpers"
    },
    {
        "commit": "fc9f59de26afb3b4a33d37f1ba51a441b050afbb",
        "message": "hrtimer_setup_on_stack() takes the callback function pointer as argument\nand initializes the timer completely.\n\nReplace hrtimer_init_on_stack() and the open coded initialization of\nhrtimer::function with the new setup mechanism.\n\nSigned-off-by: Nam Cao <namcao@linutronix.de>\nSigned-off-by: Thomas Gleixner <tglx@linutronix.de>\nLink: https://lore.kernel.org/all/f0d4ac32ec4050710a656cee8385fa4427be33aa.1730386209.git.namcao@linutronix.de",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-07 02:47:06 +0100 io_uring: Switch to use hrtimer_setup_on_stack()"
    },
    {
        "commit": "c95d36585b9f8c43a4c5d5a9fe22477a138b63f4",
        "message": "The IORING_OP_TIMEOUT command uses hrtimer underneath. The timer's callback\nfunction is setup in io_timeout(), and then the callback function is setup\nagain when the timer is rearmed.\n\nSince the callback function is the same for both cases, the latter setup is\nredundant, therefore remove it.\n\nSigned-off-by: Nam Cao <namcao@linutronix.de>\nSigned-off-by: Thomas Gleixner <tglx@linutronix.de>\nReviewed-by: Jens Axboe <axboe@kernel.dk:\nLink: https://lore.kernel.org/all/07b28dfd5691478a2d250f379c8b90dd37f9bb9a.1730386209.git.namcao@linutronix.de",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-07 02:47:05 +0100 io_uring: Remove redundant hrtimer's callback function setup"
    },
    {
        "commit": "af0a2ffef0e6d23412dd55df29f5caef8f3583f2",
        "message": "When a DEFER_TASKRUN io_uring is terminating it requeues deferred task\nwork items as normal tw, which can further fallback to kthread\nexecution. Avoid this extra step and always push them to the fallback\nkthread.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d1cd472cec2230c66bd1c8d412a5833f0af75384.1730772720.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-06 13:55:38 -0700 io_uring: avoid normal tw intermediate fallback"
    },
    {
        "commit": "6bf90bd8c58a305994948eb3409d91a7d8f2edae",
        "message": "Add the static napi tracking strategy. That allows the user to manually\nmanage the napi ids list for busy polling, and eliminate the overhead of\ndynamically updating the list from the fast path.\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nLink: https://lore.kernel.org/r/96943de14968c35a5c599352259ad98f3c0770ba.1728828877.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-06 13:55:38 -0700 io_uring/napi: add static napi tracking strategy"
    },
    {
        "commit": "71afd926f292bb8f3ca86e6c3c40123037f109c6",
        "message": "__io_napi_do_busy_loop now requires to have loop_end in its parameters.\nThis makes the code cleaner and also has the benefit of removing a\nbranch since the only caller not passing NULL for loop_end_arg is also\nsetting the value conditionally.\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nLink: https://lore.kernel.org/r/d5b9bb91b1a08fff50525e1c18d7b4709b9ca100.1728828877.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-06 13:55:38 -0700 io_uring/napi: clean up __io_napi_do_busy_loop"
    },
    {
        "commit": "db1e1adf6f993b1c2cef605d86eff709a8db5052",
        "message": "Convert napi locks to use the shiny new Scope-Based Resource Management\nmachinery.\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nLink: https://lore.kernel.org/r/2680ca47ee183cfdb89d1a40c84d349edeb620ab.1728828877.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-06 13:55:38 -0700 io_uring/napi: Use lock guards"
    },
    {
        "commit": "a5e26f49fef9485bc4ae24666d984a6de11e058c",
        "message": "1. move the sock->sk pointer validity test outside the function to\n   avoid the function call overhead and to make the function more\n   more reusable\n2. change its name to __io_napi_add_id to be more precise about it is\n   doing\n3. return an error code to report errors\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nLink: https://lore.kernel.org/r/d637fa3b437d753c0f4e44ff6a7b5bf2c2611270.1728828877.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-06 13:55:38 -0700 io_uring/napi: improve __io_napi_add"
    },
    {
        "commit": "45b3941d09d13b3503309be1f023b83deaf69b4d",
        "message": "correct 3 RCU structures modifications that were not using the RCU\nfunctions to make their update.\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nLink: https://lore.kernel.org/r/9f53b5169afa8c7bf3665a0b19dc2f7061173530.1728828877.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-06 13:55:38 -0700 io_uring/napi: fix io_napi_entry RCU accesses"
    },
    {
        "commit": "2f3cc8e441c9f657ff036c56baaab7dddbd0b350",
        "message": "io_napi_entry timeout value can be updated while accessed from the poll\nfunctions.\n\nIts concurrent accesses are wrapped with READ_ONCE()/WRITE_ONCE() macros\nto avoid incorrect compiler optimizations.\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nLink: https://lore.kernel.org/r/3de3087563cf98f75266fd9f85fdba063a8720db.1728828877.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-06 13:55:38 -0700 io_uring/napi: protect concurrent io_napi_entry timeout accesses"
    },
    {
        "commit": "483242714fcc853f3f5ef728116f5ec168468bca",
        "message": "The SQ index array consists of user provided indexes, which io_uring\nthen uses to index the SQ, and so it's susceptible to speculation. For\nall other queues io_uring tracks heads and tails in kernel, and they\nshouldn't need any special care.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c6c7a25962924a55869e317e4fdb682dfdc6b279.1730687889.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-06 13:55:38 -0700 io_uring: prevent speculating sq_array indexing"
    },
    {
        "commit": "b6f58a3f4aa8dba424356c7a69388a81f4459300",
        "message": "Rather than store the task_struct itself in struct io_kiocb, store\nthe io_uring specific task_struct. The life times are the same in terms\nof io_uring, and this avoids doing some dereferences through the\ntask_struct. For the hot path of putting local task references, we can\nderef req->tctx instead, which we'll need anyway in that function\nregardless of whether it's local or remote references.\n\nThis is mostly straight forward, except the original task PF_EXITING\ncheck needs a bit of tweaking. task_work is _always_ run from the\noriginating task, except in the fallback case, where it's run from a\nkernel thread. Replace the potentially racy (in case of fallback work)\nchecks for req->task->flags with current->flags. It's either the still\nthe original task, in which case PF_EXITING will be sane, or it has\nPF_KTHREAD set, in which case it's fallback work. Both cases should\nprevent moving forward with the given request.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-06 13:55:38 -0700 io_uring: move struct io_kiocb from task_struct to io_uring_task"
    },
    {
        "commit": "6ed368cc5d5d255ffffad33cfa02ecf2b77b7c44",
        "message": "They are only used right where they are defined, just open-code them\ninside io_put_task().\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-06 13:55:38 -0700 io_uring: remove task ref helpers"
    },
    {
        "commit": "f03baece08188f2e239c0ca0c098c14c71739ffb",
        "message": "Right now the task_struct pointer is used as the key to match a task,\nbut in preparation for some io_kiocb changes, move it to using struct\nio_uring_task instead. No functional changes intended in this patch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-06 13:55:38 -0700 io_uring: move cancelations to be io_uring_task based"
    },
    {
        "commit": "6f94cbc29adacc15007c5a16295052e674099282",
        "message": "Currently the io_rsrc_node assignment in io_kiocb is an array of two\npointers, as two nodes may be assigned to a request - one file node,\nand one buffer node. However, the buffer node can co-exist with the\nprovided buffers, as currently it's not supported to use both provided\nand registered buffers at the same time.\n\nThis crucially brings struct io_kiocb down to 4 cache lines again, as\nbefore it spilled into the 5th cacheline.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-06 13:55:36 -0700 io_uring/rsrc: split io_kiocb node type assignments"
    },
    {
        "commit": "6af82f7614a2e31e7ef23e5e160697aef31e8edd",
        "message": "Rather than keep the type field separate rom ctx, use the fact that we\ncan encode up to 4 types of nodes in the LSB of the ctx pointer. Doesn't\nreclaim any space right now on 64-bit archs, but it leaves a full int\nfor future use.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-06 13:54:15 -0700 io_uring/rsrc: encode node type and ctx together"
    },
    {
        "commit": "66d7ac6bdb07fbe69ca6971558a996ac04bbb643",
        "message": "io_uring setxattr logics duplicates stuff from fs/xattr.c; provide\nsaner helpers (filename_setxattr() and file_setxattr() resp.) and\nuse them.\n\nNB: putname(ERR_PTR()) is a no-op\n\nSigned-off-by: Al Viro <viro@zeniv.linux.org.uk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-06 12:59:39 -0500 replace do_setxattr() with saner helpers."
    },
    {
        "commit": "01ee194d1aba1202f0926d5047a2a4cf84d0e45d",
        "message": "A new hybrid poll is implemented on the io_uring layer. Once an IO is\nissued, it will not poll immediately, but rather block first and re-run\nbefore IO complete, then poll to reap IO. While this poll method could\nbe a suboptimal solution when running on a single thread, it offers\nperformance lower than regular polling but higher than IRQ, and CPU\nutilization is also lower than polling.\n\nTo use hybrid polling, the ring must be setup with both the\nIORING_SETUP_IOPOLL and IORING_SETUP_HYBRID)IOPOLL flags set. Hybrid\npolling has the same restrictions as IOPOLL, in that commands must\nexplicitly support it.\n\nSigned-off-by: hexue <xue01.he@samsung.com>\nLink: https://lore.kernel.org/r/20241101091957.564220-2-xue01.he@samsung.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-02 15:45:30 -0600 io_uring: add support for hybrid IOPOLL"
    },
    {
        "commit": "c1329532d5aabecf79788924941afb8a7b7c1024",
        "message": "Currently cloning a buffer table will fail if the destination already has\na table. But it should be possible to use it to replace existing elements.\nAdd a IORING_REGISTER_DST_REPLACE cloning flag, which if set, will allow\nthe destination to already having a buffer table. If that is the case,\nthen entries designated by offset + nr buffers will be replaced if they\nalready exist.\n\nNote that it's allowed to use IORING_REGISTER_DST_REPLACE and not have\nan existing table, in which case it'll work just like not having the\nflag set and an empty table - it'll just assign the newly created table\nfor that case.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-02 15:45:30 -0600 io_uring/rsrc: allow cloning with node replacements"
    },
    {
        "commit": "b16e920a1909da6799c43000db730d8fcdcae907",
        "message": "Right now buffer cloning is an all-or-nothing kind of thing - either the\nwhole table is cloned from a source to a destination ring, or nothing at\nall.\n\nHowever, it's not always desired to clone the whole thing. Allow for\nthe application to specify a source and destination offset, and a\nnumber of buffers to clone. If the destination offset is non-zero, then\nallocate sparse nodes upfront.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-02 15:45:30 -0600 io_uring/rsrc: allow cloning at an offset"
    },
    {
        "commit": "d50f94d761a5d9a34e03a86e512e19d88cbeaf06",
        "message": "The empty node was used as a placeholder for a sparse entry, but it\ndidn't really solve any issues. The caller still has to check for\nwhether it's the empty node or not, it may as well just check for a NULL\nreturn instead.\n\nThe dummy_ubuf was used for a sparse buffer entry, but NULL will serve\nthe same purpose there of ensuring an -EFAULT on attempted import.\n\nJust use NULL for a sparse node, regardless of whether or not it's a\nfile or buffer resource.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-02 15:45:30 -0600 io_uring/rsrc: get rid of the empty node and dummy_ubuf"
    },
    {
        "commit": "4007c3d8c22a2025367953f4ee36ae106a69d855",
        "message": "Puts and reset an existing node in a slot, if one exists. Returns true\nif a node was there, false if not. This helps cleanup some of the code\nthat does a lookup just to clear an existing node.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-02 15:45:30 -0600 io_uring/rsrc: add io_reset_rsrc_node() helper"
    },
    {
        "commit": "5f3829fdd69d746f36a5e87df21ce58470b8e9fa",
        "message": "It's only used internally, and in one spot, just open-code ti.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-02 15:45:30 -0600 io_uring/filetable: kill io_reset_alloc_hint() helper"
    },
    {
        "commit": "cb1717a7cd0fc8a063bd7fe3b4eb6fd81defb11c",
        "message": "It's only used in fdinfo, nothing really gained from having this helper.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-02 15:45:30 -0600 io_uring/filetable: remove io_file_from_index() helper"
    },
    {
        "commit": "b54a14041ee6444692d95ff38c8b3d1af682aa17",
        "message": "There are lots of spots open-coding this functionality, add a generic\nhelper that does the node lookup in a speculation safe way.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-02 15:45:30 -0600 io_uring/rsrc: add io_rsrc_node_lookup() helper"
    },
    {
        "commit": "3597f2786b687a7f26361ce00a805ea0af41b65f",
        "message": "For files, there's nr_user_files/file_table/file_data, and buffers have\nnr_user_bufs/user_bufs/buf_data. There's no reason why file_table and\nfile_data can't be the same thing, and ditto for the buffer side. That\ngets rid of more io_ring_ctx state that's in two spots rather than just\nbeing in one spot, as it should be. Put all the registered file data in\none locations, and ditto on the buffer front.\n\nThis also avoids having both io_rsrc_data->nodes being an allocated\narray, and ->user_bufs[] or ->file_table.nodes. There's no reason to\nhave this information duplicated. Keep it in one spot, io_rsrc_data,\nalong with how many resources are available.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-02 15:45:23 -0600 io_uring/rsrc: unify file and buffer resource tables"
    },
    {
        "commit": "f38f2847646f8be29a8fcb722e8b1dc8c8cb3924",
        "message": "Add the empty node initializing to the preinit part of the io_kiocb\nallocation, and reset them if they have been used.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-02 15:44:30 -0600 io_uring: only initialize io_kiocb rsrc_nodes when needed"
    },
    {
        "commit": "0701db7439208951c8a7d8600668e5cfdd5f63d2",
        "message": "Rather than allocate an io_rsrc_node for an empty/sparse buffer entry,\nadd a const entry that can be used for that. This just needs checking\nfor writing the tag, and the put check needs to check for that sparse\nnode rather than NULL for validity.\n\nThis avoids allocating rsrc nodes for sparse buffer entries.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-02 15:44:30 -0600 io_uring/rsrc: add an empty io_rsrc_node for sparse buffer entries"
    },
    {
        "commit": "fbbb8e991d86bb7539de6161746b6c747f93f533",
        "message": "It's not going to be needed in the fast path going forward, so kill it\noff.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-02 15:44:30 -0600 io_uring/rsrc: get rid of io_rsrc_node allocation cache"
    },
    {
        "commit": "7029acd8a950393ee3a3d8e1a7ee1a9b77808a3b",
        "message": "Work in progress, but get rid of the per-ring serialization of resource\nnodes, like registered buffers and files. Main issue here is that one\nnode can otherwise hold up a bunch of other nodes from getting freed,\nwhich is especially a problem for file resource nodes and networked\nworkloads where some descriptors may not see activity in a long time.\n\nAs an example, instantiate an io_uring ring fd and create a sparse\nregistered file table. Even 2 will do. Then create a socket and register\nit as fixed file 0, F0. The number of open files in the app is now 5,\nwith 0/1/2 being the usual stdin/out/err, 3 being the ring fd, and 4\nbeing the socket. Register this socket (eg \"the listener\") in slot 0 of\nthe registered file table. Now add an operation on the socket that uses\nslot 0. Finally, loop N times, where each loop creates a new socket,\nregisters said socket as a file, then unregisters the socket, and\nfinally closes the socket. This is roughly similar to what a basic\naccept loop would look like.\n\nAt the end of this loop, it's not unreasonable to expect that there\nwould still be 5 open files. Each socket created and registered in the\nloop is also unregistered and closed. But since the listener socket\nregistered first still has references to its resource node due to still\nbeing active, each subsequent socket unregistration is stuck behind it\nfor reclaim. Hence 5 + N files are still open at that point, where N is\nawaiting the final put held up by the listener socket.\n\nRewrite the io_rsrc_node handling to NOT rely on serialization. Struct\nio_kiocb now gets explicit resource nodes assigned, with each holding a\nreference to the parent node. A parent node is either of type FILE or\nBUFFER, which are the two types of nodes that exist. A request can have\ntwo nodes assigned, if it's using both registered files and buffers.\nSince request issue and task_work completion is both under the ring\nprivate lock, no atomics are needed to handle these references. It's a\nsimple unlocked inc/dec. As before, the registered buffer or file table\neach hold a reference as well to the registered nodes. Final put of the\nnode will remove the node and free the underlying resource, eg unmap the\nbuffer or put the file.\n\nOutside of removing the stall in resource reclaim described above, it\nhas the following advantages:\n\n1) It's a lot simpler than the previous scheme, and easier to follow.\n   No need to specific quiesce handling anymore.\n\n2) There are no resource node allocations in the fast path, all of that\n   happens at resource registration time.\n\n3) The structs related to resource handling can all get simplified\n   quite a bit, like io_rsrc_node and io_rsrc_data. io_rsrc_put can\n   go away completely.\n\n4) Handling of resource tags is much simpler, and doesn't require\n   persistent storage as it can simply get assigned up front at\n   registration time. Just copy them in one-by-one at registration time\n   and assign to the resource node.\n\nThe only real downside is that a request is now explicitly limited to\npinning 2 resources, one file and one buffer, where before just\nassigning a resource node to a request would pin all of them. The upside\nis that it's easier to follow now, as an individual resource is\nexplicitly referenced and assigned to the request.\n\nWith this in place, the above mentioned example will be using exactly 5\nfiles at the end of the loop, not N.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-11-02 15:44:18 -0600 io_uring/rsrc: get rid of per-ring io_rsrc_node list"
    },
    {
        "commit": "f4a1e8e36973e2034c9eac2b3538470f8b2748a4",
        "message": "Pull block fixes from Jens Axboe:\n\n - Fixup for a recent blk_rq_map_user_bvec() patch\n\n - NVMe pull request via Keith:\n     - Spec compliant identification fix (Keith)\n     - Module parameter to enable backward compatibility on unusual\n       namespace formats (Keith)\n     - Target double free fix when using keys (Vitaliy)\n     - Passthrough command error handling fix (Keith)\n\n* tag 'block-6.12-20241101' of git://git.kernel.dk/linux:\n  nvme: re-fix error-handling for io_uring nvme-passthrough\n  nvmet-auth: assign dh_key to NULL after kfree_sensitive\n  nvme: module parameter to disable pi with offsets\n  block: fix queue limits checks in blk_rq_map_user_bvec for real\n  nvme: enhance cns version checking",
        "kernel_version": "v6.12-rc6",
        "release_date": "2024-11-01 13:41:55 -1000 Merge tag 'block-6.12-20241101' of git://git.kernel.dk/linux"
    },
    {
        "commit": "f0d3699aef2b6f864c78ccfa8e2a7327f65b8841",
        "message": "Pull io_uring fix from Jens Axboe:\n\n - Fix not honoring IOCB_NOWAIT for starting buffered writes in terms of\n   calling sb_start_write(), leading to a deadlock if someone is\n   attempting to freeze the file system with writes in progress, as each\n   side will end up waiting for the other to make progress.\n\n* tag 'io_uring-6.12-20241101' of git://git.kernel.dk/linux:\n  io_uring/rw: fix missing NOWAIT check for O_DIRECT start write",
        "kernel_version": "v6.12-rc6",
        "release_date": "2024-11-01 13:38:01 -1000 Merge tag 'io_uring-6.12-20241101' of git://git.kernel.dk/linux"
    },
    {
        "commit": "d0c6cc6c6a6164a853e86206309b5a5bc5e3e72b",
        "message": "Pull NVMe fixes from Keith:\n\n\"nvme fixes for Linux 6.12\n\n - Spec compliant identification fix (Keith)\n - Module parameter to enable backward compatibility on unusual\n   namespace formats (Keith)\n - Target double free fix when using keys (Vitaliy)\n - Passthrough command error handling fix (Keith)\"\n\n* tag 'nvme-6.12-2024-10-31' of git://git.infradead.org/nvme:\n  nvme: re-fix error-handling for io_uring nvme-passthrough\n  nvmet-auth: assign dh_key to NULL after kfree_sensitive\n  nvme: module parameter to disable pi with offsets\n  nvme: enhance cns version checking",
        "kernel_version": "v6.12-rc6",
        "release_date": "2024-10-31 09:10:07 -0600 Merge tag 'nvme-6.12-2024-10-31' of git://git.infradead.org/nvme into block-6.12"
    },
    {
        "commit": "1d60d74e852647255bd8e76f5a22dc42531e4389",
        "message": "When io_uring starts a write, it'll call kiocb_start_write() to bump the\nsuper block rwsem, preventing any freezes from happening while that\nwrite is in-flight. The freeze side will grab that rwsem for writing,\nexcluding any new writers from happening and waiting for existing writes\nto finish. But io_uring unconditionally uses kiocb_start_write(), which\nwill block if someone is currently attempting to freeze the mount point.\nThis causes a deadlock where freeze is waiting for previous writes to\ncomplete, but the previous writes cannot complete, as the task that is\nsupposed to complete them is blocked waiting on starting a new write.\nThis results in the following stuck trace showing that dependency with\nthe write blocked starting a new write:\n\ntask:fio             state:D stack:0     pid:886   tgid:886   ppid:876\nCall trace:\n __switch_to+0x1d8/0x348\n __schedule+0x8e8/0x2248\n schedule+0x110/0x3f0\n percpu_rwsem_wait+0x1e8/0x3f8\n __percpu_down_read+0xe8/0x500\n io_write+0xbb8/0xff8\n io_issue_sqe+0x10c/0x1020\n io_submit_sqes+0x614/0x2110\n __arm64_sys_io_uring_enter+0x524/0x1038\n invoke_syscall+0x74/0x268\n el0_svc_common.constprop.0+0x160/0x238\n do_el0_svc+0x44/0x60\n el0_svc+0x44/0xb0\n el0t_64_sync_handler+0x118/0x128\n el0t_64_sync+0x168/0x170\nINFO: task fsfreeze:7364 blocked for more than 15 seconds.\n      Not tainted 6.12.0-rc5-00063-g76aaf945701c #7963\n\nwith the attempting freezer stuck trying to grab the rwsem:\n\ntask:fsfreeze        state:D stack:0     pid:7364  tgid:7364  ppid:995\nCall trace:\n __switch_to+0x1d8/0x348\n __schedule+0x8e8/0x2248\n schedule+0x110/0x3f0\n percpu_down_write+0x2b0/0x680\n freeze_super+0x248/0x8a8\n do_vfs_ioctl+0x149c/0x1b18\n __arm64_sys_ioctl+0xd0/0x1a0\n invoke_syscall+0x74/0x268\n el0_svc_common.constprop.0+0x160/0x238\n do_el0_svc+0x44/0x60\n el0_svc+0x44/0xb0\n el0t_64_sync_handler+0x118/0x128\n el0t_64_sync+0x168/0x170\n\nFix this by having the io_uring side honor IOCB_NOWAIT, and only attempt a\nblocking grab of the super block rwsem if it isn't set. For normal issue\nwhere IOCB_NOWAIT would always be set, this returns -EAGAIN which will\nhave io_uring core issue a blocking attempt of the write. That will in\nturn also get completions run, ensuring forward progress.\n\nSince freezing requires CAP_SYS_ADMIN in the first place, this isn't\nsomething that can be triggered by a regular user.\n\nCc: stable@vger.kernel.org # 5.10+\nReported-by: Peter Mann <peter.mann@sh.cz>\nLink: https://lore.kernel.org/io-uring/38c94aec-81c9-4f62-b44e-1d87f5597644@sh.cz\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc6",
        "release_date": "2024-10-31 08:21:02 -0600 io_uring/rw: fix missing NOWAIT check for O_DIRECT start write"
    },
    {
        "commit": "a14968aea637bbe38a99e6089944e4ad8e6c49e5",
        "message": "Fix an issue detected by the Smatch tool:\n\ndrivers/gpio/gpiolib-swnode.c:78 swnode_find_gpio() error:\nuninitialized symbol 'ret'.\n\nThe issue occurs because the 'ret' variable may be used without\ninitialization if the for_each_gpio_property_name loop does not run.\nThis could lead to returning an undefined value, causing unpredictable\nbehavior.\n\nInitialize 'ret' to 0 before the loop to ensure the function\nreturns an error code if no properties are parsed, maintaining proper\nerror handling.\n\nFixes: 9e4c6c1ad (\"Merge tag 'io_uring-6.12-20241011' of git://git.kernel.dk/linux\")\nSigned-off-by: Suraj Sonawane <surajsonawane0215@gmail.com>\nLink: https://lore.kernel.org/r/20241026090642.28633-1-surajsonawane0215@gmail.com\nSigned-off-by: Bartosz Golaszewski <bartosz.golaszewski@linaro.org>",
        "kernel_version": "v6.12-rc6",
        "release_date": "2024-10-31 13:39:25 +0100 gpio: fix uninit-value in swnode_find_gpio"
    },
    {
        "commit": "5eed4fb274cd6579f2fb4190b11c4c86c553cd06",
        "message": "This was previously fixed with commit 1147dd0503564fa0e0348\n(\"nvme: fix error-handling for io_uring nvme-passthrough\"), but the\nchange was mistakenly undone in a later commit.\n\nFixes: d6aacee9255e7f (\"nvme: use bio_integrity_map_user\")\nCc: stable@vger.kernel.org\nReported-by: Jens Axboe <axboe@kernel.dk>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Anuj Gupta <anuj20.g@samsung.com>\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nSigned-off-by: Keith Busch <kbusch@kernel.org>",
        "kernel_version": "v6.12-rc6",
        "release_date": "2024-10-30 07:19:18 -0700 nvme: re-fix error-handling for io_uring nvme-passthrough"
    },
    {
        "commit": "e410ffca588691e36d5449a5bf521a1a7b712911",
        "message": "It's only used from __io_req_set_rsrc_node(), and it takes both the ctx\nand node itself, while never using the ctx. Just open-code the basic\nrefs++ in __io_req_set_rsrc_node() instead.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:28 -0600 io_uring/rsrc: kill io_charge_rsrc_node()"
    },
    {
        "commit": "743fb58a35cde8fe27b07ee5a985ae76563845e3",
        "message": "In preparation for not pinning the whole registered file table, open\ncode the second potential direct file assignment. This will be handled\nby appropriate helpers in the future, for now just do it manually.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:28 -0600 io_uring/splice: open code 2nd direct file assignment"
    },
    {
        "commit": "aaa736b186239b7dc7778ae94c75f26c96972796",
        "message": "Doesn't matter right now as there's still some bytes left for it, but\nlet's prepare for the io_kiocb potentially growing and add a specific\nfreeptr offset for it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:28 -0600 io_uring: specify freeptr usage for SLAB_TYPESAFE_BY_RCU io_kiocb cache"
    },
    {
        "commit": "ff1256b8f3c45f222bce19fbfc1e1bc498b31d03",
        "message": "There's no need for this internal structure to be visible, move it to\nthe private rsrc.h header instead.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:28 -0600 io_uring/rsrc: move struct io_fixed_file to rsrc.h header"
    },
    {
        "commit": "a85f31052bce52111b4e9d5a536003481d0421d0",
        "message": "Useful for testing performance/efficiency impact of registered files\nand buffers, vs (particularly) non-registered files.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:28 -0600 io_uring/nop: add support for testing registered files and buffers"
    },
    {
        "commit": "aa00f67adc2c0d6439f81b5a81ff181377c47a7e",
        "message": "Generally applications have 1 or a few waits of waiting, yet they pass\nin a struct io_uring_getevents_arg every time. This needs to get copied\nand, in turn, the timeout value needs to get copied.\n\nRather than do this for every invocation, allow the application to\nregister a fixed set of wait regions that can simply be indexed when\nasking the kernel to wait on events.\n\nAt ring setup time, the application can register a number of these wait\nregions and initialize region/index 0 upfront:\n\n\tstruct io_uring_reg_wait *reg;\n\n\treg = io_uring_setup_reg_wait(ring, nr_regions, &ret);\n\n\t/* set timeout and mark as set, sigmask/sigmask_sz as needed */\n\treg->ts.tv_sec = 0;\n\treg->ts.tv_nsec = 100000;\n\treg->flags = IORING_REG_WAIT_TS;\n\nwhere nr_regions >= 1 && nr_regions <= PAGE_SIZE / sizeof(*reg). The\nabove initializes index 0, but 63 other regions can be initialized,\nif needed. Now, instead of doing:\n\n\tstruct __kernel_timespec timeout = { .tv_nsec = 100000, };\n\n\tio_uring_submit_and_wait_timeout(ring, &cqe, nr, &t, NULL);\n\nto wait for events for each submit_and_wait, or just wait, operation, it\ncan just reference the above region at offset 0 and do:\n\n\tio_uring_submit_and_wait_reg(ring, &cqe, nr, 0);\n\nto achieve the same goal of waiting 100usec without needing to copy\nboth struct io_uring_getevents_arg (24b) and struct __kernel_timeout\n(16b) for each invocation. Struct io_uring_reg_wait looks as follows:\n\nstruct io_uring_reg_wait {\n\tstruct __kernel_timespec\tts;\n\t__u32\t\t\t\tmin_wait_usec;\n\t__u32\t\t\t\tflags;\n\t__u64\t\t\t\tsigmask;\n\t__u32\t\t\t\tsigmask_sz;\n\t__u32\t\t\t\tpad[3];\n\t__u64\t\t\t\tpad2[2];\n};\n\nembedding the timeout itself in the region, rather than passing it as\na pointer as well. Note that the signal mask is still passed as a\npointer, both for compatability reasons, but also because there doesn't\nseem to be a lot of high frequency waits scenarios that involve setting\nand resetting the signal mask for each wait.\n\nThe application is free to modify any region before a wait call, or it\ncan use keep multiple regions with different settings to avoid needing to\nmodify the same one for wait calls. Up to a page size of regions is mapped\nby default, allowing PAGE_SIZE / 64 available regions for use.\n\nThe registered region must fit within a page. On a 4kb page size system,\nthat allows for 64 wait regions if a full page is used, as the size of\nstruct io_uring_reg_wait is 64b. The region registered must be aligned\nto io_uring_reg_wait in size. It's valid to register less than 64\nentries.\n\nIn network performance testing with zero-copy, this reduced the time\nspent waiting on the TX side from 3.12% to 0.3% and the RX side from 4.4%\nto 0.3%.\n\nWait regions are fixed for the lifetime of the ring - once registered,\nthey are persistent until the ring is torn down. The regions support\nminimum wait timeout as well as the regular waits.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:28 -0600 io_uring: add support for fixed wait regions"
    },
    {
        "commit": "371b47da25e1f7a1a6323f84c776bd9fa079a490",
        "message": "In scenarios where a high frequency of wait events are seen, the copy\nof the struct io_uring_getevents_arg is quite noticeable in the\nprofiles in terms of time spent. It can be seen as up to 3.5-4.5%.\nRewrite the copy-in logic, saving about 0.5% of the time.\n\nReviewed-by: Keith Busch <kbusch@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring: change io_get_ext_arg() to use uaccess begin + end"
    },
    {
        "commit": "0a54a7dd0a12b777721f5ca55c9d6331d2a46b01",
        "message": "This avoids intermediate storage for turning a __kernel_timespec\nuser pointer into an on-stack struct timespec64, only then to turn it\ninto a ktime_t.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring: switch struct ext_arg from __kernel_timespec to timespec64"
    },
    {
        "commit": "b898b8c99ead1ce8bee95083bba296e4a86a6c05",
        "message": "io_sqd_handle_event() just does a mutex unlock/lock dance when it's\nsupposed to park, somewhat relying on full ordering with the thread\ntrying to park it which does a similar unlock/lock dance on sqd->lock.\nHowever, with adaptive spinning on mutexes, this can waste an awful\nlot of time. Normally this isn't very noticeable, as parking and\nunparking the thread isn't a common (or fast path) occurence. However,\nin testing ring resizing, it's testing exactly that, as each resize\nwill require the SQPOLL to safely park and unpark.\n\nHave io_sq_thread_park() explicitly wait on sqd->park_pending being\nzero before attempting to grab the sqd->lock again.\n\nIn a resize test, this brings the runtime of SQPOLL down from about\n60 seconds to a few seconds, just like the !SQPOLL tests. And saves\na ton of spinning time on the mutex, on both sides.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring/sqpoll: wait on sqd->wait for thread parking"
    },
    {
        "commit": "79cfe9e59c2a12c3b3faeeefe38d23f3d8030972",
        "message": "Once a ring has been created, the size of the CQ and SQ rings are fixed.\nUsually this isn't a problem on the SQ ring side, as it merely controls\nthe available number of requests that can be submitted in a single\nsystem call, and there's rarely a need to change that.\n\nFor the CQ ring, it's a different story. For most efficient use of\nio_uring, it's important that the CQ ring never overflows. This means\nthat applications must size it for the worst case scenario, which can\nbe wasteful.\n\nAdd IORING_REGISTER_RESIZE_RINGS, which allows an application to resize\nthe existing rings. It takes a struct io_uring_params argument, the same\none which is used to setup the ring initially, and resizes rings\naccording to the sizes given.\n\nCertain properties are always inherited from the original ring setup,\nlike SQE128/CQE32 and other setup options. The implementation only\nallows flag associated with how the CQ ring is sized and clamped.\n\nExisting unconsumed SQE and CQE entries are copied as part of the\nprocess. If either the SQ or CQ resized destination ring cannot hold the\nentries already present in the source rings, then the operation is failed\nwith -EOVERFLOW. Any register op holds ->uring_lock, which prevents new\nsubmissions, and the internal mapping holds the completion lock as well\nacross moving CQ ring state.\n\nTo prevent races between mmap and ring resizing, add a mutex that's\nsolely used to serialize ring resize and mmap. mmap_sem can't be used\nhere, as as fork'ed process may be doing mmaps on the ring as well.\nThe ctx->resize_lock is held across mmap operations, and the resize\nwill grab it before swapping out the already mapped new data.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring/register: add IORING_REGISTER_RESIZE_RINGS"
    },
    {
        "commit": "d090bffab609762af06dec295a305ce270941b42",
        "message": "The later mapping will actually check this too, but in terms of code\nclarify, explicitly check for whether or not the rings and sqes are\nvalid during validation. That makes it explicit that if they are\nnon-NULL, they are valid and can get mapped.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring/memmap: explicitly return -EFAULT for mmap on NULL rings"
    },
    {
        "commit": "81d8191eb99d95b32e55d09d74f682d40d3e74e9",
        "message": "Abstract out a io_uring_fill_params() helper, which fills out the\nnecessary bits of struct io_uring_params. Add it to io_uring.h as well,\nin preparation for having another internal user of it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring: abstract out a bit of the ring filling logic"
    },
    {
        "commit": "09d0a8ea7facc8b1581c9bd85c3ea6f5aa62ab7d",
        "message": "In preparation for needing this somewhere else, move the definitions\nfor the maximum CQ and SQ ring size into io_uring.h. Make the\nrings_size() helper available as well, and have it take just the setup\nflags argument rather than the fill ring pointer. That's all that is\nneeded.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring: move max entry definition and ring sizing into header"
    },
    {
        "commit": "882dec6c39c40c13dd03e418952c4af38d91bb38",
        "message": "Put sr->umsg into a local variable, so it doesn't repeat \"sr->umsg->\"\nfor every field. It looks nicer, and likely without the patch it\ncompiles into a bunch of umsg memory reads.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/26c2f30b491ea7998bfdb5bb290662572a61064d.1729607201.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring/net: clean up io_msg_copy_hdr"
    },
    {
        "commit": "52838787350d4ea8132804940d5308d95ce5e035",
        "message": "We keep user pointers in an union, which could be a user buffer or a\nuser pointer to msghdr. What is confusing is that it potenitally reads\nand assigns sqe->addr as one type but then uses it as another via the\nunion. Even more, it's not even consistent across copy and zerocopy\nversions.\n\nMake send and sendmsg setup helpers read sqe->addr and treat it as the\nright type from the beginning. The end goal would be to get rid of\nthe use of struct io_sr_msg::umsg for send requests as we only need it\nat the prep side.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/685d788605f5d78af18802fcabf61ba65cfd8002.1729607201.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring/net: don't alias send user pointer reads"
    },
    {
        "commit": "ad438d070a3bf2a3ae45b59a885a5d7b0dbbc465",
        "message": "For non \"msg\" requests we copy the address at the prep stage and there\nis no need to store the address user pointer long term. Pass the SQE\ninto io_send_setup(), let it parse it, and remove struct io_sr_msg addr\naddr_len fields. It saves some space and also less confusing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/db3dce544e17ca9d4b17d2506fbbac1da8a87824.1729607201.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring/net: don't store send address ptr"
    },
    {
        "commit": "93db98f6f1d62c9e58787f6beb62245ddb91f354",
        "message": "A preparation patch splitting io_sendmsg_prep_setup into two separate\nhelpers for send and sendmsg variants.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1a2319471ba040e053b7f1d22f4af510d1118eca.1729607201.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring/net: split send and sendmsg prep helpers"
    },
    {
        "commit": "e6d43739d0ee49a39505d696ba6a656f47c2bd39",
        "message": "It's no longer being used, remove it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring: kill 'imu' from struct io_kiocb"
    },
    {
        "commit": "51c967c6c9ea6c4d480e4778ace5243db22aa27b",
        "message": "Let's keep it close with the actual import, there's no reason to do this\non the prep side. With that, we can drop one of the branches checking\nfor whether or not IORING_RECVSEND_FIXED_BUF is set.\n\nAs a side-effect, get rid of req->imu usage.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring/net: move send zc fixed buffer import to issue path"
    },
    {
        "commit": "1caa00d6b61651e04c04c2b50b3e149f24c6764d",
        "message": "All callers already hold the ring lock and hence are passing '0',\nremove the argument and the conditional locking that it controlled.\n\nSuggested-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring: remove 'issue_flags' argument for io_req_set_rsrc_node()"
    },
    {
        "commit": "003f82b58c99146dfb0c9ce1ee7ed59bc572959b",
        "message": "It's assigned in the same function that it's being used, get rid of\nit. A local variable will do just fine.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring/rw: get rid of using req->imu"
    },
    {
        "commit": "892d3e80e1b9fc09aefdfd4d31f10f3d018863a0",
        "message": "It's pretty pointless to use io_kiocb as intermediate storage for this,\nso split the validity check and the actual usage. The resource node is\nassigned upfront at prep time, to prevent it from going away. The actual\nimport is never called with the ctx->uring_lock held, so grab it for\nthe import.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring/uring_cmd: get rid of using req->imu"
    },
    {
        "commit": "c919790060230ac2b1824bbf4d3b64eb51f471ff",
        "message": "iter->bvec is already set to imu->bvec - remove the one dead assignment\nand turn the other one into an addition instead.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring/rsrc: don't assign bvec twice in io_import_fixed()"
    },
    {
        "commit": "2946f08ae9ed650b94e0ffebcdfdda8de76bd926",
        "message": "We have too many helpers posting CQEs, instead of tracing completion\nevents before filling in a CQE and thus having to pass all the data,\nset the CQE first, pass it to the tracing helper and let it extract\neverything it needs.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b83c1ca9ee5aed2df0f3bb743bf5ed699cce4c86.1729267437.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring: clean up cqe trace points"
    },
    {
        "commit": "9b296c625ac1d2ca9b129743c3f886bf7a0f471d",
        "message": "IORING_SETUP_NO_SQARRAY should be preferred and used by default by\nliburing, optimise flag checking in io_get_sqe() with a static key.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c164a48542fbb080115e2377ecf160c758562742.1729264988.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring: static_key for !IORING_SETUP_NO_SQARRAY"
    },
    {
        "commit": "1e6e7602cc9fdeaf7e2593755409e8d50545ed69",
        "message": "io_llist_xchg is only used to set the list to NULL, which can also be\ndone with llist_del_all(). Use the latter and kill io_llist_xchg.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d6765112680d2e86a58b76166b7513391ff4e5d7.1729264960.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring: kill io_llist_xchg"
    },
    {
        "commit": "b6b3eb19dd86ecc3f188bd419f12cdfcfbeda5e7",
        "message": "Convert to using kvmalloc/kfree() for the hash tables, and while at it,\nmake it handle low memory situations better.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring: move cancel hash tables to kvmalloc/kvfree"
    },
    {
        "commit": "8abf47a8d61c9e8314ae4cfa27e18c8df67c37bc",
        "message": "All it does is initialize the lists, just move the INIT_HLIST_HEAD()\ninto the one caller.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring/cancel: get rid of init_hash_table() helper"
    },
    {
        "commit": "ba4366f57b117c2eab996642288e5c75646ccfc9",
        "message": "Any access to the table is protected by ctx->uring_lock now anyway, the\nper-bucket locking doesn't buy us anything.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring/poll: get rid of per-hashtable bucket locks"
    },
    {
        "commit": "879ba46a38e67595b96c87428fbb718d63821da2",
        "message": "It serves no purposes anymore, all it does is delete the hash list\nentry. task_work always has the ring locked.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring/poll: get rid of io_poll_tw_hash_eject()"
    },
    {
        "commit": "085268829b07202cf7bf8ec1a8fb7fd9d8f6a41a",
        "message": "io_uring maintains two hash lists of inflight requests:\n\n1) ctx->cancel_table_locked. This is used when the caller has the\n   ctx->uring_lock held already. This is only an issue side parameter,\n   as removal or task_work will always have it held.\n\n2) ctx->cancel_table. This is used when the issuer does NOT have the\n   ctx->uring_lock held, and relies on the table spinlocks for access.\n\nHowever, it's pretty trivial to simply grab the lock in the one spot\nwhere we care about it, for insertion. With that, we can kill the\nunlocked table (and get rid of the _locked postfix for the other one).\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:27 -0600 io_uring/poll: get rid of unlocked cancel hash"
    },
    {
        "commit": "829ab73e7bca455e1a8718325177cfb98b63d0df",
        "message": "It's always req->ctx being used anyway, having this as a separate\nargument (that is then not even used) just makes it more confusing.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:26 -0600 io_uring/poll: remove 'ctx' argument from io_poll_req_delete()"
    },
    {
        "commit": "a377132154ab8404dafcc52e8bc0c73050a954c2",
        "message": "Normally MSG_RING requires both a source and a destination ring. But\nsome users don't always have a ring avilable to send a message from, yet\nthey still need to notify a target ring.\n\nAdd support for using io_uring_register(2) without having a source ring,\nusing a file descriptor of -1 for that. Internally those are called\nblind registration opcodes. Implement IORING_REGISTER_SEND_MSG_RING as a\nblind opcode, which simply takes an sqe that the application can put on\nthe stack and use the normal liburing helpers to initialize it. Then the\napp can call:\n\nio_uring_register(-1, IORING_REGISTER_SEND_MSG_RING, &sqe, 1);\n\nand get the same behavior in terms of the target, where a CQE is posted\nwith the details given in the sqe.\n\nFor now this takes a single sqe pointer argument, and hence arg must\nbe set to that, and nr_args must be 1. Could easily be extended to take\nan array of sqes, but for now let's keep it simple.\n\nLink: https://lore.kernel.org/r/20240924115932.116167-3-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:26 -0600 io_uring/msg_ring: add support for sending a sync message"
    },
    {
        "commit": "95d6c9229a04cc12d39034cd6be6446a55a85d6d",
        "message": "Mostly just to skip them taking an io_kiocb, rather just pass in the\nctx and io_msg directly.\n\nIn preparation for being able to issue a MSG_RING request without\nhaving an io_kiocb. No functional changes in this patch.\n\nLink: https://lore.kernel.org/r/20240924115932.116167-2-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:26 -0600 io_uring/msg_ring: refactor a few helper functions"
    },
    {
        "commit": "f4bb2f65bb8154c1a2c2d7e01db0c98dffb5918f",
        "message": "Everything else about the io_uring eventfd support is nicely kept\nprivate to that code, except the cached_cq_tail tracking. With\neverything else in place, move io_eventfd_flush_signal() to using\nthe ev_fd grab+release helpers, which then enables the direct use of\nio_ev_fd for this tracking too.\n\nLink: https://lore.kernel.org/r/20240921080307.185186-7-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:26 -0600 io_uring/eventfd: move ctx->evfd_last_cq_tail into io_ev_fd"
    },
    {
        "commit": "83a4f865e273b83426eafdd3aa51334cc21ac0fd",
        "message": "In preparation for needing the ev_fd grabbing (and releasing) from\nanother path, abstract out two helpers for that.\n\nLink: https://lore.kernel.org/r/20240921080307.185186-6-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:26 -0600 io_uring/eventfd: abstract out ev_fd grab + release helpers"
    },
    {
        "commit": "3ca5a356041438534ecbb74159df91736238c6b1",
        "message": "It's a bit hard to read what guards the triggering, move it into a\nhelper and add a comment explaining it too. This additionally moves\nthe ev_fd == NULL check in there as well.\n\nLink: https://lore.kernel.org/r/20240921080307.185186-5-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:26 -0600 io_uring/eventfd: move trigger check into a helper"
    },
    {
        "commit": "60c5f15800f21883615689e2423217a9c8a1b502",
        "message": "In preparation for using this from multiple spots, move the signaling\ninto a helper.\n\nLink: https://lore.kernel.org/r/20240921080307.185186-4-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:26 -0600 io_uring/eventfd: move actual signaling part into separate helper"
    },
    {
        "commit": "3c90b80df5b574c2c61626fd40fa3b23be21fa26",
        "message": "It's not necessary to do this post grabbing a reference. With that, we\ncan drop the out goto path as well.\n\nLink: https://lore.kernel.org/r/20240921080307.185186-3-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:26 -0600 io_uring/eventfd: check for the need to async notifier earlier"
    },
    {
        "commit": "165126dc5e23979721122dc5c7cfb28b1ca234cc",
        "message": "We call this in two spot, have a helper for it. In preparation for\nextending this part.\n\nLink: https://lore.kernel.org/r/20240921080307.185186-2-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-29 13:43:26 -0600 io_uring/eventfd: abstract out ev_fd put helper"
    },
    {
        "commit": "2ff949441802a8d076d9013c7761f63e8ae5a9bd",
        "message": "blk_rq_map_user_bvec contains a check bytes + bv->bv_len > nr_iter which\ncauses unnecessary failures in NVMe passthrough I/O, reproducible as\nfollows:\n\n- register a 2 page, page-aligned buffer against a ring\n- use that buffer to do a 1 page io_uring NVMe passthrough read\n\nThe second (i = 1) iteration of the loop in blk_rq_map_user_bvec will\nthen have nr_iter == 1 page, bytes == 1 page, bv->bv_len == 1 page, so\nthe check bytes + bv->bv_len > nr_iter will succeed, causing the I/O to\nfail. This failure is unnecessary, as when the check succeeds, it means\nwe've checked the entire buffer that will be used by the request - i.e.\nblk_rq_map_user_bvec should complete successfully. Therefore, terminate\nthe loop early and return successfully when the check bytes + bv->bv_len\n> nr_iter succeeds.\n\nWhile we're at it, also remove the check that all segments in the bvec\nare single-page. While this seems to be true for all users of the\nfunction, it doesn't appear to be required anywhere downstream.\n\nCC: stable@vger.kernel.org\nSigned-off-by: Xinyu Zhang <xizhang@purestorage.com>\nCo-developed-by: Uday Shankar <ushankar@purestorage.com>\nSigned-off-by: Uday Shankar <ushankar@purestorage.com>\nFixes: 37987547932c (\"block: extend functionality to map bvec iterator\")\nLink: https://lore.kernel.org/r/20241023211519.4177873-1-ushankar@purestorage.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc5",
        "release_date": "2024-10-23 17:02:48 -0600 block: fix sanity checks in blk_rq_map_user_bvec"
    },
    {
        "commit": "5e52f71f858eaff252a47530a5ad5e79309bd415",
        "message": "Command implementations shouldn't be directly looking into io_uring_cmd\nto carve free space. Use an io_uring helper, which will also do build\ntime size sanitisation.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nReviewed-by: Chaitanya Kulkarni <kch@nvidia.com>\nReviewed-by: Anuj Gupta <anuj20.g@samsung.com>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Keith Busch <kbusch@kernel.org>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-22 07:28:36 -0700 nvme: use helpers to access io_uring cmd space"
    },
    {
        "commit": "dc7e76ba7a6057e4c12d449db49f026d0ec238ec",
        "message": "Rejection of IOSQE_FIXED_FILE combined with IORING_OP_[GS]ETXATTR\nis fine - these do not take a file descriptor, so such combination\nmakes no sense.  The checks are misplaced, though - as it is, they\ntriggers on IORING_OP_F[GS]ETXATTR as well, and those do take\na file reference, no matter the origin.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Al Viro <viro@zeniv.linux.org.uk>",
        "kernel_version": "v6.13-rc1",
        "release_date": "2024-10-19 20:40:10 -0400 io_uring: IORING_OP_F[GS]ETXATTR is fine with REQ_F_FIXED_FILE"
    },
    {
        "commit": "715ca9dd687f89ddaac8ec8ccb3b5e5a30311a99",
        "message": "Pull one more io_uring fix from Jens Axboe:\n \"Fix for a regression introduced in 6.12-rc2, where a condition check\n  was negated and hence -EAGAIN would bubble back up up to userspace\n  rather than trigger a retry condition\"\n\n* tag 'io_uring-6.12-20241019' of git://git.kernel.dk/linux:\n  io_uring/rw: fix wrong NOWAIT check in io_rw_init_file()",
        "kernel_version": "v6.12-rc4",
        "release_date": "2024-10-19 17:04:52 -0700 Merge tag 'io_uring-6.12-20241019' of git://git.kernel.dk/linux"
    },
    {
        "commit": "ae6a888a4357131c01d85f4c91fb32552dd0bf70",
        "message": "A previous commit improved how !FMODE_NOWAIT is dealt with, but\ninadvertently negated a check whilst doing so. This caused -EAGAIN to be\nreturned from reading files with O_NONBLOCK set. Fix up the check for\nREQ_F_SUPPORT_NOWAIT.\n\nReported-by: Julian Orth <ju.orth@gmail.com>\nLink: https://github.com/axboe/liburing/issues/1270\nFixes: f7c913438533 (\"io_uring/rw: allow pollable non-blocking attempts for !FMODE_NOWAIT\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc4",
        "release_date": "2024-10-19 09:25:45 -0600 io_uring/rw: fix wrong NOWAIT check in io_rw_init_file()"
    },
    {
        "commit": "a041f47898e30e01fea5da4a47bd6bcd72d8955a",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix a regression this merge window where cloning of registered\n   buffers didn't take into account the dummy_ubuf\n\n - Fix a race with reading how many SQRING entries are available,\n   causing userspace to need to loop around io_uring_sqring_wait()\n   rather than being able to rely on SQEs being available when it\n   returned\n\n - Ensure that the SQPOLL thread is TASK_RUNNING before running\n   task_work off the cancelation exit path\n\n* tag 'io_uring-6.12-20241018' of git://git.kernel.dk/linux:\n  io_uring/sqpoll: ensure task state is TASK_RUNNING when running task_work\n  io_uring/rsrc: ignore dummy_ubuf for buffer cloning\n  io_uring/sqpoll: close race on waiting for sqring entries",
        "kernel_version": "v6.12-rc4",
        "release_date": "2024-10-18 15:38:37 -0700 Merge tag 'io_uring-6.12-20241018' of git://git.kernel.dk/linux"
    },
    {
        "commit": "8f7033aa4089fbaf7a33995f0f2ee6c9d7b9ca1b",
        "message": "When the sqpoll is exiting and cancels pending work items, it may need\nto run task_work. If this happens from within io_uring_cancel_generic(),\nthen it may be under waiting for the io_uring_task waitqueue. This\nresults in the below splat from the scheduler, as the ring mutex may be\nattempted grabbed while in a TASK_INTERRUPTIBLE state.\n\nEnsure that the task state is set appropriately for that, just like what\nis done for the other cases in io_run_task_work().\n\ndo not call blocking ops when !TASK_RUNNING; state=1 set at [<0000000029387fd2>] prepare_to_wait+0x88/0x2fc\nWARNING: CPU: 6 PID: 59939 at kernel/sched/core.c:8561 __might_sleep+0xf4/0x140\nModules linked in:\nCPU: 6 UID: 0 PID: 59939 Comm: iou-sqp-59938 Not tainted 6.12.0-rc3-00113-g8d020023b155 #7456\nHardware name: linux,dummy-virt (DT)\npstate: 61400005 (nZCv daif +PAN -UAO -TCO +DIT -SSBS BTYPE=--)\npc : __might_sleep+0xf4/0x140\nlr : __might_sleep+0xf4/0x140\nsp : ffff80008c5e7830\nx29: ffff80008c5e7830 x28: ffff0000d93088c0 x27: ffff60001c2d7230\nx26: dfff800000000000 x25: ffff0000e16b9180 x24: ffff80008c5e7a50\nx23: 1ffff000118bcf4a x22: ffff0000e16b9180 x21: ffff0000e16b9180\nx20: 000000000000011b x19: ffff80008310fac0 x18: 1ffff000118bcd90\nx17: 30303c5b20746120 x16: 74657320313d6574 x15: 0720072007200720\nx14: 0720072007200720 x13: 0720072007200720 x12: ffff600036c64f0b\nx11: 1fffe00036c64f0a x10: ffff600036c64f0a x9 : dfff800000000000\nx8 : 00009fffc939b0f6 x7 : ffff0001b6327853 x6 : 0000000000000001\nx5 : ffff0001b6327850 x4 : ffff600036c64f0b x3 : ffff8000803c35bc\nx2 : 0000000000000000 x1 : 0000000000000000 x0 : ffff0000e16b9180\nCall trace:\n __might_sleep+0xf4/0x140\n mutex_lock+0x84/0x124\n io_handle_tw_list+0xf4/0x260\n tctx_task_work_run+0x94/0x340\n io_run_task_work+0x1ec/0x3c0\n io_uring_cancel_generic+0x364/0x524\n io_sq_thread+0x820/0x124c\n ret_from_fork+0x10/0x20\n\nCc: stable@vger.kernel.org\nFixes: af5d68f8892f (\"io_uring/sqpoll: manage task_work privately\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc4",
        "release_date": "2024-10-17 08:38:04 -0600 io_uring/sqpoll: ensure task state is TASK_RUNNING when running task_work"
    },
    {
        "commit": "858e686a30d7bffba3f3527add4f78766a4389d0",
        "message": "For placeholder buffers, &dummy_ubuf is assigned which is a static\nvalue. When buffers are attempted cloned, don't attempt to grab a\nreference to it, as we both don't need it and it'll actively fail as\ndummy_ubuf doesn't have a valid reference count setup.\n\nLink: https://lore.kernel.org/io-uring/Zw8dkUzsxQ5LgAJL@ly-workstation/\nReported-by: Lai, Yi <yi1.lai@linux.intel.com>\nFixes: 7cc2a6eadcd7 (\"io_uring: add IORING_REGISTER_COPY_BUFFERS method\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc4",
        "release_date": "2024-10-16 07:09:25 -0600 io_uring/rsrc: ignore dummy_ubuf for buffer cloning"
    },
    {
        "commit": "28aabffae6be54284869a91cd8bccd3720041129",
        "message": "When an application uses SQPOLL, it must wait for the SQPOLL thread to\nconsume SQE entries, if it fails to get an sqe when calling\nio_uring_get_sqe(). It can do so by calling io_uring_enter(2) with the\nflag value of IORING_ENTER_SQ_WAIT. In liburing, this is generally done\nwith io_uring_sqring_wait(). There's a natural expectation that once\nthis call returns, a new SQE entry can be retrieved, filled out, and\nsubmitted. However, the kernel uses the cached sq head to determine if\nthe SQRING is full or not. If the SQPOLL thread is currently in the\nprocess of submitting SQE entries, it may have updated the cached sq\nhead, but not yet committed it to the SQ ring. Hence the kernel may find\nthat there are SQE entries ready to be consumed, and return successfully\nto the application. If the SQPOLL thread hasn't yet committed the SQ\nring entries by the time the application returns to userspace and\nattempts to get a new SQE, it will fail getting a new SQE.\n\nFix this by having io_sqring_full() always use the user visible SQ ring\nhead entry, rather than the internally cached one.\n\nCc: stable@vger.kernel.org # 5.10+\nLink: https://github.com/axboe/liburing/discussions/1267\nReported-by: Benedek Thaler <thaler@thaler.hu>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc4",
        "release_date": "2024-10-15 09:13:51 -0600 io_uring/sqpoll: close race on waiting for sqring entries"
    },
    {
        "commit": "9e4c6c1ad9a195f28ec3d3d5054e25f6bdde87bd",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Explicitly have a mshot_finished condition for IORING_OP_RECV in\n   multishot mode, similarly to what IORING_OP_RECVMSG has. This doesn't\n   fix a bug right now, but it makes it harder to actually have a bug\n   here if a request takes multiple iterations to finish.\n\n - Fix handling of retry of read/write of !FMODE_NOWAIT files. If they\n   are pollable, that's all we need.\n\n* tag 'io_uring-6.12-20241011' of git://git.kernel.dk/linux:\n  io_uring/rw: allow pollable non-blocking attempts for !FMODE_NOWAIT\n  io_uring/rw: fix cflags posting for single issue multishot read",
        "kernel_version": "v6.12-rc3",
        "release_date": "2024-10-11 12:00:21 -0700 Merge tag 'io_uring-6.12-20241011' of git://git.kernel.dk/linux"
    },
    {
        "commit": "f7c9134385331c5ef36252895130aa01a92de907",
        "message": "The checking for whether or not io_uring can do a non-blocking read or\nwrite attempt is gated on FMODE_NOWAIT. However, if the file is\npollable, it's feasible to just check if it's currently in a state in\nwhich it can sanely receive or send _some_ data.\n\nThis avoids unnecessary io-wq punts, and repeated worthless retries\nbefore doing that punt, by assuming that some data can get delivered\nor received if poll tells us that is true. It also allows multishot\nreads to properly work with these types of files, enabling a bit of\na cleanup of the logic that:\n\nc9d952b9103b (\"io_uring/rw: fix cflags posting for single issue multishot read\")\n\nhad to put in place.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc3",
        "release_date": "2024-10-06 20:58:53 -0600 io_uring/rw: allow pollable non-blocking attempts for !FMODE_NOWAIT"
    },
    {
        "commit": "c9d952b9103b600ddafc5d1c0e2f2dbd30f0b805",
        "message": "If multishot gets disabled, and hence the request will get terminated\nrather than persist for more iterations, then posting the CQE with the\nright cflags is still important. Most notably, the buffer reference\nneeds to be included.\n\nRefactor the return of __io_read() a bit, so that the provided buffer\nis always put correctly, and hence returned to the application.\n\nReported-by: Sharon Rosner <Sharon Rosner>\nLink: https://github.com/axboe/liburing/issues/1257\nCc: stable@vger.kernel.org\nFixes: 2a975d426c82 (\"io_uring/rw: don't allow multishot reads without NOWAIT support\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc3",
        "release_date": "2024-10-06 08:05:47 -0600 io_uring/rw: fix cflags posting for single issue multishot read"
    },
    {
        "commit": "43454e83916dc515e3d11fd07d50c40e6e555873",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix an error path memory leak, if one part fails to allocate.\n   Obviously not something that'll generally hit without error\n   injection.\n\n - Fix an io_req_flags_t cast to make sparse happier.\n\n - Improve the recv multishot termination. Not a bug now, but could be\n   one in the future. This makes it do the same thing that recvmsg does\n   in terms of when to terminate a request or not.\n\n* tag 'io_uring-6.12-20241004' of git://git.kernel.dk/linux:\n  io_uring/net: harden multishot termination case for recv\n  io_uring: fix casts to io_req_flags_t\n  io_uring: fix memory leak when cache init fail",
        "kernel_version": "v6.12-rc2",
        "release_date": "2024-10-04 10:39:36 -0700 Merge tag 'io_uring-6.12-20241004' of git://git.kernel.dk/linux"
    },
    {
        "commit": "c314094cb4cfa6fc5a17f4881ead2dfebfa717a7",
        "message": "If the recv returns zero, or an error, then it doesn't matter if more\ndata has already been received for this buffer. A condition like that\nshould terminate the multishot receive. Rather than pass in the\ncollected return value, pass in whether to terminate or keep the recv\ngoing separately.\n\nNote that this isn't a bug right now, as the only way to get there is\nvia setting MSG_WAITALL with multishot receive. And if an application\ndoes that, then -EINVAL is returned anyway. But it seems like an easy\nbug to introduce, so let's make it a bit more explicit.\n\nLink: https://github.com/axboe/liburing/issues/1246\nCc: stable@vger.kernel.org\nFixes: b3fdea6ecb55 (\"io_uring: multishot recv\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc2",
        "release_date": "2024-09-30 08:26:59 -0600 io_uring/net: harden multishot termination case for recv"
    },
    {
        "commit": "17ea56b752b6ba58fdd1fcfd24f0fd2fa2b0ade2",
        "message": "Apply __force cast to restricted io_req_flags_t type to fix\nthe following sparse warning:\n\nio_uring/io_uring.c:2026:23: sparse: warning: cast to restricted io_req_flags_t\n\nNo functional changes intended.\n\nSigned-off-by: Min-Hua Chen <minhuadotchen@gmail.com>\nLink: https://lore.kernel.org/r/20240922104132.157055-1-minhuadotchen@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc2",
        "release_date": "2024-09-24 13:31:04 -0600 io_uring: fix casts to io_req_flags_t"
    },
    {
        "commit": "3a87e264290d71ec86a210ab3e8d23b715ad266d",
        "message": "Exit the percpu ref when cache init fails to free the data memory with\nin struct percpu_ref.\n\nFixes: 206aefde4f88 (\"io_uring: reduce/pack size of io_ring_ctx\")\nSigned-off-by: Guixin Liu <kanie@linux.alibaba.com>\nReviewed-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20240923100512.64638-1-kanie@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc2",
        "release_date": "2024-09-24 13:31:00 -0600 io_uring: fix memory leak when cache init fail"
    },
    {
        "commit": "3147a0689dd9793990ff954369ffcdf2de984b46",
        "message": "Pull more io_uring updates from Jens Axboe:\n \"Mostly just a set of fixes in here, or little changes that didn't get\n  included in the initial pull request. This contains:\n\n   - Move the SQPOLL napi polling outside the submission lock (Olivier)\n\n   - Rename of the \"copy buffers\" API that got added in the 6.12 merge\n     window. There's really no copying going on, it's just referencing\n     the buffers. After a bit of consideration, decided that it was\n     better to simply rename this to avoid potential confusion (me)\n\n   - Shrink struct io_mapped_ubuf from 48 to 32 bytes, by changing it to\n     start + len tracking rather than having start / end in there, and\n     by removing the caching of folio_mask when we can just calculate it\n     from folio_shift when we need it (me)\n\n   - Fixes for the SQPOLL affinity checking (me, Felix)\n\n   - Fix for how cqring waiting checks for the presence of task_work.\n     Just check it directly rather than check for a specific\n     notification mechanism (me)\n\n   - Tweak to how request linking is represented in tracing (me)\n\n   - Fix a syzbot report that deliberately sets up a huge list of\n     overflow entries, and then hits rcu stalls when flushing this list.\n     Just check for the need to preempt, and drop/reacquire locks in the\n     loop. There's no state maintained over the loop itself, and each\n     entry is yanked from head-of-list (me)\"\n\n* tag 'for-6.12/io_uring-20240922' of git://git.kernel.dk/linux:\n  io_uring: check if we need to reschedule during overflow flush\n  io_uring: improve request linking trace\n  io_uring: check for presence of task_work rather than TIF_NOTIFY_SIGNAL\n  io_uring/sqpoll: do the napi busy poll outside the submission block\n  io_uring: clean up a type in io_uring_register_get_file()\n  io_uring/sqpoll: do not put cpumask on stack\n  io_uring/sqpoll: retain test for whether the CPU is valid\n  io_uring/rsrc: change ubuf->ubuf_end to length tracking\n  io_uring/rsrc: get rid of io_mapped_ubuf->folio_mask\n  io_uring: rename \"copy buffers\" to \"clone buffers\"",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-24 11:11:38 -0700 Merge tag 'for-6.12/io_uring-20240922' of git://git.kernel.dk/linux"
    },
    {
        "commit": "eac2ca2d682f94f46b1973bdf5e77d85d77b8e53",
        "message": "In terms of normal application usage, this list will always be empty.\nAnd if an application does overflow a bit, it'll have a few entries.\nHowever, nothing obviously prevents syzbot from running a test case\nthat generates a ton of overflow entries, and then flushing them can\ntake quite a while.\n\nCheck for needing to reschedule while flushing, and drop our locks and\ndo so if necessary. There's no state to maintain here as overflows\nalways prune from head-of-list, hence it's fine to drop and reacquire\nthe locks at the end of the loop.\n\nLink: https://lore.kernel.org/io-uring/66ed061d.050a0220.29194.0053.GAE@google.com/\nReported-by: syzbot+5fca234bd7eb378ff78e@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-20 02:51:20 -0600 io_uring: check if we need to reschedule during overflow flush"
    },
    {
        "commit": "eed138d67d99312f07ed3bc326903b6808885571",
        "message": "Right now any link trace is listed as being linked after the head\nrequest in the chain, but it's more useful to note explicitly which\nrequest a given new request is chained to. Change the link trace to dump\nthe tail request so that chains are immediately apparent when looking at\ntraces.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-20 00:17:46 -0600 io_uring: improve request linking trace"
    },
    {
        "commit": "04beb6e0e08c30c6f845f50afb7d7953603d7a6f",
        "message": "If some part of the kernel adds task_work that needs executing, in terms\nof signaling it'll generally use TWA_SIGNAL or TWA_RESUME. Those two\ndirectly translate to TIF_NOTIFY_SIGNAL or TIF_NOTIFY_RESUME, and can\nbe used for a variety of use case outside of task_work.\n\nHowever, io_cqring_wait_schedule() only tests explicitly for\nTIF_NOTIFY_SIGNAL. This means it can miss if task_work got added for\nthe task, but used a different kind of signaling mechanism (or none at\nall). Normally this doesn't matter as any task_work will be run once\nthe task exits to userspace, except if:\n\n1) The ring is setup with DEFER_TASKRUN\n2) The local work item may generate normal task_work\n\nFor condition 2, this can happen when closing a file and it's the final\nput of that file, for example. This can cause stalls where a task is\nwaiting to make progress inside io_cqring_wait(), but there's nothing else\nthat will wake it up. Hence change the \"should we schedule or loop around\"\ncheck to check for the presence of task_work explicitly, rather than just\nTIF_NOTIFY_SIGNAL as the mechanism. While in there, also change the\nordering of what type of task_work first in terms of ordering, to both\nmake it consistent with other task_work runs in io_uring, but also to\nbetter handle the case of defer task_work generating normal task_work,\nlike in the above example.\n\nReported-by: Jan Hendrik Farr <kernel@jfarr.cc>\nLink: https://github.com/axboe/liburing/issues/1235\nCc: stable@vger.kernel.org\nFixes: 846072f16eed (\"io_uring: mimimise io_cqring_wait_schedule\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-19 11:56:55 -0600 io_uring: check for presence of task_work rather than TIF_NOTIFY_SIGNAL"
    },
    {
        "commit": "bdf56c7580d267a123cc71ca0f2459c797b76fde",
        "message": "Pull slab updates from Vlastimil Babka:\n \"This time it's mostly refactoring and improving APIs for slab users in\n  the kernel, along with some debugging improvements.\n\n   - kmem_cache_create() refactoring (Christian Brauner)\n\n     Over the years have been growing new parameters to\n     kmem_cache_create() where most of them are needed only for a small\n     number of caches - most recently the rcu_freeptr_offset parameter.\n\n     To avoid adding new parameters to kmem_cache_create() and adjusting\n     all its callers, or creating new wrappers such as\n     kmem_cache_create_rcu(), we can now pass extra parameters using the\n     new struct kmem_cache_args. Not explicitly initialized fields\n     default to values interpreted as unused.\n\n     kmem_cache_create() is for now a wrapper that works both with the\n     new form: kmem_cache_create(name, object_size, args, flags) and the\n     legacy form: kmem_cache_create(name, object_size, align, flags,\n     ctor)\n\n   - kmem_cache_destroy() waits for kfree_rcu()'s in flight (Vlastimil\n     Babka, Uladislau Rezki)\n\n     Since SLOB removal, kfree() is allowed for freeing objects\n     allocated by kmem_cache_create(). By extension kfree_rcu() as\n     allowed as well, which can allow converting simple call_rcu()\n     callbacks that only do kmem_cache_free(), as there was never a\n     kmem_cache_free_rcu() variant. However, for caches that can be\n     destroyed e.g. on module removal, the cache owners knew to issue\n     rcu_barrier() first to wait for the pending call_rcu()'s, and this\n     is not sufficient for pending kfree_rcu()'s due to its internal\n     batching optimizations. Ulad has provided a new\n     kvfree_rcu_barrier() and to make the usage less error-prone,\n     kmem_cache_destroy() calls it. Additionally, destroying\n     SLAB_TYPESAFE_BY_RCU caches now again issues rcu_barrier()\n     synchronously instead of using an async work, because the past\n     motivation for async work no longer applies. Users of custom\n     call_rcu() callbacks should however keep calling rcu_barrier()\n     before cache destruction.\n\n   - Debugging use-after-free in SLAB_TYPESAFE_BY_RCU caches (Jann Horn)\n\n     Currently, KASAN cannot catch UAFs in such caches as it is legal to\n     access them within a grace period, and we only track the grace\n     period when trying to free the underlying slab page. The new\n     CONFIG_SLUB_RCU_DEBUG option changes the freeing of individual\n     object to be RCU-delayed, after which KASAN can poison them.\n\n   - Delayed memcg charging (Shakeel Butt)\n\n     In some cases, the memcg is uknown at allocation time, such as\n     receiving network packets in softirq context. With\n     kmem_cache_charge() these may be now charged later when the user\n     and its memcg is known.\n\n   - Misc fixes and improvements (Pedro Falcato, Axel Rasmussen,\n     Christoph Lameter, Yan Zhen, Peng Fan, Xavier)\"\n\n* tag 'slab-for-6.12' of git://git.kernel.org/pub/scm/linux/kernel/git/vbabka/slab: (34 commits)\n  mm, slab: restore kerneldoc for kmem_cache_create()\n  io_uring: port to struct kmem_cache_args\n  slab: make __kmem_cache_create() static inline\n  slab: make kmem_cache_create_usercopy() static inline\n  slab: remove kmem_cache_create_rcu()\n  file: port to struct kmem_cache_args\n  slab: create kmem_cache_create() compatibility layer\n  slab: port KMEM_CACHE_USERCOPY() to struct kmem_cache_args\n  slab: port KMEM_CACHE() to struct kmem_cache_args\n  slab: remove rcu_freeptr_offset from struct kmem_cache\n  slab: pass struct kmem_cache_args to do_kmem_cache_create()\n  slab: pull kmem_cache_open() into do_kmem_cache_create()\n  slab: pass struct kmem_cache_args to create_cache()\n  slab: port kmem_cache_create_usercopy() to struct kmem_cache_args\n  slab: port kmem_cache_create_rcu() to struct kmem_cache_args\n  slab: port kmem_cache_create() to struct kmem_cache_args\n  slab: add struct kmem_cache_args\n  slab: s/__kmem_cache_create/do_kmem_cache_create/g\n  memcg: add charging of already allocated slab objects\n  mm/slab: Optimize the code logic in find_mergeable()\n  ...",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-18 08:53:53 +0200 Merge tag 'slab-for-6.12' of git://git.kernel.org/pub/scm/linux/kernel/git/vbabka/slab"
    },
    {
        "commit": "53d69bdd5b19bb17602cb224e01aeed730ff3289",
        "message": "there are many small reasons justifying this change.\n\n1. busy poll must be performed even on rings that have no iopoll and no\n   new sqe. It is quite possible that a ring configured for inbound\n   traffic with multishot be several hours without receiving new request\n   submissions\n2. NAPI busy poll does not perform any credential validation\n3. If the thread is awaken by task work, processing the task work is\n   prioritary over NAPI busy loop. This is why a second loop has been\n   created after the io_sq_tw() call instead of doing the busy loop in\n   __io_sq_thread() outside its credential acquisition block.\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nLink: https://lore.kernel.org/r/de7679adf1249446bd47426db01d82b9603b7224.1726161831.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-16 20:24:37 -0600 io_uring/sqpoll: do the napi busy poll outside the submission block"
    },
    {
        "commit": "2f6a55e4235f596b7dd9e8a7cf3e07f39ac5e9c2",
        "message": "Originally \"fd\" was unsigned int but it was changed to int when we pulled\nthis code into a separate function in commit 0b6d253e084a\n(\"io_uring/register: provide helper to get io_ring_ctx from 'fd'\").  This\ndoesn't really cause a runtime problem because the call to\narray_index_nospec() will clamp negative fds to 0 and nothing else uses\nthe negative values.\n\nSigned-off-by: Dan Carpenter <dan.carpenter@linaro.org>\nLink: https://lore.kernel.org/r/6f6cb630-079f-4fdf-bf95-1082e0a3fc6e@stanley.mountain\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-16 12:04:10 -0600 io_uring: clean up a type in io_uring_register_get_file()"
    },
    {
        "commit": "7f44beadcc11adb98220556d2ddbe9c97aa6d42d",
        "message": "Putting the cpumask on the stack is deprecated for a long time (since\n2d3854a37e8), as these can be big. Given that, change the on-stack\nallocation of allowed_mask to be dynamically allocated.\n\nFixes: f011c9cf04c0 (\"io_uring/sqpoll: do not allow pinning outside of cpuset\")\nSigned-off-by: Felix Moessbauer <felix.moessbauer@siemens.com>\nLink: https://lore.kernel.org/r/20240916111150.1266191-1-felix.moessbauer@siemens.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-16 07:49:08 -0600 io_uring/sqpoll: do not put cpumask on stack"
    },
    {
        "commit": "adfc3ded5c33d67e822525f95404ef0becb099b8",
        "message": "Pull io_uring async discard support from Jens Axboe:\n \"Sitting on top of both the 6.12 block and io_uring core branches,\n  here's support for async discard through io_uring.\n\n  This allows applications to issue async discards, rather than rely on\n  the blocking sync ioctl discards we already have. The sync support is\n  difficult to use outside of idle/cleanup periods.\n\n  On a real (but slow) device, testing shows the following results when\n  compared to sync discard:\n\n\tqd64 sync discard: 21K IOPS, lat avg 3 msec (max 21 msec)\n\tqd64 async discard: 76K IOPS, lat avg 845 usec (max 2.2 msec)\n\n\tqd64 sync discard: 14K IOPS, lat avg 5 msec (max 25 msec)\n\tqd64 async discard: 56K IOPS, lat avg 1153 usec (max 3.6 msec)\n\n  and synthetic null_blk testing with the same queue depth and block\n  size settings as above shows:\n\n\tType    Trim size       IOPS    Lat avg (usec)  Lat Max (usec)\n\t==============================================================\n\tsync    4k               144K       444            20314\n\tasync   4k              1353K        47              595\n\tsync    1M                56K      1136            21031\n\tasync   1M                94K       680              760\"\n\n* tag 'for-6.12/io_uring-discard-20240913' of git://git.kernel.dk/linux:\n  block: implement async io_uring discard cmd\n  block: introduce blk_validate_byte_range()\n  filemap: introduce filemap_invalidate_pages\n  io_uring/cmd: give inline space in request to cmds\n  io_uring/cmd: expose iowq to cmds",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-16 13:50:14 +0200 Merge tag 'for-6.12/io_uring-discard-20240913' of git://git.kernel.dk/linux"
    },
    {
        "commit": "3a4d319a8fb5a9bbdf5b31ef32841eb286b1dcc2",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - NAPI fixes and cleanups (Pavel, Olivier)\n\n - Add support for absolute timeouts (Pavel)\n\n - Fixes for io-wq/sqpoll affinities (Felix)\n\n - Efficiency improvements for dealing with huge pages (Chenliang)\n\n - Support for a minwait mode, where the application essentially has two\n   timouts - one smaller one that defines the batch timeout, and the\n   overall large one similar to what we had before. This enables\n   efficient use of batching based on count + timeout, while still\n   working well with periods of less intensive workloads\n\n - Use ITER_UBUF for single segment sends\n\n - Add support for incremental buffer consumption. Right now each\n   operation will always consume a full buffer. With incremental\n   consumption, a recv/read operation only consumes the part of the\n   buffer that it needs to satisfy the operation\n\n - Add support for GCOV for io_uring, to help retain a high coverage of\n   test to code ratio\n\n - Fix regression with ocfs2, where an odd -EOPNOTSUPP wasn't correctly\n   converted to a blocking retry\n\n - Add support for cloning registered buffers from one ring to another\n\n - Misc cleanups (Anuj, me)\n\n* tag 'for-6.12/io_uring-20240913' of git://git.kernel.dk/linux: (35 commits)\n  io_uring: add IORING_REGISTER_COPY_BUFFERS method\n  io_uring/register: provide helper to get io_ring_ctx from 'fd'\n  io_uring/rsrc: add reference count to struct io_mapped_ubuf\n  io_uring/rsrc: clear 'slot' entry upfront\n  io_uring/io-wq: inherit cpuset of cgroup in io worker\n  io_uring/io-wq: do not allow pinning outside of cpuset\n  io_uring/rw: drop -EOPNOTSUPP check in __io_complete_rw_common()\n  io_uring/rw: treat -EOPNOTSUPP for IOCB_NOWAIT like -EAGAIN\n  io_uring/sqpoll: do not allow pinning outside of cpuset\n  io_uring/eventfd: move refs to refcount_t\n  io_uring: remove unused rsrc_put_fn\n  io_uring: add new line after variable declaration\n  io_uring: add GCOV_PROFILE_URING Kconfig option\n  io_uring/kbuf: add support for incremental buffer consumption\n  io_uring/kbuf: pass in 'len' argument for buffer commit\n  Revert \"io_uring: Require zeroed sqe->len on provided-buffers send\"\n  io_uring/kbuf: move io_ring_head_to_buf() to kbuf.h\n  io_uring/kbuf: add io_kbuf_commit() helper\n  io_uring/kbuf: shrink nr_iovs/mode in struct buf_sel_arg\n  io_uring: wire up min batch wake timeout\n  ...",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-16 13:29:00 +0200 Merge tag 'for-6.12/io_uring-20240913' of git://git.kernel.dk/linux"
    },
    {
        "commit": "7a40974fd0efa3698de4c6d1d0ee0436bcc4445d",
        "message": "Pull btrfs updates from David Sterba:\n \"This brings mostly refactoring, cleanups, minor performance\n  optimizations and usual fixes. The folio API conversions are most\n  noticeable.\n\n  There's one less visible change that could have a high impact. The\n  extent lock scope for read is reduced, not held for the entire\n  operation. In the buffered read case it's left to page or inode lock,\n  some direct io read synchronization is still needed.\n\n  This used to prevent deadlocks induced by page faults during direct\n  io, so there was a 4K limitation on the requests, e.g. for io_uring.\n  In the future this will allow smoother integration with iomap where\n  the extent read lock was a major obstacle.\n\n  User visible changes:\n\n   - the FSTRIM ioctl updates the processed range even after an error or\n     interruption\n\n   - cleaner thread is woken up in SYNC ioctl instead of waking the\n     transaction thread that can take some delay before waking up the\n     cleaner, this can speed up cleaning of deleted subvolumes\n\n   - print an error message when opening a device fail, e.g. when it's\n     unexpectedly read-only\n\n  Core changes:\n\n   - improved extent map handling in various ways (locking, iteration, ...)\n\n   - new assertions and locking annotations\n\n   - raid-stripe-tree locking fixes\n\n   - use xarray for tracking dirty qgroup extents, switched from rb-tree\n\n   - turn the subpage test to compile-time condition if possible (e.g.\n     on x86_64 with 4K pages), this allows to skip a lot of ifs and\n     remove dead code\n\n   - more preparatory work for compression in subpage mode\n\n  Cleanups and refactoring\n\n   - folio API conversions, many simple cases where page is passed so\n     switch it to folios\n\n   - more subpage code refactoring, update page state bitmap processing\n\n   - introduce auto free for btrfs_path structure, use for the simple\n     cases\"\n\n* tag 'for-6.12-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux: (110 commits)\n  btrfs: only unlock the to-be-submitted ranges inside a folio\n  btrfs: merge btrfs_folio_unlock_writer() into btrfs_folio_end_writer_lock()\n  btrfs: BTRFS_PATH_AUTO_FREE in orphan.c\n  btrfs: use btrfs_path auto free in zoned.c\n  btrfs: DEFINE_FREE for struct btrfs_path\n  btrfs: remove btrfs_folio_end_all_writers()\n  btrfs: constify more pointer parameters\n  btrfs: rework BTRFS_I as macro to preserve parameter const\n  btrfs: add and use helper to verify the calling task has locked the inode\n  btrfs: always update fstrim_range on failure in FITRIM ioctl\n  btrfs: convert copy_inline_to_page() to use folio\n  btrfs: convert btrfs_decompress() to take a folio\n  btrfs: convert zstd_decompress() to take a folio\n  btrfs: convert lzo_decompress() to take a folio\n  btrfs: convert zlib_decompress() to take a folio\n  btrfs: convert try_release_extent_mapping() to take a folio\n  btrfs: convert try_release_extent_state() to take a folio\n  btrfs: convert submit_eb_page() to take a folio\n  btrfs: convert submit_eb_subpage() to take a folio\n  btrfs: convert read_key_bytes() to take a folio\n  ...",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-16 13:10:46 +0200 Merge tag 'for-6.12-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux"
    },
    {
        "commit": "a09c17240bdf2e9fa6d0591afa9448b59785f7d4",
        "message": "A recent commit ensured that SQPOLL cannot be setup with a CPU that\nisn't in the current tasks cpuset, but it also dropped testing whether\nthe CPU is valid in the first place. Without that, if a task passes in\na CPU value that is too high, the following KASAN splat can get\ntriggered:\n\nBUG: KASAN: stack-out-of-bounds in io_sq_offload_create+0x858/0xaa4\nRead of size 8 at addr ffff800089bc7b90 by task wq-aff.t/1391\n\nCPU: 4 UID: 1000 PID: 1391 Comm: wq-aff.t Not tainted 6.11.0-rc7-00227-g371c468f4db6 #7080\nHardware name: linux,dummy-virt (DT)\nCall trace:\n dump_backtrace.part.0+0xcc/0xe0\n show_stack+0x14/0x1c\n dump_stack_lvl+0x58/0x74\n print_report+0x16c/0x4c8\n kasan_report+0x9c/0xe4\n __asan_report_load8_noabort+0x1c/0x24\n io_sq_offload_create+0x858/0xaa4\n io_uring_setup+0x1394/0x17c4\n __arm64_sys_io_uring_setup+0x6c/0x180\n invoke_syscall+0x6c/0x260\n el0_svc_common.constprop.0+0x158/0x224\n do_el0_svc+0x3c/0x5c\n el0_svc+0x34/0x70\n el0t_64_sync_handler+0x118/0x124\n el0t_64_sync+0x168/0x16c\n\nThe buggy address belongs to stack of task wq-aff.t/1391\n and is located at offset 48 in frame:\n io_sq_offload_create+0x0/0xaa4\n\nThis frame has 1 object:\n [32, 40) 'allowed_mask'\n\nThe buggy address belongs to the virtual mapping at\n [ffff800089bc0000, ffff800089bc9000) created by:\n kernel_clone+0x124/0x7e0\n\nThe buggy address belongs to the physical page:\npage: refcount:1 mapcount:0 mapping:0000000000000000 index:0xffff0000d740af80 pfn:0x11740a\nmemcg:ffff0000c2706f02\nflags: 0xbffe00000000000(node=0|zone=2|lastcpupid=0x1fff)\nraw: 0bffe00000000000 0000000000000000 dead000000000122 0000000000000000\nraw: ffff0000d740af80 0000000000000000 00000001ffffffff ffff0000c2706f02\npage dumped because: kasan: bad access detected\n\nMemory state around the buggy address:\n ffff800089bc7a80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n ffff800089bc7b00: 00 00 00 00 00 00 00 00 00 00 00 00 f1 f1 f1 f1\n>ffff800089bc7b80: 00 f3 f3 f3 00 00 00 00 00 00 00 00 00 00 00 00\n                         ^\n ffff800089bc7c00: 00 00 00 00 00 00 00 00 00 00 00 00 f1 f1 f1 f1\n ffff800089bc7c80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 f3\n\nReported-by: kernel test robot <oliver.sang@intel.com>\nCloses: https://lore.kernel.org/oe-lkp/202409161632.cbeeca0d-lkp@intel.com\nFixes: f011c9cf04c0 (\"io_uring/sqpoll: do not allow pinning outside of cpuset\")\nTested-by: Felix Moessbauer <felix.moessbauer@siemens.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-16 03:12:21 -0600 io_uring/sqpoll: retain test for whether the CPU is valid"
    },
    {
        "commit": "9753c642a53bc4fbdef06d372d389dce7d8cddc2",
        "message": "If we change it to tracking ubuf->start + ubuf->len, then we can reduce\nthe size of struct io_mapped_ubuf by another 4 bytes, effectively 8\nbytes, as a hole is eliminated too.\n\nThis shrinks io_mapped_ubuf to 32 bytes.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-15 09:15:22 -0600 io_uring/rsrc: change ubuf->ubuf_end to length tracking"
    },
    {
        "commit": "8b0c6025a02ddec2b497f83e7d2f27a07f1d0653",
        "message": "We don't really need to cache this, let's reclaim 8 bytes from struct\nio_mapped_ubuf and just calculate it when we need it. The only hot path\nhere is io_import_fixed().\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-15 09:15:19 -0600 io_uring/rsrc: get rid of io_mapped_ubuf->folio_mask"
    },
    {
        "commit": "636119af94f2fbf3e4458be66a1bc740ba69ce6d",
        "message": "A recent commit added support for copying registered buffers from one\nring to another. But that term is a bit confusing, as no copying of\nbuffer data is done here. What is being done is simply cloning the\nbuffer registrations from one ring to another.\n\nRename it while we still can, so that it's more descriptive. No\nfunctional changes in this patch.\n\nFixes: 7cc2a6eadcd7 (\"io_uring: add IORING_REGISTER_COPY_BUFFERS method\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-14 08:51:15 -0600 io_uring: rename \"copy buffers\" to \"clone buffers\""
    },
    {
        "commit": "7cc2a6eadcd7a5aa36ac63e6659f5c6138c7f4d2",
        "message": "Buffers can get registered with io_uring, which allows to skip the\nrepeated pin_pages, unpin/unref pages for each O_DIRECT operation. This\nreduces the overhead of O_DIRECT IO.\n\nHowever, registrering buffers can take some time. Normally this isn't an\nissue as it's done at initialization time (and hence less critical), but\nfor cases where rings can be created and destroyed as part of an IO\nthread pool, registering the same buffers for multiple rings become a\nmore time sensitive proposition. As an example, let's say an application\nhas an IO memory pool of 500G. Initial registration takes:\n\nGot 500 huge pages (each 1024MB)\nRegistered 500 pages in 409 msec\n\nor about 0.4 seconds. If we go higher to 900 1GB huge pages being\nregistered:\n\nRegistered 900 pages in 738 msec\n\nwhich is, as expected, a fully linear scaling.\n\nRather than have each ring pin/map/register the same buffer pool,\nprovide an io_uring_register(2) opcode to simply duplicate the buffers\nthat are registered with another ring. Adding the same 900GB of\nregistered buffers to the target ring can then be accomplished in:\n\nCopied 900 pages in 17 usec\n\nWhile timing differs a bit, this provides around a 25,000-40,000x\nspeedup for this use case.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-12 10:14:15 -0600 io_uring: add IORING_REGISTER_COPY_BUFFERS method"
    },
    {
        "commit": "0b6d253e084a97a05f4970dee06d9a75d29a7bda",
        "message": "Can be done in one of two ways:\n\n1) Regular file descriptor, just fget()\n2) Registered ring, index our own table for that\n\nIn preparation for adding another register use of needing to get a ctx\nfrom a file descriptor, abstract out this helper and use it in the main\nregister syscall as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-12 10:14:05 -0600 io_uring/register: provide helper to get io_ring_ctx from 'fd'"
    },
    {
        "commit": "bfc0aa7a512f9a4462a88ca7352b00b83f8d68fd",
        "message": "Currently there's a single ring owner of a mapped buffer, and hence the\nreference count will always be 1 when it's torn down and freed. However,\nin preparation for being able to link io_mapped_ubuf to different spots,\nadd a reference count to manage the lifetime of it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-11 13:54:32 -0600 io_uring/rsrc: add reference count to struct io_mapped_ubuf"
    },
    {
        "commit": "021b153f7d4115d99efa0d57ae2da6de1228295d",
        "message": "No functional changes in this patch, but clearing the slot pointer\nearlier will be required by a later change.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-11 13:52:17 -0600 io_uring/rsrc: clear 'slot' entry upfront"
    },
    {
        "commit": "50c52250e2d74b098465841163c18f4b4e9ad430",
        "message": "io_uring allows implementing custom file specific asynchronous\noperations via the fops->uring_cmd callback, a.k.a. IORING_OP_URING_CMD\nrequests or just io_uring commands. Use it to add support for async\ndiscards.\n\nNormally, it first tries to queue up bios in a non-blocking context,\nand if that fails, we'd retry from a blocking context by returning\n-EAGAIN to the core io_uring. We always get the result from bios\nasynchronously by setting a custom bi_end_io callback, at which point\nwe drag the request into the task context to either reissue or complete\nit and post a completion to the user.\n\nUnlike ioctl(BLKDISCARD) with stronger guarantees against races, we only\ndo a best effort attempt to invalidate page cache, and it can race with\nany writes and reads and leave page cache stale. It's the same kind of\nraces we allow to direct writes.\n\nAlso, apart from cases where discarding is not allowed at all, e.g.\ndiscards are not supported or the file/device is read only, the user\nshould assume that the sector range on disk is not valid anymore, even\nwhen an error was returned to the user.\n\nSuggested-by: Conrad Meyer <conradmeyer@meta.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2b5210443e4fa0257934f73dfafcc18a77cd0e09.1726072086.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-11 10:45:28 -0600 block: implement async io_uring discard cmd"
    },
    {
        "commit": "a6ccb48e13662bcb98282e051512b9686b02d353",
        "message": "Some io_uring commands can use some inline space in io_kiocb. We have 32\nbytes in struct io_uring_cmd, expose it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7ca779a61ee5e166e535d70df9c7f07b15d8a0ce.1726072086.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-11 10:44:10 -0600 io_uring/cmd: give inline space in request to cmds"
    },
    {
        "commit": "6746ee4c3a189f8b60694f01e7e29bc5ff7972e0",
        "message": "When an io_uring request needs blocking context we offload it to the\nio_uring's thread pool called io-wq. We can get there off ->uring_cmd\nby returning -EAGAIN, but there is no straightforward way of doing that\nfrom an asynchronous callback. Add a helper that would transfer a\ncommand to a blocking context.\n\nNote, we do an extra hop via task_work before io_queue_iowq(), that's a\nlimitation of io_uring infra we have that can likely be lifted later\nif that would ever become a problem.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f735f807d7c8ba50c9452c69dfe5d3e9e535037b.1726072086.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-11 10:44:10 -0600 io_uring/cmd: expose iowq to cmds"
    },
    {
        "commit": "6d0f8dcb3a634bbee46fcb028c5984c463f47812",
        "message": "* for-6.12/io_uring: (31 commits)\n  io_uring/io-wq: inherit cpuset of cgroup in io worker\n  io_uring/io-wq: do not allow pinning outside of cpuset\n  io_uring/rw: drop -EOPNOTSUPP check in __io_complete_rw_common()\n  io_uring/rw: treat -EOPNOTSUPP for IOCB_NOWAIT like -EAGAIN\n  io_uring/sqpoll: do not allow pinning outside of cpuset\n  io_uring/eventfd: move refs to refcount_t\n  io_uring: remove unused rsrc_put_fn\n  io_uring: add new line after variable declaration\n  io_uring: add GCOV_PROFILE_URING Kconfig option\n  io_uring/kbuf: add support for incremental buffer consumption\n  io_uring/kbuf: pass in 'len' argument for buffer commit\n  Revert \"io_uring: Require zeroed sqe->len on provided-buffers send\"\n  io_uring/kbuf: move io_ring_head_to_buf() to kbuf.h\n  io_uring/kbuf: add io_kbuf_commit() helper\n  io_uring/kbuf: shrink nr_iovs/mode in struct buf_sel_arg\n  io_uring: wire up min batch wake timeout\n  io_uring: add support for batch wait timeout\n  io_uring: implement our own schedule timeout handling\n  io_uring: move schedule wait logic into helper\n  io_uring: encapsulate extraneous wait flags into a separate struct\n  ...",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-11 10:42:40 -0600 Merge branch 'for-6.12/io_uring' into for-6.12/io_uring-discard"
    },
    {
        "commit": "318ad4283a6efea8ce5ec2b3c65b6cb19df6b07e",
        "message": "* for-6.12/block: (115 commits)\n  block: unpin user pages belonging to a folio at once\n  mm: release number of pages of a folio\n  block: introduce folio awareness and add a bigger size from folio\n  block: Added folio-ized version of bio_add_hw_page()\n  block, bfq: factor out a helper to split bfqq in bfq_init_rq()\n  block, bfq: remove local variable 'bfqq_already_existing' in bfq_init_rq()\n  block, bfq: remove local variable 'split' in bfq_init_rq()\n  block, bfq: remove bfq_log_bfqg()\n  block, bfq: merge bfq_release_process_ref() into bfq_put_cooperator()\n  block, bfq: fix procress reference leakage for bfqq in merge chain\n  block, bfq: fix uaf for accessing waker_bfqq after splitting\n  blk-throttle: support prioritized processing of metadata\n  blk-throttle: remove last_low_overflow_time\n  drbd: Add NULL check for net_conf to prevent dereference in state validation\n  blk-mq: add missing unplug trace event\n  mtip32xx: Remove redundant null pointer checks in mtip_hw_debugfs_init()\n  md: Add new_level sysfs interface\n  zram: Shrink zram_table_entry::flags.\n  zram: Remove ZRAM_LOCK\n  zram: Replace bit spinlocks with a spinlock_t.\n  ...",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-11 10:42:37 -0600 Merge branch 'for-6.12/block' into for-6.12/io_uring-discard"
    },
    {
        "commit": "84eacf177faa605853c58e5b1c0d9544b88c16fd",
        "message": "The io worker threads are userland threads that just never exit to the\nuserland. By that, they are also assigned to a cgroup (the group of the\ncreating task).\n\nWhen creating a new io worker, this worker should inherit the cpuset\nof the cgroup.\n\nFixes: da64d6db3bd3 (\"io_uring: One wqe per wq\")\nSigned-off-by: Felix Moessbauer <felix.moessbauer@siemens.com>\nLink: https://lore.kernel.org/r/20240910171157.166423-3-felix.moessbauer@siemens.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-11 07:27:56 -0600 io_uring/io-wq: inherit cpuset of cgroup in io worker"
    },
    {
        "commit": "0997aa5497c714edbb349ca366d28bd550ba3408",
        "message": "The io worker threads are userland threads that just never exit to the\nuserland. By that, they are also assigned to a cgroup (the group of the\ncreating task).\n\nWhen changing the affinity of the io_wq thread via syscall, we must only\nallow cpumasks within the limits defined by the cpuset controller of the\ncgroup (if enabled).\n\nFixes: da64d6db3bd3 (\"io_uring: One wqe per wq\")\nSigned-off-by: Felix Moessbauer <felix.moessbauer@siemens.com>\nLink: https://lore.kernel.org/r/20240910171157.166423-2-felix.moessbauer@siemens.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-11 07:27:56 -0600 io_uring/io-wq: do not allow pinning outside of cpuset"
    },
    {
        "commit": "90bfb28d5fa8127a113a140c9791ea0b40ab156a",
        "message": "A recent change ensured that the necessary -EOPNOTSUPP -> -EAGAIN\ntransformation happens inline on both the reader and writer side,\nand hence there's no need to check for both of these anymore on\nthe completion handler side.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-10 09:34:44 -0600 io_uring/rw: drop -EOPNOTSUPP check in __io_complete_rw_common()"
    },
    {
        "commit": "c0a9d496e0fece67db777bd48550376cf2960c47",
        "message": "Some file systems, ocfs2 in this case, will return -EOPNOTSUPP for\nan IOCB_NOWAIT read/write attempt. While this can be argued to be\ncorrect, the usual return value for something that requires blocking\nissue is -EAGAIN.\n\nA refactoring io_uring commit dropped calling kiocb_done() for\nnegative return values, which is otherwise where we already do that\ntransformation. To ensure we catch it in both spots, check it in\n__io_read() itself as well.\n\nReported-by: Robert Sander <r.sander@heinlein-support.de>\nLink: https://fosstodon.org/@gurubert@mastodon.gurubert.de/113112431889638440\nCc: stable@vger.kernel.org\nFixes: a08d195b586a (\"io_uring/rw: split io_read() into a helper\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-10 09:34:41 -0600 io_uring/rw: treat -EOPNOTSUPP for IOCB_NOWAIT like -EAGAIN"
    },
    {
        "commit": "ac325fc2aad513072722387a71bf857c938aae4e",
        "message": "Historically we've held the extent lock throughout the entire read.\nThere's been a few reasons for this, but it's mostly just caused us\nproblems.  For example, this prevents us from allowing page faults\nduring direct io reads, because we could deadlock.  This has forced us\nto only allow 4k reads at a time for io_uring NOWAIT requests because we\nhave no idea if we'll be forced to page fault and thus have to do a\nwhole lot of work.\n\nOn the buffered side we are protected by the page lock, as long as we're\nreading things like buffered writes, punch hole, and even direct IO to a\ncertain degree will get hung up on the page lock while the page is in\nflight.\n\nOn the direct side we have the dio extent lock, which acts much like the\nway the extent lock worked previously to this patch, however just for\ndirect reads.  This protects direct reads from concurrent direct writes,\nwhile we're protected from buffered writes via the inode lock.\n\nNow that we're protected in all cases, narrow the extent lock to the\npart where we're getting the extent map to submit the reads, no longer\nholding the extent lock for the entire read operation.  Push the extent\nlock down into do_readpage() so that we're only grabbing it when looking\nup the extent map.  This portion was contributed by Goldwyn.\n\nCo-developed-by: Goldwyn Rodrigues <rgoldwyn@suse.com>\nReviewed-by: Goldwyn Rodrigues <rgoldwyn@suse.com>\nSigned-off-by: Josef Bacik <josef@toxicpanda.com>\nSigned-off-by: David Sterba <dsterba@suse.com>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-10 16:51:20 +0200 btrfs: do not hold the extent lock for entire read"
    },
    {
        "commit": "a6711d1cd4e291f334ae900065e18f585732acfa",
        "message": "Port req_cachep to struct kmem_cache_args.\n\nReviewed-by: Kees Cook <kees@kernel.org>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nReviewed-by: Mike Rapoport (Microsoft) <rppt@kernel.org>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nSigned-off-by: Christian Brauner <brauner@kernel.org>\nSigned-off-by: Vlastimil Babka <vbabka@suse.cz>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-10 11:42:59 +0200 io_uring: port to struct kmem_cache_args"
    },
    {
        "commit": "f011c9cf04c06f16b24f583d313d3c012e589e50",
        "message": "The submit queue polling threads are userland threads that just never\nexit to the userland. When creating the thread with IORING_SETUP_SQ_AFF,\nthe affinity of the poller thread is set to the cpu specified in\nsq_thread_cpu. However, this CPU can be outside of the cpuset defined\nby the cgroup cpuset controller. This violates the rules defined by the\ncpuset controller and is a potential issue for realtime applications.\n\nIn b7ed6d8ffd6 we fixed the default affinity of the poller thread, in\ncase no explicit pinning is required by inheriting the one of the\ncreating task. In case of explicit pinning, the check is more\ncomplicated, as also a cpu outside of the parent cpumask is allowed.\nWe implemented this by using cpuset_cpus_allowed (that has support for\ncgroup cpusets) and testing if the requested cpu is in the set.\n\nFixes: 37d1e2e3642e (\"io_uring: move SQPOLL thread io-wq forked worker\")\nCc: stable@vger.kernel.org # 6.1+\nSigned-off-by: Felix Moessbauer <felix.moessbauer@siemens.com>\nLink: https://lore.kernel.org/r/20240909150036.55921-1-felix.moessbauer@siemens.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-09 09:09:08 -0600 io_uring/sqpoll: do not allow pinning outside of cpuset"
    },
    {
        "commit": "0e0bcf07ec5b305ce7612385b06580dcbe5bc6a5",
        "message": "atomic_t for the struct io_ev_fd references and there are no issues with\nit. While the ref getting and putting for the eventfd code is somewhat\nperformance critical for cases where eventfd signaling is used (news\nflash, you should not...), it probably doesn't warrant using an atomic_t\nfor this. Let's just move to it to refcount_t to get the added\nprotection of over/underflows.\n\nLink: https://lore.kernel.org/lkml/202409082039.hnsaIJ3X-lkp@intel.com/\nReported-by: kernel test robot <lkp@intel.com>\nCloses: https://lore.kernel.org/oe-kbuild-all/202409082039.hnsaIJ3X-lkp@intel.com/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-08 16:43:57 -0600 io_uring/eventfd: move refs to refcount_t"
    },
    {
        "commit": "54001d0f2fdbc7852136a00f3e6fc395a9547ae5",
        "message": "When asynchronous encryption is used KTLS sends out the final data at\nproto->close time. This becomes problematic when the task calling\nclose() receives a signal. In this case it can happen that\ntcp_sendmsg_locked() called at close time returns -ERESTARTSYS and the\nfinal data is not sent.\n\nThe described situation happens when KTLS is used in conjunction with\nio_uring, as io_uring uses task_work_add() to add work to the current\nuserspace task. A discussion of the problem along with a reproducer can\nbe found in [1] and [2]\n\nFix this by waiting for the asynchronous encryption to be completed on\nthe final message. With this there is no data left to be sent at close\ntime.\n\n[1] https://lore.kernel.org/all/20231010141932.GD3114228@pengutronix.de/\n[2] https://lore.kernel.org/all/20240315100159.3898944-1-s.hauer@pengutronix.de/\n\nSigned-off-by: Sascha Hauer <s.hauer@pengutronix.de>\nLink: https://patch.msgid.link/20240904-ktls-wait-async-v1-1-a62892833110@pengutronix.de\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-06 18:20:55 -0700 net: tls: wait for async completion on last message"
    },
    {
        "commit": "e58f5142f88320a5b1449f96a146f2f24615c5c7",
        "message": "When two UBLK_CMD_START_USER_RECOVERY commands are submitted, the\nfirst one sets 'ubq->ubq_daemon' to NULL, and the second one triggers\nWARN in ublk_queue_reinit() and subsequently a NULL pointer dereference\nissue.\n\nFix it by adding the check in ublk_ctrl_start_recovery() and return\nimmediately in case of zero 'ub->nr_queues_ready'.\n\n  BUG: kernel NULL pointer dereference, address: 0000000000000028\n  RIP: 0010:ublk_ctrl_start_recovery.constprop.0+0x82/0x180\n  Call Trace:\n   <TASK>\n   ? __die+0x20/0x70\n   ? page_fault_oops+0x75/0x170\n   ? exc_page_fault+0x64/0x140\n   ? asm_exc_page_fault+0x22/0x30\n   ? ublk_ctrl_start_recovery.constprop.0+0x82/0x180\n   ublk_ctrl_uring_cmd+0x4f7/0x6c0\n   ? pick_next_task_idle+0x26/0x40\n   io_uring_cmd+0x9a/0x1b0\n   io_issue_sqe+0x193/0x3f0\n   io_wq_submit_work+0x9b/0x390\n   io_worker_handle_work+0x165/0x360\n   io_wq_worker+0xcb/0x2f0\n   ? finish_task_switch.isra.0+0x203/0x290\n   ? finish_task_switch.isra.0+0x203/0x290\n   ? __pfx_io_wq_worker+0x10/0x10\n   ret_from_fork+0x2d/0x50\n   ? __pfx_io_wq_worker+0x10/0x10\n   ret_from_fork_asm+0x1a/0x30\n   </TASK>\n\nFixes: c732a852b419 (\"ublk_drv: add START_USER_RECOVERY and END_USER_RECOVERY support\")\nReported-and-tested-by: Changhui Zhong <czhong@redhat.com>\nCloses: https://lore.kernel.org/all/CAGVVp+UvLiS+bhNXV-h2icwX1dyybbYHeQUuH7RYqUvMQf6N3w@mail.gmail.com\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nSigned-off-by: Li Nan <linan122@huawei.com>\nLink: https://lore.kernel.org/r/20240904031348.4139545-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc7",
        "release_date": "2024-09-04 07:15:38 -0600 ublk_drv: fix NULL pointer dereference in ublk_ctrl_start_recovery()"
    },
    {
        "commit": "c9f9ce65c2436879779d39c6e65b95c74a206e49",
        "message": "rsrc_put_fn is declared but never used, remove it.\n\nSigned-off-by: Anuj Gupta <anuj20.g@samsung.com>\nLink: https://lore.kernel.org/r/20240902062134.136387-3-anuj20.g@samsung.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-02 09:39:57 -0600 io_uring: remove unused rsrc_put_fn"
    },
    {
        "commit": "6cf52b42c4efa4d064d19064fd2313ca4aaf9569",
        "message": "Fixes checkpatch warning\n\nSigned-off-by: Anuj Gupta <anuj20.g@samsung.com>\nLink: https://lore.kernel.org/r/20240902062134.136387-2-anuj20.g@samsung.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-09-02 09:39:57 -0600 io_uring: add new line after variable declaration"
    },
    {
        "commit": "ad246d9f04aa037f8e8bbf8573c9af527114cead",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - A fix for a regression that happened in 6.11 merge window, where the\n   copying of iovecs for compat mode applications got broken for certain\n   cases.\n\n - Fix for a bug introduced in 6.10, where if using recv/send bundles\n   with classic provided buffers, the recv/send would fail to set the\n   right iovec count. This caused 0 byte send/recv results. Found via\n   code coverage testing and writing a test case to exercise it.\n\n* tag 'io_uring-6.11-20240830' of git://git.kernel.dk/linux:\n  io_uring/kbuf: return correct iovec count from classic buffer peek\n  io_uring/rsrc: ensure compat iovecs are copied correctly",
        "kernel_version": "v6.11-rc6",
        "release_date": "2024-08-31 13:51:27 +1200 Merge tag 'io_uring-6.11-20240830' of git://git.kernel.dk/linux"
    },
    {
        "commit": "1802656ef8906cc949f58b64cb6d8d400326e163",
        "message": "If GCOV is enabled and this option is set, it enables code coverage\nprofiling of the io_uring subsystem. Only use this for test purposes,\nas it will impact the runtime performance.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-30 10:52:02 -0600 io_uring: add GCOV_PROFILE_URING Kconfig option"
    },
    {
        "commit": "f274495aea7b15225b3d83837121b22ef96e560c",
        "message": "io_provided_buffers_select() returns 0 to indicate success, but it should\nbe returning 1 to indicate that 1 vec was mapped. This causes peeking\nto fail with classic provided buffers, and while that's not a use case\nthat anyone should use, it should still work correctly.\n\nThe end result is that no buffer will be selected, and hence a completion\nwith '0' as the result will be posted, without a buffer attached.\n\nFixes: 35c8711c8fc4 (\"io_uring/kbuf: add helpers for getting/peeking multiple buffers\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc6",
        "release_date": "2024-08-30 10:45:54 -0600 io_uring/kbuf: return correct iovec count from classic buffer peek"
    },
    {
        "commit": "1c47c0d6014c832ad8e2ba04fc2c5b7070d999f7",
        "message": "For buffer registration (or updates), a userspace iovec is copied in\nand updated. If the application is within a compat syscall, then the\niovec type is compat_iovec rather than iovec. However, the type used\nin __io_sqe_buffers_update() and io_sqe_buffers_register() is always\nstruct iovec, and hence the source is incremented by the size of a\nnon-compat iovec in the loop. This misses every other iovec in the\nsource, and will run into garbage half way through the copies and\nreturn -EFAULT to the application.\n\nMaintain the source address separately and assign to our user vec\npointer, so that copies always happen from the right source address.\n\nWhile in there, correct a bad placement of __user which triggered\nthe following sparse warning prior to this fix:\n\nio_uring/rsrc.c:981:33: warning: cast removes address space '__user' of expression\nio_uring/rsrc.c:981:30: warning: incorrect type in assignment (different address spaces)\nio_uring/rsrc.c:981:30:    expected struct iovec const [noderef] __user *uvec\nio_uring/rsrc.c:981:30:    got struct iovec *[noderef] __user\n\nFixes: f4eaf8eda89e (\"io_uring/rsrc: Drop io_copy_iov in favor of iovec API\")\nReviewed-by: Gabriel Krisman Bertazi <krisman@suse.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc6",
        "release_date": "2024-08-30 07:52:43 -0600 io_uring/rsrc: ensure compat iovecs are copied correctly"
    },
    {
        "commit": "ae98dbf43d755b4e111fcd086e53939bef3e9a1a",
        "message": "By default, any recv/read operation that uses provided buffers will\nconsume at least 1 buffer fully (and maybe more, in case of bundles).\nThis adds support for incremental consumption, meaning that an\napplication may add large buffers, and each read/recv will just consume\nthe part of the buffer that it needs.\n\nFor example, let's say an application registers 1MB buffers in a\nprovided buffer ring, for streaming receives. If it gets a short recv,\nthen the full 1MB buffer will be consumed and passed back to the\napplication. With incremental consumption, only the part that was\nactually used is consumed, and the buffer remains the current one.\n\nThis means that both the application and the kernel needs to keep track\nof what the current receive point is. Each recv will still pass back a\nbuffer ID and the size consumed, the only difference is that before the\nnext receive would always be the next buffer in the ring. Now the same\nbuffer ID may return multiple receives, each at an offset into that\nbuffer from where the previous receive left off. Example:\n\nApplication registers a provided buffer ring, and adds two 32K buffers\nto the ring.\n\nBuffer1 address: 0x1000000 (buffer ID 0)\nBuffer2 address: 0x2000000 (buffer ID 1)\n\nA recv completion is received with the following values:\n\ncqe->res\t0x1000\t(4k bytes received)\ncqe->flags\t0x11\t(CQE_F_BUFFER|CQE_F_BUF_MORE set, buffer ID 0)\n\nand the application now knows that 4096b of data is available at\n0x1000000, the start of that buffer, and that more data from this buffer\nwill be coming. Now the next receive comes in:\n\ncqe->res\t0x2010\t(8k bytes received)\ncqe->flags\t0x11\t(CQE_F_BUFFER|CQE_F_BUF_MORE set, buffer ID 0)\n\nwhich tells the application that 8k is available where the last\ncompletion left off, at 0x1001000. Next completion is:\n\ncqe->res\t0x5000\t(20k bytes received)\ncqe->flags\t0x1\t(CQE_F_BUFFER set, buffer ID 0)\n\nand the application now knows that 20k of data is available at\n0x1003000, which is where the previous receive ended. CQE_F_BUF_MORE\nisn't set, as no more data is available in this buffer ID. The next\ncompletion is then:\n\ncqe->res\t0x1000\t(4k bytes received)\ncqe->flags\t0x10001\t(CQE_F_BUFFER|CQE_F_BUF_MORE set, buffer ID 1)\n\nwhich tells the application that buffer ID 1 is now the current one,\nhence there's 4k of valid data at 0x2000000. 0x2001000 will be the next\nreceive point for this buffer ID.\n\nWhen a buffer will be reused by future CQE completions,\nIORING_CQE_BUF_MORE will be set in cqe->flags. This tells the application\nthat the kernel isn't done with the buffer yet, and that it should expect\nmore completions for this buffer ID. Will only be set by provided buffer\nrings setup with IOU_PBUF_RING INC, as that's the only type of buffer\nthat will see multiple consecutive completions for the same buffer ID.\nFor any other provided buffer type, any completion that passes back\na buffer to the application is final.\n\nOnce a buffer has been fully consumed, the buffer ring head is\nincremented and the next receive will indicate the next buffer ID in the\nCQE cflags.\n\nOn the send side, the application can manage how much data is sent from\nan existing buffer by setting sqe->len to the desired send length.\n\nAn application can request incremental consumption by setting\nIOU_PBUF_RING_INC in the provided buffer ring registration. Outside of\nthat, any provided buffer ring setup and buffer additions is done like\nbefore, no changes there. The only change is in how an application may\nsee multiple completions for the same buffer ID, hence needing to know\nwhere the next receive will happen.\n\nNote that like existing provided buffer rings, this should not be used\nwith IOSQE_ASYNC, as both really require the ring to remain locked over\nthe duration of the buffer selection and the operation completion. It\nwill consume a buffer otherwise regardless of the size of the IO done.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-29 08:44:58 -0600 io_uring/kbuf: add support for incremental buffer consumption"
    },
    {
        "commit": "6733e678ba1226ad0df94f0bb095df121c54d701",
        "message": "In preparation for needing the consumed length, pass in the length being\ncompleted. Unused right now, but will be used when it is possible to\npartially consume a buffer.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-29 08:44:51 -0600 io_uring/kbuf: pass in 'len' argument for buffer commit"
    },
    {
        "commit": "641a6816795b208aa7ccac751acaae580897db10",
        "message": "This reverts commit 79996b45f7b28c0e3e08a95bab80119e95317e28.\n\nRevert the change that restricts a send provided buffer to be zero, so\nit will always consume the whole buffer. This is strictly needed for\npartial consumption, as the send may very well be a subset of the\ncurrent buffer. In fact, that's the intended use case.\n\nFor non-incremental provided buffer rings, an application should set\nsqe->len carefully to avoid the potential issue described in the\nreverted commit. It is recommended that '0' still be set for len for\nthat case, if the application is set on maintaining more than 1 send\ninflight for the same socket. This is somewhat of a nonsensical thing\nto do.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-29 08:44:46 -0600 Revert \"io_uring: Require zeroed sqe->len on provided-buffers send\""
    },
    {
        "commit": "2c8fa70bf3e981193ecda0eedf2100f933ef7085",
        "message": "In preparation for using this helper in kbuf.h as well, move it there and\nturn it into a macro.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-29 08:44:42 -0600 io_uring/kbuf: move io_ring_head_to_buf() to kbuf.h"
    },
    {
        "commit": "ecd5c9b29643f383d39320e30d21b8615bd893da",
        "message": "Committing the selected ring buffer is currently done in three different\nspots, combine it into a helper and just call that.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-29 08:44:38 -0600 io_uring/kbuf: add io_kbuf_commit() helper"
    },
    {
        "commit": "c0390d541128e8820af8177a572d9d87ff68a3bb",
        "message": "Now that we shrunk struct file to 192 bytes aka 3 cachelines reorder\nstruct file to not leave any holes or have members cross cachelines.\n\nAdd a short comment to each of the fields and mark the cachelines.\nIt's possible that we may have to tweak this based on profiling in the\nfuture. So far I had Jens test this comparing io_uring with non-fixed\nand fixed files and it improved performance. The layout is a combination\nof Jens' and my changes.\n\nLink: https: //lore.kernel.org/r/20240824-peinigen-hocken-7384b977c643@brauner\nSigned-off-by: Christian Brauner <brauner@kernel.org>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-29 15:20:25 +0200 fs: pack struct file"
    },
    {
        "commit": "120443321dfaaab8eb9290af617abcc37734c1e2",
        "message": "nr_iovs is capped at 1024, and mode only has a few low values. We can\nsafely make them u16, in preparation for adding a few more members.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:01 -0600 io_uring/kbuf: shrink nr_iovs/mode in struct buf_sel_arg"
    },
    {
        "commit": "7ed9e09e2d13d5d43385153bba4734cb0eafd7fd",
        "message": "Expose min_wait_usec in io_uring_getevents_arg, replacing the pad member\nthat is currently in there. The value is in usecs, which is explained in\nthe name as well.\n\nNote that if min_wait_usec and a normal timeout is used in conjunction,\nthe normal timeout is still relative to the base time. For example, if\nmin_wait_usec is set to 100 and the normal timeout is 1000, the max\ntotal time waited is still 1000. This also means that if the normal\ntimeout is shorter than min_wait_usec, then only the min_wait_usec will\ntake effect.\n\nSee previous commit for an explanation of how this works.\n\nIORING_FEAT_MIN_TIMEOUT is added as a feature flag for this, as\napplications doing submit_and_wait_timeout() style operations will\ngenerally not see the -EINVAL from the wait side as they return the\nnumber of IOs submitted. Only if no IOs are submitted will the -EINVAL\nbubble back up to the application.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:01 -0600 io_uring: wire up min batch wake timeout"
    },
    {
        "commit": "1100c4a2656d444785024cd9b585298729eff136",
        "message": "Waiting for events with io_uring has two knobs that can be set:\n\n1) The number of events to wake for\n2) The timeout associated with the event\n\nWaiting will abort when either of those conditions are met, as expected.\n\nThis adds support for a third event, which is associated with the number\nof events to wait for. Applications generally like to handle batches of\ncompletions, and right now they'd set a number of events to wait for and\nthe timeout for that. If no events have been received but the timeout\ntriggers, control is returned to the application and it can wait again.\nHowever, if the application doesn't have anything to do until events are\nreaped, then it's possible to make this waiting more efficient.\n\nFor example, the application may have a latency time of 50 usecs and\nwanting to handle a batch of 8 requests at the time. If it uses 50 usecs\nas the timeout, then it'll be doing 20K context switches per second even\nif nothing is happening.\n\nThis introduces the notion of min batch wait time. If the min batch wait\ntime expires, then we'll return to userspace if we have any events at all.\nIf none are available, the general wait time is applied. Any request\narriving after the min batch wait time will cause waiting to stop and\nreturn control to the application.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:01 -0600 io_uring: add support for batch wait timeout"
    },
    {
        "commit": "cebf123c634ab78d39af94caf0fc9cd2c60d82c3",
        "message": "In preparation for having two distinct timeouts and avoid waking the\ntask if we don't need to.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:01 -0600 io_uring: implement our own schedule timeout handling"
    },
    {
        "commit": "45a41e74b8f472254c64b42713bad0686350b0c6",
        "message": "In preparation for expanding how we handle waits, move the actual\nschedule and schedule_timeout() handling into a helper.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:01 -0600 io_uring: move schedule wait logic into helper"
    },
    {
        "commit": "f42b58e44802b0280a452a33fbeb37fee5b6318f",
        "message": "Rather than need to pass in 2 or 3 separate arguments, add a struct\nto encapsulate the timeout and sigset_t parts of waiting. In preparation\nfor adding another argument for waiting.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:01 -0600 io_uring: encapsulate extraneous wait flags into a separate struct"
    },
    {
        "commit": "2b8e976b984278edbeab3251d370e76d237699f9",
        "message": "Add a new registration opcode IORING_REGISTER_CLOCK, which allows the\nuser to select which clock id it wants to use with CQ waiting timeouts.\nIt only allows a subset of all posix clocks and currently supports\nCLOCK_MONOTONIC and CLOCK_BOOTTIME.\n\nSuggested-by: Lewis Baker <lewissbaker@gmail.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/98f2bc8a3c36cdf8f0e6a275245e81e903459703.1723039801.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:01 -0600 io_uring: user registered clockid for wait timeouts"
    },
    {
        "commit": "d29cb3726f03cdac7889f0109a7cb84f79e168a8",
        "message": "In addition to current relative timeouts for the waiting loop, where the\ntimespec argument specifies the maximum time it can wait for, add\nsupport for the absolute mode, with the value carrying a CLOCK_MONOTONIC\nabsolute time until which we should return control back to the user.\n\nSuggested-by: Lewis Baker <lewissbaker@gmail.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4d5b74d67ada882590b2e42aa3aa7117bbf6b55f.1723039801.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:01 -0600 io_uring: add absolute mode wait timeouts"
    },
    {
        "commit": "d5cce407e4f59b2e08d03e29d2b3c55deacc1d48",
        "message": "Remove io_napi_adjust_timeout() and move the adjustments out of the\ncommon path into __io_napi_busy_loop(). Now the limit it's calculated\nbased on struct io_wait_queue::timeout, for which we query current time\nanother time. The overhead shouldn't be a problem, it's a polling path,\nhowever that can be optimised later by additionally saving the delta\ntime value in io_cqring_wait().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/88e14686e245b3b42ff90a3c4d70895d48676206.1723039801.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:01 -0600 io_uring/napi: postpone napi timeout adjustment"
    },
    {
        "commit": "489b80060cf645e958c4755c4b5032f234409f85",
        "message": "we don't need to set ->napi_prefer_busy_poll if we're not going to poll,\ndo the checks first and all polling preparation after.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2ad7ede8cc7905328fc62e8c3805fdb11635ae0b.1723039801.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:01 -0600 io_uring/napi: refactor __io_napi_busy_loop()"
    },
    {
        "commit": "a69307a55454060b5795e68d249157f2961049c2",
        "message": "We could just move these two and save some space, but in preparation\nfor adding another flag, turn them into flags first.\n\nThis saves 8 bytes in struct io_buffer_list, making it exactly half\na cacheline on 64-bit archs now rather than 40 bytes.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:01 -0600 io_uring/kbuf: turn io_buffer_list booleans into flags"
    },
    {
        "commit": "566a424212d79b90e3a8fe6b5c7bd8f69174105c",
        "message": "Just like what is being done on the recv side, if we only map a single\nsegment, then use ITER_UBUF for mapping it. That's more efficient than\nusing an ITER_IOVEC.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:01 -0600 io_uring/net: use ITER_UBUF for single segment send maps"
    },
    {
        "commit": "03e02e8f95fee0f45124976993ed2121e2369a12",
        "message": "req->buf_list is assigned higher up and is safe to use as we remain\nwithin a locked region, as is the 'bl' variable itself from which it\nwas assigned. To improve readability, use 'bl' directly rather than\nget it from the io_kiocb, if we need to increment the head directly\nin the buffer selection path. This makes it readily apparent that\nit's the same io_buffer_list being used.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:01 -0600 io_uring/kbuf: use 'bl' directly rather than req->buf_list"
    },
    {
        "commit": "7255cd894539a96fefab9180185d268647c7341b",
        "message": "reverse the order of the element evaluation in an if statement.\n\nfor many users that are not using iopoll, the iopoll_list will always\nevaluate to false after having made a memory access whereas to_submit is\nvery likely already loaded in a register.\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/052ca60b5c49e7439e4b8bd33bfab4a09d36d3d6.1722374371.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:01 -0600 io_uring: micro optimization of __io_sq_thread() condition"
    },
    {
        "commit": "a8edbb424b1391b077407c75d8f5d2ede77aa70d",
        "message": "Add support for checking and coalescing multi-hugepage-backed fixed\nbuffers. The coalescing optimizes both time and space consumption caused\nby mapping and storing multi-hugepage fixed buffers.\n\nA coalescable multi-hugepage buffer should fully cover its folios\n(except potentially the first and last one), and these folios should\nhave the same size. These requirements are for easier processing later,\nalso we need same size'd chunks in io_import_fixed for fast iov_iter\nadjust.\n\nSigned-off-by: Chenliang Li <cliang01.li@samsung.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20240731090133.4106-3-cliang01.li@samsung.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:01 -0600 io_uring/rsrc: enable multi-hugepage buffer coalescing"
    },
    {
        "commit": "3d6106aee4732a01a275c59ec94d05302d931c4b",
        "message": "Store the folio shift and folio mask into imu struct and use it in\niov_iter adjust, as we will have non PAGE_SIZE'd chunks if a\nmulti-hugepage buffer get coalesced.\n\nSigned-off-by: Chenliang Li <cliang01.li@samsung.com>\nReviewed-by: Anuj Gupta <anuj20.g@samsung.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20240731090133.4106-2-cliang01.li@samsung.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:01 -0600 io_uring/rsrc: store folio shift and mask into imu"
    },
    {
        "commit": "d843634a95a66237f88fa6affa9203c9ee503244",
        "message": "This info may be useful when attempting to debug a problem involving a\nring using the NAPI feature.\n\nHere is an example of the output:\nip-172-31-39-89 /proc/772/fdinfo # cat 14\npos:\t0\nflags:\t02000002\nmnt_id:\t16\nino:\t10243\nSqMask:\t0xff\nSqHead:\t633\nSqTail:\t633\nCachedSqHead:\t633\nCqMask:\t0x3fff\nCqHead:\t430250\nCqTail:\t430250\nCachedCqTail:\t430250\nSQEs:\t0\nCQEs:\t0\nSqThread:\t885\nSqThreadCpu:\t0\nSqTotalTime:\t52793826\nSqWorkTime:\t3590465\nUserFiles:\t0\nUserBufs:\t0\nPollList:\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=6, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=6, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\n  op=10, task_works=0\nCqOverflowList:\nNAPI:\tenabled\nnapi_busy_poll_to:\t1\nnapi_prefer_busy_poll:\ttrue\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bb184f8b62703ddd3e6e19eae7ab6c67b97e1e10.1722293317.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.12-rc1",
        "release_date": "2024-08-25 08:27:00 -0600 io_uring: add napi busy settings to the fdinfo output"
    },
    {
        "commit": "489270f44c3fc2fb8d0e5d102ea08a90e93ca135",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single fix for provided buffer validation\"\n\n* tag 'io_uring-6.11-20240823' of git://git.kernel.dk/linux:\n  io_uring/kbuf: sanitize peek buffer setup",
        "kernel_version": "v6.11-rc5",
        "release_date": "2024-08-24 07:45:08 +0800 Merge tag 'io_uring-6.11-20240823' of git://git.kernel.dk/linux"
    },
    {
        "commit": "e0ee967630c8ee67bb47a5b38d235cd5a8789c48",
        "message": "Harden the buffer peeking a bit, by adding a sanity check for it having\na valid size. Outside of that, arg->max_len is a size_t, though it's\nonly ever set to a 32-bit value (as it's governed by MAX_RW_COUNT).\nBump our needed check to a size_t so we know it fits. Finally, cap the\ncalculated needed iov value to the PEEK_MAX_IMPORT, which is the\nmaximum number of segments that should be peeked.\n\nFixes: 35c8711c8fc4 (\"io_uring/kbuf: add helpers for getting/peeking multiple buffers\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc5",
        "release_date": "2024-08-21 07:16:38 -0600 io_uring/kbuf: sanitize peek buffer setup"
    },
    {
        "commit": "c5ac744cdddae82916d4cd35d962d3f47065e68a",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix a comment in the uapi header using the wrong member name (Caleb)\n\n - Fix KCSAN warning for a debug check in sqpoll (me)\n\n - Two more NAPI tweaks (Olivier)\n\n* tag 'io_uring-6.11-20240824' of git://git.kernel.dk/linux:\n  io_uring: fix user_data field name in comment\n  io_uring/sqpoll: annotate debug task == current with data_race()\n  io_uring/napi: remove duplicate io_napi_entry timeout assignation\n  io_uring/napi: check napi_enabled in io_napi_add() before proceeding",
        "kernel_version": "v6.11-rc4",
        "release_date": "2024-08-16 14:00:05 -0700 Merge tag 'io_uring-6.11-20240824' of git://git.kernel.dk/linux"
    },
    {
        "commit": "1fc2ac428ef7d2ab9e8e19efe7ec3e58aea51bf3",
        "message": "io_uring_cqe's user_data field refers to `sqe->data`, but io_uring_sqe\ndoes not have a data field. Fix the comment to say `sqe->user_data`.\n\nSigned-off-by: Caleb Sander Mateos <csander@purestorage.com>\nLink: https://github.com/axboe/liburing/pull/1206\nLink: https://lore.kernel.org/r/20240816181526.3642732-1-csander@purestorage.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc4",
        "release_date": "2024-08-16 12:31:26 -0600 io_uring: fix user_data field name in comment"
    },
    {
        "commit": "e4956dc7a84da074fd8dc10f7abd147f15b3ae58",
        "message": "There's a debug check in io_sq_thread_park() checking if it's the SQPOLL\nthread itself calling park. KCSAN warns about this, as we should not be\nreading sqd->thread outside of sqd->lock.\n\nJust silence this with data_race(). The pointer isn't used for anything\nbut this debug check.\n\nReported-by: syzbot+2b946a3fd80caf971b21@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc4",
        "release_date": "2024-08-13 06:10:59 -0600 io_uring/sqpoll: annotate debug task == current with data_race()"
    },
    {
        "commit": "48cc7ecd3a68e0fbfa281ef1ed6f6b6cb7638390",
        "message": "io_napi_entry() has 2 calling sites. One of them is unlikely to find an\nentry and if it does, the timeout should arguable not be updated.\n\nThe other io_napi_entry() calling site is overwriting the update made\nby io_napi_entry() so the io_napi_entry() timeout value update has no or\nlittle value and therefore is removed.\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nLink: https://lore.kernel.org/r/145b54ff179f87609e20dffaf5563c07cdbcad1a.1723423275.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc4",
        "release_date": "2024-08-12 12:11:42 -0600 io_uring/napi: remove duplicate io_napi_entry timeout assignation"
    },
    {
        "commit": "84f2eecf95018386c145ada19bb45b03bdb80d9e",
        "message": "doing so avoids the overhead of adding napi ids to all the rings that do\nnot enable napi.\n\nif no id is added to napi_list because napi is disabled,\n__io_napi_busy_loop() will not be called.\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nFixes: b4ccc4dd1330 (\"io_uring/napi: enable even with a timeout of 0\")\nLink: https://lore.kernel.org/r/bd989ccef5fda14f5fd9888faf4fefcf66bd0369.1723400131.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc4",
        "release_date": "2024-08-12 12:09:03 -0600 io_uring/napi: check napi_enabled in io_napi_add() before proceeding"
    },
    {
        "commit": "8828729c4435b85844a3b6da19cc7c148c59ec43",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Nothing major in here, just two fixes for ensuring that bundle\n  recv/send requests always get marked for cleanups, and a single fix to\n  ensure that sends with provided buffers only pick a single buffer\n  unless the bundle option has been enabled\"\n\n* tag 'io_uring-6.11-20240809' of git://git.kernel.dk/linux:\n  io_uring/net: don't pick multiple buffers for non-bundle send\n  io_uring/net: ensure expanded bundle send gets marked for cleanup\n  io_uring/net: ensure expanded bundle recv gets marked for cleanup",
        "kernel_version": "v6.11-rc3",
        "release_date": "2024-08-09 09:32:10 -0700 Merge tag 'io_uring-6.11-20240809' of git://git.kernel.dk/linux"
    },
    {
        "commit": "8fe8ac24adcd76b12edbfdefa078567bfff117d4",
        "message": "If a send is issued marked with IOSQE_BUFFER_SELECT for selecting a\nbuffer, unless it's a bundle, it should not select multiple buffers.\n\nCc: stable@vger.kernel.org\nFixes: a05d1f625c7a (\"io_uring/net: support bundles for send\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc3",
        "release_date": "2024-08-07 15:20:52 -0600 io_uring/net: don't pick multiple buffers for non-bundle send"
    },
    {
        "commit": "70ed519ed59da3a92c3acedeb84a30e5a66051ce",
        "message": "If the iovec inside the kmsg isn't already allocated AND one gets\nexpanded beyond the fixed size, then the request may not already have\nbeen marked for cleanup. Ensure that it is.\n\nCc: stable@vger.kernel.org\nFixes: a05d1f625c7a (\"io_uring/net: support bundles for send\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc3",
        "release_date": "2024-08-07 15:08:17 -0600 io_uring/net: ensure expanded bundle send gets marked for cleanup"
    },
    {
        "commit": "11893e144ed75be55d99349760513ca104781fc0",
        "message": "If the iovec inside the kmsg isn't already allocated AND one gets\nexpanded beyond the fixed size, then the request may not already have\nbeen marked for cleanup. Ensure that it is.\n\nCc: stable@vger.kernel.org\nFixes: 2f9c9515bdfd (\"io_uring/net: support bundles for recv\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc3",
        "release_date": "2024-08-07 15:06:45 -0600 io_uring/net: ensure expanded bundle recv gets marked for cleanup"
    },
    {
        "commit": "ed86525f1f4b738bae75c73e89f25430bd0af1b0",
        "message": "To pick up changes from:\n\n  d25a92ccae6b net/smc: Introduce IPPROTO_SMC\n  060f4ba6e403 io_uring/net: move charging socket out of zc io_uring\n  bb6aaf736680 net: Split a __sys_listen helper for io_uring\n  dc2e77979412 net: Split a __sys_bind helper for io_uring\n\nThis should be used to beautify socket syscall arguments and it addresses\nthese tools/perf build warnings:\n\n  Warning: Kernel ABI header differences:\n  diff -u tools/include/uapi/linux/in.h include/uapi/linux/in.h\n  diff -u tools/perf/trace/beauty/include/linux/socket.h include/linux/socket.h\n\nPlease see tools/include/uapi/README for details (it's in the first patch\nof this series).\n\nCc: \"David S. Miller\" <davem@davemloft.net>\nCc: Eric Dumazet <edumazet@google.com>\nCc: Jakub Kicinski <kuba@kernel.org>\nCc: Paolo Abeni <pabeni@redhat.com>\nCc: netdev@vger.kernel.org\nSigned-off-by: Namhyung Kim <namhyung@kernel.org>",
        "kernel_version": "v6.11-rc4",
        "release_date": "2024-08-07 10:59:07 -0700 tools/include: Sync network socket headers with the kernel sources"
    },
    {
        "commit": "17712b7ea0756799635ba159cc773082230ed028",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two minor tweaks for the NAPI handling, both from Olivier:\n\n   - Kill two unused list definitions\n\n   - Ensure that multishot NAPI doesn't age away\"\n\n* tag 'io_uring-6.11-20240802' of git://git.kernel.dk/linux:\n  io_uring: remove unused local list heads in NAPI functions\n  io_uring: keep multishot request NAPI timeout current",
        "kernel_version": "v6.11-rc2",
        "release_date": "2024-08-02 14:18:31 -0700 Merge tag 'io_uring-6.11-20240802' of git://git.kernel.dk/linux"
    },
    {
        "commit": "c3fca4fb83f7c84cd1e1aa9fe3a0e220ce8f30fb",
        "message": "These lists are unused, remove them.\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0a0ae3e955aed0f3e3d29882fb3d3cb575e0009b.1722294947.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc2",
        "release_date": "2024-07-30 06:20:20 -0600 io_uring: remove unused local list heads in NAPI functions"
    },
    {
        "commit": "2c762be5b798c443612c1bb9b011de4fdaebd1c5",
        "message": "This refresh statement was originally present in the original patch:\nhttps://lore.kernel.org/netdev/20221121191437.996297-2-shr@devkernel.io/\n\nIt has been removed with no explanation in v6:\nhttps://lore.kernel.org/netdev/20230201222254.744422-2-shr@devkernel.io/\n\nIt is important to make the refresh for multishot requests, because if no\nnew requests using the same NAPI device are added to the ring, the entry\nwill become stale and be removed silently. The unsuspecting user will\nnot know that their ring had busy polling for only 60 seconds before\nbeing pruned.\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nFixes: 8d0c12a80cdeb (\"io-uring: add napi busy poll support\")\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/r/0fe61a019ec61e5708cd117cb42ed0dab95e1617.1722294646.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc2",
        "release_date": "2024-07-30 06:18:58 -0600 io_uring: keep multishot request NAPI timeout current"
    },
    {
        "commit": "8c9307474333d8d100870b45af00bfeb1872c836",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix a syzbot issue for the msg ring cache added in this release. No\n   ill effects from this one, but it did make KMSAN unhappy (me)\n\n - Sanitize the NAPI timeout handling, by unifying the value handling\n   into all ktime_t rather than converting back and forth (Pavel)\n\n - Fail NAPI registration for IOPOLL rings, it's not supported (Pavel)\n\n - Fix a theoretical issue with ring polling and cancelations (Pavel)\n\n - Various little cleanups and fixes (Pavel)\n\n* tag 'io_uring-6.11-20240726' of git://git.kernel.dk/linux:\n  io_uring/napi: pass ktime to io_napi_adjust_timeout\n  io_uring/napi: use ktime in busy polling\n  io_uring/msg_ring: fix uninitialized use of target_req->flags\n  io_uring: align iowq and task request error handling\n  io_uring: kill REQ_F_CANCEL_SEQ\n  io_uring: simplify io_uring_cmd return\n  io_uring: fix io_match_task must_hold\n  io_uring: don't allow netpolling with SETUP_IOPOLL\n  io_uring: tighten task exit cancellations",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-27 15:22:33 -0700 Merge tag 'io_uring-6.11-20240726' of git://git.kernel.dk/linux"
    },
    {
        "commit": "358169617602f6f71b31e5c9532a09b95a34b043",
        "message": "Pass the waiting time for __io_napi_adjust_timeout as ktime and get rid\nof all timespec64 conversions. It's especially simpler since the caller\nalready have a ktime.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4f5b8e8eed4f53a1879e031a6712b25381adc23d.1722003776.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-26 08:31:59 -0600 io_uring/napi: pass ktime to io_napi_adjust_timeout"
    },
    {
        "commit": "342b2e395d5f34c9f111a818556e617939f83a8c",
        "message": "It's more natural to use ktime/ns instead of keeping around usec,\nespecially since we're comparing it against user provided timers,\nso convert napi busy poll internal handling to ktime. It's also nicer\nsince the type (ktime_t vs unsigned long) now tells the unit of measure.\n\nKeep everything as ktime, which we convert to/from micro seconds for\nIORING_[UN]REGISTER_NAPI. The net/ busy polling works seems to work with\nusec, however it's not real usec as shift by 10 is used to get it from\nnsecs, see busy_loop_current_time(), so it's easy to get truncated nsec\nback and we get back better precision.\n\nNote, we can further improve it later by removing the truncation and\nmaybe convincing net/ to use ktime/ns instead.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/95e7ec8d095069a3ed5d40a4bc6f8b586698bc7e.1722003776.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-26 08:31:59 -0600 io_uring/napi: use ktime in busy polling"
    },
    {
        "commit": "0db4618e8fabfcc404af4dda23799bba726785a5",
        "message": "syzbot reports that KMSAN complains that 'nr_tw' is an uninit-value\nwith the following report:\n\nBUG: KMSAN: uninit-value in io_req_local_work_add io_uring/io_uring.c:1192 [inline]\nBUG: KMSAN: uninit-value in io_req_task_work_add_remote+0x588/0x5d0 io_uring/io_uring.c:1240\n io_req_local_work_add io_uring/io_uring.c:1192 [inline]\n io_req_task_work_add_remote+0x588/0x5d0 io_uring/io_uring.c:1240\n io_msg_remote_post io_uring/msg_ring.c:102 [inline]\n io_msg_data_remote io_uring/msg_ring.c:133 [inline]\n io_msg_ring_data io_uring/msg_ring.c:152 [inline]\n io_msg_ring+0x1c38/0x1ef0 io_uring/msg_ring.c:305\n io_issue_sqe+0x383/0x22c0 io_uring/io_uring.c:1710\n io_queue_sqe io_uring/io_uring.c:1924 [inline]\n io_submit_sqe io_uring/io_uring.c:2180 [inline]\n io_submit_sqes+0x1259/0x2f20 io_uring/io_uring.c:2295\n __do_sys_io_uring_enter io_uring/io_uring.c:3205 [inline]\n __se_sys_io_uring_enter+0x40c/0x3ca0 io_uring/io_uring.c:3142\n __x64_sys_io_uring_enter+0x11f/0x1a0 io_uring/io_uring.c:3142\n x64_sys_call+0x2d82/0x3c10 arch/x86/include/generated/asm/syscalls_64.h:427\n do_syscall_x64 arch/x86/entry/common.c:52 [inline]\n do_syscall_64+0xcd/0x1e0 arch/x86/entry/common.c:83\n entry_SYSCALL_64_after_hwframe+0x77/0x7f\n\nwhich is the following check:\n\nif (nr_tw < nr_wait)\n\treturn;\n\nin io_req_local_work_add(). While nr_tw itself cannot be uninitialized,\nit does depend on req->flags, which off the msg ring issue path can\nindeed be uninitialized.\n\nFix this by always clearing the allocated 'req' fully if we can't grab\none from the cache itself.\n\nFixes: 50cf5f3842af (\"io_uring/msg_ring: add an alloc cache for io_kiocb entries\")\nReported-by: syzbot+82609b8937a4458106ca@syzkaller.appspotmail.com\nLink: https://lore.kernel.org/io-uring/000000000000fd3d8d061dfc0e4a@google.com/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-25 08:41:35 -0600 io_uring/msg_ring: fix uninitialized use of target_req->flags"
    },
    {
        "commit": "29d63b94036e561a016ec8878b44aad6650d23e2",
        "message": "There is a difference in how io_queue_sqe and io_wq_submit_work treat\nerror codes they get from io_issue_sqe. The first one fails anything\nunknown but latter only fails when the code is negative.\n\nIt doesn't make sense to have this discrepancy, align them to the\nio_queue_sqe behaviour.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c550e152bf4a290187f91a4322ddcb5d6d1f2c73.1721819383.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-24 08:01:49 -0600 io_uring: align iowq and task request error handling"
    },
    {
        "commit": "a2b72b81fb3ba18717fc000949ca9d45a3351130",
        "message": "We removed the reliance on the flag by the cancellation path and now\nit's unused.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e57afe566bbe4fefeb44daffb08900f2a4756577.1721819383.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-24 08:01:49 -0600 io_uring: kill REQ_F_CANCEL_SEQ"
    },
    {
        "commit": "f1dcdfcadb0c8c13dddd931c1f4dc58e54fdc9c0",
        "message": "We don't have to return error code from an op handler back to core\nio_uring, so once io_uring_cmd() sets the results and handles errors we\ncan juts return IOU_OK and simplify the code.\n\nNote, only valid with e0b23d9953b0c (\"io_uring: optimise ltimeout for\ninline execution\"), there was a problem with iopoll before.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8eae2be5b2a49236cd5f1dadbd1aa5730e9e2d4f.1721819383.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-24 08:01:49 -0600 io_uring: simplify io_uring_cmd return"
    },
    {
        "commit": "e142e9cd8891b0c6f277ac2c2c254199a6aa56e3",
        "message": "The __must_hold annotation in io_match_task() uses a non existing\nparameter \"req\", fix it.\n\nFixes: 6af3f48bf6156 (\"io_uring: fix link traversal locking\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3e65ee7709e96507cef3d93291746f2c489f2307.1721819383.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-24 08:01:49 -0600 io_uring: fix io_match_task must_hold"
    },
    {
        "commit": "bd44d7e902c2b34c217d3b48874b079760ca7b6e",
        "message": "IORING_SETUP_IOPOLL rings don't have any netpoll handling, let's fail\nattempts to register netpolling in this case, there might be people who\nwill mix up IOPOLL and netpoll.\n\nCc: stable@vger.kernel.org\nFixes: ef1186c1a875b (\"io_uring: add register/unregister napi function\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1e7553aee0a8ae4edec6742cd6dd0c1e6914fba8.1721819383.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-24 08:01:49 -0600 io_uring: don't allow netpolling with SETUP_IOPOLL"
    },
    {
        "commit": "f8b632e89a101dae349a7b212c1771d7925f441b",
        "message": "io_uring_cancel_generic() should retry if any state changes like a\nrequest is completed, however in case of a task exit it only goes for\nanother loop and avoids schedule() if any tracked (i.e. REQ_F_INFLIGHT)\nrequest got completed.\n\nLet's assume we have a non-tracked request executing in iowq and a\ntracked request linked to it. Let's also assume\nio_uring_cancel_generic() fails to find and cancel the request, i.e.\nvia io_run_local_work(), which may happen as io-wq has gaps.\nNext, the request logically completes, io-wq still hold a ref but queues\nit for completion via tw, which happens in\nio_uring_try_cancel_requests(). After, right before prepare_to_wait()\nio-wq puts the request, grabs the linked one and tries executes it, e.g.\narms polling. Finally the cancellation loop calls prepare_to_wait(),\nthere are no tw to run, no tracked request was completed, so the\ntctx_inflight() check passes and the task is put to indefinite sleep.\n\nCc: stable@vger.kernel.org\nFixes: 3f48cf18f886c (\"io_uring: unify files and task cancel\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/acac7311f4e02ce3c43293f8f1fda9c705d158f1.1721819383.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-24 08:01:49 -0600 io_uring: tighten task exit cancellations"
    },
    {
        "commit": "9deed1d5f82cf30308027f9f604a95ac7ffdbe19",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two minor fixes in here, both heading to stable. In detail:\n\n   - Fix error where forced async uring_cmd getsockopt returns the wrong\n     value on execution, leading to it never being completed (Pavel)\n\n   - Fix io_alloc_pbuf_ring() using a NULL check rather than IS_ERR\n     (Pavel)\"\n\n* tag 'io_uring-6.11-20240722' of git://git.kernel.dk/linux:\n  io_uring: fix error pbuf checking\n  io_uring: fix lost getsockopt completions",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-22 11:45:48 -0700 Merge tag 'io_uring-6.11-20240722' of git://git.kernel.dk/linux"
    },
    {
        "commit": "bcc87d978b834c298bbdd9c52454c5d0a946e97e",
        "message": "Syz reports a problem, which boils down to NULL vs IS_ERR inconsistent\nerror handling in io_alloc_pbuf_ring().\n\nKASAN: null-ptr-deref in range [0x0000000000000000-0x0000000000000007]\nRIP: 0010:__io_remove_buffers+0xac/0x700 io_uring/kbuf.c:341\nCall Trace:\n <TASK>\n io_put_bl io_uring/kbuf.c:378 [inline]\n io_destroy_buffers+0x14e/0x490 io_uring/kbuf.c:392\n io_ring_ctx_free+0xa00/0x1070 io_uring/io_uring.c:2613\n io_ring_exit_work+0x80f/0x8a0 io_uring/io_uring.c:2844\n process_one_work kernel/workqueue.c:3231 [inline]\n process_scheduled_works+0xa2c/0x1830 kernel/workqueue.c:3312\n worker_thread+0x86d/0xd40 kernel/workqueue.c:3390\n kthread+0x2f0/0x390 kernel/kthread.c:389\n ret_from_fork+0x4b/0x80 arch/x86/kernel/process.c:147\n ret_from_fork_asm+0x1a/0x30 arch/x86/entry/entry_64.S:244\n\nCc: stable@vger.kernel.org\nReported-by: syzbot+2074b1a3d447915c6f1c@syzkaller.appspotmail.com\nFixes: 87585b05757dc (\"io_uring/kbuf: use vm_insert_pages() for mmap'ed pbuf ring\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c5f9df20560bd9830401e8e48abc029e7cfd9f5e.1721329239.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-20 11:04:57 -0600 io_uring: fix error pbuf checking"
    },
    {
        "commit": "24dce1c538a7ceac43f2f97aae8dfd4bb93ea9b9",
        "message": "There is a report that iowq executed getsockopt never completes. The\nreason being that io_uring_cmd_sock() can return a positive result, and\nio_uring_cmd() propagates it back to core io_uring, instead of IOU_OK.\nIn case of io_wq_submit_work(), the request will be dropped without\ncompleting it.\n\nThe offending code was introduced by a hack in\na9c3eda7eada9 (\"io_uring: fix submission-failure handling for uring-cmd\"),\nhowever it was fine until getsockopt was introduced and started\nreturning positive results.\n\nThe right solution is to always return IOU_OK, since\ne0b23d9953b0c (\"io_uring: optimise ltimeout for inline execution\"),\nwe should be able to do it without problems, however for the sake of\nbackporting and minimising side effects, let's keep returning negative\nreturn codes and otherwise do IOU_OK.\n\nLink: https://github.com/axboe/liburing/issues/1181\nCc: stable@vger.kernel.org\nFixes: 8e9fad0e70b7b (\"io_uring: Add io_uring command support for sockets\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Breno Leitao <leitao@debian.org>\nLink: https://lore.kernel.org/r/ff349cf0654018189b6077e85feed935f0f8839e.1721149870.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-20 11:04:56 -0600 io_uring: fix lost getsockopt completions"
    },
    {
        "commit": "3f74e6bd3b84a8b6bb3cc51609c89e5b9d58eed7",
        "message": "set_initial_priority() tries to jump-start global reclaim by estimating\nthe priority based on cold/hot LRU pages.  The estimation does not account\nfor shrinker objects, and it cannot do so because their sizes can be in\ndifferent units other than page.\n\nIf shrinker objects are the majority, e.g., on TrueNAS SCALE 24.04.0 where\nZFS ARC can use almost all system memory, set_initial_priority() can\nvastly underestimate how much memory ARC shrinker can evict and assign\nextreme low values to scan_control->priority, resulting in overshoots of\nshrinker objects.\n\nTo reproduce the problem, using TrueNAS SCALE 24.04.0 with 32GB DRAM, a\ntest ZFS pool and the following commands:\n\n  fio --name=mglru.file --numjobs=36 --ioengine=io_uring \\\n      --directory=/root/test-zfs-pool/ --size=1024m --buffered=1 \\\n      --rw=randread --random_distribution=random \\\n      --time_based --runtime=1h &\n\n  for ((i = 0; i < 20; i++))\n  do\n    sleep 120\n    fio --name=mglru.anon --numjobs=16 --ioengine=mmap \\\n      --filename=/dev/zero --size=1024m --fadvise_hint=0 \\\n      --rw=randrw --random_distribution=random \\\n      --time_based --runtime=1m\n  done\n\nTo fix the problem:\n1. Cap scan_control->priority at or above DEF_PRIORITY/2, to prevent\n   the jump-start from being overly aggressive.\n2. Account for the progress from mm_account_reclaimed_pages(), to\n   prevent kswapd_shrink_node() from raising the priority\n   unnecessarily.\n\nLink: https://lkml.kernel.org/r/20240711191957.939105-2-yuzhao@google.com\nFixes: e4dde56cd208 (\"mm: multi-gen LRU: per-node lru_gen_folio lists\")\nSigned-off-by: Yu Zhao <yuzhao@google.com>\nReported-by: Alexander Motin <mav@ixsystems.com>\nCc: Wei Xu <weixugc@google.com>\nCc: <stable@vger.kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-17 21:05:18 -0700 mm/mglru: fix overshooting shrinker memory"
    },
    {
        "commit": "3a56e241732975c2c1247047ddbfc0ac6f6a4905",
        "message": "Pull io_uring updates from Jens Axboe:\n \"Here are the io_uring updates queued up for 6.11.\n\n  Nothing major this time around, various minor improvements and\n  cleanups/fixes. This contains:\n\n   - Add bind/listen opcodes. Main motivation is to support direct\n     descriptors, to avoid needing a regular fd just for doing these two\n     operations (Gabriel)\n\n   - Probe fixes (Gabriel)\n\n   - Treat io-wq work flags as atomics. Not fixing a real issue, but may\n     as well and it silences a KCSAN warning (me)\n\n   - Cleanup of rsrc __set_current_state() usage (me)\n\n   - Add 64-bit for {m,f}advise operations (me)\n\n   - Improve performance of data ring messages (me)\n\n   - Fix for ring message overflow posting (Pavel)\n\n   - Fix for freezer interaction with TWA_NOTIFY_SIGNAL. Not strictly an\n     io_uring thing, but since TWA_NOTIFY_SIGNAL was originally added\n     for faster task_work signaling for io_uring, bundling it with this\n     pull (Pavel)\n\n   - Add Pavel as a co-maintainer\n\n   - Various cleanups (me, Thorsten)\"\n\n* tag 'for-6.11/io_uring-20240714' of git://git.kernel.dk/linux: (28 commits)\n  io_uring/net: check socket is valid in io_bind()/io_listen()\n  kernel: rerun task_work while freezing in get_signal()\n  io_uring/io-wq: limit retrying worker initialisation\n  io_uring/napi: Remove unnecessary s64 cast\n  io_uring/net: cleanup io_recv_finish() bundle handling\n  io_uring/msg_ring: fix overflow posting\n  MAINTAINERS: change Pavel Begunkov from io_uring reviewer to maintainer\n  io_uring/msg_ring: use kmem_cache_free() to free request\n  io_uring/msg_ring: check for dead submitter task\n  io_uring/msg_ring: add an alloc cache for io_kiocb entries\n  io_uring/msg_ring: improve handling of target CQE posting\n  io_uring: add io_add_aux_cqe() helper\n  io_uring: add remote task_work execution helper\n  io_uring/msg_ring: tighten requirement for remote posting\n  io_uring: Allocate only necessary memory in io_probe\n  io_uring: Fix probe of disabled operations\n  io_uring: Introduce IORING_OP_LISTEN\n  io_uring: Introduce IORING_OP_BIND\n  net: Split a __sys_listen helper for io_uring\n  net: Split a __sys_bind helper for io_uring\n  ...",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-15 13:49:10 -0700 Merge tag 'for-6.11/io_uring-20240714' of git://git.kernel.dk/linux"
    },
    {
        "commit": "febc33cb352afb8c8dc87286635c35cc644fbdb9",
        "message": "Set the preferred folio order in the fgp_flags by calling\nfgf_set_order(). Page cache will try to allocate large folio of the\npreferred order whenever possible instead of allocating multiple 0 order\nfolios.\n\nThis improves the buffered write performance up to 1.25x with default\nmount options and up to 1.57x when mounted with no_data_io option with\nthe following fio workload:\n\nfio --name=bcachefs --filename=/mnt/test  --size=100G \\\n     --ioengine=io_uring --iodepth=16 --rw=write --bs=128k\n\nSigned-off-by: Pankaj Raghav <p.raghav@samsung.com>\nSigned-off-by: Kent Overstreet <kent.overstreet@linux.dev>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-14 19:00:15 -0400 bcachefs: set fgf order hint before starting a buffered write"
    },
    {
        "commit": "ad00e629145b2b9f0d78aa46e204a9df7d628978",
        "message": "We need to check that sock_from_file(req->file) != NULL.\n\nReported-by: syzbot <syzbot+1e811482aa2c70afa9a0@syzkaller.appspotmail.com>\nCloses: https://syzkaller.appspot.com/bug?extid=1e811482aa2c70afa9a0\nFixes: 7481fd93fa0a (\"io_uring: Introduce IORING_OP_BIND\")\nFixes: ff140cc8628a (\"io_uring: Introduce IORING_OP_LISTEN\")\nSigned-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>\nLink: https://lore.kernel.org/r/903da529-eaa3-43ef-ae41-d30f376c60cc@I-love.SAKURA.ne.jp\n[axboe: move assignment of sock to where the NULL check is]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-13 06:40:15 -0600 io_uring/net: check socket is valid in io_bind()/io_listen()"
    },
    {
        "commit": "943ad0b62e3c21f324c4884caa6cb4a871bca05c",
        "message": "io_uring can asynchronously add a task_work while the task is getting\nfreezed. TIF_NOTIFY_SIGNAL will prevent the task from sleeping in\ndo_freezer_trap(), and since the get_signal()'s relock loop doesn't\nretry task_work, the task will spin there not being able to sleep\nuntil the freezing is cancelled / the task is killed / etc.\n\nRun task_works in the freezer path. Keep the patch small and simple\nso it can be easily back ported, but we might need to do some cleaning\nafter and look if there are other places with similar problems.\n\nCc: stable@vger.kernel.org\nLink: https://github.com/systemd/systemd/issues/33626\nFixes: 12db8b690010c (\"entry: Add support for TIF_NOTIFY_SIGNAL\")\nReported-by: Julian Orth <ju.orth@gmail.com>\nAcked-by: Oleg Nesterov <oleg@redhat.com>\nAcked-by: Tejun Heo <tj@kernel.org>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/89ed3a52933370deaaf61a0a620a6ac91f1e754d.1720634146.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-11 01:51:44 -0600 kernel: rerun task_work while freezing in get_signal()"
    },
    {
        "commit": "0453aad676ff99787124b9b3af4a5f59fbe808e2",
        "message": "If io-wq worker creation fails, we retry it by queueing up a task_work.\ntasK_work is needed because it should be done from the user process\ncontext. The problem is that retries are not limited, and if queueing a\ntask_work is the reason for the failure, we might get into an infinite\nloop.\n\nIt doesn't seem to happen now but it would with the following patch\nexecuting task_work in the freezer's loop. For now, arbitrarily limit the\nnumber of attempts to create a worker.\n\nCc: stable@vger.kernel.org\nFixes: 3146cba99aa28 (\"io-wq: make worker creation resilient against signals\")\nReported-by: Julian Orth <ju.orth@gmail.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8280436925db88448c7c85c6656edee1a43029ea.1720634146.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-11 01:51:44 -0600 io_uring/io-wq: limit retrying worker initialisation"
    },
    {
        "commit": "f7c696a56cc7d70515774a24057b473757ec6089",
        "message": "Since the do_div() macro casts the divisor to u32 anyway, remove the\nunnecessary s64 cast and fix the following Coccinelle/coccicheck\nwarning reported by do_div.cocci:\n\n  WARNING: do_div() does a 64-by-32 division, please consider using div64_s64 instead\n\nSigned-off-by: Thorsten Blum <thorsten.blum@toblux.com>\nLink: https://lore.kernel.org/r/20240710010520.384009-2-thorsten.blum@toblux.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-10 00:20:52 -0600 io_uring/napi: Remove unnecessary s64 cast"
    },
    {
        "commit": "07838f7fd529c8a6de44b601d4b7057e6c8d36ed",
        "message": "An iommufd fault object provides an interface for delivering I/O page\nfaults to user space. These objects are created and destroyed by user\nspace, and they can be associated with or dissociated from hardware page\ntable objects during page table allocation or destruction.\n\nUser space interacts with the fault object through a file interface. This\ninterface offers a straightforward and efficient way for user space to\nhandle page faults. It allows user space to read fault messages\nsequentially and respond to them by writing to the same file. The file\ninterface supports reading messages in poll mode, so it's recommended that\nuser space applications use io_uring to enhance read and write efficiency.\n\nA fault object can be associated with any iopf-capable iommufd_hw_pgtable\nduring the pgtable's allocation. All I/O page faults triggered by devices\nwhen accessing the I/O addresses of an iommufd_hw_pgtable are routed\nthrough the fault object to user space. Similarly, user space's responses\nto these page faults are routed back to the iommu device driver through\nthe same fault object.\n\nLink: https://lore.kernel.org/r/20240702063444.105814-7-baolu.lu@linux.intel.com\nSigned-off-by: Lu Baolu <baolu.lu@linux.intel.com>\nReviewed-by: Jason Gunthorpe <jgg@nvidia.com>\nReviewed-by: Kevin Tian <kevin.tian@intel.com>\nSigned-off-by: Jason Gunthorpe <jgg@nvidia.com>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-09 13:54:32 -0300 iommufd: Add iommufd fault object"
    },
    {
        "commit": "8a9c6c40432e265600232b864f97d7c675e8be52",
        "message": "Pull io_uring fix from Jens Axboe:\n \"A fix for a feature that went into the 6.10 merge window actually\n  ended up causing a regression in building bundles for receives.\n\n  Fix that up by ensuring we don't overwrite msg_inq before we use\n  it in the loop\"\n\n* tag 'io_uring-6.10-20240703' of git://git.kernel.dk/linux:\n  io_uring/net: don't clear msg_inq before io_recv_buf_select() needs it",
        "kernel_version": "v6.10-rc7",
        "release_date": "2024-07-03 10:16:54 -0700 Merge tag 'io_uring-6.10-20240703' of git://git.kernel.dk/linux"
    },
    {
        "commit": "93d8032f4143c8d2ac3e10c6504385c26acc511f",
        "message": "Combine the two cases that check for whether or not this is a bundle,\nrather than having them as separate checks. This is easier to reduce,\nand it reduces the text associated with it as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-02 09:42:25 -0600 io_uring/net: cleanup io_recv_finish() bundle handling"
    },
    {
        "commit": "6e92c646f5a4230d939a0882f879fc50dfa116c5",
        "message": "For bundle receives to function properly, the previous iteration msg_inq\nvalue is needed to make a judgement call on how much data there is to\nreceive. A previous fix ended up clearing it earlier as an error case\nwould potentially errantly set IORING_CQE_F_SOCK_NONEMPTY if the request\ngot failed.\n\nMove the assignment to post assigning buffers for the receive, but\nensure that it's cleared for the buffer selection error case. With that,\nbuffer selection has the right msg_inq value and can correctly bundle\nreceives as designed.\n\nNoticed while testing where it was apparent than more than 1 buffer was\nnever received. After fix was in place, multiple buffers are correctly\npicked for receive. This provides a 10x speedup for the test case, as\nthe buffer size used was 64b.\n\nFixes: 18414a4a2eab (\"io_uring/net: assign kmsg inq/flags before buffer selection\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc7",
        "release_date": "2024-07-02 09:42:10 -0600 io_uring/net: don't clear msg_inq before io_recv_buf_select() needs it"
    },
    {
        "commit": "3b7c16be30e35ec035b2efcc0f7d7b368789c443",
        "message": "The caller of io_cqring_event_overflow() should be holding the\ncompletion_lock, which is violated by io_msg_tw_complete. There\nis only one caller of io_add_aux_cqe(), so just add locking there\nfor now.\n\nWARNING: CPU: 0 PID: 5145 at io_uring/io_uring.c:703 io_cqring_event_overflow+0x442/0x660 io_uring/io_uring.c:703\nRIP: 0010:io_cqring_event_overflow+0x442/0x660 io_uring/io_uring.c:703\n <TASK>\n __io_post_aux_cqe io_uring/io_uring.c:816 [inline]\n io_add_aux_cqe+0x27c/0x320 io_uring/io_uring.c:837\n io_msg_tw_complete+0x9d/0x4d0 io_uring/msg_ring.c:78\n io_fallback_req_func+0xce/0x1c0 io_uring/io_uring.c:256\n process_one_work kernel/workqueue.c:3224 [inline]\n process_scheduled_works+0xa2c/0x1830 kernel/workqueue.c:3305\n worker_thread+0x86d/0xd40 kernel/workqueue.c:3383\n kthread+0x2f0/0x390 kernel/kthread.c:389\n ret_from_fork+0x4b/0x80 arch/x86/kernel/process.c:144\n ret_from_fork_asm+0x1a/0x30 arch/x86/entry/entry_64.S:244\n </TASK>\n\nFixes: f33096a3c99c0 (\"io_uring: add io_add_aux_cqe() helper\")\nReported-by: syzbot+f7f9c893345c5c615d34@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c7350d07fefe8cce32b50f57665edbb6355ea8c1.1719927398.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-02 08:48:17 -0600 io_uring/msg_ring: fix overflow posting"
    },
    {
        "commit": "e2dd0d0593c17f32c7263e9d6f7554ecaabb0baf",
        "message": "Pavel Begunkov says:\n\n====================\nzerocopy tx cleanups\n\nAssorted zerocopy send path cleanups, the main part of which is\nmoving some net stack specific accounting out of io_uring back\nto net/ in Patch 4.\n====================\n\nLink: https://patch.msgid.link/cover.1719190216.git.asml.silence@gmail.com\nSigned-off-by: Paolo Abeni <pabeni@redhat.com>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-02 12:06:52 +0200 Merge branch 'zerocopy-tx-cleanups'"
    },
    {
        "commit": "060f4ba6e40338a70932603a3564903acf5f5734",
        "message": "Currently, io_uring's io_sg_from_iter() duplicates the part of\n__zerocopy_sg_from_iter() charging pages to the socket. It'd be too easy\nto miss while changing it in net/, the chunk is not the most\nstraightforward for outside users and full of internal implementation\ndetails. io_uring is not a good place to keep it, deduplicate it by\nmoving out of the callback into __zerocopy_sg_from_iter().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Willem de Bruijn <willemb@google.com>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Paolo Abeni <pabeni@redhat.com>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-02 12:06:50 +0200 io_uring/net: move charging socket out of zc io_uring"
    },
    {
        "commit": "9b458a260080961d5e766592b0394b08a12a0ba1",
        "message": "Pull vfs fixes from Christian Brauner:\n \"Misc:\n\n   - Don't misleadingly warn during filesystem thaw operations.\n\n     It's possible that a block device which was frozen before it was\n     mounted can cause a failing thaw operation if someone concurrently\n     tried to mount it while that thaw operation was issued and the\n     device had already been temporarily claimed for the mount (The\n     mount will of course be aborted because the device is frozen).\n\n  netfs:\n\n   - Fix io_uring based write-through. Make sure that the total request\n     length is correctly set.\n\n   - Fix partial writes to folio tail.\n\n   - Remove some xarray helpers that were intended for bounce buffers\n     which got defered to a later patch series.\n\n   - Make netfs_page_mkwrite() whether folio->mapping is vallid after\n     acquiring the folio lock.\n\n   - Make netfs_page_mkrite() flush conflicting data instead of waiting.\n\n  fsnotify:\n\n   - Ensure that fsnotify creation events are generated before fsnotify\n     open events when a file is created via ->atomic_open(). The\n     ordering was broken before.\n\n   - Ensure that no fsnotify events are generated for O_PATH file\n     descriptors. While no fsnotify open events were generated, fsnotify\n     close events were. Make it consistent and don't produce any\"\n\n* tag 'vfs-6.10-rc7.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs:\n  netfs: Fix netfs_page_mkwrite() to flush conflicting data, not wait\n  netfs: Fix netfs_page_mkwrite() to check folio->mapping is valid\n  netfs: Delete some xarray-wangling functions that aren't used\n  netfs: Fix early issue of write op on partial write to folio tail\n  netfs: Fix io_uring based write-through\n  vfs: generate FS_CREATE before FS_OPEN when ->atomic_open used.\n  fsnotify: Do not generate events for O_PATH file descriptors\n  fs: don't misleadingly warn during thaw operations",
        "kernel_version": "v6.10-rc7",
        "release_date": "2024-07-01 09:22:08 -0700 Merge tag 'vfs-6.10-rc7.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs"
    },
    {
        "commit": "8515f1661ca1f9ad63850a5e1e86599399420d2e",
        "message": "This more accurately describes Pavel's role for the project, so let's\nmake the change to reflect that.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-01 09:23:26 -0600 MAINTAINERS: change Pavel Begunkov from io_uring reviewer to maintainer"
    },
    {
        "commit": "be4f5d9c992ba1d89ce63ad9e40a99f120882038",
        "message": "The change adding caching around the request allocated and freed for\ndata messages changed a kmem_cache_free() to a kfree(), which isn't\ncorrect as the request came from slab in the first place. Fix that up\nand use the right freeing function if the cache is already at its limit.\n\nNote that the current mixing of kmem_cache_alloc and kfree is fine, but\nconsistent alloc/free functions should be used as it's otherwise somewhat\nconfusing.\n\nFixes: 50cf5f3842af (\"io_uring/msg_ring: add an alloc cache for io_kiocb entries\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-01 09:10:59 -0600 io_uring/msg_ring: use kmem_cache_free() to free request"
    },
    {
        "commit": "b0727b1243cd084260e47c51c7950020bfddb636",
        "message": "The change for improving the handling of the target CQE posting\ninadvertently dropped the NULL check for the submitter task on the target\nring, reinstate that.\n\nFixes: 0617bb500bfa (\"io_uring/msg_ring: improve handling of target CQE posting\")\nReported-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-07-01 08:45:27 -0600 io_uring/msg_ring: check for dead submitter task"
    },
    {
        "commit": "0f47788b3326150a4a3338312f03d2ef3614b53a",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Removal of a struct member that's unused since the 6.10 merge window,\n  and a fix for a regression in SQPOLL wakeups, bringing it back to how\n  it worked before the SQPOLL local task_work\"\n\n* tag 'io_uring-6.10-20240627' of git://git.kernel.dk/linux:\n  io_uring: signal SQPOLL task_work with TWA_SIGNAL_NO_IPI\n  io_uring: remove dead struct io_submit_state member",
        "kernel_version": "v6.10-rc6",
        "release_date": "2024-06-27 12:23:52 -0700 Merge tag 'io_uring-6.10-20240627' of git://git.kernel.dk/linux"
    },
    {
        "commit": "d98b7d7dda721ca009b6dc5dd3beeeb7fd46f4b4",
        "message": "[This was included in v2 of 9b038d004ce95551cb35381c49fe896c5bc11ffe, but\nv1 got pushed instead]\n\nFix netfs_unbuffered_write_iter_locked() to set the total request length in\nthe netfs_io_request struct rather than leaving it as zero.\n\nFixes: 288ace2f57c9 (\"netfs: New writeback implementation\")\nSigned-off-by: David Howells <dhowells@redhat.com>\ncc: Jeff Layton <jlayton@kernel.org>\ncc: Steve French <stfrench@microsoft.com>\ncc: Enzo Matsumiya <ematsumiya@suse.de>\ncc: Christian Brauner <brauner@kernel.org>\ncc: netfs@lists.linux.dev\ncc: v9fs@lists.linux.dev\ncc: linux-afs@lists.infradead.org\ncc: linux-cifs@vger.kernel.org\ncc: linux-fsdevel@vger.kernel.org\nLink: https://lore.kernel.org/r/20240620173137.610345-2-dhowells@redhat.com\nSigned-off-by: Christian Brauner <brauner@kernel.org>",
        "kernel_version": "v6.10-rc7",
        "release_date": "2024-06-26 14:15:26 +0200 netfs: Fix io_uring based write-through"
    },
    {
        "commit": "dbcabac138fdfc730ba458ed2199ff1f29a271fc",
        "message": "Before SQPOLL was transitioned to managing its own task_work, the core\nused TWA_SIGNAL_NO_IPI to ensure that task_work was processed. If not,\nwe can't be sure that all task_work is processed at SQPOLL thread exit\ntime.\n\nFixes: af5d68f8892f (\"io_uring/sqpoll: manage task_work privately\")\nCc: stable@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc6",
        "release_date": "2024-06-24 19:46:15 -0600 io_uring: signal SQPOLL task_work with TWA_SIGNAL_NO_IPI"
    },
    {
        "commit": "26b97668e5339434e5df8ddc7b1898a37a350112",
        "message": "When the intermediate CQE aux cache got removed, any usage of the this\nmember went away. As it isn't used anymore, kill it.\n\nFixes: 902ce82c2aa1 (\"io_uring: get rid of intermediate aux cqe caches\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc6",
        "release_date": "2024-06-24 19:46:10 -0600 io_uring: remove dead struct io_submit_state member"
    },
    {
        "commit": "50cf5f3842af3135b88b041890e7e12a74425fcb",
        "message": "With slab accounting, allocating and freeing memory has considerable\noverhead. Add a basic alloc cache for the io_kiocb allocations that\nmsg_ring needs to do. Unlike other caches, this one is used by the\nsender, grabbing it from the remote ring. When the remote ring gets\nthe posted completion, it'll free it locally. Hence it is separately\nlocked, using ctx->msg_lock.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-24 08:39:55 -0600 io_uring/msg_ring: add an alloc cache for io_kiocb entries"
    },
    {
        "commit": "0617bb500bfabf8447062f1e1edde92ed2b638f1",
        "message": "Use the exported helper for queueing task_work for message passing,\nrather than rolling our own.\n\nNote that this is only done for strict data messages for now, file\ndescriptor passing messages still rely on the kernel task_work. It could\nget converted at some point if it's performance critical.\n\nThis improves peak performance of message passing by about 5x in some\nbasic testing, with 2 threads just sending messages to each other.\nBefore this change, it was capped at around 700K/sec, with the change\nit's at over 4M/sec.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-24 08:39:50 -0600 io_uring/msg_ring: improve handling of target CQE posting"
    },
    {
        "commit": "f33096a3c99c0149be49fe1e107244a7ed860ecb",
        "message": "This helper will post a CQE, and can be called from task_work where we\nnow that the ctx is already properly locked and that deferred\ncompletions will get flushed later on.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-24 08:39:45 -0600 io_uring: add io_add_aux_cqe() helper"
    },
    {
        "commit": "c3ac76f9ca7a621428851149bc56bfca0aacaef4",
        "message": "All our task_work handling is targeted at the state in the io_kiocb\nitself, which is what it is being used for. However, MSG_RING rolls its\nown task_work handling, ignoring how that is usually done.\n\nIn preparation for switching MSG_RING to be able to use the normal\ntask_work handling, add io_req_task_work_add_remote() which allows the\ncaller to pass in the target io_ring_ctx.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-24 08:39:39 -0600 io_uring: add remote task_work execution helper"
    },
    {
        "commit": "d57afd8bb7f2c4f0d86e9e9b276f7c3a7fedfc6d",
        "message": "Currently this is gated on whether or not the target ring needs a local\ncompletion - and if so, whether or not we're running on the right task.\nThe use case for same thread cross posting is probably a lot less\nrelevant than remote posting. And since we're going to improve this\nsituation anyway, just gate it on local posting and ignore what task\nwe're currently running on.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-24 08:39:34 -0600 io_uring/msg_ring: tighten requirement for remote posting"
    },
    {
        "commit": "a502e727965dbd735145cff7ec84ad0a6060f9d2",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single cleanup for the fixed buffer iov_iter import.\n\n  More cosmetic than anything else, but let's get it cleaned up as it's\n  confusing\"\n\n* tag 'io_uring-6.10-20240621' of git://git.kernel.dk/linux:\n  io_uring/rsrc: fix incorrect assignment of iter->nr_segs in io_import_fixed",
        "kernel_version": "v6.10-rc5",
        "release_date": "2024-06-21 14:01:03 -0700 Merge tag 'io_uring-6.10-20240621' of git://git.kernel.dk/linux"
    },
    {
        "commit": "a0c6359df6c70f0754728e5353e828967910575b",
        "message": "David Wei says:\n\n====================\nbnxt_en: implement netdev_queue_mgmt_ops\n\nImplement netdev_queue_mgmt_ops for bnxt added in [1]. This will be used\nin the io_uring ZC Rx patchset to configure queues with a custom page\npool w/ a special memory provider for zero copy support.\n\nThe first two patches prep the driver, while the final patch adds the\nimplementation.\n\nAny arbitrary Rx queue can be reset without affecting other queues. V2\nand prior of this patchset was thought to only support resetting queues\nnot in the main RSS context. Upon further testing I realised moving\nqueues out and calling bnxt_hwrm_vnic_update() wasn't necessary.\n\nI didn't include the netdev core API using this netdev_queue_mgmt_ops\nbecause Mina is adding it in his devmem TCP series [2]. But I'm happy to\ninclude it if folks want to include a user with this series.\n\nI tested this series on BCM957504-N1100FY4 with FW 229.1.123.0. I\nmanually injected failures at all the places that can return an errno\nand confirmed that the device/queue is never left in a broken state.\n\n[1]: https://lore.kernel.org/netdev/20240501232549.1327174-2-shailend@google.com/\n[2]: https://lore.kernel.org/netdev/20240607005127.3078656-2-almasrymina@google.com/\n\nv3:\n - tested w/o bnxt_hwrm_vnic_update() and it works on any queue\n - removed unneeded code\n\nv2:\n - fix broken build\n - remove unused var in bnxt_init_one_rx_ring()\n====================\n\nSigned-off-by: David S. Miller <davem@davemloft.net>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-21 10:10:34 +0100 Merge branch 'bnxt_en-netdev_queue_mgmt_ops'"
    },
    {
        "commit": "a23800f08a60787dfbf2b87b2e6ed411cb629859",
        "message": "In io_import_fixed when advancing the iter within the first bvec, the\niter->nr_segs is set to bvec->bv_len. nr_segs should be the number of\nbvecs, plus we don't need to adjust it here, so just remove it.\n\nFixes: b000ae0ec2d7 (\"io_uring/rsrc: optimise single entry advance\")\nSigned-off-by: Chenliang Li <cliang01.li@samsung.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20240619063819.2445-1-cliang01.li@samsung.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc5",
        "release_date": "2024-06-20 06:51:56 -0600 io_uring/rsrc: fix incorrect assignment of iter->nr_segs in io_import_fixed"
    },
    {
        "commit": "6bc9199d0c84f5cd72922223231c7708698059a2",
        "message": "We write at most IORING_OP_LAST entries in the probe buffer, so we don't\nneed to allocate temporary space for more than that.  As a side effect,\nwe no longer can overflow \"size\".\n\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20240619020620.5301-3-krisman@suse.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-19 08:58:00 -0600 io_uring: Allocate only necessary memory in io_probe"
    },
    {
        "commit": "3e05b222382ec67dce7358d50b6006e91d028d8b",
        "message": "io_probe checks io_issue_def->not_supported, but we never really set\nthat field, as we mark non-supported functions through a specific ->prep\nhandler.  This means we end up returning IO_URING_OP_SUPPORTED, even for\ndisabled operations.  Fix it by just checking the prep handler itself.\n\nFixes: 66f4af93da57 (\"io_uring: add support for probing opcodes\")\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20240619020620.5301-2-krisman@suse.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-19 08:58:00 -0600 io_uring: Fix probe of disabled operations"
    },
    {
        "commit": "ff140cc8628abfb1755691d16cfa8788d8820ef7",
        "message": "IORING_OP_LISTEN provides the semantic of listen(2) via io_uring.  While\nthis is an essentially synchronous system call, the main point is to\nenable a network path to execute fully with io_uring registered and\ndescriptorless files.\n\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20240614163047.31581-4-krisman@suse.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-19 07:57:21 -0600 io_uring: Introduce IORING_OP_LISTEN"
    },
    {
        "commit": "7481fd93fa0a851740e26026485f56a1305454ce",
        "message": "IORING_OP_BIND provides the semantic of bind(2) via io_uring.  While\nthis is an essentially synchronous system call, the main point is to\nenable a network path to execute fully with io_uring registered and\ndescriptorless files.\n\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20240614163047.31581-3-krisman@suse.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-19 07:57:21 -0600 io_uring: Introduce IORING_OP_BIND"
    },
    {
        "commit": "bb6aaf736680f0f3c2e6281735c47c64e2042819",
        "message": "io_uring holds a reference to the file and maintains a sockaddr_storage\naddress.  Similarly to what was done to __sys_connect_file, split an\ninternal helper for __sys_listen in preparation to support an\nio_uring listen command.\n\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nReviewed-by: Kuniyuki Iwashima <kuniyu@amazon.com>\nAcked-by: Jakub Kicinski <kuba@kernel.org>\nLink: https://lore.kernel.org/r/20240614163047.31581-2-krisman@suse.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-19 07:57:21 -0600 net: Split a __sys_listen helper for io_uring"
    },
    {
        "commit": "dc2e77979412d289df9049d8c693761db8602867",
        "message": "io_uring holds a reference to the file and maintains a\nsockaddr_storage address.  Similarly to what was done to\n__sys_connect_file, split an internal helper for __sys_bind in\npreparation to supporting an io_uring bind command.\n\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nReviewed-by: Kuniyuki Iwashima <kuniyu@amazon.com>\nAcked-by: Jakub Kicinski <kuba@kernel.org>\nLink: https://lore.kernel.org/r/20240614163047.31581-1-krisman@suse.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-19 07:57:20 -0600 net: Split a __sys_bind helper for io_uring"
    },
    {
        "commit": "3b87184f7eff27fef7d7ee18b65f173152e1bb81",
        "message": "The existing fadvise/madvise support only supports 32-bit lengths. Add\nsupport for 64-bit lengths, enabled by the application setting sqe->off\nrather than sqe->len for the length. If sqe->len is set, then that is\nused as the 32-bit length. If sqe->len is zero, then sqe->off is read\nfor full 64-bit support.\n\nOlder kernels will return -EINVAL if 64-bit support isn't available.\n\nFixes: 4840e418c2fc (\"io_uring: add IORING_OP_FADVISE\")\nFixes: c1ca757bd6f4 (\"io_uring: add IORING_OP_MADVISE\")\nReported-by: Stefan <source@s.muenzel.net>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-16 14:54:55 -0600 io_uring/advise: support 64-bit lengths"
    },
    {
        "commit": "11d194669271642a5d1bfff6c8011478309e7849",
        "message": "We're guaranteed to be in a TASK_RUNNING state post schedule, so we\nnever need to set the state after that. While in there, remove the\nother __set_current_state() as well, and just call finish_wait()\nwhen we now we're going to break anyway. This is easier to grok than\nmanual __set_current_state() calls.\n\nReported-by: Linus Torvalds <torvalds@linux-foundation.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-16 14:54:55 -0600 io_uring/rsrc: remove redundant __set_current_state() post schedule()"
    },
    {
        "commit": "3474d1b93f897ab33ce160e759afd47d5f412de4",
        "message": "The work flags can be set/accessed from different tasks, both the\noriginator of the request, and the io-wq workers. While modifications\naren't concurrent, it still makes KMSAN unhappy. There's no real\ndownside to just making the flag reading/manipulation use proper\natomics here.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-16 14:54:55 -0600 io_uring/io-wq: make io_wq_work flags atomic"
    },
    {
        "commit": "f2a93294edce87c909d61e18b506404127394891",
        "message": "__io_submit_flush_completions() assigns ctx->submit_state to a local\nvariable and uses it in all but one spot, switch that forgotten\nstatement to using 'state' as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-16 14:54:55 -0600 io_uring: use 'state' consistently"
    },
    {
        "commit": "200f3abd14db55f9aadcb74f4e7a678f1c469ba1",
        "message": "This is pretty nicely abstracted already, but let's move it to a separate\nfile rather than have it in the main io_uring file. With that, we can\nalso move the io_ev_fd struct and enum out of global scope.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-16 14:54:55 -0600 io_uring/eventfd: move eventfd handling to separate file"
    },
    {
        "commit": "60b6c075e8eb8bd23c106e2ab13370a146a94a5b",
        "message": "In some ways, it just \"happens to work\" currently with using the ops\nfield for both the free and signaling bit. But it depends on ordering\nof operations in terms of freeing and signaling. Clean it up and use the\nusual refs == 0 under RCU read side lock to determine if the ev_fd is\nstill valid, and use the reference to gate the freeing as well.\n\nFixes: 21a091b970cd (\"io_uring: signal registered eventfd to process deferred task work\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-16 14:54:55 -0600 io_uring/eventfd: move to more idiomatic RCU free usage"
    },
    {
        "commit": "f4eaf8eda89e1ae5d8274297094687245293deff",
        "message": "Instead of open coding an io_uring function to copy iovs from userspace,\nrely on the existing iovec_from_user function.  While there, avoid\nrepeatedly zeroing the iov in the !arg case for io_sqe_buffer_register.\n\ntested with liburing testsuite.\n\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20240523214535.31890-1-krisman@suse.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-16 14:54:55 -0600 io_uring/rsrc: Drop io_copy_iov in favor of iovec API"
    },
    {
        "commit": "81cc927d9c5eefd4a1b08e16b0ab2263f36d03f7",
        "message": "Commit 19a63c402170 (\"io_uring/rsrc: keep one global dummy_ubuf\")\nreplaced it with a global static object but this stayed behind.\n\nFixes: 19a63c402170 (\"io_uring/rsrc: keep one global dummy_ubuf\")\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20240523214517.31803-1-krisman@suse.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.11-rc1",
        "release_date": "2024-06-16 14:54:55 -0600 io_uring: Drop per-ctx dummy_ubuf"
    },
    {
        "commit": "ac3cb72aea010510eaa1e19ab001a0d28c6eb4ab",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two fixes from Pavel headed to stable:\n\n   - Ensure that the task state is correct before attempting to grab a\n     mutex\n\n   - Split cancel sequence flag into a separate variable, as it can get\n     set by someone not owning the request (but holding the ctx lock)\"\n\n* tag 'io_uring-6.10-20240614' of git://git.kernel.dk/linux:\n  io_uring: fix cancellation overwriting req->flags\n  io_uring/rsrc: don't lock while !TASK_RUNNING",
        "kernel_version": "v6.10-rc4",
        "release_date": "2024-06-14 11:17:24 -0700 Merge tag 'io_uring-6.10-20240614' of git://git.kernel.dk/linux"
    },
    {
        "commit": "f4a1254f2a076afb0edd473589bf40f9b4d36b41",
        "message": "Only the current owner of a request is allowed to write into req->flags.\nHence, the cancellation path should never touch it. Add a new field\ninstead of the flag, move it into the 3rd cache line because it should\nalways be initialised. poll_refs can move further as polling is an\ninvolved process anyway.\n\nIt's a minimal patch, in the future we can and should find a better\nplace for it and remove now unused REQ_F_CANCEL_SEQ.\n\nFixes: 521223d7c229f (\"io_uring/cancel: don't default to setting req->work.cancel_seq\")\nCc: stable@vger.kernel.org\nReported-by: Li Shi <sl1589472800@gmail.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6827b129f8f0ad76fa9d1f0a773de938b240ffab.1718323430.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc4",
        "release_date": "2024-06-13 19:25:28 -0600 io_uring: fix cancellation overwriting req->flags"
    },
    {
        "commit": "54559642b96116b45e4b5ca7fd9f7835b8561272",
        "message": "There is a report of io_rsrc_ref_quiesce() locking a mutex while not\nTASK_RUNNING, which is due to forgetting restoring the state back after\nio_run_task_work_sig() and attempts to break out of the waiting loop.\n\ndo not call blocking ops when !TASK_RUNNING; state=1 set at\n[<ffffffff815d2494>] prepare_to_wait+0xa4/0x380\nkernel/sched/wait.c:237\nWARNING: CPU: 2 PID: 397056 at kernel/sched/core.c:10099\n__might_sleep+0x114/0x160 kernel/sched/core.c:10099\nRIP: 0010:__might_sleep+0x114/0x160 kernel/sched/core.c:10099\nCall Trace:\n <TASK>\n __mutex_lock_common kernel/locking/mutex.c:585 [inline]\n __mutex_lock+0xb4/0x940 kernel/locking/mutex.c:752\n io_rsrc_ref_quiesce+0x590/0x940 io_uring/rsrc.c:253\n io_sqe_buffers_unregister+0xa2/0x340 io_uring/rsrc.c:799\n __io_uring_register io_uring/register.c:424 [inline]\n __do_sys_io_uring_register+0x5b9/0x2400 io_uring/register.c:613\n do_syscall_x64 arch/x86/entry/common.c:52 [inline]\n do_syscall_64+0xd8/0x270 arch/x86/entry/common.c:83\n entry_SYSCALL_64_after_hwframe+0x6f/0x77\n\nReported-by: Li Shi <sl1589472800@gmail.com>\nFixes: 4ea15b56f0810 (\"io_uring/rsrc: use wq for quiescing\")\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/77966bc104e25b0534995d5dbb152332bc8f31c0.1718196953.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc4",
        "release_date": "2024-06-12 13:02:12 -0600 io_uring/rsrc: don't lock while !TASK_RUNNING"
    },
    {
        "commit": "e33915892d8871b28d17675fecc1b5b36b0d5721",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix a locking order issue with setting max async thread workers\n   (Hagar)\n\n - Fix for a NULL pointer dereference for failed async flagged requests\n   using ring provided buffers. This doesn't affect the current kernel,\n   but it does affect older kernels, and is being queued up for 6.10\n   just to make the stable process easier (me)\n\n - Fix for NAPI timeout calculations for how long to busy poll, and\n   subsequently how much to sleep post that if a wait timeout is passed\n   in (me)\n\n - Fix for a regression in this release cycle, where we could end up\n   using a partially unitialized match value for io-wq (Su)\n\n* tag 'io_uring-6.10-20240607' of git://git.kernel.dk/linux:\n  io_uring: fix possible deadlock in io_register_iowq_max_workers()\n  io_uring/io-wq: avoid garbage value of 'match' in io_wq_enqueue()\n  io_uring/napi: fix timeout calculation\n  io_uring: check for non-NULL file pointer in io_file_can_poll()",
        "kernel_version": "v6.10-rc3",
        "release_date": "2024-06-07 16:43:07 -0700 Merge tag 'io_uring-6.10-20240607' of git://git.kernel.dk/linux"
    },
    {
        "commit": "73254a297c2dd094abec7c9efee32455ae875bdf",
        "message": "The io_register_iowq_max_workers() function calls io_put_sq_data(),\nwhich acquires the sqd->lock without releasing the uring_lock.\nSimilar to the commit 009ad9f0c6ee (\"io_uring: drop ctx->uring_lock\nbefore acquiring sqd->lock\"), this can lead to a potential deadlock\nsituation.\n\nTo resolve this issue, the uring_lock is released before calling\nio_put_sq_data(), and then it is re-acquired after the function call.\n\nThis change ensures that the locks are acquired in the correct\norder, preventing the possibility of a deadlock.\n\nSuggested-by: Maximilian Heyne <mheyne@amazon.de>\nSigned-off-by: Hagar Hemdan <hagarhem@amazon.com>\nLink: https://lore.kernel.org/r/20240604130527.3597-1-hagarhem@amazon.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc3",
        "release_date": "2024-06-04 07:39:17 -0600 io_uring: fix possible deadlock in io_register_iowq_max_workers()"
    },
    {
        "commit": "91215f70ea8541e9011c0b48f8b59b9e0ce6953b",
        "message": "Clang static checker (scan-build) warning:\no_uring/io-wq.c:line 1051, column 3\nThe expression is an uninitialized value. The computed value will\nalso be garbage.\n\n'match.nr_pending' is used in io_acct_cancel_pending_work(), but it is\nnot fully initialized. Change the order of assignment for 'match' to fix\nthis problem.\n\nFixes: 42abc95f05bf (\"io-wq: decouple work_list protection from the big wqe->lock\")\nSigned-off-by: Su Hui <suhui@nfschina.com>\nLink: https://lore.kernel.org/r/20240604121242.2661244-1-suhui@nfschina.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc3",
        "release_date": "2024-06-04 07:39:00 -0600 io_uring/io-wq: avoid garbage value of 'match' in io_wq_enqueue()"
    },
    {
        "commit": "415ce0ea55c5a3afea501a773e002be9ed7149f5",
        "message": "Not quite sure what __io_napi_adjust_timeout() was attemping to do, it's\nadjusting both the NAPI timeout and the general overall timeout, and\ncalculating a value that is never used. The overall timeout is a super\nset of the NAPI timeout, and doesn't need adjusting. The only thing we\nreally need to care about is that the NAPI timeout doesn't exceed the\noverall timeout. If a user asked for a timeout of eg 5 usec and NAPI\ntimeout is 10 usec, then we should not spin for 10 usec.\n\nWhile in there, sanitize the time checking a bit. If we have a negative\nvalue in the passed in timeout, discard it. Round up the value as well,\nso we don't end up with a NAPI timeout for the majority of the wait,\nwith only a tiny sleep value at the end.\n\nHence the only case we need to care about is if the NAPI timeout is\nlarger than the overall timeout. If it is, cap the NAPI timeout at what\nthe overall timeout is.\n\nCc: stable@vger.kernel.org\nFixes: 8d0c12a80cde (\"io-uring: add napi busy poll support\")\nReported-by: Lewis Baker <lewissbaker@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc3",
        "release_date": "2024-06-04 07:32:45 -0600 io_uring/napi: fix timeout calculation"
    },
    {
        "commit": "5fc16fa5f13b3c06fdb959ef262050bd810416a2",
        "message": "In earlier kernels, it was possible to trigger a NULL pointer\ndereference off the forced async preparation path, if no file had\nbeen assigned. The trace leading to that looks as follows:\n\nBUG: kernel NULL pointer dereference, address: 00000000000000b0\nPGD 0 P4D 0\nOops: 0000 [#1] PREEMPT SMP\nCPU: 67 PID: 1633 Comm: buf-ring-invali Not tainted 6.8.0-rc3+ #1\nHardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS unknown 2/2/2022\nRIP: 0010:io_buffer_select+0xc3/0x210\nCode: 00 00 48 39 d1 0f 82 ae 00 00 00 48 81 4b 48 00 00 01 00 48 89 73 70 0f b7 50 0c 66 89 53 42 85 ed 0f 85 d2 00 00 00 48 8b 13 <48> 8b 92 b0 00 00 00 48 83 7a 40 00 0f 84 21 01 00 00 4c 8b 20 5b\nRSP: 0018:ffffb7bec38c7d88 EFLAGS: 00010246\nRAX: ffff97af2be61000 RBX: ffff97af234f1700 RCX: 0000000000000040\nRDX: 0000000000000000 RSI: ffff97aecfb04820 RDI: ffff97af234f1700\nRBP: 0000000000000000 R08: 0000000000200030 R09: 0000000000000020\nR10: ffffb7bec38c7dc8 R11: 000000000000c000 R12: ffffb7bec38c7db8\nR13: ffff97aecfb05800 R14: ffff97aecfb05800 R15: ffff97af2be5e000\nFS:  00007f852f74b740(0000) GS:ffff97b1eeec0000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00000000000000b0 CR3: 000000016deab005 CR4: 0000000000370ef0\nCall Trace:\n <TASK>\n ? __die+0x1f/0x60\n ? page_fault_oops+0x14d/0x420\n ? do_user_addr_fault+0x61/0x6a0\n ? exc_page_fault+0x6c/0x150\n ? asm_exc_page_fault+0x22/0x30\n ? io_buffer_select+0xc3/0x210\n __io_import_iovec+0xb5/0x120\n io_readv_prep_async+0x36/0x70\n io_queue_sqe_fallback+0x20/0x260\n io_submit_sqes+0x314/0x630\n __do_sys_io_uring_enter+0x339/0xbc0\n ? __do_sys_io_uring_register+0x11b/0xc50\n ? vm_mmap_pgoff+0xce/0x160\n do_syscall_64+0x5f/0x180\n entry_SYSCALL_64_after_hwframe+0x46/0x4e\nRIP: 0033:0x55e0a110a67e\nCode: ba cc 00 00 00 45 31 c0 44 0f b6 92 d0 00 00 00 31 d2 41 b9 08 00 00 00 41 83 e2 01 41 c1 e2 04 41 09 c2 b8 aa 01 00 00 0f 05 <c3> 90 89 30 eb a9 0f 1f 40 00 48 8b 42 20 8b 00 a8 06 75 af 85 f6\n\nbecause the request is marked forced ASYNC and has a bad file fd, and\nhence takes the forced async prep path.\n\nCurrent kernels with the request async prep cleaned up can no longer hit\nthis issue, but for ease of backporting, let's add this safety check in\nhere too as it really doesn't hurt. For both cases, this will inevitably\nend with a CQE posted with -EBADF.\n\nCc: stable@vger.kernel.org\nFixes: a76c0b31eef5 (\"io_uring: commit non-pollable provided mapped buffers upfront\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc3",
        "release_date": "2024-06-01 12:25:35 -0600 io_uring: check for non-NULL file pointer in io_file_can_poll()"
    },
    {
        "commit": "6d541d6672eeaf526d67b67b5407f48fe0522c6d",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A couple of minor fixes for issues introduced in the 6.10 merge window:\n\n   - Ensure that all read/write ops have an appropriate cleanup handler\n     set (Breno)\n\n   - Regression for applications still doing multiple mmaps even if\n     FEAT_SINGLE_MMAP is set (me)\n\n   - Move kmsg inquiry setting above any potential failure point,\n     avoiding a spurious NONEMPTY flag setting on early error (me)\"\n\n* tag 'io_uring-6.10-20240530' of git://git.kernel.dk/linux:\n  io_uring/net: assign kmsg inq/flags before buffer selection\n  io_uring/rw: Free iovec before cleaning async data\n  io_uring: don't attempt to mmap larger than what the user asks for",
        "kernel_version": "v6.10-rc2",
        "release_date": "2024-05-31 15:22:58 -0700 Merge tag 'io_uring-6.10-20240530' of git://git.kernel.dk/linux"
    },
    {
        "commit": "18414a4a2eabb0281d12d374c92874327e0e3fe3",
        "message": "syzbot reports that recv is using an uninitialized value:\n\n=====================================================\nBUG: KMSAN: uninit-value in io_req_cqe_overflow io_uring/io_uring.c:810 [inline]\nBUG: KMSAN: uninit-value in io_req_complete_post io_uring/io_uring.c:937 [inline]\nBUG: KMSAN: uninit-value in io_issue_sqe+0x1f1b/0x22c0 io_uring/io_uring.c:1763\n io_req_cqe_overflow io_uring/io_uring.c:810 [inline]\n io_req_complete_post io_uring/io_uring.c:937 [inline]\n io_issue_sqe+0x1f1b/0x22c0 io_uring/io_uring.c:1763\n io_wq_submit_work+0xa17/0xeb0 io_uring/io_uring.c:1860\n io_worker_handle_work+0xc04/0x2000 io_uring/io-wq.c:597\n io_wq_worker+0x447/0x1410 io_uring/io-wq.c:651\n ret_from_fork+0x6d/0x90 arch/x86/kernel/process.c:147\n ret_from_fork_asm+0x1a/0x30 arch/x86/entry/entry_64.S:244\n\nUninit was stored to memory at:\n io_req_set_res io_uring/io_uring.h:215 [inline]\n io_recv_finish+0xf10/0x1560 io_uring/net.c:861\n io_recv+0x12ec/0x1ea0 io_uring/net.c:1175\n io_issue_sqe+0x429/0x22c0 io_uring/io_uring.c:1751\n io_wq_submit_work+0xa17/0xeb0 io_uring/io_uring.c:1860\n io_worker_handle_work+0xc04/0x2000 io_uring/io-wq.c:597\n io_wq_worker+0x447/0x1410 io_uring/io-wq.c:651\n ret_from_fork+0x6d/0x90 arch/x86/kernel/process.c:147\n ret_from_fork_asm+0x1a/0x30 arch/x86/entry/entry_64.S:244\n\nUninit was created at:\n slab_post_alloc_hook mm/slub.c:3877 [inline]\n slab_alloc_node mm/slub.c:3918 [inline]\n __do_kmalloc_node mm/slub.c:4038 [inline]\n __kmalloc+0x6e4/0x1060 mm/slub.c:4052\n kmalloc include/linux/slab.h:632 [inline]\n io_alloc_async_data+0xc0/0x220 io_uring/io_uring.c:1662\n io_msg_alloc_async io_uring/net.c:166 [inline]\n io_recvmsg_prep_setup io_uring/net.c:725 [inline]\n io_recvmsg_prep+0xbe8/0x1a20 io_uring/net.c:806\n io_init_req io_uring/io_uring.c:2135 [inline]\n io_submit_sqe io_uring/io_uring.c:2182 [inline]\n io_submit_sqes+0x1135/0x2f10 io_uring/io_uring.c:2335\n __do_sys_io_uring_enter io_uring/io_uring.c:3246 [inline]\n __se_sys_io_uring_enter+0x40f/0x3c80 io_uring/io_uring.c:3183\n __x64_sys_io_uring_enter+0x11f/0x1a0 io_uring/io_uring.c:3183\n x64_sys_call+0x2c0/0x3b50 arch/x86/include/generated/asm/syscalls_64.h:427\n do_syscall_x64 arch/x86/entry/common.c:52 [inline]\n do_syscall_64+0xcf/0x1e0 arch/x86/entry/common.c:83\n entry_SYSCALL_64_after_hwframe+0x77/0x7f\n\nwhich appears to be io_recv_finish() reading kmsg->msg.msg_inq to decide\nif it needs to set IORING_CQE_F_SOCK_NONEMPTY or not. If the recv is\nentered with buffer selection, but no buffer is available, then we jump\nerror path which calls io_recv_finish() without having assigned\nkmsg->msg_inq. This might cause an errant setting of the NONEMPTY flag\nfor a request get gets errored with -ENOBUFS.\n\nReported-by: syzbot+b1647099e82b3b349fbf@syzkaller.appspotmail.com\nFixes: 4a3223f7bfda (\"io_uring/net: switch io_recv() to using io_async_msghdr\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc2",
        "release_date": "2024-05-30 14:04:37 -0600 io_uring/net: assign kmsg inq/flags before buffer selection"
    },
    {
        "commit": "e112311615a24e1618a591c73506571dc304eb8d",
        "message": "kmemleak shows that there is a memory leak in io_uring read operation,\nwhere a buffer is allocated at iovec import, but never de-allocated.\n\nThe memory is allocated at io_async_rw->free_iovec, but, then\nio_async_rw is kfreed, taking the allocated memory with it. I saw this\nhappening when the read operation fails with -11 (EAGAIN).\n\nThis is the kmemleak splat.\n\n    unreferenced object 0xffff8881da591c00 (size 256):\n...\n      backtrace (crc 7a15bdee):\n\t[<00000000256f2de4>] __kmalloc+0x2d6/0x410\n\t[<000000007a9f5fc7>] iovec_from_user.part.0+0xc6/0x160\n\t[<00000000cecdf83a>] __import_iovec+0x50/0x220\n\t[<00000000d1d586a2>] __io_import_iovec+0x13d/0x220\n\t[<0000000054ee9bd2>] io_prep_rw+0x186/0x340\n\t[<00000000a9c0372d>] io_prep_rwv+0x31/0x120\n\t[<000000001d1170b9>] io_prep_readv+0xe/0x30\n\t[<0000000070b8eb67>] io_submit_sqes+0x1bd/0x780\n\t[<00000000812496d4>] __do_sys_io_uring_enter+0x3ed/0x5b0\n\t[<0000000081499602>] do_syscall_64+0x5d/0x170\n\t[<00000000de1c5a4d>] entry_SYSCALL_64_after_hwframe+0x76/0x7e\n\nThis occurs because the async data cleanup functions are not set for\nread/write operations. As a result, the potentially allocated iovec in\nthe rw async data is not freed before the async data is released,\nleading to a memory leak.\n\nWith this following patch, kmemleak does not show the leaked memory\nanymore, and all liburing tests pass.\n\nFixes: a9165b83c193 (\"io_uring/rw: always setup io_async_rw for read/write requests\")\nSigned-off-by: Breno Leitao <leitao@debian.org>\nLink: https://lore.kernel.org/r/20240530142340.1248216-1-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc2",
        "release_date": "2024-05-30 08:33:01 -0600 io_uring/rw: Free iovec before cleaning async data"
    },
    {
        "commit": "06fe9b1df1086b42718d632aa57e8f7cd1a66a21",
        "message": "If IORING_FEAT_SINGLE_MMAP is ignored, as can happen if an application\nuses an ancient liburing or does setup manually, then 3 mmap's are\nrequired to map the ring into userspace. The kernel will still have\ncollapsed the mappings, however userspace may ask for mapping them\nindividually. If so, then we should not use the full number of ring\npages, as it may exceed the partial mapping. Doing so will yield an\n-EFAULT from vm_insert_pages(), as we pass in more pages than what the\napplication asked for.\n\nCap the number of pages to match what the application asked for, for\nthe particular mapping operation.\n\nReported-by: Lucas M\u00fclling <lmulling@proton.me>\nLink: https://github.com/axboe/liburing/issues/1157\nFixes: 3ab1db3c6039 (\"io_uring: get rid of remap_pfn_range() for mapping rings/sqes\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc2",
        "release_date": "2024-05-29 09:53:14 -0600 io_uring: don't attempt to mmap larger than what the user asks for"
    },
    {
        "commit": "e4c07ec89ef5299c7bebea6640ac82bc9f7e1c95",
        "message": "Pull vfs fixes from Christian Brauner:\n\n - Fix io_uring based write-through after converting cifs to use the\n   netfs library\n\n - Fix aio error handling when doing write-through via netfs library\n\n - Fix performance regression in iomap when used with non-large folio\n   mappings\n\n - Fix signalfd error code\n\n - Remove obsolete comment in signalfd code\n\n - Fix async request indication in netfs_perform_write() by raising\n   BDP_ASYNC when IOCB_NOWAIT is set\n\n - Yield swap device immediately to prevent spurious EBUSY errors\n\n - Don't cross a .backup mountpoint from backup volumes in afs to avoid\n   infinite loops\n\n - Fix a race between umount and async request completion in 9p after 9p\n   was converted to use the netfs library\n\n* tag 'vfs-6.10-rc2.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs:\n  netfs, 9p: Fix race between umount and async request completion\n  afs: Don't cross .backup mountpoint from backup volume\n  swap: yield device immediately\n  netfs: Fix setting of BDP_ASYNC from iocb flags\n  signalfd: drop an obsolete comment\n  signalfd: fix error return code\n  iomap: fault in smaller chunks for non-large folio mappings\n  filemap: add helper mapping_max_folio_size()\n  netfs: Fix AIO error handling when doing write-through\n  netfs: Fix io_uring based write-through",
        "kernel_version": "v6.10-rc2",
        "release_date": "2024-05-27 08:09:12 -0700 Merge tag 'vfs-6.10-rc2.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs"
    },
    {
        "commit": "9b038d004ce95551cb35381c49fe896c5bc11ffe",
        "message": "This can be triggered by mounting a cifs filesystem with a cache=strict\nmount option and then, using the fsx program from xfstests, doing:\n\n        ltp/fsx -A -d -N 1000 -S 11463 -P /tmp /cifs-mount/foo \\\n          --replay-ops=gen112-fsxops\n\nWhere gen112-fsxops holds:\n\n        fallocate 0x6be7 0x8fc5 0x377d3\n        copy_range 0x9c71 0x77e8 0x2edaf 0x377d3\n        write 0x2776d 0x8f65 0x377d3\n\nThe problem is that netfs_io_request::len is being used for two purposes\nand ends up getting set to the amount of data we transferred, not the\namount of data the caller asked to be transferred (for various reasons,\nsuch as mmap'd writes, we might end up rounding out the data written to the\nserver to include the entire folio at each end).\n\nFix this by keeping the amount we were asked to write in ->len and using\n->submitted to track what we issued ops for.  Then, when we come to calling\n->ki_complete(), ->len is the right size.\n\nThis also required netfs_cleanup_dio_write() to change since we're no\nlonger advancing wreq->len.  Use wreq->transferred instead as we might have\ndone a short read.\n\nWith this, the generic/112 xfstest passes if cifs is forced to put all\nnon-DIO opens into write-through mode.\n\nFixes: 288ace2f57c9 (\"netfs: New writeback implementation\")\nSigned-off-by: David Howells <dhowells@redhat.com>\nLink: https://lore.kernel.org/r/295086.1716298663@warthog.procyon.org.uk\ncc: Jeff Layton <jlayton@kernel.org>\ncc: Steve French <stfrench@microsoft.com>\ncc: Enzo Matsumiya <ematsumiya@suse.de>\ncc: netfs@lists.linux.dev\ncc: v9fs@lists.linux.dev\ncc: linux-afs@lists.infradead.org\ncc: linux-cifs@vger.kernel.org\ncc: linux-fsdevel@vger.kernel.org\nSigned-off-by: Christian Brauner <brauner@kernel.org>",
        "kernel_version": "v6.10-rc2",
        "release_date": "2024-05-24 13:34:06 +0200 netfs: Fix io_uring based write-through"
    },
    {
        "commit": "483a351ed4d464265aed61cab4a990b0023f8400",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Single fix here for a regression in 6.9, and then a simple cleanup\n  removing some dead code\"\n\n* tag 'io_uring-6.10-20240523' of git://git.kernel.dk/linux:\n  io_uring: remove checks for NULL 'sq_offset'\n  io_uring/sqpoll: ensure that normal task_work is also run timely",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-23 13:41:49 -0700 Merge tag 'io_uring-6.10-20240523' of git://git.kernel.dk/linux"
    },
    {
        "commit": "547988ad0f9661cd9632bdebd63cf38e008b55b2",
        "message": "Since the 5.12 kernel release, nobody has been passing NULL as the\nsq_offset pointer. Remove the checks for it being NULL or not, it will\nalways be valid.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-22 11:13:44 -0600 io_uring: remove checks for NULL 'sq_offset'"
    },
    {
        "commit": "d13ddd9c893f0e8498526bf88c6b5fad01f0edd8",
        "message": "With the move to private task_work, SQPOLL neglected to also run the\nnormal task_work, if any is pending. This will eventually get run, but\nwe should run it with the private task_work to ensure that things like\na final fput() is processed in a timely fashion.\n\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/all/313824bc-799d-414f-96b7-e6de57c7e21d@gmail.com/\nReported-by: Andrew Udvare <audvare@gmail.com>\nFixes: af5d68f8892f (\"io_uring/sqpoll: manage task_work privately\")\nTested-by: Christian Heusel <christian@heusel.eu>\nTested-by: Andrew Udvare <audvare@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-21 13:41:14 -0600 io_uring/sqpoll: ensure that normal task_work is also run timely"
    },
    {
        "commit": "89721e3038d181bacbd6be54354b513fdf1b4f10",
        "message": "Pull more io_uring updates from Jens Axboe:\n \"This adds support for IORING_CQE_F_SOCK_NONEMPTY for io_uring accept\n  requests.\n\n  This is very similar to previous work that enabled the same hint for\n  doing receives on sockets. By far the majority of the work here is\n  refactoring to enable the networking side to pass back whether or not\n  the socket had more pending requests after accepting the current one,\n  the last patch just wires it up for io_uring.\n\n  Not only does this enable applications to know whether there are more\n  connections to accept right now, it also enables smarter logic for\n  io_uring multishot accept on whether to retry immediately or wait for\n  a poll trigger\"\n\n* tag 'net-accept-more-20240515' of git://git.kernel.dk/linux:\n  io_uring/net: wire up IORING_CQE_F_SOCK_NONEMPTY for accept\n  net: pass back whether socket was empty post accept\n  net: have do_accept() take a struct proto_accept_arg argument\n  net: change proto and proto_ops accept type",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-18 10:32:39 -0700 Merge tag 'net-accept-more-20240515' of git://git.kernel.dk/linux"
    },
    {
        "commit": "91b6163be404e36baea39fc978e4739fd0448ebd",
        "message": "Pull sysctl updates from Joel Granados:\n\n - Remove sentinel elements from ctl_table structs in kernel/*\n\n   Removing sentinels in ctl_table arrays reduces the build time size\n   and runtime memory consumed by ~64 bytes per array. Removals for\n   net/, io_uring/, mm/, ipc/ and security/ are set to go into mainline\n   through their respective subsystems making the next release the most\n   likely place where the final series that removes the check for\n   proc_name == NULL will land.\n\n   This adds to removals already in arch/, drivers/ and fs/.\n\n - Adjust ctl_table definitions and references to allow constification\n     - Remove unused ctl_table function arguments\n     - Move non-const elements from ctl_table to ctl_table_header\n     - Make ctl_table pointers const in ctl_table_root structure\n\n   Making the static ctl_table structs const will increase safety by\n   keeping the pointers to proc_handler functions in .rodata. Though no\n   ctl_tables where made const in this PR, the ground work for making\n   that possible has started with these changes sent by Thomas\n   Wei\u00dfschuh.\n\n* tag 'sysctl-6.10-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/sysctl/sysctl:\n  sysctl: drop now unnecessary out-of-bounds check\n  sysctl: move sysctl type to ctl_table_header\n  sysctl: drop sysctl_is_perm_empty_ctl_table\n  sysctl: treewide: constify argument ctl_table_root::permissions(table)\n  sysctl: treewide: drop unused argument ctl_table_root::set_ownership(table)\n  bpf: Remove the now superfluous sentinel elements from ctl_table array\n  delayacct: Remove the now superfluous sentinel elements from ctl_table array\n  kprobes: Remove the now superfluous sentinel elements from ctl_table array\n  printk: Remove the now superfluous sentinel elements from ctl_table array\n  scheduler: Remove the now superfluous sentinel elements from ctl_table array\n  seccomp: Remove the now superfluous sentinel elements from ctl_table array\n  timekeeping: Remove the now superfluous sentinel elements from ctl_table array\n  ftrace: Remove the now superfluous sentinel elements from ctl_table array\n  umh: Remove the now superfluous sentinel elements from ctl_table array\n  kernel misc: Remove the now superfluous sentinel elements from ctl_table array",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-17 17:31:24 -0700 Merge tag 'sysctl-6.10-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/sysctl/sysctl"
    },
    {
        "commit": "ac287da2e0ea5be2523222981efec86f0ca977cd",
        "message": "If the given protocol supports passing back whether or not we had more\npending accept post this one, pass back this information to userspace.\nThis is done by setting IORING_CQE_F_SOCK_NONEMPTY in the CQE flags,\njust like we do for recv/recvmsg if there's more data available post\na receive operation.\n\nWe can also use this information to be smarter about multishot retry,\nas we don't need to do a pointless retry if we know for a fact that\nthere aren't any more connections to accept.\n\nSuggested-by: Norman Maurer <norman_maurer@apple.com>\nAcked-by: Jakub Kicinski <kuba@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-13 18:19:23 -0600 io_uring/net: wire up IORING_CQE_F_SOCK_NONEMPTY for accept"
    },
    {
        "commit": "9961a785944601e32f185ea696347b22ffda634c",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - Greatly improve send zerocopy performance, by enabling coalescing of\n   sent buffers.\n\n   MSG_ZEROCOPY already does this with send(2) and sendmsg(2), but the\n   io_uring side did not. In local testing, the crossover point for send\n   zerocopy being faster is now around 3000 byte packets, and it\n   performs better than the sync syscall variants as well.\n\n   This feature relies on a shared branch with net-next, which was\n   pulled into both branches.\n\n - Unification of how async preparation is done across opcodes.\n\n   Previously, opcodes that required extra memory for async retry would\n   allocate that as needed, using on-stack state until that was the\n   case. If async retry was needed, the on-stack state was adjusted\n   appropriately for a retry and then copied to the allocated memory.\n\n   This led to some fragile and ugly code, particularly for read/write\n   handling, and made storage retries more difficult than they needed to\n   be. Allocate the memory upfront, as it's cheap from our pools, and\n   use that state consistently both initially and also from the retry\n   side.\n\n - Move away from using remap_pfn_range() for mapping the rings.\n\n   This is really not the right interface to use and can cause lifetime\n   issues or leaks. Additionally, it means the ring sq/cq arrays need to\n   be physically contigious, which can cause problems in production with\n   larger rings when services are restarted, as memory can be very\n   fragmented at that point.\n\n   Move to using vm_insert_page(s) for the ring sq/cq arrays, and apply\n   the same treatment to mapped ring provided buffers. This also helps\n   unify the code we have dealing with allocating and mapping memory.\n\n   Hard to see in the diffstat as we're adding a few features as well,\n   but this kills about ~400 lines of code from the codebase as well.\n\n - Add support for bundles for send/recv.\n\n   When used with provided buffers, bundles support sending or receiving\n   more than one buffer at the time, improving the efficiency by only\n   needing to call into the networking stack once for multiple sends or\n   receives.\n\n - Tweaks for our accept operations, supporting both a DONTWAIT flag for\n   skipping poll arm and retry if we can, and a POLLFIRST flag that the\n   application can use to skip the initial accept attempt and rely\n   purely on poll for triggering the operation. Both of these have\n   identical flags on the receive side already.\n\n - Make the task_work ctx locking unconditional.\n\n   We had various code paths here that would do a mix of lock/trylock\n   and set the task_work state to whether or not it was locked. All of\n   that goes away, we lock it unconditionally and get rid of the state\n   flag indicating whether it's locked or not.\n\n   The state struct still exists as an empty type, can go away in the\n   future.\n\n - Add support for specifying NOP completion values, allowing it to be\n   used for error handling testing.\n\n - Use set/test bit for io-wq worker flags. Not strictly needed, but\n   also doesn't hurt and helps silence a KCSAN warning.\n\n - Cleanups for io-wq locking and work assignments, closing a tiny race\n   where cancelations would not be able to find the work item reliably.\n\n - Misc fixes, cleanups, and improvements\n\n* tag 'for-6.10/io_uring-20240511' of git://git.kernel.dk/linux: (97 commits)\n  io_uring: support to inject result for NOP\n  io_uring: fail NOP if non-zero op flags is passed in\n  io_uring/net: add IORING_ACCEPT_POLL_FIRST flag\n  io_uring/net: add IORING_ACCEPT_DONTWAIT flag\n  io_uring/filetable: don't unnecessarily clear/reset bitmap\n  io_uring/io-wq: Use set_bit() and test_bit() at worker->flags\n  io_uring/msg_ring: cleanup posting to IOPOLL vs !IOPOLL ring\n  io_uring: Require zeroed sqe->len on provided-buffers send\n  io_uring/notif: disable LAZY_WAKE for linked notifs\n  io_uring/net: fix sendzc lazy wake polling\n  io_uring/msg_ring: reuse ctx->submitter_task read using READ_ONCE instead of re-reading it\n  io_uring/rw: reinstate thread check for retries\n  io_uring/notif: implement notification stacking\n  io_uring/notif: simplify io_notif_flush()\n  net: add callback for setting a ubuf_info to skb\n  net: extend ubuf_info callback to ops structure\n  io_uring/net: support bundles for recv\n  io_uring/net: support bundles for send\n  io_uring/kbuf: add helpers for getting/peeking multiple buffers\n  io_uring/net: add provided buffer support for IORING_OP_SEND\n  ...",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-13 12:48:06 -0700 Merge tag 'for-6.10/io_uring-20240511' of git://git.kernel.dk/linux"
    },
    {
        "commit": "f4e8d80292859809ea135e9f4c43bae47e4f58bc",
        "message": "Pull vfs rw iterator updates from Christian Brauner:\n \"The core fs signalfd, userfaultfd, and timerfd subsystems did still\n  use f_op->read() instead of f_op->read_iter(). Convert them over since\n  we should aim to get rid of f_op->read() at some point.\n\n  Aside from that io_uring and others want to mark files as FMODE_NOWAIT\n  so it can make use of per-IO nonblocking hints to enable more\n  efficient IO. Converting those users to f_op->read_iter() allows them\n  to be marked with FMODE_NOWAIT\"\n\n* tag 'vfs-6.10.rw' of git://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs:\n  signalfd: convert to ->read_iter()\n  userfaultfd: convert to ->read_iter()\n  timerfd: convert to ->read_iter()\n  new helper: copy_to_iter_full()",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-13 12:23:17 -0700 Merge tag 'vfs-6.10.rw' of git://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs"
    },
    {
        "commit": "ad1978dbbd827c1a1a7d22d9cc9ba71989dae48a",
        "message": "* for-6.10/io_uring: (97 commits)\n  io_uring: support to inject result for NOP\n  io_uring: fail NOP if non-zero op flags is passed in\n  io_uring/net: add IORING_ACCEPT_POLL_FIRST flag\n  io_uring/net: add IORING_ACCEPT_DONTWAIT flag\n  io_uring/filetable: don't unnecessarily clear/reset bitmap\n  io_uring/io-wq: Use set_bit() and test_bit() at worker->flags\n  io_uring/msg_ring: cleanup posting to IOPOLL vs !IOPOLL ring\n  io_uring: Require zeroed sqe->len on provided-buffers send\n  io_uring/notif: disable LAZY_WAKE for linked notifs\n  io_uring/net: fix sendzc lazy wake polling\n  io_uring/msg_ring: reuse ctx->submitter_task read using READ_ONCE instead of re-reading it\n  io_uring/rw: reinstate thread check for retries\n  io_uring/notif: implement notification stacking\n  io_uring/notif: simplify io_notif_flush()\n  net: add callback for setting a ubuf_info to skb\n  net: extend ubuf_info callback to ops structure\n  io_uring/net: support bundles for recv\n  io_uring/net: support bundles for send\n  io_uring/kbuf: add helpers for getting/peeking multiple buffers\n  io_uring/net: add provided buffer support for IORING_OP_SEND\n  ...",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-11 08:25:50 -0600 Merge branch 'for-6.10/io_uring' into net-accept-more"
    },
    {
        "commit": "deb1e496a83557896fe0cca0b8af01c2a97c0dc6",
        "message": "Support to inject result for NOP so that we can inject failure from\nuserspace. It is very helpful for covering failure handling code in\nio_uring core change.\n\nWith nop flags, it becomes possible to add more test features on NOP in\nfuture.\n\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20240510035031.78874-3-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-10 06:09:45 -0600 io_uring: support to inject result for NOP"
    },
    {
        "commit": "3d8f874bd620ce03f75a5512847586828ab86544",
        "message": "The NOP op flags should have been checked from beginning like any other\nopcode, otherwise NOP may not be extended with the op flags.\n\nGiven both liburing and Rust io-uring crate always zeros SQE op flags, just\nignore users which play raw NOP uring interface without zeroing SQE, because\nNOP is just for test purpose. Then we can save one NOP2 opcode.\n\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nFixes: 2b188cc1bb85 (\"Add io_uring IO interface\")\nCc: stable@vger.kernel.org\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20240510035031.78874-2-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-10 06:09:45 -0600 io_uring: fail NOP if non-zero op flags is passed in"
    },
    {
        "commit": "d3da8e98592693811c14c31f05380f378411fea1",
        "message": "Similarly to how polling first is supported for receive, it makes sense\nto provide the same for accept. An accept operation does a lot of\nexpensive setup, like allocating an fd, a socket/inode, etc. If no\nconnection request is already pending, this is wasted and will just be\ncleaned up and freed, only to retry via the usual poll trigger.\n\nAdd IORING_ACCEPT_POLL_FIRST, which tells accept to only initiate the\naccept request if poll says we have something to accept.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-09 12:22:11 -0600 io_uring/net: add IORING_ACCEPT_POLL_FIRST flag"
    },
    {
        "commit": "7dcc758cca432510f77b2fe1077be2314bc3785b",
        "message": "This allows the caller to perform a non-blocking attempt, similarly to\nhow recvmsg has MSG_DONTWAIT. If set, and we get -EAGAIN on a connection\nattempt, propagate the result to userspace rather than arm poll and\nwait for a retry.\n\nSuggested-by: Norman Maurer <norman_maurer@apple.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-09 12:22:03 -0600 io_uring/net: add IORING_ACCEPT_DONTWAIT flag"
    },
    {
        "commit": "340f634aa43d4172771a784da31e5d4c7c7d3126",
        "message": "If we're updating an existing slot, we clear the slot bitmap only to\nset it again right after. Just leave the bit set rather than toggle\nit off and on, and move the unused slot setting into the branch of\nnot already having a file occupy this slot.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-08 08:27:45 -0600 io_uring/filetable: don't unnecessarily clear/reset bitmap"
    },
    {
        "commit": "8d2a83a97f1c86d23161b4d2e37e1b2c5220710a",
        "message": "Before deciding if we can do a NOCOW write into a range, one of the things\nwe have to do is check if there are checksum items for that range. We do\nthat through the btrfs_lookup_csums_list() function, which searches for\nchecksums and adds them to a list supplied by the caller.\n\nBut all we need is to check if there is any checksum, we don't need to\nlook for all of them and collect them into a list, which requires more\nsearch time in the checksums tree, allocating memory for checksums items\nto add to the list, copy checksums from a leaf into those list items,\nthen free that memory, etc. This is all unnecessary overhead, wasting\nmostly CPU time, and perhaps some occasional IO if we need to read from\ndisk any extent buffers.\n\nSo change btrfs_lookup_csums_list() to allow to return immediately in\ncase it finds any checksum, without the need to add it to a list and read\nit from a leaf. This is accomplished by allowing a NULL list parameter and\nmaking the function return 1 if it found any checksum, 0 if it didn't\nfound any, and a negative value in case of an error.\n\nThe following test with fio was used to measure performance:\n\n  $ cat test.sh\n  #!/bin/bash\n\n  DEV=/dev/nullb0\n  MNT=/mnt/nullb0\n\n  cat <<EOF > /tmp/fio-job.ini\n  [global]\n  name=fio-rand-write\n  filename=$MNT/fio-rand-write\n  rw=randwrite\n  bssplit=4k/20:8k/20:16k/20:32k/20:64k/20\n  direct=1\n  numjobs=16\n  fallocate=posix\n  time_based\n  runtime=300\n\n  [file1]\n  size=8G\n  ioengine=io_uring\n  iodepth=16\n  EOF\n\n  umount $MNT &> /dev/null\n  mkfs.btrfs -f $DEV\n  mount -o ssd $DEV $MNT\n\n  fio /tmp/fio-job.ini\n  umount $MNT\n\nThe test was run on a release kernel (Debian's default kernel config).\n\nThe results before this patch:\n\n  WRITE: bw=139MiB/s (146MB/s), 8204KiB/s-9504KiB/s (8401kB/s-9732kB/s), io=17.0GiB (18.3GB), run=125317-125344msec\n\nThe results after this patch:\n\n  WRITE: bw=153MiB/s (160MB/s), 9241KiB/s-10.0MiB/s (9463kB/s-10.5MB/s), io=17.0GiB (18.3GB), run=114054-114071msec\n\nReviewed-by: Qu Wenruo <wqu@suse.com>\nSigned-off-by: Filipe Manana <fdmanana@suse.com>\nReviewed-by: David Sterba <dsterba@suse.com>\nSigned-off-by: David Sterba <dsterba@suse.com>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-07 21:31:03 +0200 btrfs: make NOCOW checks for existence of checksums in a range more efficient"
    },
    {
        "commit": "8a565304927fbd28c9f028c492b5c1714002cbab",
        "message": "Utilize set_bit() and test_bit() on worker->flags within io_uring/io-wq\nto address potential data races.\n\nThe structure io_worker->flags may be accessed through various data\npaths, leading to concurrency issues. When KCSAN is enabled, it reveals\ndata races occurring in io_worker_handle_work and\nio_wq_activate_free_worker functions.\n\n\t BUG: KCSAN: data-race in io_worker_handle_work / io_wq_activate_free_worker\n\t write to 0xffff8885c4246404 of 4 bytes by task 49071 on cpu 28:\n\t io_worker_handle_work (io_uring/io-wq.c:434 io_uring/io-wq.c:569)\n\t io_wq_worker (io_uring/io-wq.c:?)\n<snip>\n\n\t read to 0xffff8885c4246404 of 4 bytes by task 49024 on cpu 5:\n\t io_wq_activate_free_worker (io_uring/io-wq.c:? io_uring/io-wq.c:285)\n\t io_wq_enqueue (io_uring/io-wq.c:947)\n\t io_queue_iowq (io_uring/io_uring.c:524)\n\t io_req_task_submit (io_uring/io_uring.c:1511)\n\t io_handle_tw_list (io_uring/io_uring.c:1198)\n<snip>\n\nLine numbers against commit 18daea77cca6 (\"Merge tag 'for-linus' of\ngit://git.kernel.org/pub/scm/virt/kvm/kvm\").\n\nThese races involve writes and reads to the same memory location by\ndifferent tasks running on different CPUs. To mitigate this, refactor\nthe code to use atomic operations such as set_bit(), test_bit(), and\nclear_bit() instead of basic \"and\" and \"or\" operations. This ensures\nthread-safe manipulation of worker flags.\n\nAlso, move `create_index` to avoid holes in the structure.\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nLink: https://lore.kernel.org/r/20240507170002.2269003-1-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-07 13:17:19 -0600 io_uring/io-wq: Use set_bit() and test_bit() at worker->flags"
    },
    {
        "commit": "59b28a6e37e650c0d601ed87875b6217140cda5d",
        "message": "Move the posting outside the checking and locking, it's cleaner that\nway.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-01 17:13:51 -0600 io_uring/msg_ring: cleanup posting to IOPOLL vs !IOPOLL ring"
    },
    {
        "commit": "79996b45f7b28c0e3e08a95bab80119e95317e28",
        "message": "When sending from a provided buffer, we set sr->len to be the smallest\nbetween the actual buffer size and sqe->len.  But, now that we\ndisconnect the buffer from the submission request, we can get in a\nsituation where the buffers and requests mismatch, and only part of a\nbuffer gets sent.  Assume:\n\n* buf[1]->len = 128; buf[2]->len = 256\n* sqe[1]->len = 128; sqe[2]->len = 256\n\nIf sqe1 runs first, it picks buff[1] and it's all good. But, if sqe[2]\nruns first, sqe[1] picks buff[2], and the last half of buff[2] is\nnever sent.\n\nWhile arguably the use-case of different-length sends is questionable,\nit has already raised confusion with potential users of this\nfeature. Let's make the interface less tricky by forcing the length to\nonly come from the buffer ring entry itself.\n\nFixes: ac5f71a3d9d7 (\"io_uring/net: add provided buffer support for IORING_OP_SEND\")\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-05-01 14:56:36 -0600 io_uring: Require zeroed sqe->len on provided-buffers send"
    },
    {
        "commit": "19352a1d395424b5f8c03289a85fbd6622d6601a",
        "message": "Notifications may now be linked and thus a single tw can post multiple\nCQEs, it's not safe to use LAZY_WAKE with them. Disable LAZY_WAKE for\nnow, if that'd prove to be a problem we can count them and pass the\nexpected number of CQEs into __io_req_task_work_add().\n\nFixes: 6fe4220912d19 (\"io_uring/notif: implement notification stacking\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0a5accdb7d2d0d27ebec14f8106e14e0192fae17.1714488419.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-30 13:06:27 -0600 io_uring/notif: disable LAZY_WAKE for linked notifs"
    },
    {
        "commit": "ef42b85a5609cd822ca0a68dd2bef2b12b5d1ca3",
        "message": "SEND[MSG]_ZC produces multiple CQEs via notifications, LAZY_WAKE doesn't\nhandle it and so disable LAZY_WAKE for sendzc polling. It should be\nfine, sends are not likely to be polled in the first place.\n\nFixes: 6ce4a93dbb5b (\"io_uring/poll: use IOU_F_TWQ_LAZY_WAKE for wakeups\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5b360fb352d91e3aec751d75c87dfb4753a084ee.1714488419.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-30 13:06:27 -0600 io_uring/net: fix sendzc lazy wake polling"
    },
    {
        "commit": "a4d416dc60980f741f0bfa1f34a1059c498c1b4e",
        "message": "In io_msg_exec_remote(), ctx->submitter_task is read using READ_ONCE at\nthe beginning of the function, checked, and then re-read from\nctx->submitter_task, voiding all guarantees of the checks. Reuse the value\nthat was read by READ_ONCE to ensure the consistency of the task struct\nthroughout the function.\n\nSigned-off-by: linke li <lilinke99@qq.com>\nLink: https://lore.kernel.org/r/tencent_F9B2296C93928D6F68FF0C95C33475C68209@qq.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-26 07:40:12 -0600 io_uring/msg_ring: reuse ctx->submitter_task read using READ_ONCE instead of re-reading it"
    },
    {
        "commit": "039a2e800bcd5beb89909d1a488abf3d647642cf",
        "message": "Allowing retries for everything is arguably the right thing to do, now\nthat every command type is async read from the start. But it's exposed a\nfew issues around missing check for a retry (which cca6571381a0 exposed),\nand the fixup commit for that isn't necessarily 100% sound in terms of\niov_iter state.\n\nFor now, just revert these two commits. This unfortunately then re-opens\nthe fact that -EAGAIN can get bubbled to userspace for some cases where\nthe kernel very well could just sanely retry them. But until we have all\nthe conditions covered around that, we cannot safely enable that.\n\nThis reverts commit df604d2ad480fcf7b39767280c9093e13b1de952.\nThis reverts commit cca6571381a0bdc88021a1f7a4c2349df21279f7.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-25 09:04:32 -0600 io_uring/rw: reinstate thread check for retries"
    },
    {
        "commit": "6fe4220912d19152a26ce19713ab232f4263018d",
        "message": "The network stack allows only one ubuf_info per skb, and unlike\nMSG_ZEROCOPY, each io_uring zerocopy send will carry a separate\nubuf_info. That means that send requests can't reuse a previosly\nallocated skb and need to get one more or more of new ones. That's fine\nfor large sends, but otherwise it would spam the stack with lots of skbs\ncarrying just a little data each.\n\nTo help with that implement linking notification (i.e. an io_uring wrapper\naround ubuf_info) into a list. Each is refcounted by skbs and the stack\nas usual. additionally all non head entries keep a reference to the\nhead, which they put down when their refcount hits 0. When the head have\nno more users, it'll efficiently put all notifications in a batch.\n\nAs mentioned previously about ->io_link_skb, the callback implementation\nalways allows to bind to an skb without a ubuf_info.\n\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bf1e7f9b72f9ecc99999fdc0d2cded5eea87fd0b.1713369317.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-22 19:31:18 -0600 io_uring/notif: implement notification stacking"
    },
    {
        "commit": "5a569469b973cb7a6c58192a37dfb8418686e518",
        "message": "io_notif_flush() is partially duplicating io_tx_ubuf_complete(), so\ninstead of duplicating it, make the flush call io_tx_ubuf_complete.\n\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/19e41652c16718b946a5c80d2ad409df7682e47e.1713369317.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-22 19:31:18 -0600 io_uring/notif: simplify io_notif_flush()"
    },
    {
        "commit": "3830fff39941bd41f50d3bb8355883276d7b0771",
        "message": "Merge net changes required for the upcoming send zerocopy improvements.\n\n* 'for-uring-ubufops' of git://git.kernel.org/pub/scm/linux/kernel/git/kuba/linux:\n  net: add callback for setting a ubuf_info to skb\n  net: extend ubuf_info callback to ops structure\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-22 19:30:05 -0600 Merge branch 'for-uring-ubufops' of git://git.kernel.org/pub/scm/linux/kernel/git/kuba/linux into for-6.10/io_uring"
    },
    {
        "commit": "af046fd169d43ef0d5f8006954fa4b2fc90974af",
        "message": "Pavel Begunkov says:\n\n====================\nimplement io_uring notification (ubuf_info) stacking (net part)\n\nTo have per request buffer notifications each zerocopy io_uring send\nrequest allocates a new ubuf_info. However, as an skb can carry only\none uarg, it may force the stack to create many small skbs hurting\nperformance in many ways.\n\nThe patchset implements notification, i.e. an io_uring's ubuf_info\nextension, stacking. It attempts to link ubuf_info's into a list,\nallowing to have multiple of them per skb.\n\nliburing/examples/send-zerocopy shows up 6 times performance improvement\nfor TCP with 4KB bytes per send, and levels it with MSG_ZEROCOPY. Without\nthe patchset it requires much larger sends to utilise all potential.\n\nbytes  | before | after (Kqps)\n1200   | 195    | 1023\n4000   | 193    | 1386\n8000   | 154    | 1058\n====================\n\nLink: https://lore.kernel.org/all/cover.1713369317.git.asml.silence@gmail.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-22 17:15:39 -0700 Merge branch 'for-uring-ubufops' into HEAD"
    },
    {
        "commit": "65bada80dec1f2108a751644773b2120bd789934",
        "message": "At the moment an skb can only have one ubuf_info associated with it,\nwhich might be a performance problem for zerocopy sends in cases like\nTCP via io_uring. Add a callback for assigning ubuf_info to skb, this\nway we will implement smarter assignment later like linking ubuf_info\ntogether.\n\nNote, it's an optional callback, which should be compatible with\nskb_zcopy_set(), that's because the net stack might potentially decide\nto clone an skb and take another reference to ubuf_info whenever it\nwishes. Also, a correct implementation should always be able to bind to\nan skb without prior ubuf_info, otherwise we could end up in a situation\nwhen the send would not be able to progress.\n\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nReviewed-by: David Ahern <dsahern@kernel.org>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Willem de Bruijn <willemb@google.com>\nLink: https://lore.kernel.org/all/b7918aadffeb787c84c9e72e34c729dc04f3a45d.1713369317.git.asml.silence@gmail.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-22 16:21:59 -0700 net: add callback for setting a ubuf_info to skb"
    },
    {
        "commit": "7ab4f16f9e2440e797eae88812f800458e5879d2",
        "message": "We'll need to associate additional callbacks with ubuf_info, introduce\na structure holding ubuf_info callbacks. Apart from a more smarter\nio_uring notification management introduced in next patches, it can be\nused to generalise msg_zerocopy_put_abort() and also store\n->sg_from_iter, which is currently passed in struct msghdr.\n\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nReviewed-by: David Ahern <dsahern@kernel.org>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Willem de Bruijn <willemb@google.com>\nLink: https://lore.kernel.org/all/a62015541de49c0e2a8a0377a1d5d0a5aeb07016.1713369317.git.asml.silence@gmail.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-22 16:21:35 -0700 net: extend ubuf_info callback to ops structure"
    },
    {
        "commit": "2f9c9515bdfde9e4df1f35782284074d3625ff8a",
        "message": "If IORING_OP_RECV is used with provided buffers, the caller may also set\nIORING_RECVSEND_BUNDLE to turn it into a multi-buffer recv. This grabs\nbuffers available and receives into them, posting a single completion for\nall of it.\n\nThis can be used with multishot receive as well, or without it.\n\nNow that both send and receive support bundles, add a feature flag for\nit as well. If IORING_FEAT_RECVSEND_BUNDLE is set after registering the\nring, then the kernel supports bundles for recv and send.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-22 11:26:11 -0600 io_uring/net: support bundles for recv"
    },
    {
        "commit": "a05d1f625c7aa681d8816bc0f10089289ad07aad",
        "message": "If IORING_OP_SEND is used with provided buffers, the caller may also\nset IORING_RECVSEND_BUNDLE to turn it into a multi-buffer send. The idea\nis that an application can fill outgoing buffers in a provided buffer\ngroup, and then arm a single send that will service them all. Once\nthere are no more buffers to send, or if the requested length has\nbeen sent, the request posts a single completion for all the buffers.\n\nThis only enables it for IORING_OP_SEND, IORING_OP_SENDMSG is coming\nin a separate patch. However, this patch does do a lot of the prep\nwork that makes wiring up the sendmsg variant pretty trivial. They\nshare the prep side.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-22 11:26:05 -0600 io_uring/net: support bundles for send"
    },
    {
        "commit": "35c8711c8fc4c16ad2749b8314da5829a493e28e",
        "message": "Our provided buffer interface only allows selection of a single buffer.\nAdd an API that allows getting/peeking multiple buffers at the same time.\n\nThis is only implemented for the ring provided buffers. It could be added\nfor the legacy provided buffers as well, but since it's strongly\nencouraged to use the new interface, let's keep it simpler and just\nprovide it for the new API. The legacy interface will always just select\na single buffer.\n\nThere are two new main functions:\n\nio_buffers_select(), which selects up as many buffers as it can. The\ncaller supplies the iovec array, and io_buffers_select() may allocate a\nbigger array if the 'out_len' being passed in is non-zero and bigger\nthan what fits in the provided iovec. Buffers grabbed with this helper\nare permanently assigned.\n\nio_buffers_peek(), which works like io_buffers_select(), except they can\nbe recycled, if needed. Callers using either of these functions should\ncall io_put_kbufs() rather than io_put_kbuf() at completion time. The\npeek interface must be called with the ctx locked from peek to\ncompletion.\n\nThis add a bit state for the request:\n\n- REQ_F_BUFFERS_COMMIT, which means that the the buffers have been\n  peeked and should be committed to the buffer ring head when they are\n  put as part of completion. Prior to this, req->buf_list was cleared to\n  NULL when committed.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-22 11:26:01 -0600 io_uring/kbuf: add helpers for getting/peeking multiple buffers"
    },
    {
        "commit": "ac5f71a3d9d7eb540f6bf7e794eb4a3e4c3f11dd",
        "message": "It's pretty trivial to wire up provided buffer support for the send\nside, just like how it's done the receive side. This enables setting up\na buffer ring that an application can use to push pending sends to,\nand then have a send pick a buffer from that ring.\n\nOne of the challenges with async IO and networking sends is that you\ncan get into reordering conditions if you have more than one inflight\nat the same time. Consider the following scenario where everything is\nfine:\n\n1) App queues sendA for socket1\n2) App queues sendB for socket1\n3) App does io_uring_submit()\n4) sendA is issued, completes successfully, posts CQE\n5) sendB is issued, completes successfully, posts CQE\n\nAll is fine. Requests are always issued in-order, and both complete\ninline as most sends do.\n\nHowever, if we're flooding socket1 with sends, the following could\nalso result from the same sequence:\n\n1) App queues sendA for socket1\n2) App queues sendB for socket1\n3) App does io_uring_submit()\n4) sendA is issued, socket1 is full, poll is armed for retry\n5) Space frees up in socket1, this triggers sendA retry via task_work\n6) sendB is issued, completes successfully, posts CQE\n7) sendA is retried, completes successfully, posts CQE\n\nNow we've sent sendB before sendA, which can make things unhappy. If\nboth sendA and sendB had been using provided buffers, then it would look\nas follows instead:\n\n1) App queues dataA for sendA, queues sendA for socket1\n2) App queues dataB for sendB queues sendB for socket1\n3) App does io_uring_submit()\n4) sendA is issued, socket1 is full, poll is armed for retry\n5) Space frees up in socket1, this triggers sendA retry via task_work\n6) sendB is issued, picks first buffer (dataA), completes successfully,\n   posts CQE (which says \"I sent dataA\")\n7) sendA is retried, picks first buffer (dataB), completes successfully,\n   posts CQE (which says \"I sent dataB\")\n\nNow we've sent the data in order, and everybody is happy.\n\nIt's worth noting that this also opens the door for supporting multishot\nsends, as provided buffers would be a prerequisite for that. Those can\ntrigger either when new buffers are added to the outgoing ring, or (if\nstalled due to lack of space) when space frees up in the socket.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-22 11:25:56 -0600 io_uring/net: add provided buffer support for IORING_OP_SEND"
    },
    {
        "commit": "3e747dedd47b6250390abfc08dc0aa4817d3c052",
        "message": "This is just moving io_recv_prep_retry() higher up so it can get used\nfor sends as well, and rename it to be generically useful for both\nsends and receives.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-22 11:25:49 -0600 io_uring/net: add generic multishot retry helper"
    },
    {
        "commit": "df604d2ad480fcf7b39767280c9093e13b1de952",
        "message": "A previous commit removed the checking on whether or not it was possible\nto retry a request, since it's now possible to retry any of them. This\nwould previously have caused the request to have been ended with an error,\nbut now the retry condition can simply get lost instead.\n\nCleanup the retry handling and always just punt it to task_work, which\nwill queue it with io-wq appropriately.\n\nReported-by: Changhui Zhong <czhong@redhat.com>\nTested-by: Ming Lei <ming.lei@redhat.com>\nFixes: cca6571381a0 (\"io_uring/rw: cleanup retry path\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-17 09:23:55 -0600 io_uring/rw: ensure retry condition isn't lost"
    },
    {
        "commit": "c4ce0ab27646f4206a9eb502d6fe45cb080e1cae",
        "message": "kmemleak complains that there's a memory leak related to connect\nhandling:\n\nunreferenced object 0xffff0001093bdf00 (size 128):\ncomm \"iou-sqp-455\", pid 457, jiffies 4294894164\nhex dump (first 32 bytes):\n02 00 fa ea 7f 00 00 01 00 00 00 00 00 00 00 00  ................\n00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................\nbacktrace (crc 2e481b1a):\n[<00000000c0a26af4>] kmemleak_alloc+0x30/0x38\n[<000000009c30bb45>] kmalloc_trace+0x228/0x358\n[<000000009da9d39f>] __audit_sockaddr+0xd0/0x138\n[<0000000089a93e34>] move_addr_to_kernel+0x1a0/0x1f8\n[<000000000b4e80e6>] io_connect_prep+0x1ec/0x2d4\n[<00000000abfbcd99>] io_submit_sqes+0x588/0x1e48\n[<00000000e7c25e07>] io_sq_thread+0x8a4/0x10e4\n[<00000000d999b491>] ret_from_fork+0x10/0x20\n\nwhich can can happen if:\n\n1) The command type does something on the prep side that triggers an\n   audit call.\n2) The thread hasn't done any operations before this that triggered\n   an audit call inside ->issue(), where we have audit_uring_entry()\n   and audit_uring_exit().\n\nWork around this by issuing a blanket NOP operation before the SQPOLL\ndoes anything.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 13:06:19 -0600 io_uring/sqpoll: work around a potential audit memory leak"
    },
    {
        "commit": "d6e295061f239bee48c9e49313f68042121e21c2",
        "message": "->account_pages is the number of pages we account against the user\nderived from unsigned len, it definitely fits into unsigned, which saves\nsome space in struct io_notif_data.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/19f2687fcb36daa74d86f4a27bfb3d35cffec318.1713185320.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:49 -0600 io_uring/notif: shrink account_pages to u32"
    },
    {
        "commit": "2e730d8de45768810df4a6859cd64c5387cf0131",
        "message": "We don't need ctx in the hottest path, i.e. registered buffers,\nlet's get it only when we need it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e7345e268ffaeaf79b4c8f3a5d019d6a87a3d1f1.1713185320.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:49 -0600 io_uring/notif: remove ctx var from io_notif_tw_complete"
    },
    {
        "commit": "7e58d0af5a587e74f46f55b91a0197f750eba78c",
        "message": "Flip the dec_and_test \"if\", that makes the function extension easier in\nthe future.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/43939e2b04dff03bff5d7227c98afedf951227b3.1713185320.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:49 -0600 io_uring/notif: refactor io_tx_ubuf_complete()"
    },
    {
        "commit": "686b56cbeedc9f4c72f9bb781918194a9a3e8334",
        "message": "A previous consolidation cleanup missed handling the case where the ring\nis dying, and __io_cqring_overflow_flush() doesn't flush entries if the\nCQ ring is already full. This is fine for the normal CQE overflow\nflushing, but if the ring is going away, we need to flush everything,\neven if it means simply freeing the overflown entries.\n\nFixes: 6c948ec44b29 (\"io_uring: consolidate overflow flushing\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:27 -0600 io_uring: ensure overflow entries are dropped when ring is exiting"
    },
    {
        "commit": "4d0f4a5413490391c6cd16407a0f71b51700a68a",
        "message": "In the __io_timeout_prep function, the io_timeout list is initialized\ntwice, removing the meaningless second initialization.\n\nSigned-off-by: Ruyi Zhang <ruyi.zhang@samsung.com>\nLink: https://lore.kernel.org/r/20240411055953.2029218-1-ruyi.zhang@samsung.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:27 -0600 io_uring/timeout: remove duplicate initialization of the io_timeout list."
    },
    {
        "commit": "6b231248e97fc37d4205449d48747b5a3b4c2fcc",
        "message": "Consolidate __io_cqring_overflow_flush and io_cqring_overflow_kill()\ninto a single function as it once was, it's easier to work with it this\nway.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/986b42c35e76a6be7aa0cdcda0a236a2222da3a7.1712708261.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:27 -0600 io_uring: consolidate overflow flushing"
    },
    {
        "commit": "8d09a88ef9d3cb7d21d45c39b7b7c31298d23998",
        "message": "Conditional locking is never great, in case of\n__io_cqring_overflow_flush(), which is a slow path, it's not justified.\nDon't handle IOPOLL separately, always grab uring_lock for overflow\nflushing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/162947df299aa12693ac4b305dacedab32ec7976.1712708261.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: always lock __io_cqring_overflow_flush"
    },
    {
        "commit": "408024b959275ba52c233c647901cacfc8d5a226",
        "message": "There is only one caller of io_cqring_overflow_flush(), open code it\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a1fecd56d9dba923ed8d4d159727fa939d3baa2a.1712708261.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: open code io_cqring_overflow_flush()"
    },
    {
        "commit": "e45ec969d17acb0a1bea78c9c6c4403fceccd599",
        "message": "c1edbf5f081be (\"io_uring: flag SQPOLL busy condition to userspace\")\nadded an extra overflowed CQE flush in the SQPOLL submission path due to\nbackpressure, which was later removed. Remove the flush and let\nio_cqring_wait() / iopoll handle it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2a83b0724ca6ca9d16c7d79a51b77c81876b2e39.1712708261.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: remove extra SQPOLL overflow flush"
    },
    {
        "commit": "a5bff51850c8d533f3696d45749ab169dd49f8dd",
        "message": "There are no users of io_req_cqe_overflow() apart from io_uring.c, make\nit static.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f4295eb2f9eb98d5db38c0578f57f0b86bfe0d8c.1712708261.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: unexport io_req_cqe_overflow()"
    },
    {
        "commit": "8c9a6f549e65912825e31dc1e0e3f7995984649d",
        "message": "We're exporting some io_uring bits to networking, e.g. for implementing\na net callback for io_uring cmds, but we don't want to expose more than\nneeded. Add a separate header for networking.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: David Wei <dw@davidwei.uk>\nLink: https://lore.kernel.org/r/20240409210554.1878789-1-dw@davidwei.uk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: separate header for exported net bits"
    },
    {
        "commit": "d285da7dbd3b3cc9b4cf822039a87ca4e4106ecf",
        "message": "We can set MSG_ZEROCOPY at the preparation step, do it so we don't have\nto care about it later in the issue callback.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c2c22aaa577624977f045979a6db2b9fb2e5648c.1712534031.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring/net: set MSG_ZEROCOPY for sendzc in advance"
    },
    {
        "commit": "6b7f864bb70591b1ba8f538c13de2a8396bfec8a",
        "message": "io_notif_complete_tw_ext() can be removed and combined with\nio_notif_complete_tw to make it simpler without sacrificing\nanything.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/025a124a5e20e2474a57e2f04f16c422eb83063c.1712534031.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring/net: get rid of io_notif_complete_tw_ext"
    },
    {
        "commit": "998632921d28a6d360d2a6d7ffcfbcf4f6f17fc2",
        "message": "Splitting io_tx_ubuf_callback_ext from io_tx_ubuf_callback is a pre\nmature optimisation that doesn't give us much. Merge the functions into\none and reclaim some simplicity back.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d44d68f6f7add33a0dcf0b7fd7b73c2dc543604f.1712534031.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring/net: merge ubuf sendzc callbacks"
    },
    {
        "commit": "bbbef3e9d2a82754bd72a86ba1352cfc17bf31a7",
        "message": "The only caller doesn't handle the return value of io_put_kbuf_comp(), so\nchange its return type into void.\n\nAlso follow Jens's suggestion to rename it as io_put_kbuf_drop().\n\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20240407132759.4056167-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: return void from io_put_kbuf_comp()"
    },
    {
        "commit": "c29006a2456bc9c2aea09e4a8958b4d9a7dfcb5a",
        "message": "io_req_put_rsrc_locked() is a weird shim function around\nio_req_put_rsrc(). All calls to io_req_put_rsrc() require holding\n->uring_lock, so we can just use it directly.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a195bc78ac3d2c6fbaea72976e982fe51e50ecdd.1712331455.git.asml.silence@gmail.com\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: remove io_req_put_rsrc_locked()"
    },
    {
        "commit": "d9713ad3fa227726a0b4d544c5a4cdd393c1933e",
        "message": "io_req_complete_post() was a sole user of ->locked_free_list, but\nsince we just gutted the function, the cache is not used anymore and\ncan be removed.\n\n->locked_free_list served as an asynhronous counterpart of the main\nrequest (i.e. struct io_kiocb) cache for all unlocked cases like io-wq.\nNow they're all forced to be completed into the main cache directly,\noff of the normal completion path or via io_free_req().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7bffccd213e370abd4de480e739d8b08ab6c1326.1712331455.git.asml.silence@gmail.com\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: remove async request cache"
    },
    {
        "commit": "de96e9ae69a134c009a6d9a7ca182fa67067ecac",
        "message": "io_req_complete_post() is now io-wq only and shouldn't be used outside\nof it, i.e. it relies that io-wq holds a ref for the request as\nexplained in a comment below. Let's add a warning to enforce the\nassumption and make sure nobody would try to do anything weird.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1013b60c35d431d0698cafbc53c06f5917348c20.1712331455.git.asml.silence@gmail.com\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: turn implicit assumptions into a warning"
    },
    {
        "commit": "f39130004d3a9155d113284c19b5a7c2eccb43fe",
        "message": "Since commit 8f6c829491fe (\"io_uring: remove struct io_tw_state::locked\"),\nio_req_complete_post() is only called from io-wq submit work, where the\nrequest reference is guaranteed to be grabbed and won't drop to zero\nin io_req_complete_post().\n\nKill the dead code, meantime add req_ref_put() to put the reference.\n\nCc: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1d8297e2046553153e763a52574f0e0f4d512f86.1712331455.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: kill dead code in io_req_complete_post"
    },
    {
        "commit": "285207f67c9bcad1d9168993f175d6d88ce310f1",
        "message": "We no longer use IO_BUFFER_LIST_BUF_PER_PAGE, kill it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring/kbuf: remove dead define"
    },
    {
        "commit": "1da2f311ba53a1ee106a637cf17aba05d2acc1ff",
        "message": "There are a few of those:\n\nio_uring/fdinfo.c:170:16: warning: declaration shadows a local variable [-Wshadow]\n  170 |                 struct file *f = io_file_from_index(&ctx->file_table, i);\n      |                              ^\nio_uring/fdinfo.c:53:67: note: previous declaration is here\n   53 | __cold void io_uring_show_fdinfo(struct seq_file *m, struct file *f)\n      |                                                                   ^\nio_uring/cancel.c:187:25: warning: declaration shadows a local variable [-Wshadow]\n  187 |                 struct io_uring_task *tctx = node->task->io_uring;\n      |                                       ^\nio_uring/cancel.c:166:31: note: previous declaration is here\n  166 |                              struct io_uring_task *tctx,\n      |                                                    ^\nio_uring/register.c:371:25: warning: declaration shadows a local variable [-Wshadow]\n  371 |                 struct io_uring_task *tctx = node->task->io_uring;\n      |                                       ^\nio_uring/register.c:312:24: note: previous declaration is here\n  312 |         struct io_uring_task *tctx = NULL;\n      |                               ^\n\nand a simple cleanup gets rid of them. For the fdinfo case, make a\ndistinction between the file being passed in (for the ring), and the\nregistered files we iterate. For the other two cases, just get rid of\nshadowed variable, there's no reason to have a new one.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: fix warnings on shadow variables"
    },
    {
        "commit": "f15ed8b4d0ce2c0831232ff85117418740f0c529",
        "message": "Move the related code from io_uring.c into memmap.c. No functional\nchanges in this patch, just cleaning it up a bit now that the full\ntransition is done.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: move mapping/allocation helpers to a separate file"
    },
    {
        "commit": "18595c0a58ae29ac6a996c5b664610119b73182d",
        "message": "There are a few cases of open-rolled loops around unpin_user_page(), use\nthe generic helper instead.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: use unpin_user_pages() where appropriate"
    },
    {
        "commit": "87585b05757dc70545efb434669708d276125559",
        "message": "Rather than use remap_pfn_range() for this and manually free later,\nswitch to using vm_insert_page() and have it Just Work.\n\nThis requires a bit of effort on the mmap lookup side, as the ctx\nuring_lock isn't held, which  otherwise protects buffer_lists from being\ntorn down, and it's not safe to grab from mmap context that would\nintroduce an ABBA deadlock between the mmap lock and the ctx uring_lock.\nInstead, lookup the buffer_list under RCU, as the the list is RCU freed\nalready. Use the existing reference count to determine whether it's\npossible to safely grab a reference to it (eg if it's not zero already),\nand drop that reference when done with the mapping. If the mmap\nreference is the last one, the buffer_list and the associated memory can\ngo away, since the vma insertion has references to the inserted pages at\nthat point.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring/kbuf: use vm_insert_pages() for mmap'ed pbuf ring"
    },
    {
        "commit": "e270bfd22a2a10d1cfbaddf23e79b6d0b405d21e",
        "message": "This avoids needing to care about HIGHMEM, and it makes the buffer\nindexing easier as both ring provided buffer methods are now virtually\nmapped in a contigious fashion.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring/kbuf: vmap pinned buffer ring"
    },
    {
        "commit": "1943f96b3816e0f0d3d6686374d6e1d617c8b42c",
        "message": "Move it into io_uring.c where it belongs, and use it in there as well\nrather than have two implementations of this.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: unify io_pin_pages()"
    },
    {
        "commit": "09fc75e0c035a2cabb8caa15cec6e85159dd94f0",
        "message": "This is the last holdout which does odd page checking, convert it to\nvmap just like what is done for the non-mmap path.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: use vmap() for ring mapping"
    },
    {
        "commit": "3ab1db3c6039e02a9deb9d5091d28d559917a645",
        "message": "Rather than use remap_pfn_range() for this and manually free later,\nswitch to using vm_insert_pages() and have it Just Work.\n\nIf possible, allocate a single compound page that covers the range that\nis needed. If that works, then we can just use page_address() on that\npage. If we fail to get a compound page, allocate single pages and use\nvmap() to map them into the kernel virtual address space.\n\nThis just covers the rings/sqes, the other remaining user of the mmap\nremap_pfn_range() user will be converted separately. Once that is done,\nwe can kill the old alloc/free code.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: get rid of remap_pfn_range() for mapping rings/sqes"
    },
    {
        "commit": "0f21a9574b1d04afbf818a3e6a60cb95eb04a616",
        "message": "While valid C, anonymous enums confuse Cython (Python to C translator),\nas reported by Ritesh (YoSTEALTH) [1] .  Since people rely on it when\nbuilding against liburing and we want to keep this header in sync with\nthe library version, let's name the existing enums in the uapi header.\n\n[1] https://github.com/cython/cython/issues/3240\n\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20240328210935.25640-1-krisman@suse.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: Avoid anonymous enums in io_uring uapi"
    },
    {
        "commit": "22537c9f79417fed70b352d54d01d2586fee9521",
        "message": "io_task_work_pending() uses wq_list_empty() on ctx->work_llist, but it's\nnot an io_wq_work_list, it's a struct llist_head. They both have\n->first as head-of-list, and it turns out the checks are identical. But\nbe proper and use the right helper.\n\nFixes: dac6a0eae793 (\"io_uring: ensure iopoll runs local task work as well\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: use the right type for work_llist empty check"
    },
    {
        "commit": "a80929d1cd530be36d482218d4e9ec09ecd5204f",
        "message": "This commit comes at the tail end of a greater effort to remove the\nempty elements at the end of the ctl_table arrays (sentinels) which will\nreduce the overall build time size of the kernel and run time memory\nbloat by ~64 bytes per sentinel (further information Link :\nhttps://lore.kernel.org/all/ZO5Yx5JFogGi%2FcBo@bombadil.infradead.org/)\n\nRemove sentinel element from kernel_io_uring_disabled_table\n\nSigned-off-by: Joel Granados <j.granados@samsung.com>\nLink: https://lore.kernel.org/r/20240328-jag-sysctl_remset_misc-v1-6-47c1463b3af2@samsung.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:26 -0600 io_uring: Remove the now superfluous sentinel elements from ctl_table array"
    },
    {
        "commit": "4e9706c6c8d1b159e2d04bee60189a1361f5afa9",
        "message": "The function are defined in the io_uring.c file, but not called\nelsewhere, so delete the unused function.\n\nio_uring/io_uring.c:646:20: warning: unused function '__io_cq_unlock'.\n\nReported-by: Abaci Robot <abaci@linux.alibaba.com>\nCloses: https://bugzilla.openanolis.cn/show_bug.cgi?id=8660\nSigned-off-by: Jiapeng Chong <jiapeng.chong@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20240328022324.78029-1-jiapeng.chong@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring: Remove unused function"
    },
    {
        "commit": "77a1cd5e795726235469c31cf13a25116bf2d712",
        "message": "The object list is a bit of a mess, with core and opcode files mixed in.\nRe-arrange it so that we have the core bits first, and then opcode\nspecific files after that.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring: re-arrange Makefile order"
    },
    {
        "commit": "05eb5fe226461c6459b81f109a9c23b46ed8bc3b",
        "message": "The allocator will generally return memory in order, but\n__io_alloc_req_refill() then adds them to a stack and we'll extract them\nin the opposite order. This obviously isn't a huge deal, but:\n\n1) it makes debugging easier when they are in order\n2) keeping them in-order is the right thing to do\n3) reduces the code for adding them to the stack\n\nJust add them in reverse to the stack.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring: refill request cache in memory order"
    },
    {
        "commit": "da22bdf38be2f2ba557d3031108614ebbba265e1",
        "message": "This should be plenty, rather than the default of 128, and matches what\nwe have on the rsrc and futex side as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/poll: shrink alloc cache size to 32"
    },
    {
        "commit": "414d0f45c316221acbf066658afdbae5b354a5cc",
        "message": "Currently lists are being used to manage this, but best practice is\nusually to have these in an array instead as that it cheaper to manage.\n\nOutside of that detail, games are also played with KASAN as the list\nis inside the cached entry itself.\n\nFinally, all users of this need a struct io_cache_entry embedded in\ntheir struct, which is union'ized with something else in there that\nisn't used across the free -> realloc cycle.\n\nGet rid of all of that, and simply have it be an array. This will not\nchange the memory used, as we're just trading an 8-byte member entry\nfor the per-elem array size.\n\nThis reduces the overhead of the recycled allocations, and it reduces\nthe amount of code code needed to support recycling to about half of\nwhat it currently is.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/alloc_cache: switch to array based caching"
    },
    {
        "commit": "e10677a8f6980dbae2e866b8320d90bae07e87ee",
        "message": "It's now unused, drop the code related to it. This includes the\nio_issue_defs->manual alloc field.\n\nWhile in there, and since ->async_size is now being used a bit more\nfrequently and in the issue path, move it to io_issue_defs[].\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring: drop ->prep_async()"
    },
    {
        "commit": "5eff57fa9f3aae3acbcaf196af507eec58955f3b",
        "message": "The previous commit turned on async data for uring_cmd, and did the\nbasic conversion of setting everything up on the prep side. However, for\na lot of use cases, -EIOCBQUEUED will get returned on issue, as the\noperation got successfully queued. For that case, a persistent SQE isn't\nneeded, as it's just used for issue.\n\nUnless execution goes async immediately, defer copying the double SQE\nuntil it's necessary.\n\nThis greatly reduces the overhead of such commands, as evidenced by\na perf diff from before and after this change:\n\n    10.60%     -8.58%  [kernel.vmlinux]  [k] io_uring_cmd_prep\n\nwhere the prep side drops from 10.60% to ~2%, which is more expected.\nPerformance also rises from ~113M IOPS to ~122M IOPS, bringing us back\nto where it was before the async command prep.\n\nTested-by: Anuj Gupta <anuj20.g@samsung.com>\nReviewed-by: Anuj Gupta <anuj20.g@samsung.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/uring_cmd: defer SQE copying until it's needed"
    },
    {
        "commit": "d10f19dff56eac5ae44dc270336b18071a8bd51c",
        "message": "Basic conversion ensuring async_data is allocated off the prep path. Adds\na basic alloc cache as well, as passthrough IO can be quite high in rate.\n\nTested-by: Anuj Gupta <anuj20.g@samsung.com>\nReviewed-by: Anuj Gupta <anuj20.g@samsung.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/uring_cmd: switch to always allocating async data"
    },
    {
        "commit": "e2ea5a7069133c01fe3dbda95d77af7f193a1a52",
        "message": "While doing that, get rid of io_async_connect and just use the generic\nio_async_msghdr. Both of them have a struct sockaddr_storage in there,\nand while io_async_msghdr is bigger, if the same type can be used then\nthe netmsg_cache can get reused for connect as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/net: move connect to always using async data"
    },
    {
        "commit": "d6f911a6b22f8e48aec82cd5f6b5a14dc76a56c3",
        "message": "Let the io_async_rw hold on to the iovec and reuse it, rather than always\nallocate and free them.\n\nAlso enables KASAN for the iovec entries, so that reuse can be detected\neven while they are in the cache.\n\nWhile doing so, shrink io_async_rw by getting rid of the bigger embedded\nfast iovec. Since iovecs are being recycled now, shrink it from 8 to 1.\nThis reduces the io_async_rw size from 264 to 160 bytes, a 40% reduction.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/rw: add iovec recycling"
    },
    {
        "commit": "cca6571381a0bdc88021a1f7a4c2349df21279f7",
        "message": "We no longer need to gate a potential retry on whether or not the\ncontext matches our original task, as all read/write operations have\nbeen fully prepared upfront. This means there's never any re-import\nneeded, and hence we can always retry requests.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/rw: cleanup retry path"
    },
    {
        "commit": "0d10bd77a1be0742a12e1bcf0554a4bcbdbc0f35",
        "message": "A separate state struct is not needed anymore, just fold it in with\nio_async_rw.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring: get rid of struct io_rw_state"
    },
    {
        "commit": "a9165b83c1937eeed1f0c731468216d6371d647f",
        "message": "read/write requests try to put everything on the stack, and then alloc\nand copy if a retry is needed. This necessitates a bunch of nasty code\nthat deals with intermediate state.\n\nGet rid of this, and have the prep side setup everything that is needed\nupfront, which greatly simplifies the opcode handlers.\n\nThis includes adding an alloc cache for io_async_rw, to make it cheap\nto handle.\n\nIn terms of cost, this should be basically free and transparent. For\nthe worst case of {READ,WRITE}_FIXED which didn't need it before,\nperformance is unaffected in the normal peak workload that is being\nused to test that. Still runs at 122M IOPS.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/rw: always setup io_async_rw for read/write requests"
    },
    {
        "commit": "d80f940701302e84d1398ecb103083468b566a69",
        "message": "Now that iovec recycling is being done, the iovec is no longer being\nfreed in there. Hence the kmsg parameter is now useless.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/net: drop 'kmsg' parameter from io_req_msg_cleanup()"
    },
    {
        "commit": "75191341785eef51f87ff54b0ed9dfbd5a72e7c2",
        "message": "Right now the io_async_msghdr is recycled to avoid the overhead of\nallocating+freeing it for every request. But the iovec is not included,\nhence that will be allocated and freed for each transfer regardless.\nThis commit enables recyling of the iovec between io_async_msghdr\nrecycles. This avoids alloc+free for each one if an iovec is used, and\non top of that, it extends the cache hot nature of msg to the iovec as\nwell.\n\nAlso enables KASAN for the iovec entries, so that reuse can be detected\neven while they are in the cache.\n\nThe io_async_msghdr also shrinks from 376 -> 288 bytes, an 88 byte\nsaving (or ~23% smaller), as the fast_iovec entry is dropped from 8\nentries to a single entry. There's no point keeping a big fast iovec\nentry, if iovecs aren't being allocated and freed continually.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/net: add iovec recycling"
    },
    {
        "commit": "9f8539fe299c250af42325eccff66e8b8d1f15da",
        "message": "All net commands have async data at this point, there's no reason to\ncheck if this is the case or not.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/net: remove (now) dead code in io_netmsg_recycle()"
    },
    {
        "commit": "6498c5c97ce73770ed227eb52b14d21c8343fd5b",
        "message": "We now ONLY call io_msg_alloc_async() from inside prep handling, which\nis always locked. No need for this helper anymore, or the check in\nio_msg_alloc_async() on whether the ring is locked or not.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring: kill io_msg_alloc_async_prep()"
    },
    {
        "commit": "50220d6ac8ff31eb065fba818e960f549fb89d4d",
        "message": "Move the io_async_msghdr out of the issue path and into prep handling,\ne it's now done unconditionally and hence does not need to be part\nof the issue path. This means any usage of io_sendrecv_prep_async() and\nio_sendmsg_prep_async(), and hence the forced async setup path is now\nunified with the normal prep setup.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/net: get rid of ->prep_async() for send side"
    },
    {
        "commit": "c6f32c7d9e09bf1368447e9a29e869193ecbb756",
        "message": "Move the io_async_msghdr out of the issue path and into prep handling,\nsince it's now done unconditionally and hence does not need to be part\nof the issue path. This reduces the footprint of the multishot fast\npath of multiple invocations of ->issue() per prep, and also means that\nusing ->prep_async() can be dropped for recvmsg asthis is now done via\nsetup on the prep side.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/net: get rid of ->prep_async() for receive side"
    },
    {
        "commit": "3ba8345aec886a3a01331e944a6a8568bf94bd10",
        "message": "We currently set this separately for async/sync entry, but let's just\nmove it to a generic pre-issue spot and eliminate the difference\nbetween the two.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/net: always set kmsg->msg.msg_control_user before issue"
    },
    {
        "commit": "790b68b32a678b65b161861f83b2b782b6b9246b",
        "message": "Rather than use an on-stack one and then need to allocate and copy if\nasync execution is required, always grab one upfront. This should be\nvery cheap, and potentially even have cache hotness benefits for\nback-to-back send/recv requests.\n\nFor any recv type of request, this is probably a good choice in general,\nas it's expected that no data is available initially. For send this is\nnot necessarily the case, as space in the socket buffer is expected to\nbe available. However, getting a cached io_async_msghdr is very cheap,\nand as it should be cache hot, probably the difference here is neglible,\nif any.\n\nA nice side benefit is that io_setup_async_msg can get killed\ncompletely, which has some nasty iovec manipulation code.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/net: always setup an io_async_msghdr"
    },
    {
        "commit": "f5b00ab2221a26202da7d10542a98203075bfdf8",
        "message": "Now that recv/recvmsg both do the same cleanup, put it in the retry and\nfinish handlers.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/net: unify cleanup handling"
    },
    {
        "commit": "4a3223f7bfda14c532856152b12aace525cf8079",
        "message": "No functional changes in this patch, just in preparation for carrying\nmore state than what is available now, if necessary.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/net: switch io_recv() to using io_async_msghdr"
    },
    {
        "commit": "54cdcca05abde32acc3233950ddc79d8be25515f",
        "message": "No functional changes in this patch, just in preparation for carrying\nmore state then what is being done now, if necessary. While unifying\nsome of this code, add a generic send setup prep handler that they can\nboth use.\n\nThis gets rid of some manual msghdr and sockaddr on the stack, and makes\nit look a bit more like the sendmsg/recvmsg variants. Going forward, more\ncan get unified on top.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/net: switch io_send() and io_send_zc() to using io_async_msghdr"
    },
    {
        "commit": "0ae9b9a14d54bd0aa68c1e8bda9dd8e6346f1d87",
        "message": "In practice, we just need to recycle a few elements for (by far) most\nuse cases. Shrink the total size down from 512 to 128, which should be\nmore than plenty.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring/alloc_cache: shrink default max entries from 512 to 128"
    },
    {
        "commit": "29f858a7c6e06060bbadee6d8502df36eed888bf",
        "message": "For historical reasons these were special cased, as they were the only\nones that needed cancelation. But now we handle cancelations generally,\nand hence there's no need to check for these in\nio_ring_ctx_wait_and_kill() when io_uring_try_cancel_requests() handles\nboth these and the rest as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:25 -0600 io_uring: remove timeout/poll specific cancelations"
    },
    {
        "commit": "254176234222c97c5da7fd33ff8c61d06480c228",
        "message": "Just like we run the inline task_work, ensure we also factor in and\nrun the fallback task_work.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:24 -0600 io_uring: flush delayed fallback task_work in cancelation"
    },
    {
        "commit": "c133b3b06b0653036b0c07675c1db0c89467ccdb",
        "message": "Move CONFIG_PROVE_LOCKING checks inside of io_lockdep_assert_cq_locked()\nand kill the else branch.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nTested-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/bbf33c429c9f6d7207a8fe66d1a5866ec2c99850.1710799188.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:24 -0600 io_uring: clean up io_lockdep_assert_cq_locked"
    },
    {
        "commit": "0667db14e1f029d56243aa2509ebc5f944388200",
        "message": "Make io_req_complete_post() to push all IORING_SETUP_IOPOLL requests\nto task_work, it's much cleaner and should normally happen. We couldn't\ndo it before because there was a possibility of looping in\n\ncomplete_post() -> tw -> complete_post() -> ...\n\nAlso, unexport the function and inline __io_req_complete_post().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nTested-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/ea19c032ace3e0dd96ac4d991a063b0188037014.1710799188.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:24 -0600 io_uring: refactor io_req_complete_post()"
    },
    {
        "commit": "23fbdde6205d9351bb52a4b8f11ec38bdbc8561a",
        "message": "task_work execution is now always locked, and we shouldn't get into\nio_req_complete_post() from them. That means that complete_post() is\nalways called out of the original task context and we don't even need to\ncheck current.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nTested-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/24ec27f27db0d8f58c974d8118dca1d345314ddc.1710799188.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:24 -0600 io_uring: remove current check from complete_post"
    },
    {
        "commit": "902ce82c2aa130bea5e3feca2d4ae62781865da7",
        "message": "io_post_aux_cqe(), which is used for multishot requests, delays\ncompletions by putting CQEs into a temporary array for the purpose\ncompletion lock/flush batching.\n\nDEFER_TASKRUN doesn't need any locking, so for it we can put completions\ndirectly into the CQ and defer post completion handling with a flag.\nThat leaves !DEFER_TASKRUN, which is not that interesting / hot for\nmultishot requests, so have conditional locking with deferred flush\nfor them.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nTested-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/b1d05a81fd27aaa2a07f9860af13059e7ad7a890.1710799188.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:24 -0600 io_uring: get rid of intermediate aux cqe caches"
    },
    {
        "commit": "e5c12945be5016d681ff305ea7306fef5902219d",
        "message": "The restriction on multishot execution context disallowing io-wq is\ndriven by rules of io_fill_cqe_req_aux(), it should only be called in\nthe master task context, either from the syscall path or in task_work.\nSince task_work now always takes the ctx lock implying\nIO_URING_F_COMPLETE_DEFER, we can just assume that the function is\nalways called with its defer argument set to true.\n\nKill the argument. Also rename the function for more consistency as\n\"fill\" in CQE related functions was usually meant for raw interfaces\nonly copying data into the CQ without any locking, waking the user\nand other accounting \"post\" functions take care of.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nTested-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/93423d106c33116c7d06bf277f651aa68b427328.1710799188.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:24 -0600 io_uring: refactor io_fill_cqe_req_aux"
    },
    {
        "commit": "8e5b3b89ecaf6d9295e561c225b35c574a5e0fe7",
        "message": "ctx is always locked for task_work now, so get rid of struct\nio_tw_state::locked. Note I'm stopping one step before removing\nio_tw_state altogether, which is not empty, because it still serves the\npurpose of indicating which function is a tw callback and forcing users\nnot to invoke them carelessly out of a wrong context. The removal can\nalways be done later.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nTested-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/e95e1ea116d0bfa54b656076e6a977bc221392a4.1710799188.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:24 -0600 io_uring: remove struct io_tw_state::locked"
    },
    {
        "commit": "92219afb980e01d88a1ac6d8fe01dcae255c4279",
        "message": "We can run normal task_work without locking the ctx, however we try to\nlock anyway and most handlers prefer or require it locked. It might have\nbeen interesting to multi-submitter ring with high contention completing\nasync read/write requests via task_work, however that will still need to\ngo through io_req_complete_post() and potentially take the lock for\nrsrc node putting or some other case.\n\nIn other words, it's hard to care about it, so alawys force the locking.\nThe case described would also because of various io_uring caches.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nTested-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/6ae858f2ef562e6ed9f13c60978c0d48926954ba.1710799188.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:24 -0600 io_uring: force tw ctx locking"
    },
    {
        "commit": "6e6b8c62120a22acd8cb759304e4cd2e3215d488",
        "message": "kiocb_done() should care to specifically redirecting requests to io-wq.\nRemove the hopping to tw to then queue an io-wq, return -EAGAIN and let\nthe core code io_uring handle offloading.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nTested-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/413564e550fe23744a970e1783dfa566291b0e6f.1710799188.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:24 -0600 io_uring/rw: avoid punting to io-wq directly"
    },
    {
        "commit": "1afdb76038e27a3a4dd4cf522f6457868051db84",
        "message": "NVMe is making up issue_flags, which is a no-no in general, and to make\nmatters worse, they are completely the wrong ones. For a pure polled\nrequest, which it does check for, we're already inside the\nctx->uring_lock when the completions are run off io_do_iopoll(). Hence\nthe correct flag would be '0' rather than IO_URING_F_UNLOCKED.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:24 -0600 nvme/io_uring: use helper for polled completions"
    },
    {
        "commit": "36a005b9c66eade68f4235e612d733510a8d52ff",
        "message": "Add comments warning users that they're only allowed to pass issue_flags\nthat were given from io_uring.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nTested-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/82ff8a45f2c3eb5f3a04a33f0692e5e4a1320455.1710799188.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:24 -0600 io_uring/cmd: document some uring_cmd related helpers"
    },
    {
        "commit": "e1eef2e56cb0db143c731b1cdc220980256d2d99",
        "message": "!IO_URING_F_UNLOCKED does not translate to availability of the deferred\ncompletion infra, IO_URING_F_COMPLETE_DEFER does, that what we should\npass and look for to use io_req_complete_defer() and other variants.\n\nLuckily, it's not a real problem as two wrongs actually made it right,\nat least as far as io_uring_cmd_work() goes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nTested-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/aef76d34fe9410df8ecc42a14544fd76cd9d8b9e.1710799188.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:24 -0600 io_uring/cmd: fix tw <-> issue_flags conversion"
    },
    {
        "commit": "6edd953b6ec758c98e9dba7234634831f1f6510d",
        "message": "io_uring cmd converts struct io_tw_state to issue_flags and later back\nto io_tw_state, it's awfully ill-fated, not to mention that intermediate\nissue_flags state is not correct.\n\nGet rid of the last conversion, drag through tw everything that came\nwith IO_URING_F_UNLOCKED, and replace io_req_complete_defer() with a\ndirect call to io_req_complete_defer(), at least for the time being.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nTested-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/c53fa3df749752bd058cf6f824a90704822d6bcc.1710799188.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:24 -0600 io_uring/cmd: kill one issue_flags to tw conversion"
    },
    {
        "commit": "da12d9ab5889b87429d9375748dcd1485b6241f3",
        "message": "io_uring_try_cancel_uring_cmd() is a part of the cmd handling so let's\nmove it closer to all cmd bits into uring_cmd.c\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nTested-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/43a3937af4933655f0fd9362c381802f804f43de.1710799188.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-15 08:10:24 -0600 io_uring/cmd: move io_uring_try_cancel_uring_cmd()"
    },
    {
        "commit": "c7adbe2eb7639c9408599dd9762ba2fa3b87297c",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix for sigmask restoring while waiting for events (Alexey)\n\n - Typo fix in comment (Haiyue)\n\n - Fix for a msg_control retstore on SEND_ZC retries (Pavel)\n\n* tag 'io_uring-6.9-20240412' of git://git.kernel.dk/linux:\n  io-uring: correct typo in comment for IOU_F_TWQ_LAZY_WAKE\n  io_uring/net: restore msg_control on sendzc retry\n  io_uring: Fix io_cqring_wait() not restoring sigmask on get_timespec64() failure",
        "kernel_version": "v6.9-rc4",
        "release_date": "2024-04-12 10:19:36 -0700 Merge tag 'io_uring-6.9-20240412' of git://git.kernel.dk/linux"
    },
    {
        "commit": "3a93daea2fb27fcefa85662654ba583a5d0c7231",
        "message": "Pull read_iter updates from Jens Axboe:\n\nThere are still a few users of fops->read() in the core parts of the\nfs stack. Which is a shame, since it'd be nice to get rid of the\nnon-iterator parts of down the line, and reclaim that part of the\nfile_operations struct.\n\nOutside of moving in that direction as a cleanup, using ->read_iter()\nenables us to mark them with FMODE_NOWAIT. This is important for users\nlike io_uring, where per-IO nonblocking hints make a difference in how\nefficiently IO can be done.\n\nThose two things are my main motivation for starting this work, with\nhopefully more to come down the line.\n\nAll patches have been booted and tested, and the corresponding test\ncases from ltp have been run.\n\n* 'read_iter' of git://git.kernel.dk/linux: (4 commits)\n  signalfd: convert to ->read_iter()\n  userfaultfd: convert to ->read_iter()\n  timerfd: convert to ->read_iter()\n  new helper: copy_to_iter_full()\n\nSigned-off-by: Christian Brauner <brauner@kernel.org>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-11 10:06:08 +0200 Merge branch 'read_iter' of git://git.kernel.dk/linux"
    },
    {
        "commit": "d94979904105a7ad8dca6fdcd8cb3fbecada22f1",
        "message": "Switch timerfd to using fops->read_iter(), so it can support not just\nO_NONBLOCK but IOCB_NOWAIT as well. With the latter, users like io_uring\ninteract with timerfds a lot better, as they can be driven purely\nby the poll trigger.\n\nManually get and install the required fd, so that FMODE_NOWAIT can be\nset before the file is installed into the file table.\n\nNo functional changes intended in this patch, it's purely a straight\nconversion to using the read iterator method.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.10-rc1",
        "release_date": "2024-04-10 16:23:02 -0600 timerfd: convert to ->read_iter()"
    },
    {
        "commit": "4fe82aedeb8a8cb09bfa60f55ab57b5c10a74ac4",
        "message": "cac9e4418f4cb (\"io_uring/net: save msghdr->msg_control for retries\")\nreinstatiates msg_control before every __sys_sendmsg_sock(), since the\nfunction can overwrite the value in msghdr. We need to do same for\nzerocopy sendmsg.\n\nCc: stable@vger.kernel.org\nFixes: 493108d95f146 (\"io_uring/net: zerocopy sendmsg\")\nLink: https://github.com/axboe/liburing/issues/1067\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/cc1d5d9df0576fa66ddad4420d240a98a020b267.1712596179.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc4",
        "release_date": "2024-04-08 21:48:41 -0600 io_uring/net: restore msg_control on sendzc retry"
    },
    {
        "commit": "978e5c19dfefc271e5550efba92fcef0d3f62864",
        "message": "This bug was introduced in commit 950e79dd7313 (\"io_uring: minor\nio_cqring_wait() optimization\"), which was made in preparation for\nadc8682ec690 (\"io_uring: Add support for napi_busy_poll\"). The latter\ngot reverted in cb3182167325 (\"Revert \"io_uring: Add support for\nnapi_busy_poll\"\"), so simply undo the former as well.\n\nCc: stable@vger.kernel.org\nFixes: 950e79dd7313 (\"io_uring: minor io_cqring_wait() optimization\")\nSigned-off-by: Alexey Izbyshev <izbyshev@ispras.ru>\nLink: https://lore.kernel.org/r/20240405125551.237142-1-izbyshev@ispras.ru\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc4",
        "release_date": "2024-04-05 20:05:41 -0600 io_uring: Fix io_cqring_wait() not restoring sigmask on get_timespec64() failure"
    },
    {
        "commit": "4f72ed492d7798919269a20d157d34495a988935",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Backport of some fixes that came up during development of the 6.10\n   io_uring patches. This includes some kbuf cleanups and reference\n   fixes.\n\n - Disable multishot read if we don't have NOWAIT support on the target\n\n - Fix for a dependency issue with workqueue flushing\n\n* tag 'io_uring-6.9-20240405' of git://git.kernel.dk/linux:\n  io_uring/kbuf: hold io_buffer_list reference over mmap\n  io_uring/kbuf: protect io_buffer_list teardown with a reference\n  io_uring/kbuf: get rid of bl->is_ready\n  io_uring/kbuf: get rid of lower BGID lists\n  io_uring: use private workqueue for exit work\n  io_uring: disable io-wq execution of multishot NOWAIT requests\n  io_uring/rw: don't allow multishot reads without NOWAIT support",
        "kernel_version": "v6.9-rc3",
        "release_date": "2024-04-05 16:58:52 -0700 Merge tag 'io_uring-6.9-20240405' of git://git.kernel.dk/linux"
    },
    {
        "commit": "04c35ab3bdae7fefbd7c7a7355f29fa03a035221",
        "message": "PAT handling won't do the right thing in COW mappings: the first PTE (or,\nin fact, all PTEs) can be replaced during write faults to point at anon\nfolios.  Reliably recovering the correct PFN and cachemode using\nfollow_phys() from PTEs will not work in COW mappings.\n\nUsing follow_phys(), we might just get the address+protection of the anon\nfolio (which is very wrong), or fail on swap/nonswap entries, failing\nfollow_phys() and triggering a WARN_ON_ONCE() in untrack_pfn() and\ntrack_pfn_copy(), not properly calling free_pfn_range().\n\nIn free_pfn_range(), we either wouldn't call memtype_free() or would call\nit with the wrong range, possibly leaking memory.\n\nTo fix that, let's update follow_phys() to refuse returning anon folios,\nand fallback to using the stored PFN inside vma->vm_pgoff for COW mappings\nif we run into that.\n\nWe will now properly handle untrack_pfn() with COW mappings, where we\ndon't need the cachemode.  We'll have to fail fork()->track_pfn_copy() if\nthe first page was replaced by an anon folio, though: we'd have to store\nthe cachemode in the VMA to make this work, likely growing the VMA size.\n\nFor now, lets keep it simple and let track_pfn_copy() just fail in that\ncase: it would have failed in the past with swap/nonswap entries already,\nand it would have done the wrong thing with anon folios.\n\nSimple reproducer to trigger the WARN_ON_ONCE() in untrack_pfn():\n\n<--- C reproducer --->\n #include <stdio.h>\n #include <sys/mman.h>\n #include <unistd.h>\n #include <liburing.h>\n\n int main(void)\n {\n         struct io_uring_params p = {};\n         int ring_fd;\n         size_t size;\n         char *map;\n\n         ring_fd = io_uring_setup(1, &p);\n         if (ring_fd < 0) {\n                 perror(\"io_uring_setup\");\n                 return 1;\n         }\n         size = p.sq_off.array + p.sq_entries * sizeof(unsigned);\n\n         /* Map the submission queue ring MAP_PRIVATE */\n         map = mmap(0, size, PROT_READ | PROT_WRITE, MAP_PRIVATE,\n                    ring_fd, IORING_OFF_SQ_RING);\n         if (map == MAP_FAILED) {\n                 perror(\"mmap\");\n                 return 1;\n         }\n\n         /* We have at least one page. Let's COW it. */\n         *map = 0;\n         pause();\n         return 0;\n }\n<--- C reproducer --->\n\nOn a system with 16 GiB RAM and swap configured:\n # ./iouring &\n # memhog 16G\n # killall iouring\n[  301.552930] ------------[ cut here ]------------\n[  301.553285] WARNING: CPU: 7 PID: 1402 at arch/x86/mm/pat/memtype.c:1060 untrack_pfn+0xf4/0x100\n[  301.553989] Modules linked in: binfmt_misc nft_fib_inet nft_fib_ipv4 nft_fib_ipv6 nft_fib nft_reject_g\n[  301.558232] CPU: 7 PID: 1402 Comm: iouring Not tainted 6.7.5-100.fc38.x86_64 #1\n[  301.558772] Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS rel-1.16.3-0-ga6ed6b701f0a-prebu4\n[  301.559569] RIP: 0010:untrack_pfn+0xf4/0x100\n[  301.559893] Code: 75 c4 eb cf 48 8b 43 10 8b a8 e8 00 00 00 3b 6b 28 74 b8 48 8b 7b 30 e8 ea 1a f7 000\n[  301.561189] RSP: 0018:ffffba2c0377fab8 EFLAGS: 00010282\n[  301.561590] RAX: 00000000ffffffea RBX: ffff9208c8ce9cc0 RCX: 000000010455e047\n[  301.562105] RDX: 07fffffff0eb1e0a RSI: 0000000000000000 RDI: ffff9208c391d200\n[  301.562628] RBP: 0000000000000000 R08: ffffba2c0377fab8 R09: 0000000000000000\n[  301.563145] R10: ffff9208d2292d50 R11: 0000000000000002 R12: 00007fea890e0000\n[  301.563669] R13: 0000000000000000 R14: ffffba2c0377fc08 R15: 0000000000000000\n[  301.564186] FS:  0000000000000000(0000) GS:ffff920c2fbc0000(0000) knlGS:0000000000000000\n[  301.564773] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n[  301.565197] CR2: 00007fea88ee8a20 CR3: 00000001033a8000 CR4: 0000000000750ef0\n[  301.565725] PKRU: 55555554\n[  301.565944] Call Trace:\n[  301.566148]  <TASK>\n[  301.566325]  ? untrack_pfn+0xf4/0x100\n[  301.566618]  ? __warn+0x81/0x130\n[  301.566876]  ? untrack_pfn+0xf4/0x100\n[  301.567163]  ? report_bug+0x171/0x1a0\n[  301.567466]  ? handle_bug+0x3c/0x80\n[  301.567743]  ? exc_invalid_op+0x17/0x70\n[  301.568038]  ? asm_exc_invalid_op+0x1a/0x20\n[  301.568363]  ? untrack_pfn+0xf4/0x100\n[  301.568660]  ? untrack_pfn+0x65/0x100\n[  301.568947]  unmap_single_vma+0xa6/0xe0\n[  301.569247]  unmap_vmas+0xb5/0x190\n[  301.569532]  exit_mmap+0xec/0x340\n[  301.569801]  __mmput+0x3e/0x130\n[  301.570051]  do_exit+0x305/0xaf0\n...\n\nLink: https://lkml.kernel.org/r/20240403212131.929421-3-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReported-by: Wupeng Ma <mawupeng1@huawei.com>\nCloses: https://lkml.kernel.org/r/20240227122814.3781907-1-mawupeng1@huawei.com\nFixes: b1a86e15dc03 (\"x86, pat: remove the dependency on 'vm_pgoff' in track/untrack pfn vma routines\")\nFixes: 5899329b1910 (\"x86: PAT: implement track/untrack of pfnmap regions for x86 - v3\")\nAcked-by: Ingo Molnar <mingo@kernel.org>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Andy Lutomirski <luto@kernel.org>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Borislav Petkov <bp@alien8.de>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: <stable@vger.kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v6.9-rc3",
        "release_date": "2024-04-05 11:21:31 -0700 x86/mm/pat: fix VM_PAT handling in COW mappings"
    },
    {
        "commit": "561e4f9451d65fc2f7eef564e0064373e3019793",
        "message": "If we look up the kbuf, ensure that it doesn't get unregistered until\nafter we're done with it. Since we're inside mmap, we cannot safely use\nthe io_uring lock. Rely on the fact that we can lookup the buffer list\nunder RCU now and grab a reference to it, preventing it from being\nunregistered until we're done with it. The lookup returns the\nio_buffer_list directly with it referenced.\n\nCc: stable@vger.kernel.org # v6.4+\nFixes: 5cf4f52e6d8a (\"io_uring: free io_buffer_list entries via RCU\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc3",
        "release_date": "2024-04-02 19:03:27 -0600 io_uring/kbuf: hold io_buffer_list reference over mmap"
    },
    {
        "commit": "6b69c4ab4f685327d9e10caf0d84217ba23a8c4b",
        "message": "No functional changes in this patch, just in preparation for being able\nto keep the buffer list alive outside of the ctx->uring_lock.\n\nCc: stable@vger.kernel.org # v6.4+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc3",
        "release_date": "2024-04-02 19:03:26 -0600 io_uring/kbuf: protect io_buffer_list teardown with a reference"
    },
    {
        "commit": "3b80cff5a4d117c53d38ce805823084eaeffbde6",
        "message": "Now that xarray is being exclusively used for the buffer_list lookup,\nthis check is no longer needed. Get rid of it and the is_ready member.\n\nCc: stable@vger.kernel.org # v6.4+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc3",
        "release_date": "2024-04-02 19:03:24 -0600 io_uring/kbuf: get rid of bl->is_ready"
    },
    {
        "commit": "09ab7eff38202159271534d2f5ad45526168f2a5",
        "message": "Just rely on the xarray for any kind of bgid. This simplifies things, and\nit really doesn't bring us much, if anything.\n\nCc: stable@vger.kernel.org # v6.4+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc3",
        "release_date": "2024-04-02 19:03:13 -0600 io_uring/kbuf: get rid of lower BGID lists"
    },
    {
        "commit": "73eaa2b583493b680c6f426531d6736c39643bfb",
        "message": "Rather than use the system unbound event workqueue, use an io_uring\nspecific one. This avoids dependencies with the tty, which also uses\nthe system_unbound_wq, and issues flushes of said workqueue from inside\nits poll handling.\n\nCc: stable@vger.kernel.org\nReported-by: Rasmus Karlsson <rasmus.karlsson@pajlada.com>\nTested-by: Rasmus Karlsson <rasmus.karlsson@pajlada.com>\nTested-by: Iskren Chernev <me@iskren.info>\nLink: https://github.com/axboe/liburing/issues/1113\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc3",
        "release_date": "2024-04-02 07:35:16 -0600 io_uring: use private workqueue for exit work"
    },
    {
        "commit": "bee1d5becdf5bf23d4ca0cd9c6b60bdf3c61d72b",
        "message": "Do the same check for direct io-wq execution for multishot requests that\ncommit 2a975d426c82 did for the inline execution, and disable multishot\nmode (and revert to single shot) if the file type doesn't support NOWAIT,\nand isn't opened in O_NONBLOCK mode. For multishot to work properly, it's\na requirement that nonblocking read attempts can be done.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc3",
        "release_date": "2024-04-01 11:46:22 -0600 io_uring: disable io-wq execution of multishot NOWAIT requests"
    },
    {
        "commit": "2a975d426c82ff05ec1f0b773798d909fe4a3105",
        "message": "Supporting multishot reads requires support for NOWAIT, as the\nalternative would be always having io-wq execute the work item whenever\nthe poll readiness triggered. Any fast file type will have NOWAIT\nsupport (eg it understands both O_NONBLOCK and IOCB_NOWAIT). If the\ngiven file type does not, then simply resort to single shot execution.\n\nCc: stable@vger.kernel.org\nFixes: fc68fcda04910 (\"io_uring/rw: add support for IORING_OP_READ_MULTISHOT\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc3",
        "release_date": "2024-04-01 11:41:50 -0600 io_uring/rw: don't allow multishot reads without NOWAIT support"
    },
    {
        "commit": "19dba097071ec4fd6486b9f0d52d12a3c5743d44",
        "message": "Pull more io_uring updates from Jens Axboe:\n \"One patch just missed the initial pull, the rest are either fixes or\n  small cleanups that make our life easier for the next kernel:\n\n   - Fix a potential leak in error handling of pinned pages, and clean\n     it up (Gabriel, Pavel)\n\n   - Fix an issue with how read multishot returns retry (me)\n\n   - Fix a problem with waitid/futex removals, if we hit the case of\n     needing to remove all of them at exit time (me)\n\n   - Fix for a regression introduced in this merge window, where we\n     don't always have sr->done_io initialized if the ->prep_async()\n     path is used (me)\n\n   - Fix for SQPOLL setup error handling (me)\n\n   - Fix for a poll removal request being delayed (Pavel)\n\n   - Rename of a struct member which had a confusing name (Pavel)\"\n\n* tag 'io_uring-6.9-20240322' of git://git.kernel.dk/linux:\n  io_uring/sqpoll: early exit thread if task_context wasn't allocated\n  io_uring: clear opcode specific data for an early failure\n  io_uring/net: ensure async prep handlers always initialize ->done_io\n  io_uring/waitid: always remove waitid entry for cancel all\n  io_uring/futex: always remove futex entry for cancel all\n  io_uring: fix poll_remove stalled req completion\n  io_uring: Fix release of pinned pages when __io_uaddr_map fails\n  io_uring/kbuf: rename is_mapped\n  io_uring: simplify io_pages_free\n  io_uring: clean rings on NO_MMAP alloc fail\n  io_uring/rw: return IOU_ISSUE_SKIP_COMPLETE for multishot retry\n  io_uring: don't save/restore iowait state",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-22 12:42:55 -0700 Merge tag 'io_uring-6.9-20240322' of git://git.kernel.dk/linux"
    },
    {
        "commit": "1251d2025c3e1bcf1f17ec0f3c0dfae5e5bbb146",
        "message": "Ideally we'd want to simply kill the task rather than wake it, but for\nnow let's just add a startup check that causes the thread to exit.\nThis can only happen if io_uring_alloc_task_context() fails, which\ngenerally requires fault injection.\n\nReported-by: Ubisectech Sirius <bugreport@ubisectech.com>\nFixes: af5d68f8892f (\"io_uring/sqpoll: manage task_work privately\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-18 20:22:42 -0600 io_uring/sqpoll: early exit thread if task_context wasn't allocated"
    },
    {
        "commit": "e21e1c45e1fe2e31732f40256b49c04e76a17cee",
        "message": "If failure happens before the opcode prep handler is called, ensure that\nwe clear the opcode specific area of the request, which holds data\nspecific to that request type. This prevents errors where opcode\nhandlers either don't get to clear per-request private data since prep\nisn't even called.\n\nReported-and-tested-by: syzbot+f8e9a371388aa62ecab4@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-16 11:24:50 -0600 io_uring: clear opcode specific data for an early failure"
    },
    {
        "commit": "f3a640cca951ef9715597e68f5363afc0f452a88",
        "message": "If we get a request with IOSQE_ASYNC set, then we first run the prep\nasync handlers. But if we then fail setting it up and want to post\na CQE with -EINVAL, we use ->done_io. This was previously guarded with\nREQ_F_PARTIAL_IO, and the normal setup handlers do set it up before any\npotential errors, but we need to cover the async setup too.\n\nFixes: 9817ad85899f (\"io_uring/net: remove dependency on REQ_F_PARTIAL_IO for sr->done_io\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-16 10:33:19 -0600 io_uring/net: ensure async prep handlers always initialize ->done_io"
    },
    {
        "commit": "2b35b8b43e07b1a6f06fdd84cf4b9eb24785896d",
        "message": "We know the request is either being removed, or already in the process of\nbeing removed through task_work, so we can delete it from our waitid list\nupfront. This is important for remove all conditions, as we otherwise\nwill find it multiple times and prevent cancelation progress.\n\nRemove the dead check in cancelation as well for the hash_node being\nempty or not. We already have a waitid reference check for ownership,\nso we don't need to check the list too.\n\nCc: stable@vger.kernel.org\nFixes: f31ecf671ddc (\"io_uring: add IORING_OP_WAITID support\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-15 15:42:49 -0600 io_uring/waitid: always remove waitid entry for cancel all"
    },
    {
        "commit": "30dab608c3cb99c2a05b76289fd05551703979ae",
        "message": "We know the request is either being removed, or already in the process of\nbeing removed through task_work, so we can delete it from our futex list\nupfront. This is important for remove all conditions, as we otherwise\nwill find it multiple times and prevent cancelation progress.\n\nCc: stable@vger.kernel.org\nFixes: 194bb58c6090 (\"io_uring: add support for futex wake and wait\")\nFixes: 8f350194d5cf (\"io_uring: add support for vectored futex waits\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-15 15:37:15 -0600 io_uring/futex: always remove futex entry for cancel all"
    },
    {
        "commit": "5e3afe580a9f5ca173a6bd55ffe10948796ef7e5",
        "message": "Taking the ctx lock is not enough to use the deferred request completion\ninfrastructure, it'll get queued into the list but no one would expect\nit there, so it will sit there until next io_submit_flush_completions().\nIt's hard to care about the cancellation path, so complete it via tw.\n\nFixes: ef7dfac51d8ed (\"io_uring/poll: serialize poll linked timer start with poll removal\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c446740bc16858f8a2a8dcdce899812f21d15f23.1710514702.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-15 09:36:56 -0600 io_uring: fix poll_remove stalled req completion"
    },
    {
        "commit": "e54e09c05c00120cbe817bdb037088035be4bd79",
        "message": "The only user of these was io_uring, and it's not using them anymore.\nMake them static and remove them from the socket header file.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nLink: https://lore.kernel.org/r/1b6089d3-c1cf-464a-abd3-b0f0b6bb2523@kernel.dk\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-14 16:48:53 -0700 net: remove {revc,send}msg_copy_msghdr() from exports"
    },
    {
        "commit": "67d1189d1095d471ed7fa426c7e384a7140a5dd7",
        "message": "Looking at the error path of __io_uaddr_map, if we fail after pinning\nthe pages for any reasons, ret will be set to -EINVAL and the error\nhandler won't properly release the pinned pages.\n\nI didn't manage to trigger it without forcing a failure, but it can\nhappen in real life when memory is heavily fragmented.\n\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nFixes: 223ef4743164 (\"io_uring: don't allow IORING_SETUP_NO_MMAP rings on highmem pages\")\nLink: https://lore.kernel.org/r/20240313213912.1920-1-krisman@suse.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-13 16:08:25 -0600 io_uring: Fix release of pinned pages when __io_uaddr_map fails"
    },
    {
        "commit": "9219e4a9d4ad57323837f7c3562964e61840b17a",
        "message": "In buffer lists we have ->is_mapped as well as ->is_mmap, it's\npretty hard to stay sane double checking which one means what,\nand in the long run there is a high chance of an eventual bug.\nRename ->is_mapped into ->is_buf_ring.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c4838f4d8ad506ad6373f1c305aee2d2c1a89786.1710343154.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-13 14:50:42 -0600 io_uring/kbuf: rename is_mapped"
    },
    {
        "commit": "2c5c0ba1179d31b0a030b45a16df6181d1bc3ea6",
        "message": "We never pass a null (top-level) pointer, remove the check.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0e1a46f9a5cd38e6876905e8030bdff9b0845e96.1710343154.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-13 14:50:42 -0600 io_uring: simplify io_pages_free"
    },
    {
        "commit": "cef59d1ea7170ec753182302645a0191c8aa3382",
        "message": "We make a few cancellation judgements based on ctx->rings, so let's\nzero it afer deallocation for IORING_SETUP_NO_MMAP just like it's\ndone with the mmap case. Likely, it's not a real problem, but zeroing\nis safer and better tested.\n\nCc: stable@vger.kernel.org\nFixes: 03d89a2de25bbc (\"io_uring: support for user allocated memory for rings/sqes\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9ff6cdf91429b8a51699c210e1f6af6ea3f8bdcf.1710255382.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-12 09:21:36 -0600 io_uring: clean rings on NO_MMAP alloc fail"
    },
    {
        "commit": "0a3737db8479b77f95f4bfda8e71b03c697eb56a",
        "message": "If read multishot is being invoked from the poll retry handler, then we\nshould return IOU_ISSUE_SKIP_COMPLETE rather than -EAGAIN. If not, then\na CQE will be posted with -EAGAIN rather than triggering the retry when\nthe file is flagged as readable again.\n\nCc: stable@vger.kernel.org\nReported-by: Sargun Dhillon <sargun@meta.com>\nFixes: fc68fcda04910 (\"io_uring/rw: add support for IORING_OP_READ_MULTISHOT\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-12 08:29:47 -0600 io_uring/rw: return IOU_ISSUE_SKIP_COMPLETE for multishot retry"
    },
    {
        "commit": "6f0974eccbf78baead1735722c4f1ee3eb9422cd",
        "message": "This kind of state is per-syscall, and since we're doing the waiting off\nentering the io_uring_enter(2) syscall, there's no way that iowait can\nalready be set for this case. Simplify it by setting it if we need to,\nand always clearing it to 0 when done.\n\nFixes: 7b72d661f1f2 (\"io_uring: gate iowait schedule on having pending requests\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-11 15:02:59 -0600 io_uring: don't save/restore iowait state"
    },
    {
        "commit": "d2c84bdce25a678c1e1f116d65b58790bd241af0",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - Make running of task_work internal loops more fair, and unify how the\n   different methods deal with them (me)\n\n - Support for per-ring NAPI. The two minor networking patches are in a\n   shared branch with netdev (Stefan)\n\n - Add support for truncate (Tony)\n\n - Export SQPOLL utilization stats (Xiaobing)\n\n - Multishot fixes (Pavel)\n\n - Fix for a race in manipulating the request flags via poll (Pavel)\n\n - Cleanup the multishot checking by making it generic, moving it out of\n   opcode handlers (Pavel)\n\n - Various tweaks and cleanups (me, Kunwu, Alexander)\n\n* tag 'for-6.9/io_uring-20240310' of git://git.kernel.dk/linux: (53 commits)\n  io_uring: Fix sqpoll utilization check racing with dying sqpoll\n  io_uring/net: dedup io_recv_finish req completion\n  io_uring: refactor DEFER_TASKRUN multishot checks\n  io_uring: fix mshot io-wq checks\n  io_uring/net: add io_req_msg_cleanup() helper\n  io_uring/net: simplify msghd->msg_inq checking\n  io_uring/kbuf: rename REQ_F_PARTIAL_IO to REQ_F_BL_NO_RECYCLE\n  io_uring/net: remove dependency on REQ_F_PARTIAL_IO for sr->done_io\n  io_uring/net: correctly handle multishot recvmsg retry setup\n  io_uring/net: clear REQ_F_BL_EMPTY in the multishot retry handler\n  io_uring: fix io_queue_proc modifying req->flags\n  io_uring: fix mshot read defer taskrun cqe posting\n  io_uring/net: fix overflow check in io_recvmsg_mshot_prep()\n  io_uring/net: correct the type of variable\n  io_uring/sqpoll: statistics of the true utilization of sq threads\n  io_uring/net: move recv/recvmsg flags out of retry loop\n  io_uring/kbuf: flag request if buffer pool is empty after buffer pick\n  io_uring/net: improve the usercopy for sendmsg/recvmsg\n  io_uring/net: move receive multishot out of the generic msghdr path\n  io_uring/net: unify how recvmsg and sendmsg copy in the msghdr\n  ...",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-11 11:35:31 -0700 Merge tag 'for-6.9/io_uring-20240310' of git://git.kernel.dk/linux"
    },
    {
        "commit": "606559dc4fa36a954a51fbf1c6c0cc320f551fe0",
        "message": "Commit 3fcb9d17206e (\"io_uring/sqpoll: statistics of the true\nutilization of sq threads\"), currently in Jens for-next branch, peeks at\nio_sq_data->thread to report utilization statistics. But, If\nio_uring_show_fdinfo races with sqpoll terminating, even though we hold\nthe ctx lock, sqd->thread might be NULL and we hit the Oops below.\n\nNote that we could technically just protect the getrusage() call and the\nsq total/work time calculations.  But showing some sq\ninformation (pid/cpu) and not other information (utilization) is more\nconfusing than not reporting anything, IMO.  So let's hide it all if we\nhappen to race with a dying sqpoll.\n\nThis can be triggered consistently in my vm setup running\nsqpoll-cancel-hang.t in a loop.\n\nBUG: kernel NULL pointer dereference, address: 00000000000007b0\nPGD 0 P4D 0\nOops: 0000 [#1] PREEMPT SMP NOPTI\nCPU: 0 PID: 16587 Comm: systemd-coredum Not tainted 6.8.0-rc3-g3fcb9d17206e-dirty #69\nHardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS unknown 2/2/2022\nRIP: 0010:getrusage+0x21/0x3e0\nCode: 90 90 90 90 90 90 90 90 90 0f 1f 44 00 00 55 48 89 d1 48 89 e5 41 57 41 56 41 55 41 54 49 89 fe 41 52 53 48 89 d3 48 83 ec 30 <4c> 8b a7 b0 07 00 00 48 8d 7a 08 65 48 8b 04 25 28 00 00 00 48 89\nRSP: 0018:ffffa166c671bb80 EFLAGS: 00010282\nRAX: 00000000000040ca RBX: ffffa166c671bc60 RCX: ffffa166c671bc60\nRDX: ffffa166c671bc60 RSI: 0000000000000000 RDI: 0000000000000000\nRBP: ffffa166c671bbe0 R08: ffff9448cc3930c0 R09: 0000000000000000\nR10: ffffa166c671bd50 R11: ffffffff9ee89260 R12: 0000000000000000\nR13: ffff9448ce099480 R14: 0000000000000000 R15: ffff9448cff5b000\nFS:  00007f786e225900(0000) GS:ffff94493bc00000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00000000000007b0 CR3: 000000010d39c000 CR4: 0000000000750ef0\nPKRU: 55555554\nCall Trace:\n <TASK>\n ? __die_body+0x1a/0x60\n ? page_fault_oops+0x154/0x440\n ? srso_alias_return_thunk+0x5/0xfbef5\n ? do_user_addr_fault+0x174/0x7c0\n ? srso_alias_return_thunk+0x5/0xfbef5\n ? exc_page_fault+0x63/0x140\n ? asm_exc_page_fault+0x22/0x30\n ? getrusage+0x21/0x3e0\n ? seq_printf+0x4e/0x70\n io_uring_show_fdinfo+0x9db/0xa10\n ? srso_alias_return_thunk+0x5/0xfbef5\n ? vsnprintf+0x101/0x4d0\n ? srso_alias_return_thunk+0x5/0xfbef5\n ? seq_vprintf+0x34/0x50\n ? srso_alias_return_thunk+0x5/0xfbef5\n ? seq_printf+0x4e/0x70\n ? seq_show+0x16b/0x1d0\n ? __pfx_io_uring_show_fdinfo+0x10/0x10\n seq_show+0x16b/0x1d0\n seq_read_iter+0xd7/0x440\n seq_read+0x102/0x140\n vfs_read+0xae/0x320\n ? srso_alias_return_thunk+0x5/0xfbef5\n ? __do_sys_newfstat+0x35/0x60\n ksys_read+0xa5/0xe0\n do_syscall_64+0x50/0x110\n entry_SYSCALL_64_after_hwframe+0x6e/0x76\nRIP: 0033:0x7f786ec1db4d\nCode: e8 46 e3 01 00 0f 1f 84 00 00 00 00 00 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 80 3d d9 ce 0e 00 00 74 17 31 c0 0f 05 <48> 3d 00 f0 ff ff 77 5b c3 66 2e 0f 1f 84 00 00 00 00 00 48 83 ec\nRSP: 002b:00007ffcb361a4b8 EFLAGS: 00000246 ORIG_RAX: 0000000000000000\nRAX: ffffffffffffffda RBX: 000055a4c8fe42f0 RCX: 00007f786ec1db4d\nRDX: 0000000000000400 RSI: 000055a4c8fe48a0 RDI: 0000000000000006\nRBP: 00007f786ecfb0b0 R08: 00007f786ecfb2a8 R09: 0000000000000001\nR10: 0000000000000000 R11: 0000000000000246 R12: 00007f786ecfaf60\nR13: 000055a4c8fe42f0 R14: 0000000000000000 R15: 00007ffcb361a628\n </TASK>\nModules linked in:\nCR2: 00000000000007b0\n---[ end trace 0000000000000000 ]---\nRIP: 0010:getrusage+0x21/0x3e0\nCode: 90 90 90 90 90 90 90 90 90 0f 1f 44 00 00 55 48 89 d1 48 89 e5 41 57 41 56 41 55 41 54 49 89 fe 41 52 53 48 89 d3 48 83 ec 30 <4c> 8b a7 b0 07 00 00 48 8d 7a 08 65 48 8b 04 25 28 00 00 00 48 89\nRSP: 0018:ffffa166c671bb80 EFLAGS: 00010282\nRAX: 00000000000040ca RBX: ffffa166c671bc60 RCX: ffffa166c671bc60\nRDX: ffffa166c671bc60 RSI: 0000000000000000 RDI: 0000000000000000\nRBP: ffffa166c671bbe0 R08: ffff9448cc3930c0 R09: 0000000000000000\nR10: ffffa166c671bd50 R11: ffffffff9ee89260 R12: 0000000000000000\nR13: ffff9448ce099480 R14: 0000000000000000 R15: ffff9448cff5b000\nFS:  00007f786e225900(0000) GS:ffff94493bc00000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00000000000007b0 CR3: 000000010d39c000 CR4: 0000000000750ef0\nPKRU: 55555554\nKernel panic - not syncing: Fatal exception\nKernel Offset: 0x1ce00000 from 0xffffffff81000000 (relocation range: 0xffffffff80000000-0xffffffffbfffffff)\n\nFixes: 3fcb9d17206e (\"io_uring/sqpoll: statistics of the true utilization of sq threads\")\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20240309003256.358-1-krisman@suse.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-09 07:27:09 -0700 io_uring: Fix sqpoll utilization check racing with dying sqpoll"
    },
    {
        "commit": "1af04699c59713a7693cc63d80b29152579e61c3",
        "message": "There are two block in io_recv_finish() completing the request, which we\ncan combine and remove jumping.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0e338dcb33c88de83809fda021cba9e7c9681620.1709905727.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-08 07:59:20 -0700 io_uring/net: dedup io_recv_finish req completion"
    },
    {
        "commit": "e0e4ab52d17096d96c21a6805ccd424b283c3c6d",
        "message": "We disallow DEFER_TASKRUN multishots from running by io-wq, which is\nchecked by individual opcodes in the issue path. We can consolidate all\nit in io_wq_submit_work() at the same time moving the checks out of the\nhot path.\n\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e492f0f11588bb5aa11d7d24e6f53b7c7628afdb.1709905727.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-08 07:58:23 -0700 io_uring: refactor DEFER_TASKRUN multishot checks"
    },
    {
        "commit": "3a96378e22cc46c7c49b5911f6c8631527a133a9",
        "message": "When checking for concurrent CQE posting, we're not only interested in\nrequests running from the poll handler but also strayed requests ended\nup in normal io-wq execution. We're disallowing multishots in general\nfrom io-wq, not only when they came in a certain way.\n\nCc: stable@vger.kernel.org\nFixes: 17add5cea2bba (\"io_uring: force multishot CQEs into task context\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d8c5b36a39258036f93301cd60d3cd295e40653d.1709905727.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-08 07:58:23 -0700 io_uring: fix mshot io-wq checks"
    },
    {
        "commit": "d9b441889c3595aa18f89ee42c6d22bb62234343",
        "message": "For the fast inline path, we manually recycle the io_async_msghdr and\nfree the iovec, and then clear the REQ_F_NEED_CLEANUP flag to avoid\nthat needing doing in the slower path. We already do that in 2 spots, and\nin preparation for adding more, add a helper and use it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-08 07:57:27 -0700 io_uring/net: add io_req_msg_cleanup() helper"
    },
    {
        "commit": "fb6328bc2ab58dcf2998bd173f1ef0f3eb7be19a",
        "message": "Just check for larger than zero rather than check for non-zero and\nnot -1. This is easier to read, and also protects against any errants\n< 0 values that aren't -1.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-08 07:56:31 -0700 io_uring/net: simplify msghd->msg_inq checking"
    },
    {
        "commit": "186daf2385295acf19ecf48f4d5214cc2d925933",
        "message": "We only use the flag for this purpose, so rename it accordingly. This\nfurther prevents various other use cases of it, keeping it clean and\nconsistent. Then we can also check it in one spot, when it's being\nattempted recycled, and remove some dead code in io_kbuf_recycle_ring().\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-08 07:56:27 -0700 io_uring/kbuf: rename REQ_F_PARTIAL_IO to REQ_F_BL_NO_RECYCLE"
    },
    {
        "commit": "9817ad85899fb695f875610fb743cb18cf087582",
        "message": "Ensure that prep handlers always initialize sr->done_io before any\npotential failure conditions, and with that, we now it's always been\nset even for the failure case.\n\nWith that, we don't need to use the REQ_F_PARTIAL_IO flag to gate on that.\nAdditionally, we should not overwrite req->cqe.res unless sr->done_io is\nactually positive.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-08 07:56:21 -0700 io_uring/net: remove dependency on REQ_F_PARTIAL_IO for sr->done_io"
    },
    {
        "commit": "deaef31bc1ec7966698a427da8c161930830e1cf",
        "message": "If we loop for multishot receive on the initial attempt, and then abort\nlater on to wait for more, we miss a case where we should be copying the\nio_async_msghdr from the stack to stable storage. This leads to the next\nretry potentially failing, if the application had the msghdr on the\nstack.\n\nCc: stable@vger.kernel.org\nFixes: 9bb66906f23e (\"io_uring: support multishot in recvmsg\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-07 17:48:03 -0700 io_uring/net: correctly handle multishot recvmsg retry setup"
    },
    {
        "commit": "b5311dbc2c2eefac00f12888dcd15e90238d1828",
        "message": "This flag should not be persistent across retries, so ensure we clear\nit before potentially attemting a retry.\n\nFixes: c3f9109dbc9e (\"io_uring/kbuf: flag request if buffer pool is empty after buffer pick\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-07 13:22:05 -0700 io_uring/net: clear REQ_F_BL_EMPTY in the multishot retry handler"
    },
    {
        "commit": "1a8ec63b2b6c91caec87d4e132b1f71b5df342be",
        "message": "With multiple poll entries __io_queue_proc() might be running in\nparallel with poll handlers and possibly task_work, we should not be\ncarelessly modifying req->flags there. io_poll_double_prepare() handles\na similar case with locking but it's much easier to move it into\n__io_arm_poll_handler().\n\nCc: stable@vger.kernel.org\nFixes: 595e52284d24a (\"io_uring/poll: don't enable lazy wake for POLLEXCLUSIVE\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/455cc49e38cf32026fa1b49670be8c162c2cb583.1709834755.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-07 11:10:28 -0700 io_uring: fix io_queue_proc modifying req->flags"
    },
    {
        "commit": "70581dcd0601a09f134f23268e3374e15d736824",
        "message": "We can't post CQEs from io-wq with DEFER_TASKRUN set, normal completions\nare handled but aux should be explicitly disallowed by opcode handlers.\n\nCc: stable@vger.kernel.org\nFixes: fc68fcda04910 (\"io_uring/rw: add support for IORING_OP_READ_MULTISHOT\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6fb7cba6f5366da25f4d3eb95273f062309d97fa.1709740837.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-07 06:30:33 -0700 io_uring: fix mshot read defer taskrun cqe posting"
    },
    {
        "commit": "8ede3db5061bb1fe28e2c9683329aafa89d2b1b4",
        "message": "The \"controllen\" variable is type size_t (unsigned long).  Casting it\nto int could lead to an integer underflow.\n\nThe check_add_overflow() function considers the type of the destination\nwhich is type int.  If we add two positive values and the result cannot\nfit in an integer then that's counted as an overflow.\n\nHowever, if we cast \"controllen\" to an int and it turns negative, then\nnegative values *can* fit into an int type so there is no overflow.\n\nGood: 100 + (unsigned long)-4 = 96  <-- overflow\n Bad: 100 + (int)-4 = 96 <-- no overflow\n\nI deleted the cast of the sizeof() as well.  That's not a bug but the\ncast is unnecessary.\n\nFixes: 9b0fc3c054ff (\"io_uring: fix types in io_recvmsg_multishot_overflow\")\nSigned-off-by: Dan Carpenter <dan.carpenter@linaro.org>\nLink: https://lore.kernel.org/r/138bd2e2-ede8-4bcc-aa7b-f3d9de167a37@moroto.mountain\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-04 16:33:15 -0700 io_uring/net: fix overflow check in io_recvmsg_mshot_prep()"
    },
    {
        "commit": "86bcacc957fc2d0403aa0e652757eec59a5fd7ca",
        "message": "The namelen is of type int. It shouldn't be made size_t which is\nunsigned. The signed number is needed for error checking before use.\n\nFixes: c55978024d12 (\"io_uring/net: move receive multishot out of the generic msghdr path\")\nSigned-off-by: Muhammad Usama Anjum <usama.anjum@collabora.com>\nLink: https://lore.kernel.org/r/20240301144349.2807544-1-usama.anjum@collabora.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-04 16:33:11 -0700 io_uring/net: correct the type of variable"
    },
    {
        "commit": "3fcb9d17206e31630f802a3ab52081d1342b8ed9",
        "message": "Count the running time and actual IO processing time of the sqpoll\nthread, and output the statistical data to fdinfo.\n\nVariable description:\n\"work_time\" in the code represents the sum of the jiffies of the sq\nthread actually processing IO, that is, how many milliseconds it\nactually takes to process IO. \"total_time\" represents the total time\nthat the sq thread has elapsed from the beginning of the loop to the\ncurrent time point, that is, how many milliseconds it has spent in\ntotal.\n\nThe test tool is fio, and its parameters are as follows:\n[global]\nioengine=io_uring\ndirect=1\ngroup_reporting\nbs=128k\nnorandommap=1\nrandrepeat=0\nrefill_buffers\nramp_time=30s\ntime_based\nruntime=1m\nclocksource=clock_gettime\noverwrite=1\nlog_avg_msec=1000\nnumjobs=1\n\n[disk0]\nfilename=/dev/nvme0n1\nrw=read\niodepth=16\nhipri\nsqthread_poll=1\n\nThe test results are as follows:\nEvery 2.0s: cat /proc/9230/fdinfo/6 | grep -E Sq\nSqMask: 0x3\nSqHead: 3197153\nSqTail: 3197153\nCachedSqHead:   3197153\nSqThread:       9231\nSqThreadCpu:    11\nSqTotalTime:    18099614\nSqWorkTime:     16748316\n\nThe test results corresponding to different iodepths are as follows:\n|-----------|-------|-------|-------|------|-------|\n|   iodepth |   1   |   4   |   8   |  16  |  64   |\n|-----------|-------|-------|-------|------|-------|\n|utilization| 2.9%  | 8.8%  | 10.9% | 92.9%| 84.4% |\n|-----------|-------|-------|-------|------|-------|\n|    idle   | 97.1% | 91.2% | 89.1% | 7.1% | 15.6% |\n|-----------|-------|-------|-------|------|-------|\n\nSigned-off-by: Xiaobing Li <xiaobing.li@samsung.com>\nLink: https://lore.kernel.org/r/20240228091251.543383-1-xiaobing.li@samsung.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-01 06:28:19 -0700 io_uring/sqpoll: statistics of the true utilization of sq threads"
    },
    {
        "commit": "eb18c29dd2a3d49cf220ee34411ff0fe60b36bf2",
        "message": "The flags don't change, just intialize them once rather than every loop\nfor multishot.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-01 06:28:19 -0700 io_uring/net: move recv/recvmsg flags out of retry loop"
    },
    {
        "commit": "76f06cbd7b310e705465e11ca93be8db247aec50",
        "message": "David Wei says:\n\n====================\nnetdevsim: link and forward skbs between ports\n\nThis patchset adds the ability to link two netdevsim ports together and\nforward skbs between them, similar to veth. The goal is to use netdevsim\nfor testing features e.g. zero copy Rx using io_uring.\n\nThis feature was tested locally on QEMU, and a selftest is included.\n\nI ran netdev selftests CI style and all tests but the following passed:\n- gro.sh\n- l2tp.sh\n- ip_local_port_range.sh\n\ngro.sh fails because virtme-ng mounts as read-only and it tries to write\nto log.txt. This issue was reported to virtme-ng upstream.\n\nl2tp.sh and ip_local_port_range.sh both fail for me on net-next/main as\nwell.\n\n---\nv13->v14:\n- implement ndo_get_iflink()\n- fix returning 0 if peer is already linked during linking or not linked\n  during unlinking\n- bump dropped counter if nsim_ipsec_tx() fails and generally reorder\n  nsim_start_xmit()\n- fix overflowing lines and indentations\n\nv12->v13:\n- wait for socat listening port to be ready before sending data in\n  selftest\n\nv11->v12:\n- fix leaked netns refs\n- fix rtnetlink.sh kci_test_ipsec_offload() selftest\n\nv10->v11:\n- add udevadm settle after creating netdevsims in selftest\n\nv9->v10:\n- fix not freeing skb when not there is no peer\n- prevent possible id clashes in selftest\n- cleanup selftest on error paths\n\nv8->v9:\n- switch to getting netns using fd rather than id\n- prevent linking a netdevsim to itself\n- update tests\n\nv7->v8:\n- fix not dereferencing RCU ptr using rcu_dereference()\n- remove unused variables in selftest\n\nv6->v7:\n- change link syntax to netnsid:ifidx\n- replace dev_get_by_index() with __dev_get_by_index()\n- check for NULL peer when linking\n- add a sysfs attribute for unlinking\n- only update Tx stats if not dropped\n- update selftest\n\nv5->v6:\n- reworked to link two netdevsims using sysfs attribute on the bus\n  device instead of debugfs due to deadlock possibility if a netdevsim\n  is removed during linking\n- removed unnecessary patch maintaining a list of probed nsim_devs\n- updated selftest\n\nv4->v5:\n- reduce nsim_dev_list_lock critical section\n- fixed missing mutex unlock during unwind ladder\n- rework nsim_dev_peer_write synchronization to take devlink lock as\n  well as rtnl_lock\n- return err msgs to user during linking if port doesn't exist or\n  linking to self\n- update tx stats outside of RCU lock\n\nv3->v4:\n- maintain a mutex protected list of probed nsim_devs instead of using\n  nsim_bus_dev\n- fixed synchronization issues by taking rtnl_lock\n- track tx_dropped skbs\n\nv2->v3:\n- take lock when traversing nsim_bus_dev_list\n- take device ref when getting a nsim_bus_dev\n- return 0 if nsim_dev_peer_read cannot find the port\n- address code formatting\n- do not hard code values in selftests\n- add Makefile for selftests\n\nv1->v2:\n- renamed debugfs file from \"link\" to \"peer\"\n- replaced strstep() with sscanf() for consistency\n- increased char[] buf sz to 22 for copying id + port from user\n- added err msg w/ expected fmt when linking as a hint to user\n- prevent linking port to itself\n- protect peer ptr using RCU\n\n====================\n\nSigned-off-by: David S. Miller <davem@davemloft.net>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-03-01 10:43:11 +0000 Merge branch 'netdevsim-link'"
    },
    {
        "commit": "13fe8e6825e44129b6cbeee41d3012554bf8d687",
        "message": "The current command UBLK_CMD_DEL_DEV won't return until the device is\nreleased, this way looks more reliable, but makes userspace more\ndifficult to implement, especially about orders: unmap command\nbuffer(which holds one ublkc reference), ublkc close,\nio_uring_file_unregister, ublkb close.\n\nAdd UBLK_CMD_DEL_DEV_ASYNC so that device deletion won't wait release,\nthen userspace needn't worry about the above order. Actually both loop\nand nbd is deleted in this async way.\n\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20240223075539.89945-3-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-28 18:47:08 -0700 ublk: add UBLK_CMD_DEL_DEV_ASYNC"
    },
    {
        "commit": "1221b9e982e181f1c37789c46fe5bfe32d97bec4",
        "message": "Firstly convert get_device() and put_device() into ublk_get_device()\nand ublk_put_device().\n\nSecondly annotate ublk_get_device() & ublk_put_device() as noinline\nfor trace, especially it is often to trigger device deletion hang\nwhen incorrect order is used on ublkc mmap, ublkc close,\nio_uring_sqe_unregister_file, ublkb close.\n\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20240223075539.89945-2-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-28 18:47:08 -0700 ublk: improve getting & putting ublk device"
    },
    {
        "commit": "c3f9109dbc9e2cd0b2c3ba0536431eef282783e9",
        "message": "Normally we do an extra roundtrip for retries even if the buffer pool has\ndepleted, as we don't check that upfront. Rather than add this check, have\nthe buffer selection methods mark the request with REQ_F_BL_EMPTY if the\nused buffer group is out of buffers after this selection. This is very\ncheap to do once we're all the way inside there anyway, and it gives the\ncaller a chance to make better decisions on how to proceed.\n\nFor example, recv/recvmsg multishot could check this flag when it\ndecides whether to keep receiving or not.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-27 11:52:45 -0700 io_uring/kbuf: flag request if buffer pool is empty after buffer pick"
    },
    {
        "commit": "792060de8b3e9ca11fab4afc0c3c5927186152a2",
        "message": "We're spending a considerable amount of the sendmsg/recvmsg time just\ncopying in the message header. And for provided buffers, the known\nsingle entry iovec.\n\nBe a bit smarter about it and enable/disable user access around our\ncopying. In a test case that does both sendmsg and recvmsg, the\nruntime before this change (averaged over multiple runs, very stable\ntimes however):\n\nKernel\t\tTime\t\tDiff\n====================================\n-git\t\t4720 usec\n-git+commit\t4311 usec\t-8.7%\n\nand looking at a profile diff, we see the following:\n\n0.25%     +9.33%  [kernel.kallsyms]     [k] _copy_from_user\n4.47%     -3.32%  [kernel.kallsyms]     [k] __io_msg_copy_hdr.constprop.0\n\nwhere we drop more than 9% of _copy_from_user() time, and consequently\nadd time to __io_msg_copy_hdr() where the copies are now attributed to,\nbut with a net win of 6%.\n\nIn comparison, the same test case with send/recv runs in 3745 usec, which\nis (expectedly) still quite a bit faster. But at least sendmsg/recvmsg is\nnow only ~13% slower, where it was ~21% slower before.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-27 11:16:00 -0700 io_uring/net: improve the usercopy for sendmsg/recvmsg"
    },
    {
        "commit": "c55978024d123d43808ab393a0a4ce3ce8568150",
        "message": "Move the actual user_msghdr / compat_msghdr into the send and receive\nsides, respectively, so we can move the uaddr receive handling into its\nown handler, and ditto the multishot with buffer selection logic.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-27 11:09:20 -0700 io_uring/net: move receive multishot out of the generic msghdr path"
    },
    {
        "commit": "52307ac4f2b507f60bae6df5be938d35e199c688",
        "message": "For recvmsg, we roll our own since we support buffer selections. This\nisn't the case for sendmsg right now, but in preparation for doing so,\nmake the recvmsg copy helpers generic so we can call them from the\nsendmsg side as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-27 09:56:50 -0700 io_uring/net: unify how recvmsg and sendmsg copy in the msghdr"
    },
    {
        "commit": "fecc51559a844b7f74119159c3cdb25b80b4e2c6",
        "message": "Cross-merge networking fixes after downstream PR.\n\nConflicts:\n\nnet/ipv4/udp.c\n  f796feabb9f5 (\"udp: add local \"peek offset enabled\" flag\")\n  56667da7399e (\"net: implement lockless setsockopt(SO_PEEK_OFF)\")\n\nAdjacent changes:\n\nnet/unix/garbage.c\n  aa82ac51d633 (\"af_unix: Drop oob_skb ref before purging queue in GC.\")\n  11498715f266 (\"af_unix: Remove io_uring code for GC.\")\n\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-22 15:29:26 -0800 Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net"
    },
    {
        "commit": "1c892cdd8fe004ed6cef4501a7141594a1616368",
        "message": "Pull vfs fixes from Christian Brauner:\n\n - Fix a memory leak in cachefiles\n\n - Restrict aio cancellations to I/O submitted through the aio\n   interfaces as this is otherwise causing issues for I/O submitted\n   via io_uring\n\n - Increase buffer for afs volume status to avoid overflow\n\n - Fix a missing zero-length check in unbuffered writes in the\n   netfs library. If generic_write_checks() returns zero make\n   netfs_unbuffered_write_iter() return right away\n\n - Prevent a leak in i_dio_count caused by netfs_begin_read() operating\n   past i_size. It will return early and leave i_dio_count incremented\n\n - Account for ipv4 addresses as well as ipv6 addresses when processing\n   incoming callbacks in afs\n\n* tag 'vfs-6.8-rc6.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs:\n  fs/aio: Restrict kiocb_set_cancel_fn() to I/O submitted via libaio\n  afs: Increase buffer size in afs_update_volume_status()\n  afs: Fix ignored callbacks over ipv4\n  cachefiles: fix memory leak in cachefiles_add_cache()\n  netfs: Fix missing zero-length check in unbuffered write\n  netfs: Fix i_dio_count leak on DIO read past i_size",
        "kernel_version": "v6.8-rc6",
        "release_date": "2024-02-22 10:06:29 -0800 Merge tag 'vfs-6.8-rc6.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs"
    },
    {
        "commit": "b820de741ae48ccf50dd95e297889c286ff4f760",
        "message": "If kiocb_set_cancel_fn() is called for I/O submitted via io_uring, the\nfollowing kernel warning appears:\n\nWARNING: CPU: 3 PID: 368 at fs/aio.c:598 kiocb_set_cancel_fn+0x9c/0xa8\nCall trace:\n kiocb_set_cancel_fn+0x9c/0xa8\n ffs_epfile_read_iter+0x144/0x1d0\n io_read+0x19c/0x498\n io_issue_sqe+0x118/0x27c\n io_submit_sqes+0x25c/0x5fc\n __arm64_sys_io_uring_enter+0x104/0xab0\n invoke_syscall+0x58/0x11c\n el0_svc_common+0xb4/0xf4\n do_el0_svc+0x2c/0xb0\n el0_svc+0x2c/0xa4\n el0t_64_sync_handler+0x68/0xb4\n el0t_64_sync+0x1a4/0x1a8\n\nFix this by setting the IOCB_AIO_RW flag for read and write I/O that is\nsubmitted by libaio.\n\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Avi Kivity <avi@scylladb.com>\nCc: Sandeep Dhavale <dhavale@google.com>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\nCc: Kent Overstreet <kent.overstreet@linux.dev>\nCc: stable@vger.kernel.org\nSigned-off-by: Bart Van Assche <bvanassche@acm.org>\nLink: https://lore.kernel.org/r/20240215204739.2677806-2-bvanassche@acm.org\nSigned-off-by: Christian Brauner <brauner@kernel.org>",
        "kernel_version": "v6.8-rc6",
        "release_date": "2024-02-21 16:31:49 +0100 fs/aio: Restrict kiocb_set_cancel_fn() to I/O submitted via libaio"
    },
    {
        "commit": "8096015082592cf282fdee9d052aa1d3bbadb805",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single fix for a regression in how overflow is handled for\n  multishot accept requests\"\n\n* tag 'io_uring-6.8-2024-02-16' of git://git.kernel.dk/linux:\n  io_uring/net: fix multishot accept overflow handling",
        "kernel_version": "v6.8-rc5",
        "release_date": "2024-02-16 13:51:20 -0800 Merge tag 'io_uring-6.8-2024-02-16' of git://git.kernel.dk/linux"
    },
    {
        "commit": "73be9a3aabdd976123e7f05dd20dbcf131347e84",
        "message": "Cross-merge networking fixes after downstream PR.\n\nNo conflicts.\n\nAdjacent changes:\n\nnet/core/dev.c\n  9f30831390ed (\"net: add rcu safety to rtnl_prop_list_size()\")\n  723de3ebef03 (\"net: free altname using an RCU callback\")\n\nnet/unix/garbage.c\n  11498715f266 (\"af_unix: Remove io_uring code for GC.\")\n  25236c91b5ab (\"af_unix: Fix task hung while purging oob_skb in GC.\")\n\ndrivers/net/ethernet/renesas/ravb_main.c\n  ed4adc07207d (\"net: ravb: Count packets instead of descriptors in GbEth RX path\"\n)\n  c2da9408579d (\"ravb: Add Rx checksum offload support for GbEth\")\n\nnet/mptcp/protocol.c\n  bdd70eb68913 (\"mptcp: drop the push_pending field\")\n  28e5c1380506 (\"mptcp: annotate lockless accesses around read-mostly fields\")\n\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-15 16:20:04 -0800 Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net"
    },
    {
        "commit": "b4ccc4dd1330a4d0db6aa4c6781631d1bab76c45",
        "message": "1 usec is not as short as it used to be, and it makes sense to allow 0\nfor a busy poll timeout - this means just do one loop to check if we\nhave anything available. Add a separate ->napi_enabled to check if napi\nhas been enabled or not.\n\nWhile at it, move the writing of the ctx napi values after we've copied\nthe old values back to userspace. This ensures that if the call fails,\nwe'll be in the same state as we were before, rather than some\nindeterminate state.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-15 15:37:28 -0700 io_uring/napi: enable even with a timeout of 0"
    },
    {
        "commit": "871760eb7af57accc5402142154e64f21701fa16",
        "message": "This function now deals only with discarding overflow entries on ring\nfree and exit, and it no longer returns whether we successfully flushed\nall entries as there's no CQE posting involved anymore. Kill the\noutdated comment.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-15 14:04:56 -0700 io_uring: kill stale comment for io_cqring_overflow_kill()"
    },
    {
        "commit": "a37ee9e117ef73bbc2f5c0b31911afd52d229861",
        "message": "If we hit CQ ring overflow when attempting to post a multishot accept\ncompletion, we don't properly save the result or return code. This\nresults in losing the accepted fd value.\n\nInstead, we return the result from the poll operation that triggered\nthe accept retry. This is generally POLLIN|POLLPRI|POLLRDNORM|POLLRDBAND\nwhich is 0xc3, or 195, which looks like a valid file descriptor, but it\nreally has no connection to that.\n\nHandle this like we do for other multishot completions - assign the\nresult, and return IOU_STOP_MULTISHOT to cancel any further completions\nfrom this request when overflow is hit. This preserves the result, as we\nshould, and tells the application that the request needs to be re-armed.\n\nCc: stable@vger.kernel.org\nFixes: 515e26961295 (\"io_uring: revert \"io_uring fix multishot accept ordering\"\")\nLink: https://github.com/axboe/liburing/issues/1062\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc5",
        "release_date": "2024-02-14 18:30:19 -0700 io_uring/net: fix multishot accept overflow handling"
    },
    {
        "commit": "c8d8fc3b2d9d4c81e3d3b23eca504ad713a91219",
        "message": "A previous commit moved to using just the private task_work list for\nSQPOLL, but it neglected to update the check for whether we have\npending task_work. Normally this is fine as we'll attempt to run it\nunconditionally, but if we race with going to sleep AND task_work\nbeing added, then we certainly need the right check here.\n\nFixes: af5d68f8892f (\"io_uring/sqpoll: manage task_work privately\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-14 13:57:20 -0700 io_uring/sqpoll: use the correct check for pending task_work"
    },
    {
        "commit": "78f9b61bd8e5466f0e90823e64e3d87c41f6258c",
        "message": "If there's no current work on the list, we still need to potentially\nwake the SQPOLL task if it is sleeping. This is ordered with the\nwait queue addition in sqpoll, which adds to the wait queue before\nchecking for pending work items.\n\nFixes: af5d68f8892f (\"io_uring/sqpoll: manage task_work privately\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-14 13:56:08 -0700 io_uring: wake SQPOLL task when task_work is added to an empty queue"
    },
    {
        "commit": "428f13826855e3eea44bf13cedbf33f382ef8794",
        "message": "While testing io_uring NAPI with DEFER_TASKRUN, I ran into slowdowns and\nstalls in packet delivery. Turns out that while\nio_napi_busy_loop_should_end() aborts appropriately on regular\ntask_work, it does not abort if we have local task_work pending.\n\nMove io_has_work() into the private io_uring.h header, and gate whether\nwe should continue polling on that as well. This makes NAPI polling on\nsend/receive work as designed with IORING_SETUP_DEFER_TASKRUN as well.\n\nFixes: 8d0c12a80cde (\"io-uring: add napi busy poll support\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-14 13:01:25 -0700 io_uring/napi: ensure napi polling is aborted when work is available"
    },
    {
        "commit": "3fb1764c6b57808ddab7fe7c242fa04c2479ef0a",
        "message": "Changes to AF_UNIX trigger rebuild of io_uring, but io_uring does\nnot use AF_UNIX anymore.\n\nLet's not include af_unix.h and instead include necessary headers.\n\nSigned-off-by: Kuniyuki Iwashima <kuniyu@amazon.com>\nLink: https://lore.kernel.org/r/20240212234236.63714-1-kuniyu@amazon.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-12 19:02:11 -0700 io_uring: Don't include af_unix.h."
    },
    {
        "commit": "ef1186c1a875bfa8a8cbfc2a9670b14b082187a9",
        "message": "This adds an api to register and unregister the napi for io-uring. If\nthe arg value is specified when unregistering, the current napi setting\nfor the busy poll timeout is copied into the user structure. If this is\nnot required, NULL can be passed as the arg value.\n\nSigned-off-by: Stefan Roesch <shr@devkernel.io>\nAcked-by: Jakub Kicinski <kuba@kernel.org>\nLink: https://lore.kernel.org/r/20230608163839.2891748-7-shr@devkernel.io\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-09 11:54:32 -0700 io_uring: add register/unregister napi function"
    },
    {
        "commit": "8d0c12a80cdeb80d5e0510e96d38fe551ed8e9b5",
        "message": "This adds the napi busy polling support in io_uring.c. It adds a new\nnapi_list to the io_ring_ctx structure. This list contains the list of\nnapi_id's that are currently enabled for busy polling. The list is\nsynchronized by the new napi_lock spin lock. The current default napi\nbusy polling time is stored in napi_busy_poll_to. If napi busy polling\nis not enabled, the value is 0.\n\nIn addition there is also a hash table. The hash table store the napi\nid and the pointer to the above list nodes. The hash table is used to\nspeed up the lookup to the list elements. The hash table is synchronized\nwith rcu.\n\nThe NAPI_TIMEOUT is stored as a timeout to make sure that the time a\nnapi entry is stored in the napi list is limited.\n\nThe busy poll timeout is also stored as part of the io_wait_queue. This\nis necessary as for sq polling the poll interval needs to be adjusted\nand the napi callback allows only to pass in one value.\n\nThis has been tested with two simple programs from the liburing library\nrepository: the napi client and the napi server program. The client\nsends a request, which has a timestamp in its payload and the server\nreplies with the same payload. The client calculates the roundtrip time\nand stores it to calculate the results.\n\nThe client is running on host1 and the server is running on host 2 (in\nthe same rack). The measured times below are roundtrip times. They are\naverage times over 5 runs each. Each run measures 1 million roundtrips.\n\n                   no rx coal          rx coal: frames=88,usecs=33\nDefault              57us                    56us\n\nclient_poll=100us    47us                    46us\n\nserver_poll=100us    51us                    46us\n\nclient_poll=100us+   40us                    40us\nserver_poll=100us\n\nclient_poll=100us+   41us                    39us\nserver_poll=100us+\nprefer napi busy poll on client\n\nclient_poll=100us+   41us                    39us\nserver_poll=100us+\nprefer napi busy poll on server\n\nclient_poll=100us+   41us                    39us\nserver_poll=100us+\nprefer napi busy poll on client + server\n\nSigned-off-by: Stefan Roesch <shr@devkernel.io>\nSuggested-by: Olivier Langlois <olivier@trillion01.com>\nAcked-by: Jakub Kicinski <kuba@kernel.org>\nLink: https://lore.kernel.org/r/20230608163839.2891748-5-shr@devkernel.io\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-09 11:54:19 -0700 io-uring: add napi busy poll support"
    },
    {
        "commit": "adaad27980a370179c0eef96b4afe6a459f856a7",
        "message": "Pull netdev side of the io_uring napi support.\n\n* 'for-io_uring-add-napi-busy-polling-support' of git://git.kernel.org/pub/scm/linux/kernel/git/kuba/linux:\n  net: add napi_busy_loop_rcu()\n  net: split off __napi_busy_poll from napi_busy_poll",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-09 11:53:39 -0700 Merge branch 'for-io_uring-add-napi-busy-polling-support' of git://git.kernel.org/pub/scm/linux/kernel/git/kuba/linux into for-6.9/io_uring"
    },
    {
        "commit": "f6ce9a1f6aecccac25635125f6bd9072322c3747",
        "message": "Merge netdev bits of io_uring busy polling support.\n\nJens Axboe says:\n\n====================\nio_uring: add napi busy polling support\n\nI finally got around to testing this patchset in its current form, and\nresults look fine to me. It Works. Using the basic ping/pong test that's\npart of the liburing addition, without enabling NAPI I get:\n\nStock settings, no NAPI, 100k packets:\n\n rtt(us) min/avg/max/mdev = 31.730/37.006/87.960/0.497\n\n and with -t10 -b enabled:\n\n rtt(us) min/avg/max/mdev = 23.250/29.795/63.511/1.203\n\nIn short, this patchset enables per io_uring NAPI enablement, rather\nthan need to enable that globally. This allows targeted NAPI usage with\nio_uring.\n\nHere's Stefan's v15 posting, which predates this one:\n\nhttps://lore.kernel.org/io-uring/20230608163839.2891748-1-shr@devkernel.io/\n====================\n\nLink: https://lore.kernel.org/r/20240206163422.646218-1-axboe@kernel.dk\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-09 10:35:34 -0800 Merge branch 'for-io_uring-add-napi-busy-polling-support'"
    },
    {
        "commit": "b4bb1900c12e6a0fe11ff51e1aa6eea19a4ad635",
        "message": "Adds support for doing truncate through io_uring, eliminating\nthe need for applications to roll their own thread pool or offload\nmechanism to be able to do non-blocking truncates.\n\nSigned-off-by: Tony Solomonik <tony.solomonik@gmail.com>\nLink: https://lore.kernel.org/r/20240202121724.17461-3-tony.solomonik@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-09 09:04:39 -0700 io_uring: add support for ftruncate"
    },
    {
        "commit": "3be042cf46feeedf664152d063376b5c17026d1d",
        "message": "Cross-merge networking fixes after downstream PR.\n\nNo conflicts.\n\nAdjacent changes:\n\ndrivers/net/ethernet/stmicro/stmmac/common.h\n  38cc3c6dcc09 (\"net: stmmac: protect updates of 64-bit statistics counters\")\n  fd5a6a71313e (\"net: stmmac: est: Per Tx-queue error count for HLBF\")\n  c5c3e1bfc9e0 (\"net: stmmac: Offload queueMaxSDU from tc-taprio\")\n\ndrivers/net/wireless/microchip/wilc1000/netdev.c\n  c9013880284d (\"wifi: fill in MODULE_DESCRIPTION()s for wilc1000\")\n  328efda22af8 (\"wifi: wilc1000: do not realloc workqueue everytime an interface is added\")\n\nnet/unix/garbage.c\n  11498715f266 (\"af_unix: Remove io_uring code for GC.\")\n  1279f9d9dec2 (\"af_unix: Call kfree_skb() for dead unix_(sk)->oob_skb in GC.\")\n\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 15:30:33 -0800 Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net"
    },
    {
        "commit": "a6e959bd3d6bfe8a3daeeacb714a299b9094a6cb",
        "message": "commit 0a31bd5f2bbb (\"KMEM_CACHE(): simplify slab cache creation\")\nintroduces a new macro.\nUse the new KMEM_CACHE() macro instead of direct kmem_cache_create\nto simplify the creation of SLAB caches.\n\nSigned-off-by: Kunwu Chan <chentao@kylinos.cn>\nLink: https://lore.kernel.org/r/20240130100247.81460-1-chentao@kylinos.cn\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring: Simplify the allocation of slab caches"
    },
    {
        "commit": "da08d2edb020026beac01d087d3b37e479fdb7e9",
        "message": "Nothing major here, just moving a few things around to reduce the\npadding. This reduces the size on a non-debug kernel from 1536 to\n1472 bytes, saving a full cacheline.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring: re-arrange struct io_ring_ctx to reduce padding"
    },
    {
        "commit": "af5d68f8892f8ee8f137648b79ceb2abc153a19b",
        "message": "Decouple from task_work running, and cap the number of entries we process\nat the time. If we exceed that number, push remaining entries to a retry\nlist that we'll process first next time.\n\nWe cap the number of entries to process at 8, which is fairly random.\nWe just want to get enough per-ctx batching here, while not processing\nendlessly.\n\nSince we manually run PF_IO_WORKER related task_work anyway as the task\nnever exits to userspace, with this we no longer need to add an actual\ntask_work item to the per-process list.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring/sqpoll: manage task_work privately"
    },
    {
        "commit": "2708af1adc11700c6c3ce4109e3b133079a36a78",
        "message": "No functional changes in this patch, just in preparation for returning\nsomething other than count from this helper.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring: pass in counter to handle_tw_list() rather than return it"
    },
    {
        "commit": "42c0905f0cac9a86d2cb8138665a6d62ea607078",
        "message": "Now that we don't loop around task_work anymore, there's no point in\nmaintaining the ring and locked state outside of handle_tw_list(). Get\nrid of passing in those pointers (and pointers to pointers) and just do\nthe management internally in handle_tw_list().\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring: cleanup handle_tw_list() calling convention"
    },
    {
        "commit": "3cdc4be114a9be61b7041a53e44aa71718a7cf28",
        "message": "This overly long line is hard to read. Break it up by AND'ing the\nref mask first, then perform the atomic_sub_return() with the value\nitself.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring/poll: improve readability of poll reference decrementing"
    },
    {
        "commit": "9fe3eaea4a3530ca34a8d8ff00b1848c528789ca",
        "message": "If we have a ton of notifications coming in, we can be looping in here\nfor a long time. This can be problematic for various reasons, mostly\nbecause we can starve userspace. If the application is waiting on N\nevents, then only re-run if we need more events.\n\nFixes: c0e0d6ba25f1 (\"io_uring: add IORING_SETUP_DEFER_TASKRUN\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring: remove unconditional looping in local task_work handling"
    },
    {
        "commit": "670d9d3df8808b39430ade7a04b38363971167f5",
        "message": "We just reversed the task_work list and that will have touched requests\nas well, just get rid of this optimization as it should not make a\ndifference anymore.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring: remove next io_kiocb fetch in task_work running"
    },
    {
        "commit": "170539bdf1094e6e43e9aa86bf2dcaff0857df41",
        "message": "For local task_work, which is used if IORING_SETUP_DEFER_TASKRUN is set,\nwe reverse the order of the lockless list before processing the work.\nThis is done to process items in the order in which they were queued, as\nthe llist always adds to the head.\n\nDo the same for traditional task_work, so we have the same behavior for\nboth types.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring: handle traditional task_work in FIFO order"
    },
    {
        "commit": "4c98b89175a229a1eb9e6db67b4b7c0d712c86a7",
        "message": "We no longer loop in task_work handling, hence delete the argument from\nthe tracepoint as it's always 1 and hence not very informative.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring: remove 'loops' argument from trace_io_uring_task_work_run()"
    },
    {
        "commit": "592b4805432af075468876771c0f7d41273ccb3c",
        "message": "A previous commit added looping around handling traditional task_work\nas an optimization, and while that may seem like a good idea, it's also\npossible to run into application starvation doing so. If the task_work\ngeneration is bursty, we can get very deep task_work queues, and we can\nend up looping in here for a very long time.\n\nOne immediately observable problem with that is handling network traffic\nusing provided buffers, where flooding incoming traffic and looping\ntask_work handling will very quickly lead to buffer starvation as we\nkeep running task_work rather than returning to the application so it\ncan handle the associated CQEs and also provide buffers back.\n\nFixes: 3a0c037b0e16 (\"io_uring: batch task_work\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring: remove looping around handling traditional task_work"
    },
    {
        "commit": "8435c6f380d622639d8acbc0af585d941396fa57",
        "message": "We have various functions calculating the CQE cflags we need to pass\nback, but it's all the same everywhere. Make a number of the putting\nfunctions void, and just have the two main helps for this, io_put_kbuf()\nand io_put_kbuf_comp() calculate the actual mask and pass it back.\n\nWhile at it, cleanup how we put REQ_F_BUFFER_RING buffers. Before\nthis change, we would call into __io_put_kbuf() only to go right back\nin to the header defined functions. As clearing this type of buffer\nis just re-assigning the buf_index and incrementing the head, this\nis very wasteful.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring/kbuf: cleanup passing back cflags"
    },
    {
        "commit": "949249e25f1098315a971b70b893c1a2e2e4a819",
        "message": "Any read/write opcode has needs_file == true, which means that we\nwould've failed the request long before reaching the issue stage if we\ndidn't successfully assign a file. This check has been dead forever,\nand is really a leftover from generic code.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring/rw: remove dead file == NULL check"
    },
    {
        "commit": "4caa74fdce7d59582b3e3f95b718b47e043f276e",
        "message": "Move the ctx declaration and assignment up to be generally available\nin the function, as we use req->ctx at the top anyway.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring: cleanup io_req_complete_post()"
    },
    {
        "commit": "bfe30bfde279529011161a60e5a7ca4be83de422",
        "message": "Any of the fast paths will already have this locked, this helper only\nexists to deal with io-wq invoking request issue where we do not have\nthe ctx->uring_lock held already. This means that any common or fast\npath will already have this locked, mark it as such.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring: mark the need to lock/unlock the ring as unlikely"
    },
    {
        "commit": "95041b93e90a06bb613ec4bef9cd4d61570f68e4",
        "message": "This adds a flag to avoid dipping dereferencing file and then f_op to\nfigure out if the file has a poll handler defined or not. We generally\ncall this at least twice for networked workloads, and if using ring\nprovided buffers, we do it on every buffer selection. Particularly the\nlatter is troublesome, as it's otherwise a very fast operation.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring: add io_file_can_poll() helper"
    },
    {
        "commit": "521223d7c229f83915619f888c99e952f24dc39f",
        "message": "Just leave it unset by default, avoiding dipping into the last\ncacheline (which is otherwise untouched) for the fast path of using\npoll to drive networked traffic. Add a flag that tells us if the\nsequence is valid or not, and then we can defer actually assigning\nthe flag and sequence until someone runs cancelations.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:06 -0700 io_uring/cancel: don't default to setting req->work.cancel_seq"
    },
    {
        "commit": "4bcb982cce74e18155fba0d97394ca9634e0d8f0",
        "message": "We're out of space here, and none of the flags are easily reclaimable.\nBump it to 64-bits and re-arrange the struct a bit to avoid gaps.\n\nAdd a specific bitwise type for the request flags, io_request_flags_t.\nThis will help catch violations of casting this value to a smaller type\non 32-bit archs, like unsigned int.\n\nThis creates a hole in the io_kiocb, so move nr_tw up and rsrc_node down\nto retain needing only cacheline 0 and 1 for non-polled opcodes.\n\nNo functional changes intended in this patch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 13:27:03 -0700 io_uring: expand main struct io_kiocb flags to 64-bits"
    },
    {
        "commit": "e516c3fc6c182736aec5418a73f15199640491e2",
        "message": "When enlisting a bio into ->free_list_irq we protect the list by\ndisabling irqs. It's likely they're already disabled and performance of\nlocal_irq_{save,restore}() is decent, but it's not zero cost.\n\nLet's only use the irq cache when when we're serving a hard irq, which\nallows to remove local_irq_{save,restore}(), and fall back to bio_free()\nin all left cases.\n\nProfiles indicate that the bio_put() cost is reduced by ~3.5 times\n(1.76% -> 0.49%), and total throughput of a CPU bound benchmark improve\nby around 1% (t/io_uring with high QD and several drives).\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/36d207540b7046c653cc16e5ff08fe7234b19f81.1707314970.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-08 10:18:48 -0700 block: optimise in irq bio put caching"
    },
    {
        "commit": "5492a490e64ed47483a0b28f10ba07eef4a47edb",
        "message": "Let's use file_mnt_idmap() as we do that across the tree.\n\nNo functional impact.\n\nCc: Christian Brauner <brauner@kernel.org>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Pavel Begunkov <asml.silence@gmail.com>\nCc: io-uring@vger.kernel.org\nCc: linux-fsdevel@vger.kernel.org\nCc: linux-kernel@vger.kernel.org\nSigned-off-by: Alexander Mikhalitsyn <aleksandr.mikhalitsyn@canonical.com>\nLink: https://lore.kernel.org/r/20240129180024.219766-2-aleksandr.mikhalitsyn@canonical.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-02-06 19:55:14 -0700 io_uring: use file_mnt_idmap helper"
    },
    {
        "commit": "1279f9d9dec2d7462823a18c29ad61359e0a007d",
        "message": "syzbot reported a warning [0] in __unix_gc() with a repro, which\ncreates a socketpair and sends one socket's fd to itself using the\npeer.\n\n  socketpair(AF_UNIX, SOCK_STREAM, 0, [3, 4]) = 0\n  sendmsg(4, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"\\360\", iov_len=1}],\n          msg_iovlen=1, msg_control=[{cmsg_len=20, cmsg_level=SOL_SOCKET,\n                                      cmsg_type=SCM_RIGHTS, cmsg_data=[3]}],\n          msg_controllen=24, msg_flags=0}, MSG_OOB|MSG_PROBE|MSG_DONTWAIT|MSG_ZEROCOPY) = 1\n\nThis forms a self-cyclic reference that GC should finally untangle\nbut does not due to lack of MSG_OOB handling, resulting in memory\nleak.\n\nRecently, commit 11498715f266 (\"af_unix: Remove io_uring code for\nGC.\") removed io_uring's dead code in GC and revealed the problem.\n\nThe code was executed at the final stage of GC and unconditionally\nmoved all GC candidates from gc_candidates to gc_inflight_list.\nThat papered over the reported problem by always making the following\nWARN_ON_ONCE(!list_empty(&gc_candidates)) false.\n\nThe problem has been there since commit 2aab4b969002 (\"af_unix: fix\nstruct pid leaks in OOB support\") added full scm support for MSG_OOB\nwhile fixing another bug.\n\nTo fix this problem, we must call kfree_skb() for unix_sk(sk)->oob_skb\nif the socket still exists in gc_candidates after purging collected skb.\n\nThen, we need to set NULL to oob_skb before calling kfree_skb() because\nit calls last fput() and triggers unix_release_sock(), where we call\nduplicate kfree_skb(u->oob_skb) if not NULL.\n\nNote that the leaked socket remained being linked to a global list, so\nkmemleak also could not detect it.  We need to check /proc/net/protocol\nto notice the unfreed socket.\n\n[0]:\nWARNING: CPU: 0 PID: 2863 at net/unix/garbage.c:345 __unix_gc+0xc74/0xe80 net/unix/garbage.c:345\nModules linked in:\nCPU: 0 PID: 2863 Comm: kworker/u4:11 Not tainted 6.8.0-rc1-syzkaller-00583-g1701940b1a02 #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/25/2024\nWorkqueue: events_unbound __unix_gc\nRIP: 0010:__unix_gc+0xc74/0xe80 net/unix/garbage.c:345\nCode: 8b 5c 24 50 e9 86 f8 ff ff e8 f8 e4 22 f8 31 d2 48 c7 c6 30 6a 69 89 4c 89 ef e8 97 ef ff ff e9 80 f9 ff ff e8 dd e4 22 f8 90 <0f> 0b 90 e9 7b fd ff ff 48 89 df e8 5c e7 7c f8 e9 d3 f8 ff ff e8\nRSP: 0018:ffffc9000b03fba0 EFLAGS: 00010293\nRAX: 0000000000000000 RBX: ffffc9000b03fc10 RCX: ffffffff816c493e\nRDX: ffff88802c02d940 RSI: ffffffff896982f3 RDI: ffffc9000b03fb30\nRBP: ffffc9000b03fce0 R08: 0000000000000001 R09: fffff52001607f66\nR10: 0000000000000003 R11: 0000000000000002 R12: dffffc0000000000\nR13: ffffc9000b03fc10 R14: ffffc9000b03fc10 R15: 0000000000000001\nFS:  0000000000000000(0000) GS:ffff8880b9400000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00005559c8677a60 CR3: 000000000d57a000 CR4: 00000000003506f0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\nCall Trace:\n <TASK>\n process_one_work+0x889/0x15e0 kernel/workqueue.c:2633\n process_scheduled_works kernel/workqueue.c:2706 [inline]\n worker_thread+0x8b9/0x12a0 kernel/workqueue.c:2787\n kthread+0x2c6/0x3b0 kernel/kthread.c:388\n ret_from_fork+0x45/0x80 arch/x86/kernel/process.c:147\n ret_from_fork_asm+0x1b/0x30 arch/x86/entry/entry_64.S:242\n </TASK>\n\nReported-by: syzbot+fa3ef895554bdbfd1183@syzkaller.appspotmail.com\nCloses: https://syzkaller.appspot.com/bug?extid=fa3ef895554bdbfd1183\nFixes: 2aab4b969002 (\"af_unix: fix struct pid leaks in OOB support\")\nSigned-off-by: Kuniyuki Iwashima <kuniyu@amazon.com>\nReviewed-by: Eric Dumazet <edumazet@google.com>\nLink: https://lore.kernel.org/r/20240203183149.63573-1-kuniyu@amazon.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.8-rc4",
        "release_date": "2024-02-06 18:33:07 -0800 af_unix: Call kfree_skb() for dead unix_(sk)->oob_skb in GC."
    },
    {
        "commit": "717ca0b8e55eea49c5d71c026eafbe1e64d4b556",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix for missing retry for read multishot.\n\n   If we trigger the execution of it and there's more than one buffer to\n   be read, then we don't always read more than the first one. As it's\n   edge triggered, this can lead to stalls.\n\n - Limit inline receive multishot retries for fairness reasons.\n\n   If we have a very bursty socket receiving data, we still need to\n   ensure we process other requests as well. This is really two minor\n   cleanups, then adding a way for poll reissue to trigger a requeue,\n   and then finally having multishot receive utilize that.\n\n - Fix for a weird corner case for non-multishot receive with\n   MSG_WAITALL, using provided buffers, and setting the length to\n   zero (to let the buffer dictate the receive size).\n\n* tag 'io_uring-6.8-2024-02-01' of git://git.kernel.dk/linux:\n  io_uring/net: fix sr->len for IORING_OP_RECV with MSG_WAITALL and buffers\n  io_uring/net: limit inline multishot retries\n  io_uring/poll: add requeue return code from poll multishot handling\n  io_uring/net: un-indent mshot retry path in io_recv_finish()\n  io_uring/poll: move poll execution helpers higher up\n  io_uring/rw: ensure poll based multishot read retries appropriately",
        "kernel_version": "v6.8-rc3",
        "release_date": "2024-02-02 10:45:17 -0800 Merge tag 'io_uring-6.8-2024-02-01' of git://git.kernel.dk/linux"
    },
    {
        "commit": "72bd80252feeb3bef8724230ee15d9f7ab541c6e",
        "message": "If we use IORING_OP_RECV with provided buffers and pass in '0' as the\nlength of the request, the length is retrieved from the selected buffer.\nIf MSG_WAITALL is also set and we get a short receive, then we may hit\nthe retry path which decrements sr->len and increments the buffer for\na retry. However, the length is still zero at this point, which means\nthat sr->len now becomes huge and import_ubuf() will cap it to\nMAX_RW_COUNT and subsequently return -EFAULT for the range as a whole.\n\nFix this by always assigning sr->len once the buffer has been selected.\n\nCc: stable@vger.kernel.org\nFixes: 7ba89d2af17a (\"io_uring: ensure recv and recvmsg handle MSG_WAITALL correctly\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc3",
        "release_date": "2024-02-01 06:42:36 -0700 io_uring/net: fix sr->len for IORING_OP_RECV with MSG_WAITALL and buffers"
    },
    {
        "commit": "567058d398aa97beaec62252fcc3bb78d34fa270",
        "message": "Kuniyuki Iwashima says:\n\n====================\naf_unix: Remove io_uring dead code in GC.\n\nI will post another series that rewrites the garbage collector for\nAF_UNIX socket.\n\nThis is a prep series to clean up changes to GC made by io_uring but\nnow not necessary.\n====================\n\nLink: https://lore.kernel.org/r/20240129190435.57228-1-kuniyu@amazon.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-01-31 16:41:18 -0800 Merge branch 'af_unix-remove-io_uring-dead-code-in-gc'"
    },
    {
        "commit": "99a7a5b9943ea2d05fb0dee38e4ae2290477ed83",
        "message": "Originally, the code related to garbage collection was all in garbage.c.\n\nCommit f4e65870e5ce (\"net: split out functions related to registering\ninflight socket files\") moved some functions to scm.c for io_uring and\nadded CONFIG_UNIX_SCM just in case AF_UNIX was built as module.\n\nHowever, since commit 97154bcf4d1b (\"af_unix: Kconfig: make CONFIG_UNIX\nbool\"), AF_UNIX is no longer built separately.  Also, io_uring does not\nsupport SCM_RIGHTS now.\n\nLet's move the functions back to garbage.c\n\nSigned-off-by: Kuniyuki Iwashima <kuniyu@amazon.com>\nAcked-by: Jens Axboe <axboe@kernel.dk>\nLink: https://lore.kernel.org/r/20240129190435.57228-4-kuniyu@amazon.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-01-31 16:41:16 -0800 af_unix: Remove CONFIG_UNIX_SCM."
    },
    {
        "commit": "11498715f266a3fb4caabba9dd575636cbcaa8f1",
        "message": "Since commit 705318a99a13 (\"io_uring/af_unix: disable sending\nio_uring over sockets\"), io_uring's unix socket cannot be passed\nvia SCM_RIGHTS, so it does not contribute to cyclic reference and\nno longer be candidate for garbage collection.\n\nAlso, commit 6e5e6d274956 (\"io_uring: drop any code related to\nSCM_RIGHTS\") cleaned up SCM_RIGHTS code in io_uring.\n\nLet's do it in AF_UNIX as well by reverting commit 0091bfc81741\n(\"io_uring/af_unix: defer registered files gc to io_uring release\")\nand commit 10369080454d (\"net: reclaim skb->scm_io_uring bit\").\n\nSigned-off-by: Kuniyuki Iwashima <kuniyu@amazon.com>\nAcked-by: Jens Axboe <axboe@kernel.dk>\nLink: https://lore.kernel.org/r/20240129190435.57228-3-kuniyu@amazon.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-01-31 16:41:16 -0800 af_unix: Remove io_uring code for GC."
    },
    {
        "commit": "76b367a2d83163cf19173d5cb0b562acbabc8eac",
        "message": "If we have multiple clients and some/all are flooding the receives to\nsuch an extent that we can retry a LOT handling multishot receives, then\nwe can be starving some clients and hence serving traffic in an\nimbalanced fashion.\n\nLimit multishot retry attempts to some arbitrary value, whose only\npurpose serves to ensure that we don't keep serving a single connection\nfor way too long. We default to 32 retries, which should be more than\nenough to provide fairness, yet not so small that we'll spend too much\ntime requeuing rather than handling traffic.\n\nCc: stable@vger.kernel.org\nDepends-on: 704ea888d646 (\"io_uring/poll: add requeue return code from poll multishot handling\")\nDepends-on: 1e5d765a82f (\"io_uring/net: un-indent mshot retry path in io_recv_finish()\")\nDepends-on: e84b01a880f6 (\"io_uring/poll: move poll execution helpers higher up\")\nFixes: b3fdea6ecb55 (\"io_uring: multishot recv\")\nFixes: 9bb66906f23e (\"io_uring: support multishot in recvmsg\")\nLink: https://github.com/axboe/liburing/issues/1043\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc3",
        "release_date": "2024-01-29 13:19:58 -0700 io_uring/net: limit inline multishot retries"
    },
    {
        "commit": "704ea888d646cb9d715662944cf389c823252ee0",
        "message": "Since our poll handling is edge triggered, multishot handlers retry\ninternally until they know that no more data is available. In\npreparation for limiting these retries, add an internal return code,\nIOU_REQUEUE, which can be used to inform the poll backend about the\nhandler wanting to retry, but that this should happen through a normal\ntask_work requeue rather than keep hammering on the issue side for this\none request.\n\nNo functional changes in this patch, nobody is using this return code\njust yet.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc3",
        "release_date": "2024-01-29 13:19:47 -0700 io_uring/poll: add requeue return code from poll multishot handling"
    },
    {
        "commit": "91e5d765a82fb2c9d0b7ad930d8953208081ddf1",
        "message": "In preparation for putting some retry logic in there, have the done\npath just skip straight to the end rather than have too much nesting\nin here.\n\nNo functional changes in this patch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc3",
        "release_date": "2024-01-29 13:19:26 -0700 io_uring/net: un-indent mshot retry path in io_recv_finish()"
    },
    {
        "commit": "e84b01a880f635e3084a361afba41f95ff500d12",
        "message": "In preparation for calling __io_poll_execute() higher up, move the\nfunctions to avoid forward declarations.\n\nNo functional changes in this patch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc3",
        "release_date": "2024-01-29 13:19:17 -0700 io_uring/poll: move poll execution helpers higher up"
    },
    {
        "commit": "c79f52f0656eeb3e4a12f7f358f760077ae111b6",
        "message": "io_read_mshot() always relies on poll triggering retries, and this works\nfine as long as we do a retry per size of the buffer being read. The\nbuffer size is given by the size of the buffer(s) in the given buffer\ngroup ID.\n\nBut if we're reading less than what is available, then we don't always\nget to read everything that is available. For example, if the buffers\navailable are 32 bytes and we have 64 bytes to read, then we'll\ncorrectly read the first 32 bytes and then wait for another poll trigger\nbefore we attempt the next read. This next poll trigger may never\nhappen, in which case we just sit forever and never make progress, or it\nmay trigger at some point in the future, and now we're just delivering\nthe available data much later than we should have.\n\nio_read_mshot() could do retries itself, but that is wasteful as we'll\nbe going through all of __io_read() again, and most likely in vain.\nRather than do that, bump our poll reference count and have\nio_poll_check_events() do one more loop and check with vfs_poll() if we\nhave more data to read. If we do, io_read_mshot() will get invoked again\ndirectly and we'll read the next chunk.\n\nio_poll_multishot_retry() must only get called from inside\nio_poll_issue(), which is our multishot retry handler, as we know we\nalready \"own\" the request at this point.\n\nCc: stable@vger.kernel.org\nLink: https://github.com/axboe/liburing/issues/1041\nFixes: fc68fcda0491 (\"io_uring/rw: add support for IORING_OP_READ_MULTISHOT\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc3",
        "release_date": "2024-01-28 20:37:11 -0700 io_uring/rw: ensure poll based multishot read retries appropriately"
    },
    {
        "commit": "1cf05e2508ab70ec1af56d1e62e79520d7b8dd6e",
        "message": "Kuniyuki Iwashima says:\n\n====================\naf_unix: Random improvements for GC.\n\nIf more than 16000 inflight AF_UNIX sockets exist on a host, each\nsendmsg() will be forced to wait for unix_gc() even if a process\nis not sending any FD.\n\nThis series tries not to impose such a penalty on sane users who\ndo not send AF_UNIX FDs or do not have inflight sockets more than\nSCM_MAX_FD * 8.\n\nThe first patch can be backported to -stable.\n\nCleanup patches for commit 69db702c8387 (\"io_uring/af_unix: disable\nsending io_uring over sockets\") and large refactoring of GC will\nbe followed later.\n\nv4: https://lore.kernel.org/netdev/20231219030102.27509-1-kuniyu@amazon.com/\nv3: https://lore.kernel.org/netdev/20231218075020.60826-1-kuniyu@amazon.com/\nv2: https://lore.kernel.org/netdev/20231123014747.66063-1-kuniyu@amazon.com/\nv1: https://lore.kernel.org/netdev/20231122013629.28554-1-kuniyu@amazon.com/\n====================\n\nLink: https://lore.kernel.org/r/20240123170856.41348-1-kuniyu@amazon.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-01-26 20:34:28 -0800 Merge branch 'af_unix-random-improvements-for-gc'"
    },
    {
        "commit": "cced1c5e72b7466e6c9091370eaf5d55a4ddeecb",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single tweak to the newly added IORING_OP_FIXED_FD_INSTALL from\n  Paul, ensuring it goes via the audit path and playing it safe by\n  excluding it from using registered creds\"\n\n* tag 'io_uring-6.8-2024-01-26' of git://git.kernel.dk/linux:\n  io_uring: enable audit and restrict cred override for IORING_OP_FIXED_FD_INSTALL",
        "kernel_version": "v6.8-rc2",
        "release_date": "2024-01-26 15:17:42 -0800 Merge tag 'io_uring-6.8-2024-01-26' of git://git.kernel.dk/linux"
    },
    {
        "commit": "082fd1ea1f98e6bb1189213a2404ddd774de3843",
        "message": "If parent inode is not watching, check for the event in masks of\nsb/mount/inode masks early to optimize out most of the code in\n__fsnotify_parent() and avoid calling fsnotify().\n\nJens has reported that this optimization improves BW and IOPS in an\nio_uring benchmark by more than 10% and reduces perf reported CPU usage.\n\nbefore:\n\n+    4.51%  io_uring  [kernel.vmlinux]  [k] fsnotify\n+    3.67%  io_uring  [kernel.vmlinux]  [k] __fsnotify_parent\n\nafter:\n\n+    2.37%  io_uring  [kernel.vmlinux]  [k] __fsnotify_parent\n\nReported-and-tested-by: Jens Axboe <axboe@kernel.dk>\nLink: https://lore.kernel.org/linux-fsdevel/b45bd8ff-5654-4e67-90a6-aad5e6759e0b@kernel.dk/\nSigned-off-by: Amir Goldstein <amir73il@gmail.com>\nSigned-off-by: Jan Kara <jack@suse.cz>\nMessage-Id: <20240116113247.758848-1-amir73il@gmail.com>",
        "kernel_version": "v6.9-rc1",
        "release_date": "2024-01-24 16:26:03 +0100 fsnotify: optimize the case of no parent watcher"
    },
    {
        "commit": "16bae3e1377846734ec6b87eee459c0f3551692c",
        "message": "We need to correct some aspects of the IORING_OP_FIXED_FD_INSTALL\ncommand to take into account the security implications of making an\nio_uring-private file descriptor generally accessible to a userspace\ntask.\n\nThe first change in this patch is to enable auditing of the FD_INSTALL\noperation as installing a file descriptor into a task's file descriptor\ntable is a security relevant operation and something that admins/users\nmay want to audit.\n\nThe second change is to disable the io_uring credential override\nfunctionality, also known as io_uring \"personalities\", in the\nFD_INSTALL command.  The credential override in FD_INSTALL is\nparticularly problematic as it affects the credentials used in the\nsecurity_file_receive() LSM hook.  If a task were to request a\ncredential override via REQ_F_CREDS on a FD_INSTALL operation, the LSM\nwould incorrectly check to see if the overridden credentials of the\nio_uring were able to \"receive\" the file as opposed to the task's\ncredentials.  After discussions upstream, it's difficult to imagine a\nuse case where we would want to allow a credential override on a\nFD_INSTALL operation so we are simply going to block REQ_F_CREDS on\nIORING_OP_FIXED_FD_INSTALL operations.\n\nFixes: dc18b89ab113 (\"io_uring/openclose: add support for IORING_OP_FIXED_FD_INSTALL\")\nSigned-off-by: Paul Moore <paul@paul-moore.com>\nLink: https://lore.kernel.org/r/20240123215501.289566-2-paul@paul-moore.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc2",
        "release_date": "2024-01-23 15:25:14 -0700 io_uring: enable audit and restrict cred override for IORING_OP_FIXED_FD_INSTALL"
    },
    {
        "commit": "e9a5a78d1ad8ceb4e3df6d6ad93360094c84ac40",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Nothing major in here, just a few fixes and cleanups that arrived\n  after the initial merge window pull request got finalized, as well as\n  a fix for a patch that got merged earlier\"\n\n* tag 'for-6.8/io_uring-2024-01-18' of git://git.kernel.dk/linux:\n  io_uring: combine cq_wait_nr checks\n  io_uring: clean *local_work_add var naming\n  io_uring: clean up local tw add-wait sync\n  io_uring: adjust defer tw counting\n  io_uring/register: guard compat syscall with CONFIG_COMPAT\n  io_uring/rsrc: improve code generation for fixed file assignment\n  io_uring/rw: cleanup io_rw_done()",
        "kernel_version": "v6.8-rc4",
        "release_date": "2024-01-18 18:17:57 -0800 Merge tag 'for-6.8/io_uring-2024-01-18' of git://git.kernel.dk/linux"
    },
    {
        "commit": "c2459ce011f65487231c6340486d5acdaceac6a7",
        "message": "Pull vfs fixes from Christian Brauner:\n \"This contains two fixes for the current merge window. The listmount\n  changes that you requested and a fix for a fsnotify performance\n  regression:\n\n   - The proposed listmount changes are currently under my authorship. I\n     wasn't sure whether you'd wanted to be author as the patch wasn't\n     signed off. If you do I'm happy if you just apply your own patch.\n\n     I've tested the patch with my sh4 cross-build setup. And confirmed\n     that a) the build failure with sh on current upstream is\n     reproducible and that b) the proposed patch fixes the build\n     failure. That should only leave the task of fixing put_user on sh.\n\n   - The fsnotify regression was caused by moving one of the hooks out\n     of the security hook in preparation for other fsnotify work. This\n     meant that CONFIG_SECURITY would have compiled out the fsnotify\n     hook before but didn't do so now.\n\n     That lead to up to 6% performance regression in some io_uring\n     workloads that compile all fsnotify and security checks out. Fix\n     this by making sure that the relevant hooks are covered by the\n     already existing CONFIG_FANOTIFY_ACCESS_PERMISSIONS where the\n     relevant hook belongs\"\n\n* tag 'vfs-6.8-rc1.fixes' of gitolite.kernel.org:pub/scm/linux/kernel/git/vfs/vfs:\n  fs: rework listmount() implementation\n  fsnotify: compile out fsnotify permission hooks if !FANOTIFY_ACCESS_PERMISSIONS",
        "kernel_version": "v6.8-rc2",
        "release_date": "2024-01-17 09:34:25 -0800 Merge tag 'vfs-6.8-rc1.fixes' of gitolite.kernel.org:pub/scm/linux/kernel/git/vfs/vfs"
    },
    {
        "commit": "b4bc35cf8704db86203c0739711dab1804265bf3",
        "message": "Instead of explicitly checking ->cq_wait_nr for whether there are\nwaiting, which is currently represented by 0, we can store there a\nlarge value and the nr_tw will automatically filter out those cases.\nAdd a named constant for that and for the wake up bias value.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/38def30282654d980673976cd42fde9bab19b297.1705438669.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc4",
        "release_date": "2024-01-17 09:45:24 -0700 io_uring: combine cq_wait_nr checks"
    },
    {
        "commit": "e8c407717b4814dac5641d93cbbbb9fc394f7cf0",
        "message": "if (!first) { ... }\n\nWhile it reads as do something if it's not the first entry, it does\nexactly the opposite because \"first\" here is a pointer to the first\nentry. Remove the confusion by renaming it into \"head\".\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3b8be483b52f58a524185bb88694b8a268e7e85d.1705438669.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc4",
        "release_date": "2024-01-17 09:45:24 -0700 io_uring: clean *local_work_add var naming"
    },
    {
        "commit": "d381099f980b5f6c3c7e150baf13b0aaefc66c29",
        "message": "Kill a smp_mb__after_atomic() right before wake_up, it's useless, and\nadd a comment explaining implicit barriers from cmpxchg and\nsynchronsation around ->cq_wait_nr with the waiter.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3007f3c2d53c72b61de56919ef56b53158b8276f.1705438669.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc4",
        "release_date": "2024-01-17 09:45:24 -0700 io_uring: clean up local tw add-wait sync"
    },
    {
        "commit": "dc12d1799ce710fd90abbe0ced71e7e1ae0894fc",
        "message": "The UINT_MAX work item counting bias in io_req_local_work_add() in case\nof !IOU_F_TWQ_LAZY_WAKE works in a sense that we will not miss a wake up,\nhowever it's still eerie. In particular, if we add a lazy work item\nafter a non-lazy one, we'll increment it and get nr_tw==0, and\nsubsequent adds may try to unnecessarily wake up the task, which is\nthough not so likely to happen in real workloads.\n\nHalf the bias, it's still large enough to be larger than any valid\n->cq_wait_nr, which is limited by IORING_MAX_CQ_ENTRIES, but further\nhave a good enough of space before it overflows.\n\nFixes: 8751d15426a31 (\"io_uring: reduce scheduling due to tw\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/108b971e958deaf7048342930c341ba90f75d806.1705438669.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc4",
        "release_date": "2024-01-17 09:45:24 -0700 io_uring: adjust defer tw counting"
    },
    {
        "commit": "baf59771343dc0c2ef9ac3189bf9df2d6143654f",
        "message": "Add compat.h include to avoid a potential build issue:\n\nio_uring/register.c:281:6: error: call to undeclared function 'in_compat_syscall'; ISO C99 and later do not support implicit function declarations [-Werror,-Wimplicit-function-declaration]\n\nif (in_compat_syscall()) {\n    ^\n1 warning generated.\nio_uring/register.c:282:9: error: call to undeclared function 'compat_get_bitmap'; ISO C99 and later do not support implicit function declarations [-Werror,-Wimplicit-function-declaration]\nret = compat_get_bitmap(cpumask_bits(new_mask),\n      ^\n\nFixes: c43203154d8a (\"io_uring/register: move io_uring_register(2) related code to register.c\")\nReported-by: Manu Bretelle <chantra@meta.com>\nReviewed-by: Jeff Moyer <jmoyer@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc4",
        "release_date": "2024-01-17 09:45:18 -0700 io_uring/register: guard compat syscall with CONFIG_COMPAT"
    },
    {
        "commit": "4c72e2b8c42e57f65d8fbfb01329e79d2b450653",
        "message": "Pull io_uring updates from Jens Axboe:\n \"Mostly just come fixes and cleanups, but one feature as well. In\n  detail:\n\n   - Harden the check for handling IOPOLL based on return (Pavel)\n\n   - Various minor optimizations (Pavel)\n\n   - Drop remnants of SCM_RIGHTS fd passing support, now that it's no\n     longer supported since 6.7 (me)\n\n   - Fix for a case where bytes_done wasn't initialized properly on a\n     failure condition for read/write requests (me)\n\n   - Move the register related code to a separate file (me)\n\n   - Add support for returning the provided ring buffer head (me)\n\n   - Add support for adding a direct descriptor to the normal file table\n     (me, Christian Brauner)\n\n   - Fix for ensuring pending task_work for a ring with DEFER_TASKRUN is\n     run even if we timeout waiting (me)\"\n\n* tag 'for-6.8/io_uring-2024-01-08' of git://git.kernel.dk/linux:\n  io_uring: ensure local task_work is run on wait timeout\n  io_uring/kbuf: add method for returning provided buffer ring head\n  io_uring/rw: ensure io->bytes_done is always initialized\n  io_uring: drop any code related to SCM_RIGHTS\n  io_uring/unix: drop usage of io_uring socket\n  io_uring/register: move io_uring_register(2) related code to register.c\n  io_uring/openclose: add support for IORING_OP_FIXED_FD_INSTALL\n  io_uring/cmd: inline io_uring_cmd_get_task\n  io_uring/cmd: inline io_uring_cmd_do_in_task_lazy\n  io_uring: split out cmd api into a separate header\n  io_uring: optimise ltimeout for inline execution\n  io_uring: don't check iopoll if request completes",
        "kernel_version": "v6.8-rc5",
        "release_date": "2024-01-11 14:19:23 -0800 Merge tag 'for-6.8/io_uring-2024-01-08' of git://git.kernel.dk/linux"
    },
    {
        "commit": "3f302388d45855c0b24802e7b414e3fb29f172e3",
        "message": "For the normal read/write path, we have already locked the ring\nsubmission side when assigning the file. This causes branch\nmispredictions when we then check and try and lock again in\nio_req_set_rsrc_node(). As this is a very hot path, this matters.\n\nAdd a basic helper that already assumes we already have it locked,\nand use that in io_file_get_fixed().\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc4",
        "release_date": "2024-01-11 13:37:31 -0700 io_uring/rsrc: improve code generation for fixed file assignment"
    },
    {
        "commit": "fe80eb15dea5125ea64845c9de0dd7f8478dd267",
        "message": "This originally came from the aio side, and it's laid out rather oddly.\nThe common case here is that we either get -EIOCBQUEUED from submitting\nan async request, or that we complete the request correctly with the\ngiven number of bytes. Handling the odd internal restart error codes\nis not a common operation.\n\nLay it out a bit more optimally that better explains the normal flow,\nand switch to avoiding the indirect call completely as this is our\nkiocb and we know the completion handler can only be one of two\npossible variants. While at it, move it to where it belongs in the\nfile, with fellow end IO helpers.\n\nOutside of being easier to read, this also reduces the text size of the\nfunction by 24 bytes for me on arm64.\n\nReviewed-by: Keith Busch <kbusch@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc4",
        "release_date": "2024-01-10 11:46:48 -0700 io_uring/rw: cleanup io_rw_done()"
    },
    {
        "commit": "95e7249691f082a5178d4d6f60fcdee91da458ab",
        "message": "This adds polling support to virtio-scsi. It's based on and works similar\nto virtblk support where we add a module param to specify the number of\npoll queues then subtract to calculate the IO queues.\n\nWhen using 8 poll queues and a vhost worker per queue we see 4K IOPs\nwith fio:\n\nfio --filename=/dev/sda --direct=1 --rw=randread --bs=4k \\\n--ioengine=io_uring --hipri --iodepth=128  --numjobs=$NUM_JOBS\n\nincrease like:\n\njobs\tbase\tpoll\n1\t207K\t296K\n2\t392K\t552K\n3\t581K\t860K\n4\t765K\t1235K\n5\t936K\t1598K\n6\t1104K\t1880K\n7\t1253K\t2095K\n8\t1311k\t2187K\n\nSigned-off-by: Mike Christie <michael.christie@oracle.com>\nMessage-Id: <20231214052649.57743-1-michael.christie@oracle.com>\nSigned-off-by: Michael S. Tsirkin <mst@redhat.com>",
        "kernel_version": "v6.8-rc2",
        "release_date": "2024-01-10 13:01:37 -0500 scsi: virtio_scsi: Add mq_poll support"
    },
    {
        "commit": "6ff1407e24e6fdfa4a16ba9ba551e3d253a26391",
        "message": "A previous commit added an earlier break condition here, which is fine if\nwe're using non-local task_work as it'll be run on return to userspace.\nHowever, if DEFER_TASKRUN is used, then we could be leaving local\ntask_work that is ready to process in the ctx list until next time that\nwe enter the kernel to wait for events.\n\nMove the break condition to _after_ we have run task_work.\n\nCc: stable@vger.kernel.org\nFixes: 846072f16eed (\"io_uring: mimimise io_cqring_wait_schedule\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc5",
        "release_date": "2024-01-04 12:21:08 -0700 io_uring: ensure local task_work is run on wait timeout"
    },
    {
        "commit": "8ab3b09755d926afc3bdd2fadff7f159310440c2",
        "message": "Use the proper kasan_mempool_unpoison_object hook for unpoisoning cached\nobjects.\n\nA future change might also update io_uring to check the return value of\nkasan_mempool_poison_object to prevent double-free and invalid-free bugs. \nThis proves to be non-trivial with the current way io_uring caches\nobjects, so this is left out-of-scope of this series.\n\nLink: https://lkml.kernel.org/r/eca18d6cbf676ed784f1a1f209c386808a8087c5.1703024586.git.andreyknvl@google.com\nSigned-off-by: Andrey Konovalov <andreyknvl@google.com>\nCc: Alexander Lobakin <alobakin@pm.me>\nCc: Alexander Potapenko <glider@google.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Breno Leitao <leitao@debian.org>\nCc: Dmitry Vyukov <dvyukov@google.com>\nCc: Evgenii Stepanov <eugenis@google.com>\nCc: Marco Elver <elver@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v6.8-rc3",
        "release_date": "2023-12-29 11:58:41 -0800 io_uring: use mempool KASAN hook"
    },
    {
        "commit": "280ec6ccb6422aa4a04f9ac4216ddcf055acc95d",
        "message": "Patch series \"kasan: save mempool stack traces\".\n\nThis series updates KASAN to save alloc and free stack traces for\nsecondary-level allocators that cache and reuse allocations internally\ninstead of giving them back to the underlying allocator (e.g.  mempool).\n\nAs a part of this change, introduce and document a set of KASAN hooks:\n\nbool kasan_mempool_poison_pages(struct page *page, unsigned int order);\nvoid kasan_mempool_unpoison_pages(struct page *page, unsigned int order);\nbool kasan_mempool_poison_object(void *ptr);\nvoid kasan_mempool_unpoison_object(void *ptr, size_t size);\n\nand use them in the mempool code.\n\nBesides mempool, skbuff and io_uring also cache allocations and already\nuse KASAN hooks to poison those.  Their code is updated to use the new\nmempool hooks.\n\nThe new hooks save alloc and free stack traces (for normal kmalloc and\nslab objects; stack traces for large kmalloc objects and page_alloc are\nnot supported by KASAN yet), improve the readability of the users' code,\nand also allow the users to prevent double-free and invalid-free bugs; see\nthe patches for the details.\n\n\nThis patch (of 21):\n\nRename kasan_slab_free_mempool to kasan_mempool_poison_object.\n\nkasan_slab_free_mempool is a slightly confusing name: it is unclear\nwhether this function poisons the object when it is freed into mempool or\ndoes something when the object is freed from mempool to the underlying\nallocator.\n\nThe new name also aligns with other mempool-related KASAN hooks added in\nthe following patches in this series.\n\nLink: https://lkml.kernel.org/r/cover.1703024586.git.andreyknvl@google.com\nLink: https://lkml.kernel.org/r/c5618685abb7cdbf9fb4897f565e7759f601da84.1703024586.git.andreyknvl@google.com\nSigned-off-by: Andrey Konovalov <andreyknvl@google.com>\nCc: Alexander Lobakin <alobakin@pm.me>\nCc: Alexander Potapenko <glider@google.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Breno Leitao <leitao@debian.org>\nCc: Dmitry Vyukov <dvyukov@google.com>\nCc: Evgenii Stepanov <eugenis@google.com>\nCc: Marco Elver <elver@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v6.8-rc3",
        "release_date": "2023-12-29 11:58:36 -0800 kasan: rename kasan_slab_free_mempool to kasan_mempool_poison_object"
    },
    {
        "commit": "d293b1a89694fc4918d9a4330a71ba2458f9d581",
        "message": "The tail of the provided ring buffer is shared between the kernel and\nthe application, but the head is private to the kernel as the\napplication doesn't need to see it. However, this also prevents the\napplication from knowing how many buffers the kernel has consumed.\nUsually this is fine, as the information is inherently racy in that\nthe kernel could be consuming buffers continually, but for cleanup\npurposes it may be relevant to know how many buffers are still left\nin the ring.\n\nAdd IORING_REGISTER_PBUF_STATUS which will return status for a given\nprovided buffer ring. Right now it just returns the head, but space\nis reserved for more information later in, if needed.\n\nLink: https://github.com/axboe/liburing/discussions/1020\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc5",
        "release_date": "2023-12-21 09:47:06 -0700 io_uring/kbuf: add method for returning provided buffer ring head"
    },
    {
        "commit": "0a535eddbe0dc1de4386046ab849f08aeb2f8faf",
        "message": "If IOSQE_ASYNC is set and we fail importing an iovec for a readv or\nwritev request, then we leave ->bytes_done uninitialized and hence the\neventual failure CQE posted can potentially have a random res value\nrather than the expected -EINVAL.\n\nSetup ->bytes_done before potentially failing, so we have a consistent\nvalue if we fail the request early.\n\nCc: stable@vger.kernel.org\nReported-by: xingwei lee <xrivendell7@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc5",
        "release_date": "2023-12-21 08:49:18 -0700 io_uring/rw: ensure io->bytes_done is always initialized"
    },
    {
        "commit": "6e5e6d274956305f1fc0340522b38f5f5be74bdb",
        "message": "This is dead code after we dropped support for passing io_uring fds\nover SCM_RIGHTS, get rid of it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc5",
        "release_date": "2023-12-19 12:36:34 -0700 io_uring: drop any code related to SCM_RIGHTS"
    },
    {
        "commit": "a4104821ad651d8a0b374f0b2474c345bbb42f82",
        "message": "Since we no longer allow sending io_uring fds over SCM_RIGHTS, move to\nusing io_is_uring_fops() to detect whether this is a io_uring fd or not.\nWith that done, kill off io_uring_get_socket() as nobody calls it\nanymore.\n\nThis is in preparation to yanking out the rest of the core related to\nunix gc with io_uring.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc5",
        "release_date": "2023-12-19 12:33:50 -0700 io_uring/unix: drop usage of io_uring socket"
    },
    {
        "commit": "c43203154d8ac579537aa0c7802b77d463b1f53a",
        "message": "Most of this code is basically self contained, move it out of the core\nio_uring file to bring a bit more separation to the registration related\nbits. This moves another ~10% of the code into register.c.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc5",
        "release_date": "2023-12-19 08:54:20 -0700 io_uring/register: move io_uring_register(2) related code to register.c"
    },
    {
        "commit": "3bd7d748816927202268cb335921f7f68b3ca723",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Just two minor fixes:\n\n   - Fix for the io_uring socket option commands using the wrong value\n     on some archs (Al)\n\n   - Tweak to the poll lazy wake enable (me)\"\n\n* tag 'io_uring-6.7-2023-12-15' of git://git.kernel.dk/linux:\n  io_uring/cmd: fix breakage in SOCKET_URING_OP_SIOC* implementation\n  io_uring/poll: don't enable lazy wake for POLLEXCLUSIVE",
        "kernel_version": "v6.7-rc6",
        "release_date": "2023-12-15 12:20:14 -0800 Merge tag 'io_uring-6.7-2023-12-15' of git://git.kernel.dk/linux"
    },
    {
        "commit": "1ba0e9d69b2000e95267c888cbfa91d823388d47",
        "message": "In 8e9fad0e70b7 \"io_uring: Add io_uring command support for sockets\"\nyou've got an include of asm-generic/ioctls.h done in io_uring/uring_cmd.c.\nThat had been done for the sake of this chunk -\n+               ret = prot->ioctl(sk, SIOCINQ, &arg);\n+               if (ret)\n+                       return ret;\n+               return arg;\n+       case SOCKET_URING_OP_SIOCOUTQ:\n+               ret = prot->ioctl(sk, SIOCOUTQ, &arg);\n\nSIOC{IN,OUT}Q are defined to symbols (FIONREAD and TIOCOUTQ) that come from\nioctls.h, all right, but the values vary by the architecture.\n\nFIONREAD is\n\t0x467F on mips\n\t0x4004667F on alpha, powerpc and sparc\n\t0x8004667F on sh and xtensa\n\t0x541B everywhere else\nTIOCOUTQ is\n\t0x7472 on mips\n\t0x40047473 on alpha, powerpc and sparc\n\t0x80047473 on sh and xtensa\n\t0x5411 everywhere else\n\n->ioctl() expects the same values it would've gotten from userland; all\nplaces where we compare with SIOC{IN,OUT}Q are using asm/ioctls.h, so\nthey pick the correct values.  io_uring_cmd_sock(), OTOH, ends up\npassing the default ones.\n\nFixes: 8e9fad0e70b7 (\"io_uring: Add io_uring command support for sockets\")\nCc:  <stable@vger.kernel.org>\nSigned-off-by: Al Viro <viro@zeniv.linux.org.uk>\nLink: https://lore.kernel.org/r/20231214213408.GT1674809@ZenIV\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc6",
        "release_date": "2023-12-14 16:52:13 -0700 io_uring/cmd: fix breakage in SOCKET_URING_OP_SIOC* implementation"
    },
    {
        "commit": "c7402612e2e61b76177f22e6e7f705adcbecc6fe",
        "message": "Pull networking fixes from Paolo Abeni:\n\"Current release - regressions:\n\n   - tcp: fix tcp_disordered_ack() vs usec TS resolution\n\n  Current release - new code bugs:\n\n   - dpll: sanitize possible null pointer dereference in\n     dpll_pin_parent_pin_set()\n\n   - eth: octeon_ep: initialise control mbox tasks before using APIs\n\n  Previous releases - regressions:\n\n   - io_uring/af_unix: disable sending io_uring over sockets\n\n   - eth: mlx5e:\n       - TC, don't offload post action rule if not supported\n       - fix possible deadlock on mlx5e_tx_timeout_work\n\n   - eth: iavf: fix iavf_shutdown to call iavf_remove instead iavf_close\n\n   - eth: bnxt_en: fix skb recycling logic in bnxt_deliver_skb()\n\n   - eth: ena: fix DMA syncing in XDP path when SWIOTLB is on\n\n   - eth: team: fix use-after-free when an option instance allocation\n     fails\n\n  Previous releases - always broken:\n\n   - neighbour: don't let neigh_forced_gc() disable preemption for long\n\n   - net: prevent mss overflow in skb_segment()\n\n   - ipv6: support reporting otherwise unknown prefix flags in\n     RTM_NEWPREFIX\n\n   - tcp: remove acked SYN flag from packet in the transmit queue\n     correctly\n\n   - eth: octeontx2-af:\n       - fix a use-after-free in rvu_nix_register_reporters\n       - fix promisc mcam entry action\n\n   - eth: dwmac-loongson: make sure MDIO is initialized before use\n\n   - eth: atlantic: fix double free in ring reinit logic\"\n\n* tag 'net-6.7-rc6' of git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net: (62 commits)\n  net: atlantic: fix double free in ring reinit logic\n  appletalk: Fix Use-After-Free in atalk_ioctl\n  net: stmmac: Handle disabled MDIO busses from devicetree\n  net: stmmac: dwmac-qcom-ethqos: Fix drops in 10M SGMII RX\n  dpaa2-switch: do not ask for MDB, VLAN and FDB replay\n  dpaa2-switch: fix size of the dma_unmap\n  net: prevent mss overflow in skb_segment()\n  vsock/virtio: Fix unsigned integer wrap around in virtio_transport_has_space()\n  Revert \"tcp: disable tcp_autocorking for socket when TCP_NODELAY flag is set\"\n  MIPS: dts: loongson: drop incorrect dwmac fallback compatible\n  stmmac: dwmac-loongson: drop useless check for compatible fallback\n  stmmac: dwmac-loongson: Make sure MDIO is initialized before use\n  tcp: disable tcp_autocorking for socket when TCP_NODELAY flag is set\n  dpll: sanitize possible null pointer dereference in dpll_pin_parent_pin_set()\n  net: ena: Fix XDP redirection error\n  net: ena: Fix DMA syncing in XDP path when SWIOTLB is on\n  net: ena: Fix xdp drops handling due to multibuf packets\n  net: ena: Destroy correct number of xdp queues upon failure\n  net: Remove acked SYN flag from packet in the transmit queue correctly\n  qed: Fix a potential use-after-free in qed_cxt_tables_alloc\n  ...",
        "kernel_version": "v6.7-rc6",
        "release_date": "2023-12-14 13:11:49 -0800 Merge tag 'net-6.7-rc6' of git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net"
    },
    {
        "commit": "595e52284d24adc376890d3fc93bdca4707d9aca",
        "message": "There are a few quirks around using lazy wake for poll unconditionally,\nand one of them is related the EPOLLEXCLUSIVE. Those may trigger\nexclusive wakeups, which wake a limited number of entries in the wait\nqueue. If that wake number is less than the number of entries someone is\nwaiting for (and that someone is also using DEFER_TASKRUN), then we can\nget stuck waiting for more entries while we should be processing the ones\nwe already got.\n\nIf we're doing exclusive poll waits, flag the request as not being\ncompatible with lazy wakeups.\n\nReported-by: Pavel Begunkov <asml.silence@gmail.com>\nFixes: 6ce4a93dbb5b (\"io_uring/poll: use IOU_F_TWQ_LAZY_WAKE for wakeups\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc6",
        "release_date": "2023-12-13 08:58:15 -0700 io_uring/poll: don't enable lazy wake for POLLEXCLUSIVE"
    },
    {
        "commit": "dc18b89ab113e9c6c7a529316ddf7029fb55132d",
        "message": "io_uring can currently open/close regular files or fixed/direct\ndescriptors. Or you can instantiate a fixed descriptor from a regular\none, and then close the regular descriptor. But you currently can't turn\na purely fixed/direct descriptor into a regular file descriptor.\n\nIORING_OP_FIXED_FD_INSTALL adds support for installing a direct\ndescriptor into the normal file table, just like receiving a file\ndescriptor or opening a new file would do. This is all nicely abstracted\ninto receive_fd(), and hence adding support for this is truly trivial.\n\nSince direct descriptors are only usable within io_uring itself, it can\nbe useful to turn them into real file descriptors if they ever need to\nbe accessed via normal syscalls. This can either be a transitory thing,\nor just a permanent transition for a given direct descriptor.\n\nBy default, new fds are installed with O_CLOEXEC set. The application\ncan disable O_CLOEXEC by setting IORING_FIXED_FD_NO_CLOEXEC in the\nsqe->install_fd_flags member.\n\nSuggested-by: Christian Brauner <brauner@kernel.org>\nReviewed-by: Christian Brauner <brauner@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc5",
        "release_date": "2023-12-12 07:42:57 -0700 io_uring/openclose: add support for IORING_OP_FIXED_FD_INSTALL"
    },
    {
        "commit": "055c15626a45b1ebc9f2f34981e705e1af171236",
        "message": "With io_uring_types.h we see all required definitions to inline\nio_uring_cmd_get_task().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/aa8e317f09e651a5f3e72f8c0ad3902084c1f930.1701391955.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc5",
        "release_date": "2023-12-12 07:42:52 -0700 io_uring/cmd: inline io_uring_cmd_get_task"
    },
    {
        "commit": "6b04a3737057ddfed396c954f9e4be4fe6d53c62",
        "message": "Now as we can easily include io_uring_types.h, move IOU_F_TWQ_LAZY_WAKE\nand inline io_uring_cmd_do_in_task_lazy().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/2ec9fb31dd192d1c5cf26d0a2dec5657d88a8e48.1701391955.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc5",
        "release_date": "2023-12-12 07:42:52 -0700 io_uring/cmd: inline io_uring_cmd_do_in_task_lazy"
    },
    {
        "commit": "b66509b8497f2b002a2654e386a440f1274ddcc7",
        "message": "linux/io_uring.h is slowly becoming a rubbish bin where we put\nanything exposed to other subsystems. For instance, the task exit\nhooks and io_uring cmd infra are completely orthogonal and don't need\neach other's definitions. Start cleaning it up by splitting out all\ncommand bits into a new header file.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7ec50bae6e21f371d3850796e716917fc141225a.1701391955.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc5",
        "release_date": "2023-12-12 07:42:52 -0700 io_uring: split out cmd api into a separate header"
    },
    {
        "commit": "e0b23d9953b0c1cf498f1ae2cba8032d0fb733cb",
        "message": "At one point in time we had an optimisation that would not spin up a\nlinked timeout timer when the master request successfully completes\ninline (during the first nowait execution attempt). We somehow lost it,\nso this patch restores it back.\n\nNote, that it's fine using io_arm_ltimeout() after the io_issue_sqe()\ncompletes the request because of delayed completion, but that that adds\nunwanted overhead.\n\nReported-by: Christian Mazakas <christian.mazakas@gmail.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8bf69c2a4beec14c565c85c86edb871ca8b8bcc8.1701390926.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc5",
        "release_date": "2023-12-12 07:42:52 -0700 io_uring: optimise ltimeout for inline execution"
    },
    {
        "commit": "9b43ef3d52532a0175ed6654618f7db61d390d2e",
        "message": "IOPOLL request should never return IOU_OK, so the following iopoll\nqueueing check in io_issue_sqe() after getting IOU_OK doesn't make any\nsense as would never turn true. Let's optimise on that and return a bit\nearlier. It's also much more resilient to potential bugs from\nmischieving iopoll implementations.\n\nCc:  <stable@vger.kernel.org>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2f8690e2fa5213a2ff292fac29a7143c036cdd60.1701390926.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc5",
        "release_date": "2023-12-12 07:42:52 -0700 io_uring: don't check iopoll if request completes"
    },
    {
        "commit": "2394b311c6b5bab225e5307c819eb1ea59de49de",
        "message": "Merge vfs.file from the VFS tree to avoid conflicts with receive_fd() now\nhaving 3 arguments rather than just 2.\n\n* 'vfs.file' of git://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs:\n  file: remove __receive_fd()\n  file: stop exposing receive_fd_user()\n  fs: replace f_rcuhead with f_task_work\n  file: remove pointless wrapper\n  file: s/close_fd_get_file()/file_close_fd()/g\n  Improve __fget_files_rcu() code generation (and thus __fget_light())\n  file: massage cleanup of files that failed to open",
        "kernel_version": "v6.8-rc5",
        "release_date": "2023-12-12 07:42:24 -0700 Merge branch 'vfs.file' of git://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs into for-6.8/io_uring"
    },
    {
        "commit": "24fa3ae9467f49dd9698fd884f2c6b13cc8ea12d",
        "message": "Only io_uring uses __close_fd_get_file(). All it does is hide\ncurrent->files but io_uring accesses files_struct directly right now\nanyway so it's a bit pointless. Just rename pick_file() to\nfile_close_fd_locked() and let io_uring use it. Add a lockdep assert in\nthere that we expect the caller to hold file_lock while we're at it.\n\nLink: https://lore.kernel.org/r/20231130-vfs-files-fixes-v1-2-e73ca6f4ea83@kernel.org\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nReviewed-by: Jan Kara <jack@suse.cz>\nSigned-off-by: Christian Brauner <brauner@kernel.org>",
        "kernel_version": "v6.8-rc5",
        "release_date": "2023-12-12 14:24:13 +0100 file: remove pointless wrapper"
    },
    {
        "commit": "69db702c83874fbaa2a51af761e35a8e5a593b95",
        "message": "File reference cycles have caused lots of problems for io_uring\nin the past, and it still doesn't work exactly right and races with\nunix_stream_read_generic(). The safest fix would be to completely\ndisallow sending io_uring files via sockets via SCM_RIGHT, so there\nare no possible cycles invloving registered files and thus rendering\nSCM accounting on the io_uring side unnecessary.\n\nCc: stable@vger.kernel.org\nFixes: 0091bfc81741b (\"io_uring/af_unix: defer registered files gc to io_uring release\")\nReported-and-suggested-by: Jann Horn <jannh@google.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nAcked-by: Jakub Kicinski <kuba@kernel.org>\nSigned-off-by: David S. Miller <davem@davemloft.net>",
        "kernel_version": "v6.7-rc6",
        "release_date": "2023-12-09 21:20:33 +0000 io_uring/af_unix: disable sending io_uring over sockets"
    },
    {
        "commit": "689659c988193f1e16bc34bfda3f333b11528c1f",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two minor fixes for issues introduced in this release cycle, and two\n  fixes for issues or potential issues that are heading to stable.\n\n  One of these ends up disabling passing io_uring file descriptors via\n  SCM_RIGHTS. There really shouldn't be an overlap between that kind of\n  historic use case and modern usage of io_uring, which is why this was\n  deemed appropriate\"\n\n* tag 'io_uring-6.7-2023-12-08' of git://git.kernel.dk/linux:\n  io_uring/af_unix: disable sending io_uring over sockets\n  io_uring/kbuf: check for buffer list readiness after NULL check\n  io_uring/kbuf: Fix an NULL vs IS_ERR() bug in io_alloc_pbuf_ring()\n  io_uring: fix mutex_unlock with unreferenced ctx",
        "kernel_version": "v6.7-rc5",
        "release_date": "2023-12-08 12:32:38 -0800 Merge tag 'io_uring-6.7-2023-12-08' of git://git.kernel.dk/linux"
    },
    {
        "commit": "705318a99a138c29a512a72c3e0043b3cd7f55f4",
        "message": "File reference cycles have caused lots of problems for io_uring\nin the past, and it still doesn't work exactly right and races with\nunix_stream_read_generic(). The safest fix would be to completely\ndisallow sending io_uring files via sockets via SCM_RIGHT, so there\nare no possible cycles invloving registered files and thus rendering\nSCM accounting on the io_uring side unnecessary.\n\nCc:  <stable@vger.kernel.org>\nFixes: 0091bfc81741b (\"io_uring/af_unix: defer registered files gc to io_uring release\")\nReported-and-suggested-by: Jann Horn <jannh@google.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c716c88321939156909cfa1bd8b0faaf1c804103.1701868795.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc5",
        "release_date": "2023-12-07 10:35:19 -0700 io_uring/af_unix: disable sending io_uring over sockets"
    },
    {
        "commit": "a58a173444a68412bb08849bd81c679395f20ca0",
        "message": "io_uring sets up the io worker kernel thread via a syscall out of an\nuser space prrocess. This process might have used FPU and since\ncopy_thread() didn't clear FPU states for kernel threads a BUG()\nis triggered for using FPU inside kernel. Move code around\nto always clear FPU state for user and kernel threads.\n\nCc: stable@vger.kernel.org\nReported-by: Aurelien Jarno <aurel32@debian.org>\nCloses: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=1055021\nSuggested-by: Jiaxun Yang <jiaxun.yang@flygoat.com>\nReviewed-by: Jiaxun Yang <jiaxun.yang@flygoat.com>\nSigned-off-by: Thomas Bogendoerfer <tsbogend@alpha.franken.de>",
        "kernel_version": "v6.7-rc5",
        "release_date": "2023-12-05 18:47:11 +0100 MIPS: kernel: Clear FPU states when setting up kernel threads"
    },
    {
        "commit": "9865346b7e8374b57f1c3ccacdc77846c6352ff4",
        "message": "Move the buffer list 'is_ready' check below the validity check for\nthe buffer list for a given group.\n\nFixes: 5cf4f52e6d8a (\"io_uring: free io_buffer_list entries via RCU\")\nReported-by: Dan Carpenter <dan.carpenter@linaro.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc5",
        "release_date": "2023-12-05 07:02:13 -0700 io_uring/kbuf: check for buffer list readiness after NULL check"
    },
    {
        "commit": "e53f7b54b1fdecae897f25002ff0cff04faab228",
        "message": "The io_mem_alloc() function returns error pointers, not NULL.  Update\nthe check accordingly.\n\nFixes: b10b73c102a2 (\"io_uring/kbuf: recycle freed mapped buffer ring entries\")\nSigned-off-by: Dan Carpenter <dan.carpenter@linaro.org>\nLink: https://lore.kernel.org/r/5ed268d3-a997-4f64-bd71-47faa92101ab@moroto.mountain\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc5",
        "release_date": "2023-12-05 06:59:56 -0700 io_uring/kbuf: Fix an NULL vs IS_ERR() bug in io_alloc_pbuf_ring()"
    },
    {
        "commit": "f7b32e785042d2357c5abc23ca6db1b92c91a070",
        "message": "Callers of mutex_unlock() have to make sure that the mutex stays alive\nfor the whole duration of the function call. For io_uring that means\nthat the following pattern is not valid unless we ensure that the\ncontext outlives the mutex_unlock() call.\n\nmutex_lock(&ctx->uring_lock);\nreq_put(req); // typically via io_req_task_submit()\nmutex_unlock(&ctx->uring_lock);\n\nMost contexts are fine: io-wq pins requests, syscalls hold the file,\ntask works are taking ctx references and so on. However, the task work\nfallback path doesn't follow the rule.\n\nCc:  <stable@vger.kernel.org>\nFixes: 04fc6c802d (\"io_uring: save ctx put/get for task_work submit\")\nReported-by: Jann Horn <jannh@google.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/io-uring/CAG48ez3xSoYb+45f1RLtktROJrpiDQ1otNvdR+YLQf7m+Krj5Q@mail.gmail.com/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc5",
        "release_date": "2023-12-03 19:09:28 -0700 io_uring: fix mutex_unlock with unreferenced ctx"
    },
    {
        "commit": "8fadb86d4ced8b8349a3b227d6d66736ff150819",
        "message": "No more users of this field.\n\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Martin K. Petersen <martin.petersen@oracle.com>\nSigned-off-by: Keith Busch <kbusch@kernel.org>\nLink: https://lore.kernel.org/r/20231130215309.2923568-5-kbusch@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.8-rc5",
        "release_date": "2023-12-01 18:29:18 -0700 io_uring: remove uring_cmd cookie"
    },
    {
        "commit": "c9a925b7bcd9552f19ba50519c6a49ed7ca61691",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix an issue with discontig page checking for IORING_SETUP_NO_MMAP\n\n - Fix an issue with not allowing IORING_SETUP_NO_MMAP also disallowing\n   mmap'ed buffer rings\n\n - Fix an issue with deferred release of memory mapped pages\n\n - Fix a lockdep issue with IORING_SETUP_NO_MMAP\n\n - Use fget/fput consistently, even from our sync system calls. No real\n   issue here, but if we were ever to allow closing io_uring descriptors\n   it would be required. Let's play it safe and just use the full ref\n   counted versions upfront. Most uses of io_uring are threaded anyway,\n   and hence already doing the full version underneath.\n\n* tag 'io_uring-6.7-2023-11-30' of git://git.kernel.dk/linux:\n  io_uring: use fget/fput consistently\n  io_uring: free io_buffer_list entries via RCU\n  io_uring/kbuf: prune deferred locked cache when tearing down\n  io_uring/kbuf: recycle freed mapped buffer ring entries\n  io_uring/kbuf: defer release of mapped buffer rings\n  io_uring: enable io_mem_alloc/free to be used in other parts\n  io_uring: don't guard IORING_OFF_PBUF_RING with SETUP_NO_MMAP\n  io_uring: don't allow discontig pages for IORING_SETUP_NO_MMAP",
        "kernel_version": "v6.7-rc4",
        "release_date": "2023-12-02 06:47:32 +0900 Merge tag 'io_uring-6.7-2023-11-30' of git://git.kernel.dk/linux"
    },
    {
        "commit": "73363c262d6a7d26063da96610f61baf69a70f7c",
        "message": "Normally within a syscall it's fine to use fdget/fdput for grabbing a\nfile from the file table, and it's fine within io_uring as well. We do\nthat via io_uring_enter(2), io_uring_register(2), and then also for\ncancel which is invoked from the latter. io_uring cannot close its own\nfile descriptors as that is explicitly rejected, and for the cancel\nside of things, the file itself is just used as a lookup cookie.\n\nHowever, it is more prudent to ensure that full references are always\ngrabbed. For anything threaded, either explicitly in the application\nitself or through use of the io-wq worker threads, this is what happens\nanyway. Generalize it and use fget/fput throughout.\n\nAlso see the below link for more details.\n\nLink: https://lore.kernel.org/io-uring/CAG48ez1htVSO3TqmrF8QcX2WFuYTRM-VZ_N10i-VZgbtg=NNqw@mail.gmail.com/\nSuggested-by: Jann Horn <jannh@google.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc4",
        "release_date": "2023-11-28 11:56:29 -0700 io_uring: use fget/fput consistently"
    },
    {
        "commit": "5cf4f52e6d8aa2d3b7728f568abbf9d42a3af252",
        "message": "mmap_lock nests under uring_lock out of necessity, as we may be doing\nuser copies with uring_lock held. However, for mmap of provided buffer\nrings, we attempt to grab uring_lock with mmap_lock already held from\ndo_mmap(). This makes lockdep, rightfully, complain:\n\nWARNING: possible circular locking dependency detected\n6.7.0-rc1-00009-gff3337ebaf94-dirty #4438 Not tainted\n------------------------------------------------------\nbuf-ring.t/442 is trying to acquire lock:\nffff00020e1480a8 (&ctx->uring_lock){+.+.}-{3:3}, at: io_uring_validate_mmap_request.isra.0+0x4c/0x140\n\nbut task is already holding lock:\nffff0000dc226190 (&mm->mmap_lock){++++}-{3:3}, at: vm_mmap_pgoff+0x124/0x264\n\nwhich lock already depends on the new lock.\n\nthe existing dependency chain (in reverse order) is:\n\n-> #1 (&mm->mmap_lock){++++}-{3:3}:\n       __might_fault+0x90/0xbc\n       io_register_pbuf_ring+0x94/0x488\n       __arm64_sys_io_uring_register+0x8dc/0x1318\n       invoke_syscall+0x5c/0x17c\n       el0_svc_common.constprop.0+0x108/0x130\n       do_el0_svc+0x2c/0x38\n       el0_svc+0x4c/0x94\n       el0t_64_sync_handler+0x118/0x124\n       el0t_64_sync+0x168/0x16c\n\n-> #0 (&ctx->uring_lock){+.+.}-{3:3}:\n       __lock_acquire+0x19a0/0x2d14\n       lock_acquire+0x2e0/0x44c\n       __mutex_lock+0x118/0x564\n       mutex_lock_nested+0x20/0x28\n       io_uring_validate_mmap_request.isra.0+0x4c/0x140\n       io_uring_mmu_get_unmapped_area+0x3c/0x98\n       get_unmapped_area+0xa4/0x158\n       do_mmap+0xec/0x5b4\n       vm_mmap_pgoff+0x158/0x264\n       ksys_mmap_pgoff+0x1d4/0x254\n       __arm64_sys_mmap+0x80/0x9c\n       invoke_syscall+0x5c/0x17c\n       el0_svc_common.constprop.0+0x108/0x130\n       do_el0_svc+0x2c/0x38\n       el0_svc+0x4c/0x94\n       el0t_64_sync_handler+0x118/0x124\n       el0t_64_sync+0x168/0x16c\n\nFrom that mmap(2) path, we really just need to ensure that the buffer\nlist doesn't go away from underneath us. For the lower indexed entries,\nthey never go away until the ring is freed and we can always sanely\nreference those as long as the caller has a file reference. For the\nhigher indexed ones in our xarray, we just need to ensure that the\nbuffer list remains valid while we return the address of it.\n\nFree the higher indexed io_buffer_list entries via RCU. With that we can\navoid needing ->uring_lock inside mmap(2), and simply hold the RCU read\nlock around the buffer list lookup and address check.\n\nTo ensure that the arrayed lookup either returns a valid fully formulated\nentry via RCU lookup, add an 'is_ready' flag that we access with store\nand release memory ordering. This isn't needed for the xarray lookups,\nbut doesn't hurt either. Since this isn't a fast path, retain it across\nboth types. Similarly, for the allocated array inside the ctx, ensure\nwe use the proper load/acquire as setup could in theory be running in\nparallel with mmap.\n\nWhile in there, add a few lockdep checks for documentation purposes.\n\nCc: stable@vger.kernel.org\nFixes: c56e022c0a27 (\"io_uring: add support for user mapped provided buffer ring\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc4",
        "release_date": "2023-11-28 11:45:02 -0700 io_uring: free io_buffer_list entries via RCU"
    },
    {
        "commit": "07d6063d3d3beb3168d3ac9fdef7bca81254d983",
        "message": "We used to just use our page list for final teardown, which would ensure\nthat we got all the buffers, even the ones that were not on the normal\ncached list. But while moving to slab for the io_buffers, we know only\nprune this list, not the deferred locked list that we have. This can\ncause a leak of memory, if the workload ends up using the intermediate\nlocked list.\n\nFix this by always pruning both lists when tearing down.\n\nFixes: b3a4dbc89d40 (\"io_uring/kbuf: Use slab for struct io_buffer objects\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc4",
        "release_date": "2023-11-28 11:45:02 -0700 io_uring/kbuf: prune deferred locked cache when tearing down"
    },
    {
        "commit": "b10b73c102a2eab91e1cd62a03d6446f1dfecc64",
        "message": "Right now we stash any potentially mmap'ed provided ring buffer range\nfor freeing at release time, regardless of when they get unregistered.\nSince we're keeping track of these ranges anyway, keep track of their\nregistration state as well, and use that to recycle ranges when\nappropriate rather than always allocate new ones.\n\nThe lookup is a basic scan of entries, checking for the best matching\nfree entry.\n\nFixes: c392cbecd8ec (\"io_uring/kbuf: defer release of mapped buffer rings\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc4",
        "release_date": "2023-11-28 11:45:02 -0700 io_uring/kbuf: recycle freed mapped buffer ring entries"
    },
    {
        "commit": "c392cbecd8eca4c53f2bf508731257d9d0a21c2d",
        "message": "If a provided buffer ring is setup with IOU_PBUF_RING_MMAP, then the\nkernel allocates the memory for it and the application is expected to\nmmap(2) this memory. However, io_uring uses remap_pfn_range() for this\noperation, so we cannot rely on normal munmap/release on freeing them\nfor us.\n\nStash an io_buf_free entry away for each of these, if any, and provide\na helper to free them post ->release().\n\nCc: stable@vger.kernel.org\nFixes: c56e022c0a27 (\"io_uring: add support for user mapped provided buffer ring\")\nReported-by: Jann Horn <jannh@google.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc4",
        "release_date": "2023-11-28 07:56:16 -0700 io_uring/kbuf: defer release of mapped buffer rings"
    },
    {
        "commit": "120ae58593630819209a011a3f9c89f73bcc9894",
        "message": "The eventfd_signal_mask() helper was introduced for io_uring and similar\nto eventfd_signal() it always passed 1 for @n. So don't bother with that\nargument at all.\n\nLink: https://lore.kernel.org/r/20231122-vfs-eventfd-signal-v2-3-bd549b14ce0c@kernel.org\nReviewed-by: Jan Kara <jack@suse.cz>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Christian Brauner <brauner@kernel.org>",
        "kernel_version": "v6.8-rc3",
        "release_date": "2023-11-28 14:08:46 +0100 eventfd: simplify eventfd_signal_mask()"
    },
    {
        "commit": "edecf1689768452ba1a64b7aaf3a47a817da651a",
        "message": "In preparation for using these helpers, make them non-static and add\nthem to our internal header.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc4",
        "release_date": "2023-11-27 20:53:52 -0700 io_uring: enable io_mem_alloc/free to be used in other parts"
    },
    {
        "commit": "6f007b1406637d3d73d42e41d7e8d9b245185e69",
        "message": "This flag only applies to the SQ and CQ rings, it's perfectly valid\nto use a mmap approach for the provided ring buffers. Move the\ncheck into where it belongs.\n\nCc: stable@vger.kernel.org\nFixes: 03d89a2de25b (\"io_uring: support for user allocated memory for rings/sqes\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc4",
        "release_date": "2023-11-27 17:10:56 -0700 io_uring: don't guard IORING_OFF_PBUF_RING with SETUP_NO_MMAP"
    },
    {
        "commit": "820d070feb668aab5bc9413c285a1dda2a70e076",
        "message": "io_sqes_map() is used rather than io_mem_alloc(), if the application\npasses in memory for mapping rather than have the kernel allocate it and\nthen mmap(2) the ranges. This then calls __io_uaddr_map() to perform the\npage mapping and pinning, which checks if we end up with the same pages,\nif more than one page is mapped. But this check is incorrect and only\nchecks if the first and last pages are the same, where it really should\nbe checking if the mapped pages are contigous. This allows mapping a\nsingle normal page, or a huge page range.\n\nDown the line we can add support for remapping pages to be virtually\ncontigous, which is really all that io_uring cares about.\n\nCc: stable@vger.kernel.org\nFixes: 03d89a2de25b (\"io_uring: support for user allocated memory for rings/sqes\")\nReported-by: Jann Horn <jannh@google.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc4",
        "release_date": "2023-11-27 08:28:56 -0700 io_uring: don't allow discontig pages for IORING_SETUP_NO_MMAP"
    },
    {
        "commit": "004442384416cbb3cedf99eb8ab5f10c32e4dd34",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A fix for ensuring that LINKAT always propagates flags correctly, and\n  a fix for an off-by-one in segment skipping for registered buffers.\n\n  Both heading to stable as well\"\n\n* tag 'io_uring-6.7-2023-11-23' of git://git.kernel.dk/linux:\n  io_uring: fix off-by one bvec index\n  io_uring/fs: consider link->flags when getting path for LINKAT",
        "kernel_version": "v6.7-rc3",
        "release_date": "2023-11-23 17:36:29 -0800 Merge tag 'io_uring-6.7-2023-11-23' of git://git.kernel.dk/linux"
    },
    {
        "commit": "d6fef34ee4d102be448146f24caf96d7b4a05401",
        "message": "If the offset equals the bv_len of the first registered bvec, then the\nrequest does not include any of that first bvec. Skip it so that drivers\ndon't have to deal with a zero length bvec, which was observed to break\nNVMe's PRP list creation.\n\nCc: stable@vger.kernel.org\nFixes: bd11b3a391e3 (\"io_uring: don't use iov_iter_advance() for fixed buffers\")\nSigned-off-by: Keith Busch <kbusch@kernel.org>\nLink: https://lore.kernel.org/r/20231120221831.2646460-1-kbusch@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc3",
        "release_date": "2023-11-20 15:21:38 -0700 io_uring: fix off-by one bvec index"
    },
    {
        "commit": "8479063f1fbee201a8739130e816cc331b675838",
        "message": "In order for `AT_EMPTY_PATH` to work as expected, the fact\nthat the user wants that behavior needs to make it to `getname_flags`\nor it will return ENOENT.\n\nFixes: cf30da90bc3a (\"io_uring: add support for IORING_OP_LINKAT\")\nCc:  <stable@vger.kernel.org>\nLink: https://github.com/axboe/liburing/issues/995\nSigned-off-by: Charles Mirabile <cmirabil@redhat.com>\nLink: https://lore.kernel.org/r/20231120105545.1209530-1-cmirabil@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc3",
        "release_date": "2023-11-20 09:01:42 -0700 io_uring/fs: consider link->flags when getting path for LINKAT"
    },
    {
        "commit": "0e413c2a737f4ce8924645ee80438c3be7c44ee3",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single fixup for a change we made in this release, which caused\n  a regression in sometimes missing fdinfo output if the SQPOLL thread\n  had the lock held when fdinfo output was retrieved.\n\n  This brings us back on par with what we had before, where just the\n  main uring_lock will prevent that output. We'd love to get rid of that\n  too, but that is beyond the scope of this release and will have to\n  wait for 6.8\"\n\n* tag 'io_uring-6.7-2023-11-17' of git://git.kernel.dk/linux:\n  io_uring/fdinfo: remove need for sqpoll lock for thread/pid retrieval",
        "kernel_version": "v6.7-rc2",
        "release_date": "2023-11-17 14:03:18 -0800 Merge tag 'io_uring-6.7-2023-11-17' of git://git.kernel.dk/linux"
    },
    {
        "commit": "a0d45c3f596be53c1bd8822a1984532d14fdcea9",
        "message": "A previous commit added a trylock for getting the SQPOLL thread info via\nfdinfo, but this introduced a regression where we often fail to get it if\nthe thread is busy. For that case, we end up not printing the current CPU\nand PID info.\n\nRather than rely on this lock, just print the pid we already stored in\nthe io_sq_data struct, and ensure we update the current CPU every time\nwe've slept or potentially rescheduled. The latter won't potentially be\n100% accurate, but that wasn't the case before either as the task can\nget migrated at any time unless it has been pinned at creation time.\n\nWe retain keeping the io_sq_data dereference inside the ctx->uring_lock,\nas it has always been, as destruction of the thread and data happen below\nthat. We could make this RCU safe, but there's little point in doing that.\n\nWith this, we always print the last valid information we had, rather than\nhave spurious outputs with missing information.\n\nFixes: 7644b1a1c9a7 (\"io_uring/fdinfo: lock SQ thread while retrieving thread cpu/pid\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc2",
        "release_date": "2023-11-15 06:35:46 -0700 io_uring/fdinfo: remove need for sqpoll lock for thread/pid retrieval"
    },
    {
        "commit": "4f0b9194bc119a9850a99e5e824808e2f468c348",
        "message": "The call to the inode_init_security_anon() LSM hook is not the sole\nreason to use anon_inode_getfile_secure() or anon_inode_getfd_secure().\nFor example, the functions also allow one to create a file with non-zero\nsize, without needing a full-blown filesystem.  In this case, you don't\nneed a \"secure\" version, just unique inodes; the current name of the\nfunctions is confusing and does not explain well the difference with\nthe more \"standard\" anon_inode_getfile() and anon_inode_getfd().\n\nOf course, there is another side of the coin; neither io_uring nor\nuserfaultfd strictly speaking need distinct inodes, and it is not\nthat clear anymore that anon_inode_create_get{file,fd}() allow the LSM\nto intercept and block the inode's creation.  If one was so inclined,\nanon_inode_getfile_secure() and anon_inode_getfd_secure() could be kept,\nusing the shared inode or a new one depending on CONFIG_SECURITY.\nHowever, this is probably overkill, and potentially a cause of bugs in\ndifferent configurations.  Therefore, just add a comment to io_uring\nand userfaultfd explaining the choice of the function.\n\nWhile at it, remove the export for what is now anon_inode_create_getfd().\nThere is no in-tree module that uses it, and the old name is gone anyway.\nIf anybody actually needs the symbol, they can ask or they can just use\nanon_inode_create_getfile(), which will be exported very soon for use\nin KVM.\n\nSuggested-by: Christian Brauner <brauner@kernel.org>\nReviewed-by: Christian Brauner <brauner@kernel.org>\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>",
        "kernel_version": "v6.8-rc2",
        "release_date": "2023-11-14 08:00:57 -0500 fs: Rename anon_inode_getfile_secure() and anon_inode_getfd_secure()"
    },
    {
        "commit": "b712075e03cf95d9009c99230775dc41195bde8a",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Mostly just a few fixes and cleanups caused by the read multishot\n  support.\n\n  Outside of that, a stable fix for how a connect retry is done\"\n\n* tag 'io_uring-6.7-2023-11-10' of git://git.kernel.dk/linux:\n  io_uring: do not clamp read length for multishot read\n  io_uring: do not allow multishot read to set addr or len\n  io_uring: indicate if io_kbuf_recycle did recycle anything\n  io_uring/rw: add separate prep handler for fixed read/write\n  io_uring/rw: add separate prep handler for readv/writev\n  io_uring/net: ensure socket is marked connected on connect retry\n  io_uring/rw: don't attempt to allocate async data if opcode doesn't need it",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-11-10 11:25:58 -0800 Merge tag 'io_uring-6.7-2023-11-10' of git://git.kernel.dk/linux"
    },
    {
        "commit": "4b803784178d1721ba26cadd8aef13b2c7456730",
        "message": "Pull block fixes from Jens Axboe:\n\n - NVMe pull request via Keith:\n      - nvme keyring config compile fixes (Hannes and Arnd)\n      - fabrics keep alive fixes (Hannes)\n      - tcp authentication fixes (Mark)\n      - io_uring_cmd error handling fix (Anuj)\n      - stale firmware attribute fix (Daniel)\n      - tcp memory leak (Christophe)\n      - crypto library usage simplification (Eric)\n\n - nbd use-after-free fix. May need a followup, but at least it's better\n   than what it was before (Li)\n\n - Rate limit write on read-only device warnings (Yu)\n\n* tag 'block-6.7-2023-11-10' of git://git.kernel.dk/linux:\n  nvme: keyring: fix conditional compilation\n  nvme: common: make keyring and auth separate modules\n  blk-core: use pr_warn_ratelimited() in bio_check_ro()\n  nbd: fix uaf in nbd_open\n  nvme: start keep-alive after admin queue setup\n  nvme-loop: always quiesce and cancel commands before destroying admin q\n  nvme-tcp: avoid open-coding nvme_tcp_teardown_admin_queue()\n  nvme-auth: always set valid seq_num in dhchap reply\n  nvme-auth: add flag for bi-directional auth\n  nvme-auth: auth success1 msg always includes resp\n  nvme: fix error-handling for io_uring nvme-passthrough\n  nvme: update firmware version after commit\n  nvme-tcp: Fix a memory leak\n  nvme-auth: use crypto_shash_tfm_digest()",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-11-10 11:20:33 -0800 Merge tag 'block-6.7-2023-11-10' of git://git.kernel.dk/linux"
    },
    {
        "commit": "37d9486874ec925fa298bcd7ba628a9b206e812f",
        "message": "Pull NVMe fixes from Keith:\n\n\"nvme fixes for 6.7\n\n - nvme keyring config compile fixes (Hannes and Arnd)\n - fabrics keep alive fixes (Hannes)\n - tcp authentication fixes (Mark)\n - io_uring_cmd error handling fix (Anuj)\n - stale firmware attribute fix (Daniel)\n - tcp memory leak (Christophe)\n - cytpo library usage simplification (Eric)\"\n\n* tag 'nvme-6.7-2023-11-8' of git://git.infradead.org/nvme:\n  nvme: keyring: fix conditional compilation\n  nvme: common: make keyring and auth separate modules\n  nvme: start keep-alive after admin queue setup\n  nvme-loop: always quiesce and cancel commands before destroying admin q\n  nvme-tcp: avoid open-coding nvme_tcp_teardown_admin_queue()\n  nvme-auth: always set valid seq_num in dhchap reply\n  nvme-auth: add flag for bi-directional auth\n  nvme-auth: auth success1 msg always includes resp\n  nvme: fix error-handling for io_uring nvme-passthrough\n  nvme: update firmware version after commit\n  nvme-tcp: Fix a memory leak\n  nvme-auth: use crypto_shash_tfm_digest()",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-11-08 09:19:16 -0700 Merge tag 'nvme-6.7-2023-11-8' of git://git.infradead.org/nvme into block-6.7"
    },
    {
        "commit": "e53759298a7d7e98c3e5c2440d395d19cea7d6bf",
        "message": "When doing a multishot read, the code path reuses the old read\npaths. However this breaks an assumption built into those paths,\nnamely that struct io_rw::len is available for reuse by __io_import_iovec.\n\nFor multishot this results in len being set for the first receive\ncall, and then subsequent calls are clamped to that buffer length\nincorrectly.\n\nInstead keep len as zero after recycling buffers, to reuse the full\nbuffer size of the next selected buffer.\n\nFixes: fc68fcda0491 (\"io_uring/rw: add support for IORING_OP_READ_MULTISHOT\")\nSigned-off-by: Dylan Yudaken <dyudaken@gmail.com>\nLink: https://lore.kernel.org/r/20231106203909.197089-4-dyudaken@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-11-06 13:41:58 -0700 io_uring: do not clamp read length for multishot read"
    },
    {
        "commit": "49fbe99486786661994a55ced855c31d966bbdf0",
        "message": "For addr: this field is not used, since buffer select is forced.\nBut by forcing it to be zero it leaves open future uses of the field.\n\nlen is actually usable, you could imagine that you want to receive\nmultishot up to a certain length.\nHowever right now this is not how it is implemented, and it seems\nsafer to force this to be zero.\n\nFixes: fc68fcda0491 (\"io_uring/rw: add support for IORING_OP_READ_MULTISHOT\")\nSigned-off-by: Dylan Yudaken <dyudaken@gmail.com>\nLink: https://lore.kernel.org/r/20231106203909.197089-3-dyudaken@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-11-06 13:41:58 -0700 io_uring: do not allow multishot read to set addr or len"
    },
    {
        "commit": "89d528ba2f8281de61163c6b62e598b64d832175",
        "message": "It can be useful to know if io_kbuf_recycle did actually recycle the\nbuffer on the request, or if it left the request alone.\n\nSigned-off-by: Dylan Yudaken <dyudaken@gmail.com>\nLink: https://lore.kernel.org/r/20231106203909.197089-2-dyudaken@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-11-06 13:41:58 -0700 io_uring: indicate if io_kbuf_recycle did recycle anything"
    },
    {
        "commit": "1147dd0503564fa0e03489a039f9e0c748a03db4",
        "message": "Driver may return an error before submitting the command to the device.\nEnsure that such error is propagated up.\n\nFixes: 456cba386e94 (\"nvme: wire-up uring-cmd support for io-passthru on char-device.\")\nSigned-off-by: Anuj Gupta <anuj20.g@samsung.com>\nSigned-off-by: Kanchan Joshi <joshi.k@samsung.com>\nReviewed-by: Niklas Cassel <niklas.cassel@wdc.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Keith Busch <kbusch@kernel.org>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-11-06 08:09:31 -0800 nvme: fix error-handling for io_uring nvme-passthrough"
    },
    {
        "commit": "f688944cfb810986c626cb13d95bc666e5c8a36c",
        "message": "Rather than sprinkle opcode checks in the generic read/write prep handler,\nhave a separate prep handler for the vectored readv/writev operation.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-11-06 07:43:16 -0700 io_uring/rw: add separate prep handler for fixed read/write"
    },
    {
        "commit": "0e984ec88da9747549227900e5215c5e6a1b65ae",
        "message": "Rather than sprinkle opcode checks in the generic read/write prep handler,\nhave a separate prep handler for the vectored readv/writev operation.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-11-06 07:41:17 -0700 io_uring/rw: add separate prep handler for readv/writev"
    },
    {
        "commit": "6bdfe2d88b9ff8b0cce32ce87cd47c0e9d665f48",
        "message": "Pull apparmor updates from John Johansen:\n \"This adds initial support for mediating io_uring and userns creation.\n  Adds a new restriction that tightens the use of change_profile, and a\n  couple of optimizations to reduce performance bottle necks that have\n  been found when retrieving the current task's secid and allocating\n  work buffers.\n\n  The majority of the patch set continues cleaning up and simplifying\n  the code (fixing comments, removing now dead functions, and macros\n  etc). Finally there are 4 bug fixes, with the regression fix having\n  had a couple months of testing.\n\n  Features:\n   - optimize retrieving current task secid\n   - add base io_uring mediation\n   - add base userns mediation\n   - improve buffer allocation\n   - allow restricting unprivilege change_profile\n\n  Cleanups:\n   - Fix kernel doc comments\n   - remove unused declarations\n   - remove unused functions\n   - remove unneeded #ifdef\n   - remove unused macros\n   - mark fns static\n   - cleanup fn with unused return values\n   - cleanup audit data\n   - pass cred through to audit data\n   - refcount the pdb instead of using duplicates\n   - make SK_CTX macro an inline fn\n   - some comment cleanups\n\n  Bug fixes:\n   - fix regression in mount mediation\n   - fix invalid refenece\n   - use passed in gfp flags\n   - advertise avaiability of extended perms and disconnected.path\"\n\n* tag 'apparmor-pr-2023-11-03' of git://git.kernel.org/pub/scm/linux/kernel/git/jj/linux-apparmor: (39 commits)\n  apparmor: Fix some kernel-doc comments\n  apparmor: Fix one kernel-doc comment\n  apparmor: Fix some kernel-doc comments\n  apparmor: mark new functions static\n  apparmor: Fix regression in mount mediation\n  apparmor: cache buffers on percpu list if there is lock contention\n  apparmor: add io_uring mediation\n  apparmor: add user namespace creation mediation\n  apparmor: allow restricting unprivileged change_profile\n  apparmor: advertise disconnected.path is available\n  apparmor: refcount the pdb\n  apparmor: provide separate audit messages for file and policy checks\n  apparmor: pass cred through to audit info.\n  apparmor: rename audit_data->label to audit_data->subj_label\n  apparmor: combine common_audit_data and apparmor_audit_data\n  apparmor: rename SK_CTX() to aa_sock and make it an inline fn\n  apparmor: Optimize retrieving current task secid\n  apparmor: remove unused functions in policy_ns.c/.h\n  apparmor: remove unneeded #ifdef in decompress_zstd()\n  apparmor: fix invalid reference on profile->disconnected\n  ...",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-11-03 09:48:17 -1000 Merge tag 'apparmor-pr-2023-11-03' of git://git.kernel.org/pub/scm/linux/kernel/git/jj/linux-apparmor"
    },
    {
        "commit": "f8f9ab2d98116e79d220f1d089df7464ad4e026d",
        "message": "io_uring does non-blocking connection attempts, which can yield some\nunexpected results if a connect request is re-attempted by an an\napplication. This is equivalent to the following sync syscall sequence:\n\nsock = socket(AF_INET, SOCK_STREAM | SOCK_NONBLOCK, IPPROTO_TCP);\nconnect(sock, &addr, sizeof(addr);\n\nret == -1 and errno == EINPROGRESS expected here. Now poll for POLLOUT\non sock, and when that returns, we expect the socket to be connected.\nBut if we follow that procedure with:\n\nconnect(sock, &addr, sizeof(addr));\n\nyou'd expect ret == -1 and errno == EISCONN here, but you actually get\nret == 0. If we attempt the connection one more time, then we get EISCON\nas expected.\n\nio_uring used to do this, but turns out that bluetooth fails with EBADFD\nif you attempt to re-connect. Also looks like EISCONN _could_ occur with\nthis sequence.\n\nRetain the ->in_progress logic, but work-around a potential EISCONN or\nEBADFD error and only in those cases look at the sock_error(). This\nshould work in general and avoid the odd sequence of a repeated connect\nrequest returning success when the socket is already connected.\n\nThis is all a side effect of the socket state being in a CONNECTING\nstate when we get EINPROGRESS, and only a re-connect or other related\noperation will turn that into CONNECTED.\n\nCc: stable@vger.kernel.org\nFixes: 3fb1bd688172 (\"io_uring/net: handle -EINPROGRESS correct for IORING_OP_CONNECT\")\nLink: https://github.com/axboe/liburing/issues/980\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-11-03 13:25:50 -0600 io_uring/net: ensure socket is marked connected on connect retry"
    },
    {
        "commit": "0df96fb71a395b4fc9c80180306420c743f395a8",
        "message": "The new read multishot method doesn't need to allocate async data ever,\nas it doesn't do vectored IO and it must only be used with provided\nbuffers. While it doesn't have ->prep_async() set, it also sets\n->async_size to 0, which is different from any other read/write type we\notherwise support.\n\nIf it's used on a file type that isn't pollable, we do try and allocate\nthis async data, and then try and use that data. But since we passed in\na size of 0 for the data, we get a NULL back on data allocation. We then\nproceed to dereference that to copy state, and that obviously won't end\nwell.\n\nAdd a check in io_setup_async_rw() for this condition, and avoid copying\nstate. Also add a check for whether or not buffer selection is specified\nin prep while at it.\n\nFixes: fc68fcda0491 (\"io_uring/rw: add support for IORING_OP_READ_MULTISHOT\")\nLink: https://bugzilla.kernel.org/show_bug.cgi?id=218101\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-11-03 09:31:21 -0600 io_uring/rw: don't attempt to allocate async data if opcode doesn't need it"
    },
    {
        "commit": "4de520f1fcefd4ebb7dddcf28bde1b330c2f6b5d",
        "message": "Pull io_uring futex support from Jens Axboe:\n \"This adds support for using futexes through io_uring - first futex\n  wake and wait, and then the vectored variant of waiting, futex waitv.\n\n  For both wait/wake/waitv, we support the bitset variant, as the\n  'normal' variants can be easily implemented on top of that.\n\n  PI and requeue are not supported through io_uring, just the above\n  mentioned parts. This may change in the future, but in the spirit of\n  keeping this small (and based on what people have been asking for),\n  this is what we currently have.\n\n  Wake support is pretty straight forward, most of the thought has gone\n  into the wait side to avoid needing to offload wait operations to a\n  blocking context. Instead, we rely on the usual callbacks to retry and\n  post a completion event, when appropriate.\n\n  As far as I can recall, the first request for futex support with\n  io_uring came from Andres Freund, working on postgres. His aio rework\n  of postgres was one of the early adopters of io_uring, and futex\n  support was a natural extension for that. This is relevant from both a\n  usability point of view, as well as for effiency and performance. In\n  Andres's words, for the former:\n\n     Futex wait support in io_uring makes it a lot easier to avoid\n     deadlocks in concurrent programs that have their own buffer pool:\n     Obviously pages in the application buffer pool have to be locked\n     during IO. If the initiator of IO A needs to wait for a held lock\n     B, the holder of lock B might wait for the IO A to complete. The\n     ability to wait for a lock and IO completions at the same time\n     provides an efficient way to avoid such deadlocks\n\n  and in terms of effiency, even without unlocking the full potential\n  yet, Andres says:\n\n     Futex wake support in io_uring is useful because it allows for more\n     efficient directed wakeups. For some \"locks\" postgres has queues\n     implemented in userspace, with wakeup logic that cannot easily be\n     implemented with FUTEX_WAKE_BITSET on a single \"futex word\"\n     (imagine waiting for journal flushes to have completed up to a\n     certain point).\n\n     Thus a \"lock release\" sometimes need to wake up many processes in a\n     row. A quick-and-dirty conversion to doing these wakeups via\n     io_uring lead to a 3% throughput increase, with 12% fewer context\n     switches, albeit in a fairly extreme workload\"\n\n* tag 'io_uring-futex-2023-10-30' of git://git.kernel.dk/linux:\n  io_uring: add support for vectored futex waits\n  futex: make the vectored futex operations available\n  futex: make futex_parse_waitv() available as a helper\n  futex: add wake_data to struct futex_q\n  io_uring: add support for futex wake and wait\n  futex: abstract out a __futex_wake_mark() helper\n  futex: factor out the futex wake handling\n  futex: move FUTEX2_VALID_MASK to futex.h",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-11-01 11:25:08 -1000 Merge tag 'io_uring-futex-2023-10-30' of git://git.kernel.dk/linux"
    },
    {
        "commit": "f5277ad1e9768dbd05b1ae8dcdba690215d8c5b7",
        "message": "Pull io_uring {get,set}sockopt support from Jens Axboe:\n \"This adds support for using getsockopt and setsockopt via io_uring.\n\n  The main use cases for this is to enable use of direct descriptors,\n  rather than first instantiating a normal file descriptor, doing the\n  option tweaking needed, then turning it into a direct descriptor. With\n  this support, we can avoid needing a regular file descriptor\n  completely.\n\n  The net and bpf bits have been signed off on their side\"\n\n* tag 'for-6.7/io_uring-sockopt-2023-10-30' of git://git.kernel.dk/linux:\n  selftests/bpf/sockopt: Add io_uring support\n  io_uring/cmd: Introduce SOCKET_URING_OP_SETSOCKOPT\n  io_uring/cmd: Introduce SOCKET_URING_OP_GETSOCKOPT\n  io_uring/cmd: return -EOPNOTSUPP if net is disabled\n  selftests/net: Extract uring helpers to be reusable\n  tools headers: Grab copy of io_uring.h\n  io_uring/cmd: Pass compat mode in issue_flags\n  net/socket: Break down __sys_getsockopt\n  net/socket: Break down __sys_setsockopt\n  bpf: Add sockptr support for setsockopt\n  bpf: Add sockptr support for getsockopt",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-11-01 11:16:34 -1000 Merge tag 'for-6.7/io_uring-sockopt-2023-10-30' of git://git.kernel.dk/linux"
    },
    {
        "commit": "ffa059b262ba72571e7fefe7fa2b4ebb6776b277",
        "message": "Pull io_uring updates from Jens Axboe:\n \"This contains the core io_uring updates, of which there are not many,\n  and adds support for using WAITID through io_uring and hence not\n  needing to block on these kinds of events.\n\n  Outside of that, tweaks to the legacy provided buffer handling and\n  some cleanups related to cancelations for uring_cmd support\"\n\n* tag 'for-6.7/io_uring-2023-10-30' of git://git.kernel.dk/linux:\n  io_uring/poll: use IOU_F_TWQ_LAZY_WAKE for wakeups\n  io_uring/kbuf: Use slab for struct io_buffer objects\n  io_uring/kbuf: Allow the full buffer id space for provided buffers\n  io_uring/kbuf: Fix check of BID wrapping in provided buffers\n  io_uring/rsrc: cleanup io_pin_pages()\n  io_uring: cancelable uring_cmd\n  io_uring: retain top 8bits of uring_cmd flags for kernel internal use\n  io_uring: add IORING_OP_WAITID support\n  exit: add internal include file with helpers\n  exit: add kernel_waitid_prepare() helper\n  exit: move core of do_wait() into helper\n  exit: abstract out should_wake helper for child_wait_callback()\n  io_uring/rw: add support for IORING_OP_READ_MULTISHOT\n  io_uring/rw: mark readv/writev as vectored in the opcode definition\n  io_uring/rw: split io_read() into a helper",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-11-01 11:09:19 -1000 Merge tag 'for-6.7/io_uring-2023-10-30' of git://git.kernel.dk/linux"
    },
    {
        "commit": "3b3f874cc1d074bdcffc224d683925fd11808fe7",
        "message": "Pull misc vfs updates from Christian Brauner:\n \"This contains the usual miscellaneous features, cleanups, and fixes\n  for vfs and individual fses.\n\n  Features:\n\n   - Rename and export helpers that get write access to a mount. They\n     are used in overlayfs to get write access to the upper mount.\n\n   - Print the pretty name of the root device on boot failure. This\n     helps in scenarios where we would usually only print\n     \"unknown-block(1,2)\".\n\n   - Add an internal SB_I_NOUMASK flag. This is another part in the\n     endless POSIX ACL saga in a way.\n\n     When POSIX ACLs are enabled via SB_POSIXACL the vfs cannot strip\n     the umask because if the relevant inode has POSIX ACLs set it might\n     take the umask from there. But if the inode doesn't have any POSIX\n     ACLs set then we apply the umask in the filesytem itself. So we end\n     up with:\n\n      (1) no SB_POSIXACL -> strip umask in vfs\n      (2) SB_POSIXACL    -> strip umask in filesystem\n\n     The umask semantics associated with SB_POSIXACL allowed filesystems\n     that don't even support POSIX ACLs at all to raise SB_POSIXACL\n     purely to avoid umask stripping. That specifically means NFS v4 and\n     Overlayfs. NFS v4 does it because it delegates this to the server\n     and Overlayfs because it needs to delegate umask stripping to the\n     upper filesystem, i.e., the filesystem used as the writable layer.\n\n     This went so far that SB_POSIXACL is raised eve on kernels that\n     don't even have POSIX ACL support at all.\n\n     Stop this blatant abuse and add SB_I_NOUMASK which is an internal\n     superblock flag that filesystems can raise to opt out of umask\n     handling. That should really only be the two mentioned above. It's\n     not that we want any filesystems to do this. Ideally we have all\n     umask handling always in the vfs.\n\n   - Make overlayfs use SB_I_NOUMASK too.\n\n   - Now that we have SB_I_NOUMASK, stop checking for SB_POSIXACL in\n     IS_POSIXACL() if the kernel doesn't have support for it. This is a\n     very old patch but it's only possible to do this now with the wider\n     cleanup that was done.\n\n   - Follow-up work on fake path handling from last cycle. Citing mostly\n     from Amir:\n\n     When overlayfs was first merged, overlayfs files of regular files\n     and directories, the ones that are installed in file table, had a\n     \"fake\" path, namely, f_path is the overlayfs path and f_inode is\n     the \"real\" inode on the underlying filesystem.\n\n     In v6.5, we took another small step by introducing of the\n     backing_file container and the file_real_path() helper. This change\n     allowed vfs and filesystem code to get the \"real\" path of an\n     overlayfs backing file. With this change, we were able to make\n     fsnotify work correctly and report events on the \"real\" filesystem\n     objects that were accessed via overlayfs.\n\n     This method works fine, but it still leaves the vfs vulnerable to\n     new code that is not aware of files with fake path. A recent\n     example is commit db1d1e8b9867 (\"IMA: use vfs_getattr_nosec to get\n     the i_version\"). This commit uses direct referencing to f_path in\n     IMA code that otherwise uses file_inode() and file_dentry() to\n     reference the filesystem objects that it is measuring.\n\n     This contains work to switch things around: instead of having\n     filesystem code opt-in to get the \"real\" path, have generic code\n     opt-in for the \"fake\" path in the few places that it is needed.\n\n     Is it far more likely that new filesystems code that does not use\n     the file_dentry() and file_real_path() helpers will end up causing\n     crashes or averting LSM/audit rules if we keep the \"fake\" path\n     exposed by default.\n\n     This change already makes file_dentry() moot, but for now we did\n     not change this helper just added a WARN_ON() in ovl_d_real() to\n     catch if we have made any wrong assumptions.\n\n     After the dust settles on this change, we can make file_dentry() a\n     plain accessor and we can drop the inode argument to ->d_real().\n\n   - Switch struct file to SLAB_TYPESAFE_BY_RCU. This looks like a small\n     change but it really isn't and I would like to see everyone on\n     their tippie toes for any possible bugs from this work.\n\n     Essentially we've been doing most of what SLAB_TYPESAFE_BY_RCU for\n     files since a very long time because of the nasty interactions\n     between the SCM_RIGHTS file descriptor garbage collection. So\n     extending it makes a lot of sense but it is a subtle change. There\n     are almost no places that fiddle with file rcu semantics directly\n     and the ones that did mess around with struct file internal under\n     rcu have been made to stop doing that because it really was always\n     dodgy.\n\n     I forgot to put in the link tag for this change and the discussion\n     in the commit so adding it into the merge message:\n\n       https://lore.kernel.org/r/20230926162228.68666-1-mjguzik@gmail.com\n\n  Cleanups:\n\n   - Various smaller pipe cleanups including the removal of a spin lock\n     that was only used to protect against writes without pipe_lock()\n     from O_NOTIFICATION_PIPE aka watch queues. As that was never\n     implemented remove the additional locking from pipe_write().\n\n   - Annotate struct watch_filter with the new __counted_by attribute.\n\n   - Clarify do_unlinkat() cleanup so that it doesn't look like an extra\n     iput() is done that would cause issues.\n\n   - Simplify file cleanup when the file has never been opened.\n\n   - Use module helper instead of open-coding it.\n\n   - Predict error unlikely for stale retry.\n\n   - Use WRITE_ONCE() for mount expiry field instead of just commenting\n     that one hopes the compiler doesn't get smart.\n\n  Fixes:\n\n   - Fix readahead on block devices.\n\n   - Fix writeback when layztime is enabled and inodes whose timestamp\n     is the only thing that changed reside on wb->b_dirty_time. This\n     caused excessively large zombie memory cgroup when lazytime was\n     enabled as such inodes weren't handled fast enough.\n\n   - Convert BUG_ON() to WARN_ON_ONCE() in open_last_lookups()\"\n\n* tag 'vfs-6.7.misc' of gitolite.kernel.org:pub/scm/linux/kernel/git/vfs/vfs: (26 commits)\n  file, i915: fix file reference for mmap_singleton()\n  vfs: Convert BUG_ON to WARN_ON_ONCE in open_last_lookups\n  writeback, cgroup: switch inodes with dirty timestamps to release dying cgwbs\n  chardev: Simplify usage of try_module_get()\n  ovl: rely on SB_I_NOUMASK\n  fs: fix umask on NFS with CONFIG_FS_POSIX_ACL=n\n  fs: store real path instead of fake path in backing file f_path\n  fs: create helper file_user_path() for user displayed mapped file path\n  fs: get mnt_writers count for an open backing file's real path\n  vfs: stop counting on gcc not messing with mnt_expiry_mark if not asked\n  vfs: predict the error in retry_estale as unlikely\n  backing file: free directly\n  vfs: fix readahead(2) on block devices\n  io_uring: use files_lookup_fd_locked()\n  file: convert to SLAB_TYPESAFE_BY_RCU\n  vfs: shave work on failed file open\n  fs: simplify misleading code to remove ambiguity regarding ihold()/iput()\n  watch_queue: Annotate struct watch_filter with __counted_by\n  fs/pipe: use spinlock in pipe_read() only if there is a watch_queue\n  fs/pipe: remove unnecessary spinlock from pipe_write()\n  ...",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-30 09:14:19 -1000 Merge tag 'vfs-6.7.misc' of gitolite.kernel.org:pub/scm/linux/kernel/git/vfs/vfs"
    },
    {
        "commit": "d1b0949f23a343d3153d5c681fb1866538534227",
        "message": "Pull misc filesystem fixes from Al Viro:\n \"Assorted fixes all over the place: literally nothing in common, could\n  have been three separate pull requests.\n\n  All are simple regression fixes, but not for anything from this cycle\"\n\n* tag 'pull-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:\n  ceph_wait_on_conflict_unlink(): grab reference before dropping ->d_lock\n  io_uring: kiocb_done() should *not* trust ->ki_pos if ->{read,write}_iter() failed\n  sparc32: fix a braino in fault handling in csum_and_copy_..._user()",
        "kernel_version": "v6.6",
        "release_date": "2023-10-27 16:44:58 -1000 Merge tag 'pull-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs"
    },
    {
        "commit": "1939316bf988f3e49a07d9c4dd6f660bf4daa53d",
        "message": "->ki_pos value is unreliable in such cases.  For an obvious example,\nconsider O_DSYNC write - we feed the data to page cache and start IO,\nthen we make sure it's completed.  Update of ->ki_pos is dealt with\nby the first part; failure in the second ends up with negative value\nreturned _and_ ->ki_pos left advanced as if sync had been successful.\nIn the same situation write(2) does not advance the file position\nat all.\n\nReviewed-by: Christian Brauner <brauner@kernel.org>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Al Viro <viro@zeniv.linux.org.uk>",
        "kernel_version": "v6.6",
        "release_date": "2023-10-27 20:14:11 -0400 io_uring: kiocb_done() should *not* trust ->ki_pos if ->{read,write}_iter() failed"
    },
    {
        "commit": "56567a20b22bdbf85c3e55eee3bf2bd23fa2f108",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Fix for an issue reported where reading fdinfo could find a NULL\n  thread as we didn't properly synchronize, and then a disable for the\n  IOCB_DIO_CALLER_COMP optimization as a recent reported highlighted how\n  that could lead to deadlocks if the task issued async O_DIRECT writes\n  and then proceeded to do sync fallocate() calls\"\n\n* tag 'io_uring-6.6-2023-10-27' of git://git.kernel.dk/linux:\n  io_uring/rw: disable IOCB_DIO_CALLER_COMP\n  io_uring/fdinfo: lock SQ thread while retrieving thread cpu/pid",
        "kernel_version": "v6.6",
        "release_date": "2023-10-27 14:10:32 -1000 Merge tag 'io_uring-6.6-2023-10-27' of git://git.kernel.dk/linux"
    },
    {
        "commit": "838b35bb6a89c36da07ca39520ec071d9250334d",
        "message": "If an application does O_DIRECT writes with io_uring and the file system\nsupports IOCB_DIO_CALLER_COMP, then completions of the dio write side is\ndone from the task_work that will post the completion event for said\nwrite as well.\n\nWhenever a dio write is done against a file, the inode i_dio_count is\nelevated. This enables other callers to use inode_dio_wait() to wait for\nprevious writes to complete. If we defer the full dio completion to\ntask_work, we are dependent on that task_work being run before the\ninode i_dio_count can be decremented.\n\nIf the same task that issues io_uring dio writes with\nIOCB_DIO_CALLER_COMP performs a synchronous system call that calls\ninode_dio_wait(), then we can deadlock as we're blocked sleeping on\nthe event to become true, but not processing the completions that will\nresult in the inode i_dio_count being decremented.\n\nUntil we can guarantee that this is the case, then disable the deferred\ncaller completions.\n\nFixes: 099ada2c8726 (\"io_uring/rw: add write support for IOCB_DIO_CALLER_COMP\")\nReported-by: Andres Freund <andres@anarazel.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6",
        "release_date": "2023-10-25 08:02:29 -0600 io_uring/rw: disable IOCB_DIO_CALLER_COMP"
    },
    {
        "commit": "7644b1a1c9a7ae8ab99175989bfc8676055edb46",
        "message": "We could race with SQ thread exit, and if we do, we'll hit a NULL pointer\ndereference when the thread is cleared. Grab the SQPOLL data lock before\nattempting to get the task cpu and pid for fdinfo, this ensures we have a\nstable view of it.\n\nCc: stable@vger.kernel.org\nLink: https://bugzilla.kernel.org/show_bug.cgi?id=218032\nReviewed-by: Gabriel Krisman Bertazi <krisman@suse.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6",
        "release_date": "2023-10-25 07:44:14 -0600 io_uring/fdinfo: lock SQ thread while retrieving thread cpu/pid"
    },
    {
        "commit": "7060d3ccdd4cf042749664d2fe93f978a777e3e9",
        "message": "Two new functions were introduced as global functions when they are\nonly called from inside the file that defines them and should have\nbeen static:\n\nsecurity/apparmor/lsm.c:658:5: error: no previous prototype for 'apparmor_uring_override_creds' [-Werror=missing-prototypes]\nsecurity/apparmor/lsm.c:682:5: error: no previous prototype for 'apparmor_uring_sqpoll' [-Werror=missing-prototypes]\n\nFixes: c4371d90633b7 (\"apparmor: add io_uring mediation\")\nSigned-off-by: Arnd Bergmann <arnd@arndb.de>\nSigned-off-by: John Johansen <john.johansen@canonical.com>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-22 00:45:53 -0700 apparmor: mark new functions static"
    },
    {
        "commit": "747b7628ca66de3806e6988d3a6e0c9c48d33694",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single fix for a bug report that came in, fixing a case where\n  failure to init a ring with IORING_SETUP_NO_MMAP can trigger a NULL\n  pointer dereference\"\n\n* tag 'io_uring-6.6-2023-10-20' of git://git.kernel.dk/linux:\n  io_uring: fix crash with IORING_SETUP_NO_MMAP and invalid SQ ring address",
        "kernel_version": "v6.6-rc7",
        "release_date": "2023-10-20 10:28:46 -0700 Merge tag 'io_uring-6.6-2023-10-20' of git://git.kernel.dk/linux"
    },
    {
        "commit": "b9ec913212e6e91efa5a0a612c4a8ec4cf5da896",
        "message": "Expand the sockopt test to use also check for io_uring {g,s}etsockopt\ncommands operations.\n\nThis patch starts by marking each test if they support io_uring support\nor not.\n\nRight now, io_uring cmd getsockopt() has a limitation of only\naccepting level == SOL_SOCKET, otherwise it returns -EOPNOTSUPP. Since\nthere aren't any test exercising getsockopt(level == SOL_SOCKET), this\npatch changes two tests to use level == SOL_SOCKET, they are\n\"getsockopt: support smaller ctx->optlen\" and \"getsockopt: read\nctx->optlen\".\nThere is no limitation for the setsockopt() part.\n\nLater, each test runs using regular {g,s}etsockopt systemcalls, and, if\nliburing is supported, execute the same test (again), but calling\nliburing {g,s}setsockopt commands.\n\nThis patch also changes the level of two tests to use SOL_SOCKET for the\nfollowing two tests. This is going to help to exercise the io_uring\nsubsystem:\n * getsockopt: read ctx->optlen\n * getsockopt: support smaller ctx->optlen\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nLink: https://lore.kernel.org/r/20231016134750.1381153-12-leitao@debian.org\nAcked-by: Martin KaFai Lau <martin.lau@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-19 16:42:04 -0600 selftests/bpf/sockopt: Add io_uring support"
    },
    {
        "commit": "4232c6e349f3a591fd0f432e6b858d32095adce6",
        "message": "Add initial support for SOCKET_URING_OP_SETSOCKOPT. This new command is\nsimilar to setsockopt. This implementation leverages the function\ndo_sock_setsockopt(), which is shared with the setsockopt() system call\npath.\n\nImportant to say that userspace needs to keep the pointer's memory alive\nuntil the operation is completed. I.e, the memory could not be\ndeallocated before the CQE is returned to userspace.\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nReviewed-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20231016134750.1381153-11-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-19 16:42:03 -0600 io_uring/cmd: Introduce SOCKET_URING_OP_SETSOCKOPT"
    },
    {
        "commit": "a5d2f99aff6b6f9cd6a1ab6907d8be8066114791",
        "message": "Add support for getsockopt command (SOCKET_URING_OP_GETSOCKOPT), where\nlevel is SOL_SOCKET. This is leveraging the sockptr_t infrastructure,\nwhere a sockptr_t is either userspace or kernel space, and handled as\nsuch.\n\nDifferently from the getsockopt(2), the optlen field is not a userspace\npointers. In getsockopt(2), userspace provides optlen pointer, which is\noverwritten by the kernel.  In this implementation, userspace passes a\nu32, and the new value is returned in cqe->res. I.e., optlen is not a\npointer.\n\nImportant to say that userspace needs to keep the pointer alive until\nthe CQE is completed.\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nReviewed-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20231016134750.1381153-10-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-19 16:42:03 -0600 io_uring/cmd: Introduce SOCKET_URING_OP_GETSOCKOPT"
    },
    {
        "commit": "d2cac3ec823798861f8f39b4aa0ec960ffa997b0",
        "message": "Protect io_uring_cmd_sock() to be called if CONFIG_NET is not set. If\nnetwork is not enabled, but io_uring is, then we want to return\n-EOPNOTSUPP for any possible socket operation.\n\nThis is helpful because io_uring_cmd_sock() can now call functions that\nonly exits if CONFIG_NET is enabled without having #ifdef CONFIG_NET\ninside the function itself.\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nLink: https://lore.kernel.org/r/20231016134750.1381153-9-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-19 16:42:03 -0600 io_uring/cmd: return -EOPNOTSUPP if net is disabled"
    },
    {
        "commit": "ba6e0e5cb5b2c2e736e16b4aead816450a8718e6",
        "message": "Instead of defining basic io_uring functions in the test case, move them\nto a common directory, so, other tests can use them.\n\nThis simplify the test code and reuse the common liburing\ninfrastructure. This is basically a copy of what we have in\nio_uring_zerocopy_tx with some minor improvements to make checkpatch\nhappy.\n\nA follow-up test will use the same helpers in a BPF sockopt test.\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nLink: https://lore.kernel.org/r/20231016134750.1381153-8-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-19 16:42:03 -0600 selftests/net: Extract uring helpers to be reusable"
    },
    {
        "commit": "7746a6adfc81e2e0386a85117d5e8fd824da367b",
        "message": "This file will be used by mini_uring.h and allow tests to run without\nthe need of installing liburing to run the tests.\n\nThis is needed to run io_uring tests in BPF, such as\n(tools/testing/selftests/bpf/prog_tests/sockopt.c).\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nLink: https://lore.kernel.org/r/20231016134750.1381153-7-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-19 16:42:03 -0600 tools headers: Grab copy of io_uring.h"
    },
    {
        "commit": "5fea44a6e05b86bf49019fbbf2ab30098d03e0dc",
        "message": "Create a new flag to track if the operation is running compat mode.\nThis basically check the context->compat and pass it to the issue_flags,\nso, it could be queried later in the callbacks.\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nReviewed-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20231016134750.1381153-6-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-19 16:42:03 -0600 io_uring/cmd: Pass compat mode in issue_flags"
    },
    {
        "commit": "0b05b0cd78c92371fdde6333d006f39eaf9e0860",
        "message": "Split __sys_getsockopt() into two functions by removing the core\nlogic into a sub-function (do_sock_getsockopt()). This will avoid\ncode duplication when doing the same operation in other callers, for\ninstance.\n\ndo_sock_getsockopt() will be called by io_uring getsockopt() command\noperation in the following patch.\n\nThe same was done for the setsockopt pair.\n\nSuggested-by: Martin KaFai Lau <martin.lau@linux.dev>\nSigned-off-by: Breno Leitao <leitao@debian.org>\nAcked-by: Jakub Kicinski <kuba@kernel.org>\nAcked-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20231016134750.1381153-5-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-19 16:41:37 -0600 net/socket: Break down __sys_getsockopt"
    },
    {
        "commit": "1406245c29454ff84919736be83e14cdaba7fec1",
        "message": "Split __sys_setsockopt() into two functions by removing the core\nlogic into a sub-function (do_sock_setsockopt()). This will avoid\ncode duplication when doing the same operation in other callers, for\ninstance.\n\ndo_sock_setsockopt() will be called by io_uring setsockopt() command\noperation in the following patch.\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nReviewed-by: Willem de Bruijn <willemb@google.com>\nAcked-by: Jakub Kicinski <kuba@kernel.org>\nAcked-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20231016134750.1381153-4-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-19 14:05:49 -0600 net/socket: Break down __sys_setsockopt"
    },
    {
        "commit": "3f31e0d14d44ad491a81b7c1f83f32fbc300a867",
        "message": "The whole network stack uses sockptr, and while it doesn't move to\nsomething more modern, let's use sockptr in setsockptr BPF hooks, so, it\ncould be used by other callers.\n\nThe main motivation for this change is to use it in the io_uring\n{g,s}etsockopt(), which will use a userspace pointer for *optval, but, a\nkernel value for optlen.\n\nLink: https://lore.kernel.org/all/ZSArfLaaGcfd8LH8@gmail.com/\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nAcked-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20231016134750.1381153-3-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-19 14:05:37 -0600 bpf: Add sockptr support for setsockopt"
    },
    {
        "commit": "a615f67e1a426f35366b8398c11f31c148e7df48",
        "message": "The whole network stack uses sockptr, and while it doesn't move to\nsomething more modern, let's use sockptr in getsockptr BPF hooks, so, it\ncould be used by other callers.\n\nThe main motivation for this change is to use it in the io_uring\n{g,s}etsockopt(), which will use a userspace pointer for *optval, but, a\nkernel value for optlen.\n\nLink: https://lore.kernel.org/all/ZSArfLaaGcfd8LH8@gmail.com/\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nAcked-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20231016134750.1381153-2-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-19 14:05:28 -0600 bpf: Add sockptr support for getsockopt"
    },
    {
        "commit": "ea1cc20cd4ce55dd920a87a317c43da03ccea192",
        "message": "Pull vfs fix from Christian Brauner:\n \"An openat() call from io_uring triggering an audit call can apparently\n  cause the refcount of struct filename to be incremented from multiple\n  threads concurrently during async execution, triggering a refcount\n  underflow and hitting a BUG_ON(). That bug has been lurking around\n  since at least v5.16 apparently.\n\n  Switch to an atomic counter to fix that. The underflow check is\n  downgraded from a BUG_ON() to a WARN_ON_ONCE() but we could easily\n  remove that check altogether tbh\"\n\n* tag 'v6.6-rc7.vfs.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs:\n  audit,io_uring: io_uring openat triggers audit reference count underflow",
        "kernel_version": "v6.6-rc7",
        "release_date": "2023-10-19 09:37:41 -0700 Merge tag 'v6.6-rc7.vfs.fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs"
    },
    {
        "commit": "6ce4a93dbb5bd93bc2bdf14da63f9360a4dcd6a1",
        "message": "With poll triggered retries, each event trigger will cause a task_work\nitem to be added for processing. If the ring is setup with\nIORING_SETUP_DEFER_TASKRUN and a task is waiting on multiple events to\ncomplete, any task_work addition will wake the task for processing these\nitems. This can cause more context switches than we would like, if the\napplication is deliberately waiting on multiple items to increase\nefficiency.\n\nFor example, if an application has receive multishot armed for sockets\nand wants to wait for N to complete within M usec of time, we should not\nbe waking up and processing these items until we have all the events we\nasked for. By switching the poll trigger to lazy wake, we'll process\nthem when they are all ready, in one swoop, rather than wake multiple\ntimes only to process one and then go back to sleep.\n\nAt some point we probably want to look at just making the lazy wake\nthe default, but for now, let's just selectively enable it where it\nmakes sense.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-19 06:42:29 -0600 io_uring/poll: use IOU_F_TWQ_LAZY_WAKE for wakeups"
    },
    {
        "commit": "50d910d27362d6809a0668f0f1cb5220bc7dc6a0",
        "message": "While valid we don't need to open-code rcu dereferences if we're\nacquiring file_lock anyway.\n\nSuggested-by: Al Viro <viro@zeniv.linux.org.uk>\nLink: https://lore.kernel.org/r/20231010030615.GO800259@ZenIV\nSigned-off-by: Christian Brauner <brauner@kernel.org>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-19 11:02:49 +0200 io_uring: use files_lookup_fd_locked()"
    },
    {
        "commit": "c4371d90633b73cf6e86aff43ff2b5d95ad2b9eb",
        "message": "For now, the io_uring mediation is limited to sqpoll and\noverride_creds.\n\nSigned-off-by: Georgia Garcia <georgia.garcia@canonical.com>\nSigned-off-by: John Johansen <john.johansen@canonical.com>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-18 15:58:49 -0700 apparmor: add io_uring mediation"
    },
    {
        "commit": "8b51a3956d44ea6ade962874ade14de9a7d16556",
        "message": "If we specify a valid CQ ring address but an invalid SQ ring address,\nwe'll correctly spot this and free the allocated pages and clear them\nto NULL. However, we don't clear the ring page count, and hence will\nattempt to free the pages again. We've already cleared the address of\nthe page array when freeing them, but we don't check for that. This\ncauses the following crash:\n\nUnable to handle kernel NULL pointer dereference at virtual address 0000000000000000\nOops [#1]\nModules linked in:\nCPU: 0 PID: 20 Comm: kworker/u2:1 Not tainted 6.6.0-rc5-dirty #56\nHardware name: ucbbar,riscvemu-bare (DT)\nWorkqueue: events_unbound io_ring_exit_work\nepc : io_pages_free+0x2a/0x58\n ra : io_rings_free+0x3a/0x50\n epc : ffffffff808811a2 ra : ffffffff80881406 sp : ffff8f80000c3cd0\n status: 0000000200000121 badaddr: 0000000000000000 cause: 000000000000000d\n [<ffffffff808811a2>] io_pages_free+0x2a/0x58\n [<ffffffff80881406>] io_rings_free+0x3a/0x50\n [<ffffffff80882176>] io_ring_exit_work+0x37e/0x424\n [<ffffffff80027234>] process_one_work+0x10c/0x1f4\n [<ffffffff8002756e>] worker_thread+0x252/0x31c\n [<ffffffff8002f5e4>] kthread+0xc4/0xe0\n [<ffffffff8000332a>] ret_from_fork+0xa/0x1c\n\nCheck for a NULL array in io_pages_free(), but also clear the page counts\nwhen we free them to be on the safer side.\n\nReported-by: rtm@csail.mit.edu\nFixes: 03d89a2de25b (\"io_uring: support for user allocated memory for rings/sqes\")\nCc: stable@vger.kernel.org\nReviewed-by: Jeff Moyer <jmoyer@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc7",
        "release_date": "2023-10-18 09:22:14 -0600 io_uring: fix crash with IORING_SETUP_NO_MMAP and invalid SQ ring address"
    },
    {
        "commit": "216c8f5ef0f209a3797292c487bdaa6991ab4b92",
        "message": "Monitor work actually introduces one extra context for handling abort, this\nway is easy to cause race, and also introduce extra delay when handling\naborting.\n\nNow we start to support cancelable uring_cmd, so use it instead:\n\n1) this cancel callback is either run from the uring cmd submission task\ncontext or called after the io_uring context is exit, so the callback is\nrun exclusively with ublk_ch_uring_cmd() and __ublk_rq_task_work().\n\n2) the previous patch freezes request queue when calling ublk_abort_queue(),\nwhich is now completely exclusive with ublk_queue_rq() and\nublk_ch_uring_cmd()/__ublk_rq_task_work().\n\n3) in timeout handler, if all IOs are in-flight, then all uring commands\nare completed, uring command canceling can't help us to provide forward\nprogress any more, so call ublk_abort_requests() in timeout handler.\n\nThis way simplifies aborting queue, and is helpful for adding new feature,\nsuch as, relax the limit of using single task for handling one queue.\n\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20231009093324.957829-7-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-17 08:27:56 -0600 ublk: replace monitor with cancelable uring_cmd"
    },
    {
        "commit": "85248d670b71d9edda9459ee14fdc85c8e9632c0",
        "message": "ublk_cancel_dev() just calls ublk_cancel_queue() to cancel all pending\nio commands after ublk request queue is idle. The only protection is just\nthe read & write of ubq->nr_io_ready and avoid duplicated command cancel,\nso add one per-queue lock with cancel flag for providing this protection,\nmeantime move ublk_cancel_dev() out of ub->mutex.\n\nThen we needn't to call io_uring_cmd_complete_in_task() to cancel\npending command. And the same cancel logic will be re-used for\ncancelable uring command.\n\nThis patch basically reverts commit ac5902f84bb5 (\"ublk: fix AB-BA lockdep warning\").\n\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20231009093324.957829-4-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-17 08:27:55 -0600 ublk: move ublk_cancel_dev() out of ub->mutex"
    },
    {
        "commit": "3421c7f68bba52281bbb38bc76c18dc03cb689e4",
        "message": "In well-done ublk server implementation, ublk io command won't be\nlinked into any link chain. Meantime they are always handled in no-wait\nstyle, so basically io cmd is always handled in submitter task context.\n\nHowever, the server may set IOSQE_ASYNC, or io command is linked to one\nchain mistakenly, then we may still run into io-wq context and\nctx->uring_lock isn't held.\n\nSo in case of IO_URING_F_UNLOCKED, schedule this command by\nio_uring_cmd_complete_in_task to force running it in submitter task. Then\nublk_ch_uring_cmd_local() is guaranteed to run with context uring_lock held,\nand we needn't to worry about sync among submission code path any more.\n\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20231009093324.957829-3-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-17 08:27:55 -0600 ublk: make sure io cmd handled in submitter task context"
    },
    {
        "commit": "d451fdd0fe8323bf970b735cf276d4e11ae8cdcc",
        "message": "Merge in io_uring fixes, as the ublk simplifying cancelations and\naborts depend on the two patches from Ming adding cancelation support\nfor uring_cmd.\n\n* for-6.7/io_uring:\n  io_uring/kbuf: Use slab for struct io_buffer objects\n  io_uring/kbuf: Allow the full buffer id space for provided buffers\n  io_uring/kbuf: Fix check of BID wrapping in provided buffers\n  io_uring/rsrc: cleanup io_pin_pages()\n  io_uring: cancelable uring_cmd\n  io_uring: retain top 8bits of uring_cmd flags for kernel internal use\n  io_uring: add IORING_OP_WAITID support\n  exit: add internal include file with helpers\n  exit: add kernel_waitid_prepare() helper\n  exit: move core of do_wait() into helper\n  exit: abstract out should_wake helper for child_wait_callback()\n  io_uring/rw: add support for IORING_OP_READ_MULTISHOT\n  io_uring/rw: mark readv/writev as vectored in the opcode definition\n  io_uring/rw: split io_read() into a helper",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-17 08:26:38 -0600 Merge branch 'for-6.7/io_uring' into for-6.7/block"
    },
    {
        "commit": "8d211285c6d48baca4934c54b1965d4e75ce35e2",
        "message": "This adds set of tests which use io_uring for rx/tx. This test suite is\nimplemented as separated util like 'vsock_test' and has the same set of\ninput arguments as 'vsock_test'. These tests only cover cases of data\ntransmission (no connect/bind/accept etc).\n\nSigned-off-by: Arseniy Krasnov <avkrasnov@salutedevices.com>\nReviewed-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-15 13:19:43 +0100 test/vsock: io_uring rx/tx tests"
    },
    {
        "commit": "dcc55d7bb23016e7ae335c8558e1937d7a551b35",
        "message": "This bit is used by io_uring in case of zerocopy tx mode. io_uring code\nchecks, that socket has this feature. This patch sets it in two places:\n1) For socket in 'connect()' call.\n2) For new socket which is returned by 'accept()' call.\n\nSigned-off-by: Arseniy Krasnov <avkrasnov@salutedevices.com>\nReviewed-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-15 13:19:42 +0100 vsock: enable SOCK_SUPPORT_ZC bit"
    },
    {
        "commit": "03adc61edad49e1bbecfb53f7ea5d78f398fe368",
        "message": "An io_uring openat operation can update an audit reference count\nfrom multiple threads resulting in the call trace below.\n\nA call to io_uring_submit() with a single openat op with a flag of\nIOSQE_ASYNC results in the following reference count updates.\n\nThese first part of the system call performs two increments that do not race.\n\ndo_syscall_64()\n  __do_sys_io_uring_enter()\n    io_submit_sqes()\n      io_openat_prep()\n        __io_openat_prep()\n          getname()\n            getname_flags()       /* update 1 (increment) */\n              __audit_getname()   /* update 2 (increment) */\n\nThe openat op is queued to an io_uring worker thread which starts the\nopportunity for a race.  The system call exit performs one decrement.\n\ndo_syscall_64()\n  syscall_exit_to_user_mode()\n    syscall_exit_to_user_mode_prepare()\n      __audit_syscall_exit()\n        audit_reset_context()\n           putname()              /* update 3 (decrement) */\n\nThe io_uring worker thread performs one increment and two decrements.\nThese updates can race with the system call decrement.\n\nio_wqe_worker()\n  io_worker_handle_work()\n    io_wq_submit_work()\n      io_issue_sqe()\n        io_openat()\n          io_openat2()\n            do_filp_open()\n              path_openat()\n                __audit_inode()   /* update 4 (increment) */\n            putname()             /* update 5 (decrement) */\n        __audit_uring_exit()\n          audit_reset_context()\n            putname()             /* update 6 (decrement) */\n\nThe fix is to change the refcnt member of struct audit_names\nfrom int to atomic_t.\n\nkernel BUG at fs/namei.c:262!\nCall Trace:\n...\n ? putname+0x68/0x70\n audit_reset_context.part.0.constprop.0+0xe1/0x300\n __audit_uring_exit+0xda/0x1c0\n io_issue_sqe+0x1f3/0x450\n ? lock_timer_base+0x3b/0xd0\n io_wq_submit_work+0x8d/0x2b0\n ? __try_to_del_timer_sync+0x67/0xa0\n io_worker_handle_work+0x17c/0x2b0\n io_wqe_worker+0x10a/0x350\n\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/lkml/MW2PR2101MB1033FFF044A258F84AEAA584F1C9A@MW2PR2101MB1033.namprd21.prod.outlook.com/\nFixes: 5bd2182d58e9 (\"audit,io_uring,io-wq: add some basic audit support to io_uring\")\nSigned-off-by: Dan Clash <daclash@linux.microsoft.com>\nLink: https://lore.kernel.org/r/20231012215518.GA4048@linuxonhyperv3.guj3yctzbm1etfxqx2vob5hsef.xx.internal.cloudapp.net\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Christian Brauner <brauner@kernel.org>",
        "kernel_version": "v6.6-rc7",
        "release_date": "2023-10-13 18:34:46 +0200 audit,io_uring: io_uring openat triggers audit reference count underflow"
    },
    {
        "commit": "a88c38694714f70b2bc72f33ca125bf06c0f62f2",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - syzbot report on a crash on 32-bit arm with highmem, and went digging\n   to check for potentially similar issues and found one more (me)\n\n - Fix a syzbot report with PROVE_LOCKING=y and setting up the ring in a\n   disabled state (me)\n\n - Fix for race with CPU hotplut and io-wq init (Jeff)\n\n* tag 'io_uring-6.6-2023-10-06' of git://git.kernel.dk/linux:\n  io-wq: fully initialize wqe before calling cpuhp_state_add_instance_nocalls()\n  io_uring: don't allow IORING_SETUP_NO_MMAP rings on highmem pages\n  io_uring: ensure io_lockdep_assert_cq_locked() handles disabled rings\n  io_uring/kbuf: don't allow registered buffer rings on highmem pages",
        "kernel_version": "v6.6-rc5",
        "release_date": "2023-10-06 15:41:18 -0700 Merge tag 'io_uring-6.6-2023-10-06' of git://git.kernel.dk/linux"
    },
    {
        "commit": "b3a4dbc89d4021b3f90ff6a13537111a004f9d07",
        "message": "The allocation of struct io_buffer for metadata of provided buffers is\ndone through a custom allocator that directly gets pages and\nfragments them.  But, slab would do just fine, as this is not a hot path\n(in fact, it is a deprecated feature) and, by keeping a custom allocator\nimplementation we lose benefits like tracking, poisoning,\nsanitizers. Finally, the custom code is more complex and requires\nkeeping the list of pages in struct ctx for no good reason.  This patch\ncleans this path up and just uses slab.\n\nI microbenchmarked it by forcing the allocation of a large number of\nobjects with the least number of io_uring commands possible (keeping\nnbufs=USHRT_MAX), with and without the patch.  There is a slight\nincrease in time spent in the allocation with slab, of course, but even\nwhen allocating to system resources exhaustion, which is not very\nrealistic and happened around 1/2 billion provided buffers for me, it\nwasn't a significant hit in system time.  Specially if we think of a\nreal-world scenario, an application doing register/unregister of\nprovided buffers will hit ctx->io_buffers_cache more often than actually\ngoing to slab.\n\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20231005000531.30800-4-krisman@suse.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-05 08:38:17 -0600 io_uring/kbuf: Use slab for struct io_buffer objects"
    },
    {
        "commit": "f74c746e476b9dad51448b9a9421aae72b60e25f",
        "message": "nbufs tracks the number of buffers and not the last bgid. In 16-bit, we\nhave 2^16 valid buffers, but the check mistakenly rejects the last\nbid. Let's fix it to make the interface consistent with the\ndocumentation.\n\nFixes: ddf0322db79c (\"io_uring: add IORING_OP_PROVIDE_BUFFERS\")\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20231005000531.30800-3-krisman@suse.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-05 08:38:15 -0600 io_uring/kbuf: Allow the full buffer id space for provided buffers"
    },
    {
        "commit": "ab69838e7c75b0edb699c1a8f42752b30333c46f",
        "message": "Commit 3851d25c75ed0 (\"io_uring: check for rollover of buffer ID when\nproviding buffers\") introduced a check to prevent wrapping the BID\ncounter when sqe->off is provided, but it's off-by-one too\nrestrictive, rejecting the last possible BID (65534).\n\ni.e., the following fails with -EINVAL.\n\n     io_uring_prep_provide_buffers(sqe, addr, size, 0xFFFF, 0, 0);\n\nFixes: 3851d25c75ed (\"io_uring: check for rollover of buffer ID when providing buffers\")\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20231005000531.30800-2-krisman@suse.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-05 08:38:08 -0600 io_uring/kbuf: Fix check of BID wrapping in provided buffers"
    },
    {
        "commit": "223ef474316466e9f61f6e0064f3a6fe4923a2c5",
        "message": "On at least arm32, but presumably any arch with highmem, if the\napplication passes in memory that resides in highmem for the rings,\nthen we should fail that ring creation. We fail it with -EINVAL, which\nis what kernels that don't support IORING_SETUP_NO_MMAP will do as well.\n\nCc: stable@vger.kernel.org\nFixes: 03d89a2de25b (\"io_uring: support for user allocated memory for rings/sqes\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc5",
        "release_date": "2023-10-03 09:59:58 -0600 io_uring: don't allow IORING_SETUP_NO_MMAP rings on highmem pages"
    },
    {
        "commit": "1658633c04653578429ff5dfc62fdc159203a8f2",
        "message": "io_lockdep_assert_cq_locked() checks that locking is correctly done when\na CQE is posted. If the ring is setup in a disabled state with\nIORING_SETUP_R_DISABLED, then ctx->submitter_task isn't assigned until\nthe ring is later enabled. We generally don't post CQEs in this state,\nas no SQEs can be submitted. However it is possible to generate a CQE\nif tagged resources are being updated. If this happens and PROVE_LOCKING\nis enabled, then the locking check helper will dereference\nctx->submitter_task, which hasn't been set yet.\n\nFixup io_lockdep_assert_cq_locked() to handle this case correctly. While\nat it, convert it to a static inline as well, so that generated line\noffsets will actually reflect which condition failed, rather than just\nthe line offset for io_lockdep_assert_cq_locked() itself.\n\nReported-and-tested-by: syzbot+efc45d4e7ba6ab4ef1eb@syzkaller.appspotmail.com\nFixes: f26cc9593581 (\"io_uring: lockdep annotate CQ locking\")\nCc: stable@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc5",
        "release_date": "2023-10-03 08:12:54 -0600 io_uring: ensure io_lockdep_assert_cq_locked() handles disabled rings"
    },
    {
        "commit": "f8024f1f36a30a082b0457d5779c8847cea57f57",
        "message": "syzbot reports that registering a mapped buffer ring on arm32 can\ntrigger an OOPS. Registered buffer rings have two modes, one of them\nis the application passing in the memory that the buffer ring should\nreside in. Once those pages are mapped, we use page_address() to get\na virtual address. This will obviously fail on highmem pages, which\naren't mapped.\n\nAdd a check if we have any highmem pages after mapping, and fail the\nattempt to register a provided buffer ring if we do. This will return\nthe same error as kernels that don't support provided buffer rings to\nbegin with.\n\nLink: https://lore.kernel.org/io-uring/000000000000af635c0606bcb889@google.com/\nFixes: c56e022c0a27 (\"io_uring: add support for user mapped provided buffer ring\")\nCc: stable@vger.kernel.org\nReported-by: syzbot+2113e61b8848fa7951d8@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc5",
        "release_date": "2023-10-03 08:12:28 -0600 io_uring/kbuf: don't allow registered buffer rings on highmem pages"
    },
    {
        "commit": "922a2c78f13611e2c08fc48f615c0cd367dcb6da",
        "message": "This function is overly convoluted with a goto error path, and checks\nunder the mmap_read_lock() that don't need to be at all. Rearrange it\na bit so the checks and errors fall out naturally, rather than needing\nto jump around for it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-10-02 18:25:23 -0600 io_uring/rsrc: cleanup io_pin_pages()"
    },
    {
        "commit": "a98b95959b98f0015ea159292f516f9e8cc9e4db",
        "message": "Pull io_uring fix from Jens Axboe:\n \"A single fix going to stable for the IORING_OP_LINKAT flag handling\"\n\n* tag 'io_uring-6.6-2023-09-28' of git://git.kernel.dk/linux:\n  io_uring/fs: remove sqe->rw_flags checking from LINKAT",
        "kernel_version": "v6.6-rc4",
        "release_date": "2023-09-29 12:56:34 -0700 Merge tag 'io_uring-6.6-2023-09-28' of git://git.kernel.dk/linux"
    },
    {
        "commit": "a52d4f657568d6458e873f74a9602e022afe666f",
        "message": "This is unionized with the actual link flags, so they can of course be\nset and they will be evaluated further down. If not we fail any LINKAT\nthat has to set option flags.\n\nFixes: cf30da90bc3a (\"io_uring: add support for IORING_OP_LINKAT\")\nCc: stable@vger.kernel.org\nReported-by: Thomas Leonard <talex5@gmail.com>\nLink: https://github.com/axboe/liburing/issues/955\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc4",
        "release_date": "2023-09-29 03:07:09 -0600 io_uring/fs: remove sqe->rw_flags checking from LINKAT"
    },
    {
        "commit": "8f350194d5cfd7016d4cd44e433df0faa4d4a703",
        "message": "This adds support for IORING_OP_FUTEX_WAITV, which allows registering a\nnotification for a number of futexes at once. If one of the futexes are\nwoken, then the request will complete with the index of the futex that got\nwoken as the result. This is identical to what the normal vectored futex\nwaitv operation does.\n\nUse like IORING_OP_FUTEX_WAIT, except sqe->addr must now contain a\npointer to a struct futex_waitv array, and sqe->off must now contain the\nnumber of elements in that array. As flags are passed in the futex_vector\narray, and likewise for the value and futex address(es), sqe->addr2\nand sqe->addr3 are also reserved for IORING_OP_FUTEX_WAITV.\n\nFor cancelations, FUTEX_WAITV does not rely on the futex_unqueue()\nreturn value as we're dealing with multiple futexes. Instead, a separate\nper io_uring request atomic is used to claim ownership of the request.\n\nWaiting on N futexes could be done with IORING_OP_FUTEX_WAIT as well,\nbut that punts a lot of the work to the application:\n\n1) Application would need to submit N IORING_OP_FUTEX_WAIT requests,\n   rather than just a single IORING_OP_FUTEX_WAITV.\n\n2) When one futex is woken, application would need to cancel the\n   remaining N-1 requests that didn't trigger.\n\nWhile this is of course doable, having a single vectored futex wait\nmakes for much simpler application code.\n\nAcked-by: Peter Zijlstra (Intel) <peterz@infradead.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-09-29 02:37:08 -0600 io_uring: add support for vectored futex waits"
    },
    {
        "commit": "e9a56c9325ef28d5481712e85dd5d3f8b2a68e88",
        "message": "Rename unqueue_multiple() as futex_unqueue_multiple(), and make both\nthat and futex_wait_multiple_setup() available for external users. This\nis in preparation for wiring up vectored waits in io_uring.\n\nAcked-by: Peter Zijlstra (Intel) <peterz@infradead.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-09-29 02:37:07 -0600 futex: make the vectored futex operations available"
    },
    {
        "commit": "194bb58c6090e39bd7d9b9c888a079213628e1f6",
        "message": "Add support for FUTEX_WAKE/WAIT primitives.\n\nIORING_OP_FUTEX_WAKE is mix of FUTEX_WAKE and FUTEX_WAKE_BITSET, as\nit does support passing in a bitset.\n\nSimilary, IORING_OP_FUTEX_WAIT is a mix of FUTEX_WAIT and\nFUTEX_WAIT_BITSET.\n\nFor both of them, they are using the futex2 interface.\n\nFUTEX_WAKE is straight forward, as those can always be done directly from\nthe io_uring submission without needing async handling. For FUTEX_WAIT,\nthings are a bit more complicated. If the futex isn't ready, then we\nrely on a callback via futex_queue->wake() when someone wakes up the\nfutex. From that calback, we queue up task_work with the original task,\nwhich will post a CQE and wake it, if necessary.\n\nCancelations are supported, both from the application point-of-view,\nbut also to be able to cancel pending waits if the ring exits before\nall events have occurred. The return value of futex_unqueue() is used\nto gate who wins the potential race between cancelation and futex\nwakeups. Whomever gets a 'ret == 1' return from that claims ownership\nof the io_uring futex request.\n\nThis is just the barebones wait/wake support. PI or REQUEUE support is\nnot added at this point, unclear if we might look into that later.\n\nLikewise, explicit timeouts are not supported either. It is expected\nthat users that need timeouts would do so via the usual io_uring\nmechanism to do that using linked timeouts.\n\nThe SQE format is as follows:\n\n`addr`\t\tAddress of futex\n`fd`\t\tfutex2(2) FUTEX2_* flags\n`futex_flags`\tio_uring specific command flags. None valid now.\n`addr2`\t\tValue of futex\n`addr3`\t\tMask to wake/wait\n\nAcked-by: Peter Zijlstra (Intel) <peterz@infradead.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-09-29 02:36:57 -0600 io_uring: add support for futex wake and wait"
    },
    {
        "commit": "12a4be50aff30ee8f2c6a64020c82a4e997e8d6c",
        "message": "In preparation for having another waker that isn't futex_wake_mark(),\nadd a wake handler in futex_q. No extra data is associated with the\nhandler outside of struct futex_q itself. futex_wake_mark() is defined as\nthe standard wakeup helper, now set through futex_q_init like other\ndefaults.\n\nNormal sync futex waiting relies on wake_q holding tasks that should\nbe woken up. This is what futex_wake_mark() does, it'll unqueue the\nfutex and add the associated task to the wake queue. For async usage of\nfutex waiting, rather than having tasks sleeping on the futex, we'll\nneed to deal with a futex wake differently. For the planned io_uring\ncase, that means posting a completion event for the task in question.\nHaving a definable wake handler can help support that use case.\n\nAcked-by: Peter Zijlstra (Intel) <peterz@infradead.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-09-29 02:36:50 -0600 futex: factor out the futex wake handling"
    },
    {
        "commit": "52e856c38761bae0cea09b25cfbb4d46cd930d45",
        "message": "Pull in locking/core from the tip tree, to get the futex2 dependencies\nfrom Peter Zijlstra.\n\n* 'locking/core' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)\n  locking/ww_mutex/test: Make sure we bail out instead of livelock\n  locking/ww_mutex/test: Fix potential workqueue corruption\n  locking/ww_mutex/test: Use prng instead of rng to avoid hangs at bootup\n  futex: Add sys_futex_requeue()\n  futex: Add flags2 argument to futex_requeue()\n  futex: Propagate flags into get_futex_key()\n  futex: Add sys_futex_wait()\n  futex: FLAGS_STRICT\n  futex: Add sys_futex_wake()\n  futex: Validate futex value against futex size\n  futex: Flag conversion\n  futex: Extend the FUTEX2 flags\n  futex: Clarify FUTEX2 flags\n  asm-generic: ticket-lock: Optimize arch_spin_value_unlocked()\n  futex/pi: Fix recursive rt_mutex waiter state\n  locking/rtmutex: Add a lockdep assert to catch potential nested blocking\n  locking/rtmutex: Use rt_mutex specific scheduler helpers\n  sched: Provide rt_mutex specific scheduler helpers\n  sched: Extract __schedule_loop()\n  locking/rtmutex: Avoid unconditional slowpath for DEBUG_RT_MUTEXES\n  ...",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-09-28 07:47:07 -0600 Merge branch 'locking/core' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip into io_uring-futex"
    },
    {
        "commit": "73c7e7a91f041f4c2e3c0db1e727163b331c60c9",
        "message": "* for-6.7/io_uring:\n  io_uring: cancelable uring_cmd\n  io_uring: retain top 8bits of uring_cmd flags for kernel internal use\n  io_uring: add IORING_OP_WAITID support\n  exit: add internal include file with helpers\n  exit: add kernel_waitid_prepare() helper\n  exit: move core of do_wait() into helper\n  exit: abstract out should_wake helper for child_wait_callback()\n  io_uring/rw: add support for IORING_OP_READ_MULTISHOT\n  io_uring/rw: mark readv/writev as vectored in the opcode definition\n  io_uring/rw: split io_read() into a helper",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-09-28 07:46:55 -0600 Merge branch 'for-6.7/io_uring' into io_uring-futex"
    },
    {
        "commit": "93b8cc60c37b9d17732b7a297e5dca29b50a990d",
        "message": "uring_cmd may never complete, such as ublk, in which uring cmd isn't\ncompleted until one new block request is coming from ublk block device.\n\nAdd cancelable uring_cmd to provide mechanism to driver for cancelling\npending commands in its own way.\n\nAdd API of io_uring_cmd_mark_cancelable() for driver to mark one command as\ncancelable, then io_uring will cancel this command in\nio_uring_cancel_generic(). ->uring_cmd() callback is reused for canceling\ncommand in driver's way, then driver gets notified with the cancelling\nfrom io_uring.\n\nAdd API of io_uring_cmd_get_task() to help driver cancel handler\ndeal with the canceling.\n\nReviewed-by: Gabriel Krisman Bertazi <krisman@suse.de>\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-09-28 07:36:00 -0600 io_uring: cancelable uring_cmd"
    },
    {
        "commit": "528ce6781726e022bc5dc84034360e6e8f1b89bd",
        "message": "Retain top 8bits of uring_cmd flags for kernel internal use, so that we\ncan move IORING_URING_CMD_POLLED out of uapi header.\n\nReviewed-by: Gabriel Krisman Bertazi <krisman@suse.de>\nReviewed-by: Anuj Gupta <anuj20.g@samsung.com>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-09-28 07:31:41 -0600 io_uring: retain top 8bits of uring_cmd flags for kernel internal use"
    },
    {
        "commit": "d78bfa1346ab1fe04d20aa45a0678d1fc866f37c",
        "message": "Add batched mq_ops.queue_rqs() support in null_blk for testing. The\nimplementation is much easy since null_blk doesn't have commit_rqs().\n\nWe simply handle each request one by one, if errors are encountered,\nleave them in the passed in list and return back.\n\nThere is about 3.6% improvement in IOPS of fio/t/io_uring on null_blk\nwith hw_queue_depth=256 on my test VM, from 1.09M to 1.13M.\n\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20230913151616.3164338-6-chengming.zhou@linux.dev\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-09-22 08:52:13 -0600 block/null_blk: add queue_rqs() support"
    },
    {
        "commit": "f31ecf671ddc498f20219453395794ff2383e06b",
        "message": "This adds support for an async version of waitid(2), in a fully async\nversion. If an event isn't immediately available, wait for a callback\nto trigger a retry.\n\nThe format of the sqe is as follows:\n\nsqe->len\t\tThe 'which', the idtype being queried/waited for.\nsqe->fd\t\t\tThe 'pid' (or id) being waited for.\nsqe->file_index\t\tThe 'options' being set.\nsqe->addr2\t\tA pointer to siginfo_t, if any, being filled in.\n\nbuf_index, add3, and waitid_flags are reserved/unused for now.\nwaitid_flags will be used for options for this request type. One\ninteresting use case may be to add multi-shot support, so that the\nrequest stays armed and posts a notification every time a monitored\nprocess state change occurs.\n\nNote that this does not support rusage, on Arnd's recommendation.\n\nSee the waitid(2) man page for details on the arguments.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-09-21 12:04:45 -0600 io_uring: add IORING_OP_WAITID support"
    },
    {
        "commit": "2e521a2064bf8b26cf178c0f7644a70ed1a512fa",
        "message": "Move struct wait_opts and waitid_info into kernel/exit.h, and include\nfunction declarations for the recently added helpers. Make them\nnon-static as well.\n\nThis is in preparation for adding a waitid operation through io_uring.\nWith the abtracted helpers, this is now possible.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-09-21 12:03:50 -0600 exit: add internal include file with helpers"
    },
    {
        "commit": "fc68fcda049108478ee4704d8a3ad3e05cc72fd0",
        "message": "This behaves like IORING_OP_READ, except:\n\n1) It only supports pollable files (eg pipes, sockets, etc). Note that\n   for sockets, you probably want to use recv/recvmsg with multishot\n   instead.\n\n2) It supports multishot mode, meaning it will repeatedly trigger a\n   read and fill a buffer when data is available. This allows similar\n   use to recv/recvmsg but on non-sockets, where a single request will\n   repeatedly post a CQE whenever data is read from it.\n\n3) Because of #2, it must be used with provided buffers. This is\n   uniformly true across any request type that supports multishot and\n   transfers data, with the reason being that it's obviously not\n   possible to pass in a single buffer for the data, as multiple reads\n   may very well trigger before an application has a chance to process\n   previous CQEs and the data passed from them.\n\nReviewed-by: Gabriel Krisman Bertazi <krisman@suse.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-09-21 12:02:30 -0600 io_uring/rw: add support for IORING_OP_READ_MULTISHOT"
    },
    {
        "commit": "d2d778fbf9964e4e5b8d7420eba8ec5ce938e794",
        "message": "This is cleaner than gating on the opcode type, particularly as more\nread/write type opcodes may be added.\n\nThen we can use that for the data import, and for __io_read() on\nwhether or not we need to copy state.\n\nReviewed-by: Gabriel Krisman Bertazi <krisman@suse.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-09-21 12:00:46 -0600 io_uring/rw: mark readv/writev as vectored in the opcode definition"
    },
    {
        "commit": "a08d195b586a217d76b42062f88f375a3eedda4d",
        "message": "Add __io_read() which does the grunt of the work, leaving the completion\nside to the new io_read(). No functional changes in this patch.\n\nReviewed-by: Gabriel Krisman Bertazi <krisman@suse.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.7-rc1",
        "release_date": "2023-09-21 12:00:41 -0600 io_uring/rw: split io_read() into a helper"
    },
    {
        "commit": "31d8fddb588bc7e3ba40bffa7573b7f7c7c73aa3",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single fix, fixing a regression with poll first, recvmsg, and\n  using a provided buffer\"\n\n* tag 'io_uring-6.6-2023-09-15' of git://git.kernel.dk/linux:\n  io_uring/net: fix iter retargeting for selected buf",
        "kernel_version": "v6.6-rc2",
        "release_date": "2023-09-15 13:55:29 -0700 Merge tag 'io_uring-6.6-2023-09-15' of git://git.kernel.dk/linux"
    },
    {
        "commit": "c21a8027ad8a68c340d0d58bf1cc61dcb0bc4d2f",
        "message": "When using selected buffer feature, io_uring delays data iter setup\nuntil later. If io_setup_async_msg() is called before that it might see\nnot correctly setup iterator. Pre-init nr_segs and judge from its state\nwhether we repointing.\n\nCc: stable@vger.kernel.org\nReported-by: syzbot+a4c6e5ef999b68b26ed1@syzkaller.appspotmail.com\nFixes: 0455d4ccec548 (\"io_uring: add POLL_FIRST support for send/sendmsg and recv/recvmsg\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0000000000002770be06053c7757@google.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc2",
        "release_date": "2023-09-14 10:12:55 -0600 io_uring/net: fix iter retargeting for selected buf"
    },
    {
        "commit": "7ccc3ebf0c575728bff2d3cb4719ccd84aa186ab",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A few fixes that should go into the 6.6-rc merge window:\n\n   - Fix for a regression this merge window caused by the SQPOLL\n     affinity patch, where we can race with SQPOLL thread shutdown and\n     cause an oops when trying to set affinity (Gabriel)\n\n   - Fix for a regression this merge window where fdinfo reading with\n     for a ring setup with IORING_SETUP_NO_SQARRAY will attempt to\n     deference the non-existing SQ ring array (me)\n\n   - Add the patch that allows more finegrained control over who can use\n     io_uring (Matteo)\n\n   - Locking fix for a regression added this merge window for IOPOLL\n     overflow (Pavel)\n\n   - IOPOLL fix for stable, breaking our loop if helper threads are\n     exiting (Pavel)\n\n  Also had a fix for unreaped iopoll requests from io-wq from Ming, but\n  we found an issue with that and hence it got reverted. Will get this\n  sorted for a future rc\"\n\n* tag 'io_uring-6.6-2023-09-08' of git://git.kernel.dk/linux:\n  Revert \"io_uring: fix IO hang in io_wq_put_and_exit from do_exit()\"\n  io_uring: fix unprotected iopoll overflow\n  io_uring: break out of iowq iopoll on teardown\n  io_uring: add a sysctl to disable io_uring system-wide\n  io_uring/fdinfo: only print ->sq_array[] if it's there\n  io_uring: fix IO hang in io_wq_put_and_exit from do_exit()\n  io_uring: Don't set affinity on a dying sqpoll thread",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-09-08 21:32:28 -0700 Merge tag 'io_uring-6.6-2023-09-08' of git://git.kernel.dk/linux"
    },
    {
        "commit": "023464fe33a53d7e3fa0a1967a2adcb17e5e40e3",
        "message": "This reverts commit b484a40dc1f16edb58e5430105a021e1916e6f27.\n\nThis commit cancels all requests with io-wq, not just the ones from the\noriginating task. This breaks use cases that have thread pools, or just\nmultiple tasks issuing requests on the same ring. The liburing\nregression test for this also shows that problem:\n\n$ test/thread-exit.t\ncqe->res=-125, Expected 512\n\nwhere an IO thread gets its request canceled rather than complete\nsuccessfully.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-09-07 09:41:49 -0600 Revert \"io_uring: fix IO hang in io_wq_put_and_exit from do_exit()\""
    },
    {
        "commit": "27122c079f5b4b4ecf1323b65700edc57e07bf6e",
        "message": "[   71.490669] WARNING: CPU: 3 PID: 17070 at io_uring/io_uring.c:769\nio_cqring_event_overflow+0x47b/0x6b0\n[   71.498381] Call Trace:\n[   71.498590]  <TASK>\n[   71.501858]  io_req_cqe_overflow+0x105/0x1e0\n[   71.502194]  __io_submit_flush_completions+0x9f9/0x1090\n[   71.503537]  io_submit_sqes+0xebd/0x1f00\n[   71.503879]  __do_sys_io_uring_enter+0x8c5/0x2380\n[   71.507360]  do_syscall_64+0x39/0x80\n\nWe decoupled CQ locking from ->task_complete but haven't fixed up places\nforcing locking for CQ overflows.\n\nFixes: ec26c225f06f5 (\"io_uring: merge iopoll and normal completion paths\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-09-07 09:02:29 -0600 io_uring: fix unprotected iopoll overflow"
    },
    {
        "commit": "45500dc4e01c167ee063f3dcc22f51ced5b2b1e9",
        "message": "io-wq will retry iopoll even when it failed with -EAGAIN. If that\nraces with task exit, which sets TIF_NOTIFY_SIGNAL for all its workers,\nsuch workers might potentially infinitely spin retrying iopoll again and\nagain and each time failing on some allocation / waiting / etc. Don't\nkeep spinning if io-wq is dying.\n\nFixes: 561fb04a6a225 (\"io_uring: replace workqueue usage with io-wq\")\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-09-07 09:02:27 -0600 io_uring: break out of iowq iopoll on teardown"
    },
    {
        "commit": "76d3ccecfa186af3120e206d62f03db1a94a535f",
        "message": "Introduce a new sysctl (io_uring_disabled) which can be either 0, 1, or\n2. When 0 (the default), all processes are allowed to create io_uring\ninstances, which is the current behavior.  When 1, io_uring creation is\ndisabled (io_uring_setup() will fail with -EPERM) for unprivileged\nprocesses not in the kernel.io_uring_group group.  When 2, calls to\nio_uring_setup() fail with -EPERM regardless of privilege.\n\nSigned-off-by: Matteo Rizzo <matteorizzo@google.com>\n[JEM: modified to add io_uring_group]\nSigned-off-by: Jeff Moyer <jmoyer@redhat.com>\nLink: https://lore.kernel.org/r/x49y1i42j1z.fsf@segfault.boston.devel.redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-09-05 08:34:07 -0600 io_uring: add a sysctl to disable io_uring system-wide"
    },
    {
        "commit": "32f5dea040ee6e3cc30ac52d23f1674fd5110d03",
        "message": "If a ring is setup with IORING_SETUP_NO_SQARRAY, then we don't have\nthe SQ array. Don't try to dump info from it through fdinfo if that\nis the case.\n\nReported-by: syzbot+216e2ea6e0bf4a0acdd7@syzkaller.appspotmail.com\nFixes: 2af89abda7d9 (\"io_uring: add option to remove SQ indirection\")\nReviewed-by: Gabriel Krisman Bertazi <krisman@suse.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-09-01 15:08:29 -0600 io_uring/fdinfo: only print ->sq_array[] if it's there"
    },
    {
        "commit": "5a26e45edb4690d58406178b5a9ea4c6dcf2c105",
        "message": "When doing io_uring benchmark on /dev/nullb0, it's easy to crash the\nkernel if poll requests timeout triggered, as reported by David. [1]\n\nBUG: kernel NULL pointer dereference, address: 0000000000000008\nWorkqueue: kblockd blk_mq_timeout_work\nRIP: 0010:null_timeout_rq+0x4e/0x91\nCall Trace:\n ? null_timeout_rq+0x4e/0x91\n blk_mq_handle_expired+0x31/0x4b\n bt_iter+0x68/0x84\n ? bt_tags_iter+0x81/0x81\n __sbitmap_for_each_set.constprop.0+0xb0/0xf2\n ? __blk_mq_complete_request_remote+0xf/0xf\n bt_for_each+0x46/0x64\n ? __blk_mq_complete_request_remote+0xf/0xf\n ? percpu_ref_get_many+0xc/0x2a\n blk_mq_queue_tag_busy_iter+0x14d/0x18e\n blk_mq_timeout_work+0x95/0x127\n process_one_work+0x185/0x263\n worker_thread+0x1b5/0x227\n\nThis is indeed a race problem between null_timeout_rq() and null_poll().\n\nnull_poll()\t\t\t\tnull_timeout_rq()\n  spin_lock(&nq->poll_lock)\n  list_splice_init(&nq->poll_list, &list)\n  spin_unlock(&nq->poll_lock)\n\n  while (!list_empty(&list))\n    req = list_first_entry()\n    list_del_init()\n    ...\n    blk_mq_add_to_batch()\n    // req->rq_next = NULL\n\t\t\t\t\tspin_lock(&nq->poll_lock)\n\n\t\t\t\t\t// rq->queuelist->next == NULL\n\t\t\t\t\tlist_del_init(&rq->queuelist)\n\n\t\t\t\t\tspin_unlock(&nq->poll_lock)\n\nFix these problems by setting requests state to MQ_RQ_COMPLETE under\nnq->poll_lock protection, in which null_timeout_rq() can safely detect\nthis race and early return.\n\nNote this patch just fix the kernel panic when request timeout happen.\n\n[1] https://lore.kernel.org/all/3893581.1691785261@warthog.procyon.org.uk/\n\nFixes: 0a593fbbc245 (\"null_blk: poll queue support\")\nReported-by: David Howells <dhowells@redhat.com>\nTested-by: David Howells <dhowells@redhat.com>\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nLink: https://lore.kernel.org/r/20230901120306.170520-2-chengming.zhou@linux.dev\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-09-01 08:18:25 -0600 null_blk: fix poll request timeout handling"
    },
    {
        "commit": "b484a40dc1f16edb58e5430105a021e1916e6f27",
        "message": "io_wq_put_and_exit() is called from do_exit(), but all FIXED_FILE requests\nin io_wq aren't canceled in io_uring_cancel_generic() called from do_exit().\nMeantime io_wq IO code path may share resource with normal iopoll code\npath.\n\nSo if any HIPRI request is submittd via io_wq, this request may not get resouce\nfor moving on, given iopoll isn't possible in io_wq_put_and_exit().\n\nThe issue can be triggered when terminating 't/io_uring -n4 /dev/nullb0'\nwith default null_blk parameters.\n\nFix it by always cancelling all requests in io_wq by adding helper of\nio_uring_cancel_wq(), and this way is reasonable because io_wq destroying\nfollows canceling requests immediately.\n\nCloses: https://lore.kernel.org/linux-block/3893581.1691785261@warthog.procyon.org.uk/\nReported-by: David Howells <dhowells@redhat.com>\nCc: Chengming Zhou <zhouchengming@bytedance.com>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20230901134916.2415386-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-09-01 07:54:06 -0600 io_uring: fix IO hang in io_wq_put_and_exit from do_exit()"
    },
    {
        "commit": "bd6fc5da4c51107e1e0cec4a3a07963d1dae2c84",
        "message": "Syzbot reported a null-ptr-deref of sqd->thread inside\nio_sqpoll_wq_cpu_affinity.  It turns out the sqd->thread can go away\nfrom under us during io_uring_register, in case the process gets a\nfatal signal during io_uring_register.\n\nIt is not particularly hard to hit the race, and while I am not sure\nthis is the exact case hit by syzbot, it solves it.  Finally, checking\n->thread is enough to close the race because we locked sqd while\n\"parking\" the thread, thus preventing it from going away.\n\nI reproduced it fairly consistently with a program that does:\n\nint main(void) {\n  ...\n  io_uring_queue_init(RING_LEN, &ring1, IORING_SETUP_SQPOLL);\n  while (1) {\n    io_uring_register_iowq_aff(ring, 1, &mask);\n  }\n}\n\nExecuted in a loop with timeout to trigger SIGTERM:\n  while true; do timeout 1 /a.out ; done\n\nThis will hit the following BUG() in very few attempts.\n\nBUG: kernel NULL pointer dereference, address: 00000000000007a8\nPGD 800000010e949067 P4D 800000010e949067 PUD 10e46e067 PMD 0\nOops: 0000 [#1] PREEMPT SMP PTI\nCPU: 0 PID: 15715 Comm: dead-sqpoll Not tainted 6.5.0-rc7-next-20230825-g193296236fa0-dirty #23\nHardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 0.0.0 02/06/2015\nRIP: 0010:io_sqpoll_wq_cpu_affinity+0x27/0x70\nCode: 90 90 90 0f 1f 44 00 00 55 53 48 8b 9f 98 03 00 00 48 85 db 74 4f\n48 89 df 48 89 f5 e8 e2 f8 ff ff 48 8b 43 38 48 85 c0 74 22 <48> 8b b8\na8 07 00 00 48 89 ee e8 ba b1 00 00 48 89 df 89 c5 e8 70\nRSP: 0018:ffffb04040ea7e70 EFLAGS: 00010282\nRAX: 0000000000000000 RBX: ffff93c010749e40 RCX: 0000000000000001\nRDX: 0000000000000000 RSI: ffffffffa7653331 RDI: 00000000ffffffff\nRBP: ffffb04040ea7eb8 R08: 0000000000000000 R09: c0000000ffffdfff\nR10: ffff93c01141b600 R11: ffffb04040ea7d18 R12: ffff93c00ea74840\nR13: 0000000000000011 R14: 0000000000000000 R15: ffff93c00ea74800\nFS:  00007fb7c276ab80(0000) GS:ffff93c36f200000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00000000000007a8 CR3: 0000000111634003 CR4: 0000000000370ef0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\nCall Trace:\n <TASK>\n ? __die_body+0x1a/0x60\n ? page_fault_oops+0x154/0x440\n ? do_user_addr_fault+0x174/0x7b0\n ? exc_page_fault+0x63/0x140\n ? asm_exc_page_fault+0x22/0x30\n ? io_sqpoll_wq_cpu_affinity+0x27/0x70\n __io_register_iowq_aff+0x2b/0x60\n __io_uring_register+0x614/0xa70\n __x64_sys_io_uring_register+0xaa/0x1a0\n do_syscall_64+0x3a/0x90\n entry_SYSCALL_64_after_hwframe+0x6e/0xd8\nRIP: 0033:0x7fb7c226fec9\nCode: 2e 00 b8 ca 00 00 00 0f 05 eb a5 66 0f 1f 44 00 00 48 89 f8 48 89\nf7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01\nf0 ff ff 73 01 c3 48 8b 0d 97 7f 2d 00 f7 d8 64 89 01 48\nRSP: 002b:00007ffe2c0674f8 EFLAGS: 00000246 ORIG_RAX: 00000000000001ab\nRAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007fb7c226fec9\nRDX: 00007ffe2c067530 RSI: 0000000000000011 RDI: 0000000000000003\nRBP: 00007ffe2c0675d0 R08: 00007ffe2c067550 R09: 00007ffe2c067550\nR10: 0000000000000001 R11: 0000000000000246 R12: 0000000000000000\nR13: 00007ffe2c067750 R14: 0000000000000000 R15: 0000000000000000\n </TASK>\nModules linked in:\nCR2: 00000000000007a8\n---[ end trace 0000000000000000 ]---\n\nReported-by: syzbot+c74fea926a78b8a91042@syzkaller.appspotmail.com\nFixes: ebdfefc09c6d (\"io_uring/sqpoll: fix io-wq affinity when IORING_SETUP_SQPOLL is used\")\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/87v8cybuo6.fsf@suse.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-30 09:53:44 -0600 io_uring: Don't set affinity on a dying sqpoll thread"
    },
    {
        "commit": "c1b7fcf3f6d94c2c3528bf77054bf174a5ef63d7",
        "message": "Pull io_uring updates from Jens Axboe:\n \"Fairly quiet round in terms of features, mostly just improvements all\n  over the map for existing code. In detail:\n\n   - Initial support for socket operations through io_uring. Latter half\n     of this will likely land with the 6.7 kernel, then allowing things\n     like get/setsockopt (Breno)\n\n   - Cleanup of the cancel code, and then adding support for canceling\n     requests with the opcode as the key (me)\n\n   - Improvements for the io-wq locking (me)\n\n   - Fix affinity setting for SQPOLL based io-wq (me)\n\n   - Remove the io_uring userspace code. These were added initially as\n     copies from liburing, but all of them have since bitrotted and are\n     way out of date at this point. Rather than attempt to keep them in\n     sync, just get rid of them. People will have liburing available\n     anyway for these examples. (Pavel)\n\n   - Series improving the CQ/SQ ring caching (Pavel)\n\n   - Misc fixes and cleanups (Pavel, Yue, me)\"\n\n* tag 'for-6.6/io_uring-2023-08-28' of git://git.kernel.dk/linux: (47 commits)\n  io_uring: move iopoll ctx fields around\n  io_uring: move multishot cqe cache in ctx\n  io_uring: separate task_work/waiting cache line\n  io_uring: banish non-hot data to end of io_ring_ctx\n  io_uring: move non aligned field to the end\n  io_uring: add option to remove SQ indirection\n  io_uring: compact SQ/CQ heads/tails\n  io_uring: force inline io_fill_cqe_req\n  io_uring: merge iopoll and normal completion paths\n  io_uring: reorder cqring_flush and wakeups\n  io_uring: optimise extra io_get_cqe null check\n  io_uring: refactor __io_get_cqe()\n  io_uring: simplify big_cqe handling\n  io_uring: cqe init hardening\n  io_uring: improve cqe !tracing hot path\n  io_uring/rsrc: Annotate struct io_mapped_ubuf with __counted_by\n  io_uring/sqpoll: fix io-wq affinity when IORING_SETUP_SQPOLL is used\n  io_uring: simplify io_run_task_work_sig return\n  io_uring/rsrc: keep one global dummy_ubuf\n  io_uring: never overflow io_aux_cqe\n  ...",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-29 20:11:33 -0700 Merge tag 'for-6.6/io_uring-2023-08-28' of git://git.kernel.dk/linux"
    },
    {
        "commit": "6016fc9162245c5b109305841f76cca59c20a273",
        "message": "Pull iomap updates from Darrick Wong:\n \"We've got some big changes for this release -- I'm very happy to be\n  landing willy's work to enable large folios for the page cache for\n  general read and write IOs when the fs can make contiguous space\n  allocations, and Ritesh's work to track sub-folio dirty state to\n  eliminate the write amplification problems inherent in using large\n  folios.\n\n  As a bonus, io_uring can now process write completions in the caller's\n  context instead of bouncing through a workqueue, which should reduce\n  io latency dramatically. IOWs, XFS should see a nice performance bump\n  for both IO paths.\n\n  Summary:\n\n   - Make large writes to the page cache fill sparse parts of the cache\n     with large folios, then use large memcpy calls for the large folio.\n\n   - Track the per-block dirty state of each large folio so that a\n     buffered write to a single byte on a large folio does not result in\n     a (potentially) multi-megabyte writeback IO.\n\n   - Allow some directio completions to be performed in the initiating\n     task's context instead of punting through a workqueue. This will\n     reduce latency for some io_uring requests\"\n\n* tag 'iomap-6.6-merge-3' of git://git.kernel.org/pub/scm/fs/xfs/xfs-linux: (26 commits)\n  iomap: support IOCB_DIO_CALLER_COMP\n  io_uring/rw: add write support for IOCB_DIO_CALLER_COMP\n  fs: add IOCB flags related to passing back dio completions\n  iomap: add IOMAP_DIO_INLINE_COMP\n  iomap: only set iocb->private for polled bio\n  iomap: treat a write through cache the same as FUA\n  iomap: use an unsigned type for IOMAP_DIO_* defines\n  iomap: cleanup up iomap_dio_bio_end_io()\n  iomap: Add per-block dirty state tracking to improve performance\n  iomap: Allocate ifs in ->write_begin() early\n  iomap: Refactor iomap_write_delalloc_punch() function out\n  iomap: Use iomap_punch_t typedef\n  iomap: Fix possible overflow condition in iomap_write_delalloc_scan\n  iomap: Add some uptodate state handling helpers for ifs state bitmap\n  iomap: Drop ifs argument from iomap_set_range_uptodate()\n  iomap: Rename iomap_page to iomap_folio_state and others\n  iomap: Copy larger chunks from userspace\n  iomap: Create large folios in the buffered write path\n  filemap: Allow __filemap_get_folio to allocate large folios\n  filemap: Add fgf_t typedef\n  ...",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-28 11:59:52 -0700 Merge tag 'iomap-6.6-merge-3' of git://git.kernel.org/pub/scm/fs/xfs/xfs-linux"
    },
    {
        "commit": "de16588a7737b12e63ec646d72b45befb2b1f8f7",
        "message": "Pull misc vfs updates from Christian Brauner:\n \"This contains the usual miscellaneous features, cleanups, and fixes\n  for vfs and individual filesystems.\n\n  Features:\n\n   - Block mode changes on symlinks and rectify our broken semantics\n\n   - Report file modifications via fsnotify() for splice\n\n   - Allow specifying an explicit timeout for the \"rootwait\" kernel\n     command line option. This allows to timeout and reboot instead of\n     always waiting indefinitely for the root device to show up\n\n   - Use synchronous fput for the close system call\n\n  Cleanups:\n\n   - Get rid of open-coded lockdep workarounds for async io submitters\n     and replace it all with a single consolidated helper\n\n   - Simplify epoll allocation helper\n\n   - Convert simple_write_begin and simple_write_end to use a folio\n\n   - Convert page_cache_pipe_buf_confirm() to use a folio\n\n   - Simplify __range_close to avoid pointless locking\n\n   - Disable per-cpu buffer head cache for isolated cpus\n\n   - Port ecryptfs to kmap_local_page() api\n\n   - Remove redundant initialization of pointer buf in pipe code\n\n   - Unexport the d_genocide() function which is only used within core\n     vfs\n\n   - Replace printk(KERN_ERR) and WARN_ON() with WARN()\n\n  Fixes:\n\n   - Fix various kernel-doc issues\n\n   - Fix refcount underflow for eventfds when used as EFD_SEMAPHORE\n\n   - Fix a mainly theoretical issue in devpts\n\n   - Check the return value of __getblk() in reiserfs\n\n   - Fix a racy assert in i_readcount_dec\n\n   - Fix integer conversion issues in various functions\n\n   - Fix LSM security context handling during automounts that prevented\n     NFS superblock sharing\"\n\n* tag 'v6.6-vfs.misc' of git://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs: (39 commits)\n  cachefiles: use kiocb_{start,end}_write() helpers\n  ovl: use kiocb_{start,end}_write() helpers\n  aio: use kiocb_{start,end}_write() helpers\n  io_uring: use kiocb_{start,end}_write() helpers\n  fs: create kiocb_{start,end}_write() helpers\n  fs: add kerneldoc to file_{start,end}_write() helpers\n  io_uring: rename kiocb_end_write() local helper\n  splice: Convert page_cache_pipe_buf_confirm() to use a folio\n  libfs: Convert simple_write_begin and simple_write_end to use a folio\n  fs/dcache: Replace printk and WARN_ON by WARN\n  fs/pipe: remove redundant initialization of pointer buf\n  fs: Fix kernel-doc warnings\n  devpts: Fix kernel-doc warnings\n  doc: idmappings: fix an error and rephrase a paragraph\n  init: Add support for rootwait timeout parameter\n  vfs: fix up the assert in i_readcount_dec\n  fs: Fix one kernel-doc comment\n  docs: filesystems: idmappings: clarify from where idmappings are taken\n  fs/buffer.c: disable per-CPU buffer_head cache for isolated CPUs\n  vfs, security: Fix automount superblock LSM init problem, preventing NFS sb sharing\n  ...",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-28 10:17:14 -0700 Merge tag 'v6.6-vfs.misc' of git://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs"
    },
    {
        "commit": "194505b55dd7899da114a4d47825204eefc0fff5",
        "message": "The commit referenced below opened up concurrent unaligned dio under\nshared locking for pure overwrites. In doing so, it enabled use of\nthe IOMAP_DIO_OVERWRITE_ONLY flag and added a warning on unexpected\n-EAGAIN returns as an extra precaution, since ext4 does not retry\nwrites in such cases. The flag itself is advisory in this case since\next4 checks for unaligned I/Os and uses appropriate locking up\nfront, rather than on a retry in response to -EAGAIN.\n\nAs it turns out, the warning check is susceptible to false positives\nbecause there are scenarios where -EAGAIN can be expected from lower\nlayers without necessarily having IOCB_NOWAIT set on the iocb. For\nexample, one instance of the warning has been seen where io_uring\nsets IOCB_HIPRI, which in turn results in REQ_POLLED|REQ_NOWAIT on\nthe bio. This results in -EAGAIN if the block layer is unable to\nallocate a request, etc. [Note that there is an outstanding patch to\nuntangle REQ_POLLED and REQ_NOWAIT such that the latter relies on\nIOCB_NOWAIT, which would also address this instance of the warning.]\n\nAnother instance of the warning has been reproduced by syzbot. A dio\nwrite is interrupted down in __get_user_pages_locked() waiting on\nthe mm lock and returns -EAGAIN up the stack. If the iomap dio\niteration layer has made no progress on the write to this point,\n-EAGAIN returns up to the filesystem and triggers the warning.\n\nThis use of the overwrite flag in ext4 is precautionary and\nhalf-baked. I.e., ext4 doesn't actually implement overwrite checking\nin the iomap callbacks when the flag is set, so the only extra\nverification it provides are i_size checks in the generic iomap dio\nlayer. Combined with the tendency for false positives, the added\nverification is not worth the extra trouble. Remove the flag,\nassociated warning, and update the comments to document when\nconcurrent unaligned dio writes are allowed and why said flag is not\nused.\n\nCc: stable@kernel.org\nReported-by: syzbot+5050ad0fb47527b1808a@syzkaller.appspotmail.com\nReported-by: Pengfei Xu <pengfei.xu@intel.com>\nFixes: 310ee0902b8d (\"ext4: allow concurrent unaligned dio overwrites\")\nSigned-off-by: Brian Foster <bfoster@redhat.com>\nReviewed-by: Jan Kara <jack@suse.cz>\nLink: https://lore.kernel.org/r/20230810165559.946222-1-bfoster@redhat.com\nSigned-off-by: Theodore Ts'o <tytso@mit.edu>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-27 11:27:12 -0400 ext4: drop dio overwrite only flag and associated warning"
    },
    {
        "commit": "644c4a7a721fb90356cdd42219c9928a3c386230",
        "message": "Move poll_multi_queue and iopoll_list to the submission cache line, it\ndoesn't make much sense to keep them separately, and is better place\nfor it in general.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5b03cf7e6652e350e6e70a917eec72ba9f33b97b.1692916914.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-24 17:16:20 -0600 io_uring: move iopoll ctx fields around"
    },
    {
        "commit": "0aa7aa5f766933d4f91b22d9658cd688e1f15dab",
        "message": "We cache multishot CQEs before flushing them to the CQ in\nsubmit_state.cqe. It's a 16 entry cache totalling 256 bytes in the\nmiddle of the io_submit_state structure. Move it out of there, it\nshould help with CPU caches for the submission state, and shouldn't\naffect cached CQEs.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/dbe1f39c043ee23da918836be44fcec252ce6711.1692916914.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-24 17:16:20 -0600 io_uring: move multishot cqe cache in ctx"
    },
    {
        "commit": "c9def23dde5238184777340ad811e4903f216a2d",
        "message": "task_work's are typically queued up from IRQ/softirq potentially by a\nrandom CPU like in case of networking. Batch ctx fields bouncing as this\ninto a separate cache line.\n\nWe also move ->cq_timeouts there because waiters have to read and check\nit. We can also conditionally hide ->cq_timeouts in the future from the\nCQ wait path as a not really useful rudiment.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b7f3fcb5b6b9cca0238778262c1fdb7ada6286b7.1692916914.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-24 17:16:20 -0600 io_uring: separate task_work/waiting cache line"
    },
    {
        "commit": "18df385f42f0b3310ed2e4a3e39264bf5e784692",
        "message": "Let's move all slow path, setup/init and so on fields to the end of\nio_ring_ctx, that makes ctx reorganisation later easier. That includes,\npage arrays used only on tear down, CQ overflow list, old provided\nbuffer caches and used by io-wq poll hashes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/fc471b63925a0bf90a34943c4d36163c523cfb43.1692916914.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-24 17:16:20 -0600 io_uring: banish non-hot data to end of io_ring_ctx"
    },
    {
        "commit": "d7f06fea5d6be78403d42c9637f67bc883870094",
        "message": "Move not cache aligned fields down in io_ring_ctx, should change\nanything, but makes further refactoring easier.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/518e95d7888e9d481b2c5968dcf3f23db9ea47a5.1692916914.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-24 17:16:19 -0600 io_uring: move non aligned field to the end"
    },
    {
        "commit": "2af89abda7d9c2aeb573677e2c498ddb09f8058a",
        "message": "Not many aware, but io_uring submission queue has two levels. The first\nlevel usually appears as sq_array and stores indexes into the actual SQ.\n\nTo my knowledge, no one has ever seriously used it, nor liburing exposes\nit to users. Add IORING_SETUP_NO_SQARRAY, when set we don't bother\ncreating and using the sq_array and SQ heads/tails will be pointing\ndirectly into the SQ. Improves memory footprint, in term of both\nallocations as well as cache usage, and also should make io_get_sqe()\nless branchy in the end.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0ffa3268a5ef61d326201ff43a233315c96312e0.1692916914.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-24 17:16:19 -0600 io_uring: add option to remove SQ indirection"
    },
    {
        "commit": "e5598d6ae62626d261b046a2f19347c38681ff51",
        "message": "Queues heads and tails cache line aligned. That makes sq, cq taking 4\nlines or 5 lines if we include the rest of struct io_rings (e.g.\nsq_flags is frequently accessed).\n\nSince modern io_uring is mostly single threaded, it doesn't make much\nsend to spread them as such, it wastes space and puts additional pressure\non caches. Put them all into a single line.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9c8deddf9a7ed32069235a530d1e117fb460bc4c.1692916914.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-24 17:16:19 -0600 io_uring: compact SQ/CQ heads/tails"
    },
    {
        "commit": "093a650b757210bc856ca7f5349fb5a4bb9d4bd6",
        "message": "There are only 2 callers of io_fill_cqe_req left, and one of them is\nextremely hot. Force inline the function.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ffce4fc5e3521966def848a4d930586dfe33ae11.1692916914.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-24 17:16:19 -0600 io_uring: force inline io_fill_cqe_req"
    },
    {
        "commit": "ec26c225f06f5993f8891fa6c79fab3c92981181",
        "message": "io_do_iopoll() and io_submit_flush_completions() are pretty similar,\nboth filling CQEs and then free a list of requests. Don't duplicate it\nand make iopoll use __io_submit_flush_completions(), which also helps\nwith inlining and other optimisations.\n\nFor that, we need to first find all completed iopoll requests and splice\nthem from the iopoll list and then pass it down. This adds one extra\nlist traversal, which should be fine as requests will stay hot in cache.\n\nCQ locking is already conditional, introduce ->lockless_cq and skip\nlocking for IOPOLL as it's protected by ->uring_lock.\n\nWe also add a wakeup optimisation for IOPOLL to __io_cq_unlock_post(),\nso it works just like io_cqring_ev_posted_iopoll().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3840473f5e8a960de35b77292026691880f6bdbc.1692916914.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-24 17:16:19 -0600 io_uring: merge iopoll and normal completion paths"
    },
    {
        "commit": "54927baf6c195fb512ac38b26a041ca44edb2e29",
        "message": "Unlike in the past, io_commit_cqring_flush() doesn't do anything that\nmay need io_cqring_wake() to be issued after, all requests it completes\nwill go via task_work. Do io_commit_cqring_flush() after\nio_cqring_wake() to clean up __io_cq_unlock_post().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ed32dcfeec47e6c97bd6b18c152ddce5b218403f.1692916914.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-24 17:16:19 -0600 io_uring: reorder cqring_flush and wakeups"
    },
    {
        "commit": "59fbc409e71649f558fb4578cdbfac67acb824dc",
        "message": "If the cached cqe check passes in io_get_cqe*() it already means that\nthe cqe we return is valid and non-zero, however the compiler is unable\nto optimise null checks like in io_fill_cqe_req().\n\nDo a bit of trickery, return success/fail boolean from io_get_cqe*()\nand store cqe in the cqe parameter. That makes it do the right thing,\nerasing the check together with the introduced indirection.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/322ea4d3377d3d4efd8ae90ab8ed28a99f518210.1692916914.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-24 17:16:19 -0600 io_uring: optimise extra io_get_cqe null check"
    },
    {
        "commit": "20d6b633870495fda1d92d283ebf890d80f68ecd",
        "message": "Make __io_get_cqe simpler by not grabbing the cqe from refilled cached,\nbut letting io_get_cqe() do it for us. That's cleaner and removes some\nduplication.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/74dc8fdf2657e438b2e05e1d478a3596924604e9.1692916914.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-24 17:16:19 -0600 io_uring: refactor __io_get_cqe()"
    },
    {
        "commit": "b24c5d752962fa0970cd7e3d74b1cd0e843358de",
        "message": "Don't keep big_cqe bits of req in a union with hash_node, find a\nseparate space for it. It's bit safer, but also if we keep it always\ninitialised, we can get rid of ugly REQ_F_CQE32_INIT handling.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/447aa1b2968978c99e655ba88db536e903df0fe9.1692916914.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-24 17:16:19 -0600 io_uring: simplify big_cqe handling"
    },
    {
        "commit": "31d3ba924fd86add6d14f9085fdd2f4ec0879631",
        "message": "io_kiocb::cqe stores the completion info which we'll memcpy to\nuserspace, and we rely on callbacks and other later steps to populate\nit with right values. We have never had problems with that, but it would\nstill be safer to zero it on allocation.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b16a3b64dde678686460d3c3792c3ba6d3d1bc7a.1692916914.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-24 17:16:19 -0600 io_uring: cqe init hardening"
    },
    {
        "commit": "a0727c738309a06ef5579c1742f8f0def63aa883",
        "message": "While looking at io_fill_cqe_req()'s asm I stumbled on our trace points\nturning into the chunk below:\n\ntrace_io_uring_complete(req->ctx, req, req->cqe.user_data,\n\t\t\treq->cqe.res, req->cqe.flags,\n\t\t\treq->extra1, req->extra2);\n\nio_uring/io_uring.c:898: \ttrace_io_uring_complete(req->ctx, req, req->cqe.user_data,\n\tmovq\t232(%rbx), %rdi\t# req_44(D)->big_cqe.extra2, _5\n\tmovq\t224(%rbx), %rdx\t# req_44(D)->big_cqe.extra1, _6\n\tmovl\t84(%rbx), %r9d\t# req_44(D)->cqe.D.81184.flags, _7\n\tmovl\t80(%rbx), %r8d\t# req_44(D)->cqe.res, _8\n\tmovq\t72(%rbx), %rcx\t# req_44(D)->cqe.user_data, _9\n\tmovq\t88(%rbx), %rsi\t# req_44(D)->ctx, _10\n./arch/x86/include/asm/jump_label.h:27: \tasm_volatile_goto(\"1:\"\n\t1:jmp .L1772 # objtool NOPs this \t#\n\t...\n\nIt does a jump_label for actual tracing, but those 6 moves will stay\nthere in the hottest io_uring path. As an optimisation, add a\ntrace_io_uring_complete_enabled() check, which is also uses jump_labels,\nit tricks the compiler into behaving. It removes the junk without\nchanging anything else int the hot path.\n\nNote: apparently, it's not only me noticing it, and people are also\nworking it around. We should remove the check when it's solved\ngenerically or rework tracing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/555d8312644b3776f4be7e23f9b92943875c4bc7.1692916914.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-24 17:16:19 -0600 io_uring: improve cqe !tracing hot path"
    },
    {
        "commit": "99a9e0b83ab9955e604397717b82267feb021df3",
        "message": "Patch series \"Remove _folio_dtor and _folio_order\", v2.\n\n\nThis patch (of 13):\n\nfolio_put() is the standard way to write this, and it's not appreciably\nslower.  This is an enabling patch for removing free_compound_page()\nentirely.\n\nLink: https://lkml.kernel.org/r/20230816151201.3655946-1-willy@infradead.org\nLink: https://lkml.kernel.org/r/20230816151201.3655946-2-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nCc: Sidhartha Kumar <sidhartha.kumar@oracle.com>\nCc: Yanteng Si <siyanteng@loongson.cn>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-21 14:28:42 -0700 io_uring: stop calling free_compound_page()"
    },
    {
        "commit": "e484fd73f4bdcb00c2188100c2d84e9f3f5c9f7d",
        "message": "Use helpers instead of the open coded dance to silence lockdep warnings.\n\nSuggested-by: Jan Kara <jack@suse.cz>\nSigned-off-by: Amir Goldstein <amir73il@gmail.com>\nReviewed-by: Jan Kara <jack@suse.cz>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nMessage-Id: <20230817141337.1025891-5-amir73il@gmail.com>\nSigned-off-by: Christian Brauner <brauner@kernel.org>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-21 17:27:26 +0200 io_uring: use kiocb_{start,end}_write() helpers"
    },
    {
        "commit": "ed0360bbab72b829437b67ebb2f9cfac19f59dfe",
        "message": "aio, io_uring, cachefiles and overlayfs, all open code an ugly variant\nof file_{start,end}_write() to silence lockdep warnings.\n\nCreate helpers for this lockdep dance so we can use the helpers in all\nthe callers.\n\nSuggested-by: Jan Kara <jack@suse.cz>\nSigned-off-by: Amir Goldstein <amir73il@gmail.com>\nReviewed-by: Jan Kara <jack@suse.cz>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nMessage-Id: <20230817141337.1025891-4-amir73il@gmail.com>\nSigned-off-by: Christian Brauner <brauner@kernel.org>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-21 17:27:26 +0200 fs: create kiocb_{start,end}_write() helpers"
    },
    {
        "commit": "a370167fe526123637965f60859a9f1f3e1a58b7",
        "message": "This helper does not take a kiocb as input and we want to create a\ncommon helper by that name that takes a kiocb as input.\n\nSigned-off-by: Amir Goldstein <amir73il@gmail.com>\nReviewed-by: Jan Kara <jack@suse.cz>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nMessage-Id: <20230817141337.1025891-2-amir73il@gmail.com>\nSigned-off-by: Christian Brauner <brauner@kernel.org>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-21 17:27:25 +0200 io_uring: rename kiocb_end_write() local helper"
    },
    {
        "commit": "04d9244c9420db33149608a566399176d57690f8",
        "message": "Prepare for the coming implementation by GCC and Clang of the __counted_by\nattribute. Flexible array members annotated with __counted_by can have\ntheir accesses bounds-checked at run-time checking via CONFIG_UBSAN_BOUNDS\n(for array indexing) and CONFIG_FORTIFY_SOURCE (for strcpy/memcpy-family\nfunctions).\n\nAs found with Coccinelle[1], add __counted_by for struct io_mapped_ubuf.\n\n[1] https://github.com/kees/kernel-tools/blob/trunk/coccinelle/examples/counted_by.cocci\n\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Pavel Begunkov <asml.silence@gmail.com>\nCc: io-uring@vger.kernel.org\nSigned-off-by: Kees Cook <keescook@chromium.org>\nReviewed-by: \"Gustavo A. R. Silva\" <gustavoars@kernel.org>\nLink: https://lore.kernel.org/r/20230817212146.never.853-kees@kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-17 19:14:47 -0600 io_uring/rsrc: Annotate struct io_mapped_ubuf with __counted_by"
    },
    {
        "commit": "ebdfefc09c6de7897962769bd3e63a2ff443ebf5",
        "message": "If we setup the ring with SQPOLL, then that polling thread has its\nown io-wq setup. This means that if the application uses\nIORING_REGISTER_IOWQ_AFF to set the io-wq affinity, we should not be\nsetting it for the invoking task, but rather the sqpoll task.\n\nAdd an sqpoll helper that parks the thread and updates the affinity,\nand use that one if we're using SQPOLL.\n\nFixes: fe76421d1da1 (\"io_uring: allow user configurable IO thread CPU affinity\")\nCc: stable@vger.kernel.org # 5.10+\nLink: https://github.com/axboe/liburing/discussions/884\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-16 13:40:28 -0600 io_uring/sqpoll: fix io-wq affinity when IORING_SETUP_SQPOLL is used"
    },
    {
        "commit": "2e40ed24e1696e47e94e804d09ef88ecb6617201",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A followup fix for the parisc/SHM_COLOUR fix, also from Helge, which\n  is heading to stable.\n\n  And then just the io_uring equivalent of the RESOLVE_CACHED fix in\n  commit a0fc452a5d7f from last week for build_open_flags()\"\n\n* tag 'io_uring-6.5-2023-08-11' of git://git.kernel.dk/linux:\n  io_uring/parisc: Adjust pgoff in io_uring mmap() for parisc\n  io_uring: correct check for O_TMPFILE",
        "kernel_version": "v6.5-rc6",
        "release_date": "2023-08-11 12:06:51 -0700 Merge tag 'io_uring-6.5-2023-08-11' of git://git.kernel.dk/linux"
    },
    {
        "commit": "d246c759c47eafe4688613e89b337e48c39c5968",
        "message": "Nobody cares about io_run_task_work_sig returning 1, we only check for\nnegative errors. Simplify by keeping to 0/-error returns.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3aec8a532c003d6e50739b969a82989402696170.1691757663.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-11 10:42:57 -0600 io_uring: simplify io_run_task_work_sig return"
    },
    {
        "commit": "19a63c4021702e389a559726b16fcbf07a8a05f9",
        "message": "We set empty registered buffers to dummy_ubuf as an optimisation.\nCurrently, we allocate the dummy entry for each ring, whenever we can\nsimply have one global instance.\n\nWe're casting out const on assignment, it's fine as we're not going to\nchange the content of the dummy, the constness gives us an extra layer\nof protection if sth ever goes wrong.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e4a96dda35ab755914bc43f6781bba0df97ac489.1691757663.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-11 10:42:57 -0600 io_uring/rsrc: keep one global dummy_ubuf"
    },
    {
        "commit": "b6b2bb58a75407660f638a68e6e34a07036146d0",
        "message": "Now all callers of io_aux_cqe() set allow_overflow to false, remove the\nparameter and not allow overflowing auxilary multishot cqes.\n\nWhen CQ is full the function callers and all multishot requests in\ngeneral are expected to complete the request. That prevents indefinite\nin-background grows of the overflow list and let's the userspace to\nhandle the backlog at its own pace.\n\nResubmitting a request should also be faster than accounting a bunch of\noverflows, so it should be better for perf when it happens, but a well\nbehaving userspace should be trying to avoid overflows in any case.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bb20d14d708ea174721e58bb53786b0521e4dd6d.1691757663.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-11 10:42:57 -0600 io_uring: never overflow io_aux_cqe"
    },
    {
        "commit": "056695bffa4beed5668dd4aa11efb696eacb3ed9",
        "message": "Nobody checks io_req_cqe_overflow()'s return, make it return void.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8f2029ad0c22f73451664172d834372608ee0a77.1691757663.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-11 10:42:57 -0600 io_uring: remove return from io_req_cqe_overflow()"
    },
    {
        "commit": "00b0db562485fbb259cd4054346208ad0885d662",
        "message": "io_fill_cqe_req() is only called from one place, open code it, and\nrename __io_fill_cqe_req().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f432ce75bb1c94cadf0bd2add4d6aa510bd1fb36.1691757663.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-11 10:42:57 -0600 io_uring: open code io_fill_cqe_req()"
    },
    {
        "commit": "b2e74db55dd93d6db22a813c9a775b5dbf87c560",
        "message": "Don't allow overflowing multishot recv CQEs, it might get out of\nhand, hurt performance, and in the worst case scenario OOM the task.\n\nCc: stable@vger.kernel.org\nFixes: b3fdea6ecb55c (\"io_uring: multishot recv\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0b295634e8f1b71aa764c984608c22d85f88f75c.1691757663.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-11 10:42:51 -0600 io_uring/net: don't overflow multishot recv"
    },
    {
        "commit": "1bfed23349716a7811645336a7ce42c4b8f250bc",
        "message": "Don't allow overflowing multishot accept CQEs, we want to limit\nthe grows of the overflow list.\n\nCc: stable@vger.kernel.org\nFixes: 4e86a2c980137 (\"io_uring: implement multishot mode for accept\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7d0d749649244873772623dd7747966f516fe6e2.1691757663.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-11 10:42:34 -0600 io_uring/net: don't overflow multishot accept"
    },
    {
        "commit": "22f7fb80e6d91980785d235dc939695d3a271c3b",
        "message": "All we really care about is finding a free worker. If said worker is\nalready running, it's either starting new work already or it's just\nfinishing up existing work. For the latter, we'll be finding this work\nitem next anyway, and for the former, if the worker does go to sleep,\nit'll create a new worker anyway as we have pending items.\n\nThis reduces try_to_wake_up() overhead considerably:\n\n23.16%    -10.46%  [kernel.kallsyms]      [k] try_to_wake_up\n\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-11 10:36:20 -0600 io_uring/io-wq: don't gate worker wake up success on wake_up_process()"
    },
    {
        "commit": "de36a15f9a3842be24ca220060b77925f2f5435b",
        "message": "When we check if we have work to run, we grab the acct lock, check,\ndrop it, and then return the result. If we do have work to run, then\nrunning the work will again grab acct->lock and get the work item.\n\nThis causes us to grab acct->lock more frequently than we need to.\nIf we have work to do, have io_acct_run_queue() return with the acct\nlock still acquired. io_worker_handle_work() is then always invoked\nwith the acct lock already held.\n\nIn a simple test cases that stats files (IORING_OP_STATX always hits\nio-wq), we see a nice reduction in locking overhead with this change:\n\n19.32%   -12.55%  [kernel.kallsyms]      [k] __cmpwait_case_32\n20.90%   -12.07%  [kernel.kallsyms]      [k] queued_spin_lock_slowpath\n\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-11 10:36:17 -0600 io_uring/io-wq: reduce frequency of acct->lock acquisitions"
    },
    {
        "commit": "78848b9b05623cfddb790d23b0dc38a275eb0763",
        "message": "The worker free list is RCU protected, and checks for workers going away\nwhen iterating it. There's no need to hold the wq->lock around the\nlookup.\n\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-11 10:36:12 -0600 io_uring/io-wq: don't grab wq->lock for worker activation"
    },
    {
        "commit": "89226307b109f828566f0e024ee97b722167927c",
        "message": "We never use io_move_task_work_from_local() before it's defined in the\nfile anyway, so kill the forward declaration.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-10 15:01:58 -0600 io_uring: remove unnecessary forward declaration"
    },
    {
        "commit": "17bc28374cd06b7d2d3f1e88470ef89f9cd3a497",
        "message": "No functional changes in this patch, just a prep patch for needing the\nrequest in io_file_put().\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-10 10:27:46 -0600 io_uring: have io_file_put() take an io_kiocb rather than the file"
    },
    {
        "commit": "9f69a259576ad46e6a13812b2c272bc9a89f8e03",
        "message": "No point in using io_file_put() here, as we need to check if it's a\nfixed file in the caller anyway.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-10 10:24:25 -0600 io_uring/splice: use fput() directly"
    },
    {
        "commit": "3aaf22b62a9270b90524cd257755b960461a7614",
        "message": "The caller holds a reference to the ring itself, so by definition\nthe ring cannot go away. There's no need to play games with tryget\nfor the reference, as we don't need an extra reference at all.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-10 10:24:19 -0600 io_uring/fdinfo: get rid of ref tryget"
    },
    {
        "commit": "fe9da61ffccad80ae79fadad836971acf0d465bd",
        "message": "Commit 16d7fd3cfa72 (\"zonefs: use iomap for synchronous direct writes\")\nchanges zonefs code from a self-built zone append BIO to using iomap for\nsynchronous direct writes. This change relies on iomap submit BIO\ncallback to change the write BIO built by iomap to a zone append BIO.\nHowever, this change overlooked the fact that a write BIO may be very\nlarge as it is split when issued. The change from a regular write to a\nzone append operation for the built BIO can result in a block layer\nwarning as zone append BIO are not allowed to be split.\n\nWARNING: CPU: 18 PID: 202210 at block/bio.c:1644 bio_split+0x288/0x350\nCall Trace:\n? __warn+0xc9/0x2b0\n? bio_split+0x288/0x350\n? report_bug+0x2e6/0x390\n? handle_bug+0x41/0x80\n? exc_invalid_op+0x13/0x40\n? asm_exc_invalid_op+0x16/0x20\n? bio_split+0x288/0x350\nbio_split_rw+0x4bc/0x810\n? __pfx_bio_split_rw+0x10/0x10\n? lockdep_unlock+0xf2/0x250\n__bio_split_to_limits+0x1d8/0x900\nblk_mq_submit_bio+0x1cf/0x18a0\n? __pfx_iov_iter_extract_pages+0x10/0x10\n? __pfx_blk_mq_submit_bio+0x10/0x10\n? find_held_lock+0x2d/0x110\n? lock_release+0x362/0x620\n? mark_held_locks+0x9e/0xe0\n__submit_bio+0x1ea/0x290\n? __pfx___submit_bio+0x10/0x10\n? seqcount_lockdep_reader_access.constprop.0+0x82/0x90\nsubmit_bio_noacct_nocheck+0x675/0xa20\n? __pfx_bio_iov_iter_get_pages+0x10/0x10\n? __pfx_submit_bio_noacct_nocheck+0x10/0x10\niomap_dio_bio_iter+0x624/0x1280\n__iomap_dio_rw+0xa22/0x18a0\n? lock_is_held_type+0xe3/0x140\n? __pfx___iomap_dio_rw+0x10/0x10\n? lock_release+0x362/0x620\n? zonefs_file_write_iter+0x74c/0xc80 [zonefs]\n? down_write+0x13d/0x1e0\niomap_dio_rw+0xe/0x40\nzonefs_file_write_iter+0x5ea/0xc80 [zonefs]\ndo_iter_readv_writev+0x18b/0x2c0\n? __pfx_do_iter_readv_writev+0x10/0x10\n? inode_security+0x54/0xf0\ndo_iter_write+0x13b/0x7c0\n? lock_is_held_type+0xe3/0x140\nvfs_writev+0x185/0x550\n? __pfx_vfs_writev+0x10/0x10\n? __handle_mm_fault+0x9bd/0x1c90\n? find_held_lock+0x2d/0x110\n? lock_release+0x362/0x620\n? find_held_lock+0x2d/0x110\n? lock_release+0x362/0x620\n? __up_read+0x1ea/0x720\n? do_pwritev+0x136/0x1f0\ndo_pwritev+0x136/0x1f0\n? __pfx_do_pwritev+0x10/0x10\n? syscall_enter_from_user_mode+0x22/0x90\n? lockdep_hardirqs_on+0x7d/0x100\ndo_syscall_64+0x58/0x80\n\nThis error depends on the hardware used, specifically on the max zone\nappend bytes and max_[hw_]sectors limits. Tests using AMD Epyc machines\nthat have low limits did not reveal this issue while runs on Intel Xeon\nmachines with larger limits trigger it.\n\nManually splitting the zone append BIO using bio_split_rw() can solve\nthis issue but also requires issuing the fragment BIOs synchronously\nwith submit_bio_wait(), to avoid potential reordering of the zone append\nBIO fragments, which would lead to data corruption. That is, this\nsolution is not better than using regular write BIOs which are subject\nto serialization using zone write locking at the IO scheduler level.\n\nGiven this, fix the issue by removing zone append support and using\nregular write BIOs for synchronous direct writes. This allows preseving\nthe use of iomap and having identical synchronous and asynchronous\nsequential file write path. Zone append support will be reintroduced\nlater through io_uring commands to ensure that the needed special\nhandling is done correctly.\n\nReported-by: Shin'ichiro Kawasaki <shinichiro.kawasaki@wdc.com>\nFixes: 16d7fd3cfa72 (\"zonefs: use iomap for synchronous direct writes\")\nCc: stable@vger.kernel.org\nSigned-off-by: Damien Le Moal <dlemoal@kernel.org>\nTested-by: Shin'ichiro Kawasaki <shinichiro.kawasaki@wdc.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>",
        "kernel_version": "v6.5-rc6",
        "release_date": "2023-08-10 12:59:47 +0900 zonefs: fix synchronous direct writes to sequential files"
    },
    {
        "commit": "2bc057692599a5b3dc93d75a3dff34f72576355d",
        "message": "Normally these two flags do go together, as the issuer of polled IO\ngenerally cannot wait for resources that will get freed as part of IO\ncompletion. This is because that very task is the one that will complete\nthe request and free those resources, hence that would introduce a\ndeadlock.\n\nBut it is possible to have someone else issue the polled IO, eg via\nio_uring if the request is punted to io-wq. For that case, it's fine to\nhave the task block on IO submission, as it is not the same task that\nwill be completing the IO.\n\nIt's completely up to the caller to ask for both polled and nowait IO\nseparately! If we don't allow polled IO where IOCB_NOWAIT isn't set in\nthe kiocb, then we can run into repeated -EAGAIN submissions and not\nmake any progress.\n\nReviewed-by: Bart Van Assche <bvanassche@acm.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc6",
        "release_date": "2023-08-09 16:04:07 -0600 block: don't make REQ_POLLED imply REQ_NOWAIT"
    },
    {
        "commit": "9e4bef2ba9e0c5fd0e0ae383f0c0fb0e338aafad",
        "message": "We return 0 for success, or -error when there's an error. Move the 'ret'\nvariable into the loop where we are actually using it, to make it\nclearer that we don't carry this variable forward for return outside of\nthe loop.\n\nWhile at it, also move the need_resched() break condition out of the\nwhile check itself, keeping it with the signal pending check.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-09 10:46:46 -0600 io_uring: cleanup 'ret' handling in io_iopoll_check()"
    },
    {
        "commit": "dc314886cb3d0e4ab2858003e8de2917f8a3ccbd",
        "message": "Don't keep spinning iopoll with a signal set. It'll eventually return\nback, e.g. by virtue of need_resched(), but it's not a nice user\nexperience.\n\nCc: stable@vger.kernel.org\nFixes: def596e9557c9 (\"io_uring: support for IO polling\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/eeba551e82cad12af30c3220125eb6cb244cc94c.1691594339.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-09 10:46:46 -0600 io_uring: break iopolling on signal"
    },
    {
        "commit": "17619322e56bce68290842889658ec5981f00a42",
        "message": "There are tons of io_uring tests and examples in liburing and on the\nInternet. If you're looking for a benchmark, io_uring-bench.c is just an\nacutely outdated version of fio/io_uring. And for basic condensed init\ntemplate for likes of selftests take a peek at io_uring_zerocopy_tx.c.\n\nKill tools/io_uring/, it's a burden keeping it here.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7c740701d3b475dcad8c92602a551044f72176b4.1691543666.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-09 10:46:46 -0600 io_uring: kill io_uring userspace examples"
    },
    {
        "commit": "569f5308e54352a12181cc0185f848024c5443e8",
        "message": "io_req_local_work_add() peeks into the work list, which can be executed\nin the meanwhile. It's completely fine without KASAN as we're in an RCU\nread section and it's SLAB_TYPESAFE_BY_RCU. With KASAN though it may\ntrigger a false positive warning because internal io_uring caches are\nsanitised.\n\nRemove sanitisation from the io_uring request cache for now.\n\nCc: stable@vger.kernel.org\nFixes: 8751d15426a31 (\"io_uring: reduce scheduling due to tw\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c6fbf7a82a341e66a0007c76eefd9d57f2d3ba51.1691541473.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-09 10:46:46 -0600 io_uring: fix false positive KASAN warnings"
    },
    {
        "commit": "cfdbaa3a291d6fd2cb4a1a70d74e63b4abc2f5ec",
        "message": "cq_extra is protected by ->completion_lock, which io_get_sqe() misses.\nThe bug is harmless as it doesn't happen in real life, requires invalid\nSQ index array and racing with submission, and only messes up the\nuserspace, i.e. stall requests execution but will be cleaned up on\nring destruction.\n\nFixes: 15641e427070f (\"io_uring: don't cache number of dropped SQEs\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/66096d54651b1a60534bb2023f2947f09f50ef73.1691538547.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-09 10:46:46 -0600 io_uring: fix drain stalls by invalid SQE"
    },
    {
        "commit": "d4b30eed51d79361c290dc25a1386f5611f4982a",
        "message": "Commit 36b9818a5a84 (\"io_uring/rsrc: don't offload node free\")\nremoved the implementation but leave declaration.\n\nSigned-off-by: Yue Haibing <yuehaibing@huawei.com>\nLink: https://lore.kernel.org/r/20230808151058.4572-1-yuehaibing@huawei.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-09 10:46:46 -0600 io_uring/rsrc: Remove unused declaration io_rsrc_put_tw()"
    },
    {
        "commit": "b97f96e22f051d59d07a527dbd7d90408b661ca8",
        "message": "When compiling the kernel with clang and having HARDENED_USERCOPY\nenabled, the liburing openat2.t test case fails during request setup:\n\nusercopy: Kernel memory overwrite attempt detected to SLUB object 'io_kiocb' (offset 24, size 24)!\n------------[ cut here ]------------\nkernel BUG at mm/usercopy.c:102!\ninvalid opcode: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC\nCPU: 3 PID: 413 Comm: openat2.t Tainted: G                 N 6.4.3-g6995e2de6891-dirty #19\nHardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.16.1-0-g3208b098f51a-prebuilt.qemu.org 04/01/2014\nRIP: 0010:usercopy_abort+0x84/0x90\nCode: ce 49 89 ce 48 c7 c3 68 48 98 82 48 0f 44 de 48 c7 c7 56 c6 94 82 4c 89 de 48 89 c1 41 52 41 56 53 e8 e0 51 c5 00 48 83 c4 18 <0f> 0b 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 44 00 00 55 41 57 41 56\nRSP: 0018:ffffc900016b3da0 EFLAGS: 00010296\nRAX: 0000000000000062 RBX: ffffffff82984868 RCX: 4e9b661ac6275b00\nRDX: ffff8881b90ec580 RSI: ffffffff82949a64 RDI: 00000000ffffffff\nRBP: 0000000000000018 R08: 0000000000000000 R09: 0000000000000000\nR10: ffffc900016b3c88 R11: ffffc900016b3c30 R12: 00007ffe549659e0\nR13: ffff888119014000 R14: 0000000000000018 R15: 0000000000000018\nFS:  00007f862e3ca680(0000) GS:ffff8881b90c0000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00005571483542a8 CR3: 0000000118c11000 CR4: 00000000003506e0\nCall Trace:\n <TASK>\n ? __die_body+0x63/0xb0\n ? die+0x9d/0xc0\n ? do_trap+0xa7/0x180\n ? usercopy_abort+0x84/0x90\n ? do_error_trap+0xc6/0x110\n ? usercopy_abort+0x84/0x90\n ? handle_invalid_op+0x2c/0x40\n ? usercopy_abort+0x84/0x90\n ? exc_invalid_op+0x2f/0x40\n ? asm_exc_invalid_op+0x16/0x20\n ? usercopy_abort+0x84/0x90\n __check_heap_object+0xe2/0x110\n __check_object_size+0x142/0x3d0\n io_openat2_prep+0x68/0x140\n io_submit_sqes+0x28a/0x680\n __se_sys_io_uring_enter+0x120/0x580\n do_syscall_64+0x3d/0x80\n entry_SYSCALL_64_after_hwframe+0x46/0xb0\nRIP: 0033:0x55714834de26\nCode: ca 01 0f b6 82 d0 00 00 00 8b ba cc 00 00 00 45 31 c0 31 d2 41 b9 08 00 00 00 83 e0 01 c1 e0 04 41 09 c2 b8 aa 01 00 00 0f 05 <c3> 66 0f 1f 84 00 00 00 00 00 89 30 eb 89 0f 1f 40 00 8b 00 a8 06\nRSP: 002b:00007ffe549659c8 EFLAGS: 00000246 ORIG_RAX: 00000000000001aa\nRAX: ffffffffffffffda RBX: 00007ffe54965a50 RCX: 000055714834de26\nRDX: 0000000000000000 RSI: 0000000000000001 RDI: 0000000000000003\nRBP: 0000000000000000 R08: 0000000000000000 R09: 0000000000000008\nR10: 0000000000000000 R11: 0000000000000246 R12: 000055714834f057\nR13: 00007ffe54965a50 R14: 0000000000000001 R15: 0000557148351dd8\n </TASK>\nModules linked in:\n---[ end trace 0000000000000000 ]---\n\nwhen it tries to copy struct open_how from userspace into the per-command\nspace in the io_kiocb. There's nothing wrong with the copy, but we're\nmissing the appropriate annotations for allowing user copies to/from the\nio_kiocb slab.\n\nAllow copies in the per-command area, which is from the 'file' pointer to\nwhen 'opcode' starts. We do have existing user copies there, but they are\nnot all annotated like the one that openat2_prep() uses,\ncopy_struct_from_user(). But in practice opcodes should be allowed to\ncopy data into their per-command area in the io_kiocb.\n\nReported-by: Breno Leitao <leitao@debian.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-09 10:46:44 -0600 io_uring: annotate the struct io_kiocb slab for appropriate user copy"
    },
    {
        "commit": "8e9fad0e70b7b62848e0aeb1a873903b9ce4d7c4",
        "message": "Enable io_uring commands on network sockets. Create two new\nSOCKET_URING_OP commands that will operate on sockets.\n\nIn order to call ioctl on sockets, use the file_operations->io_uring_cmd\ncallbacks, and map it to a uring socket function, which handles the\nSOCKET_URING_OP accordingly, and calls socket ioctls.\n\nThis patches was tested by creating a new test case in liburing.\nLink: https://github.com/leitao/liburing/tree/io_uring_cmd\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nAcked-by: Jakub Kicinski <kuba@kernel.org>\nLink: https://lore.kernel.org/r/20230627134424.2784797-1-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-09 10:46:15 -0600 io_uring: Add io_uring command support for sockets"
    },
    {
        "commit": "56675f8b9f9b15b024b8e3145fa289b004916ab7",
        "message": "The changes from commit 32832a407a71 (\"io_uring: Fix io_uring mmap() by\nusing architecture-provided get_unmapped_area()\") to the parisc\nimplementation of get_unmapped_area() broke glibc's locale-gen\nexecutable when running on parisc.\n\nThis patch reverts those architecture-specific changes, and instead\nadjusts in io_uring_mmu_get_unmapped_area() the pgoff offset which is\nthen given to parisc's get_unmapped_area() function.  This is much\ncleaner than the previous approach, and we still will get a coherent\naddresss.\n\nThis patch has no effect on other architectures (SHM_COLOUR is only\ndefined on parisc), and the liburing testcase stil passes on parisc.\n\nCc: stable@vger.kernel.org # 6.4\nSigned-off-by: Helge Deller <deller@gmx.de>\nReported-by: Christoph Biedl <linux-kernel.bfrz@manchmal.in-ulm.de>\nFixes: 32832a407a71 (\"io_uring: Fix io_uring mmap() by using architecture-provided get_unmapped_area()\")\nFixes: d808459b2e31 (\"io_uring: Adjust mapping wrt architecture aliasing requirements\")\nLink: https://lore.kernel.org/r/ZNEyGV0jyI8kOOfz@p100\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc6",
        "release_date": "2023-08-08 12:37:01 -0600 io_uring/parisc: Adjust pgoff in io_uring mmap() for parisc"
    },
    {
        "commit": "72dbde0f2afbe4af8e8595a89c650ae6b9d9c36f",
        "message": "O_TMPFILE is actually __O_TMPFILE|O_DIRECTORY. This means that the old\ncheck for whether RESOLVE_CACHED can be used would incorrectly think\nthat O_DIRECTORY could not be used with RESOLVE_CACHED.\n\nCc: stable@vger.kernel.org # v5.12+\nFixes: 3a81fd02045c (\"io_uring: enable LOOKUP_CACHED path resolution for filename lookups\")\nSigned-off-by: Aleksa Sarai <cyphar@cyphar.com>\nLink: https://lore.kernel.org/r/20230807-resolve_cached-o_tmpfile-v3-1-e49323e1ef6f@cyphar.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc6",
        "release_date": "2023-08-07 12:34:23 -0600 io_uring: correct check for O_TMPFILE"
    },
    {
        "commit": "377698d4abe2cd118dd866d5ef19e2f1aa6b9758",
        "message": "Improve iomap/xfs async dio write performance\n\niomap always punts async dio write completions to a workqueue, which has\na cost in terms of efficiency (now you need an unrelated worker to\nprocess it) and latency (now you're bouncing a completion through an\nasync worker, which is a classic slowdown scenario).\n\nio_uring handles IRQ completions via task_work, and for writes that\ndon't need to do extra IO at completion time, we can safely complete\nthem inline from that. This patchset adds IOCB_DIO_CALLER_COMP, which an\nIO issuer can set to inform the completion side that any extra work that\nneeds doing for that completion can be punted to a safe task context.\n\nThe iomap dio completion will happen in hard/soft irq context, and we\nneed a saner context to process these completions. IOCB_DIO_CALLER_COMP\nis added, which can be set in a struct kiocb->ki_flags by the issuer. If\nthe completion side of the iocb handling understands this flag, it can\nchoose to set a kiocb->dio_complete() handler and just call ki_complete\nfrom IRQ context. The issuer must then ensure that this callback is\nprocessed from a task. io_uring punts IRQ completions to task_work\nalready, so it's trivial wire it up to run more of the completion before\nposting a CQE. This is good for up to a 37% improvement in\nthroughput/latency for low queue depth IO, patch 5 has the details.\n\nIf we need to do real work at completion time, iomap will clear the\nIOMAP_DIO_CALLER_COMP flag.\n\nThis work came about when Andres tested low queue depth dio writes for\npostgres and compared it to doing sync dio writes, showing that the\nasync processing slows us down a lot.\n\n* tag 'xfs-async-dio.6-2023-08-01' of git://git.kernel.dk/linux:\n  iomap: support IOCB_DIO_CALLER_COMP\n  io_uring/rw: add write support for IOCB_DIO_CALLER_COMP\n  fs: add IOCB flags related to passing back dio completions\n  iomap: add IOMAP_DIO_INLINE_COMP\n  iomap: only set iocb->private for polled bio\n  iomap: treat a write through cache the same as FUA\n  iomap: use an unsigned type for IOMAP_DIO_* defines\n  iomap: cleanup up iomap_dio_bio_end_io()\n\nSigned-off-by: Darrick J. Wong <djwong@kernel.org>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-01 16:41:49 -0700 Merge tag 'xfs-async-dio.6-2023-08-01' of git://git.kernel.dk/linux into iomap-6.6-mergeA"
    },
    {
        "commit": "8c052fb3002e6e7eccdc138d2e459cd03f019ade",
        "message": "If IOCB_DIO_CALLER_COMP is set, utilize that to set kiocb->dio_complete\nhandler and data for that callback. Rather than punt the completion to a\nworkqueue, we pass back the handler and data to the issuer and will get\na callback from a safe task context.\n\nUsing the following fio job to randomly dio write 4k blocks at\nqueue depths of 1..16:\n\nfio --name=dio-write --filename=/data1/file --time_based=1 \\\n--runtime=10 --bs=4096 --rw=randwrite --norandommap --buffered=0 \\\n--cpus_allowed=4 --ioengine=io_uring --iodepth=$depth\n\nshows the following results before and after this patch:\n\n\tStock\tPatched\t\tDiff\n=======================================\nQD1\t155K\t162K\t\t+ 4.5%\nQD2\t290K\t313K\t\t+ 7.9%\nQD4\t533K\t597K\t\t+12.0%\nQD8\t604K\t827K\t\t+36.9%\nQD16\t615K\t845K\t\t+37.4%\n\nwhich shows nice wins all around. If we factored in per-IOP efficiency,\nthe wins look even nicer. This becomes apparent as queue depth rises,\nas the offloaded workqueue completions runs out of steam.\n\nReviewed-by: Darrick J. Wong <djwong@kernel.org>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Dave Chinner <dchinner@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-01 17:32:49 -0600 iomap: support IOCB_DIO_CALLER_COMP"
    },
    {
        "commit": "099ada2c87260e5c52fbdad4ca40c165fc194a2e",
        "message": "If the filesystem dio handler understands IOCB_DIO_CALLER_COMP, we'll\nget a kiocb->ki_complete() callback with kiocb->dio_complete set. In\nthat case, rather than complete the IO directly through task_work, queue\nup an intermediate task_work handler that first processes this callback\nand then immediately completes the request.\n\nFor XFS, this avoids a punt through a workqueue, which is a lot less\nefficient and adds latency to lower queue depth (or sync) O_DIRECT\nwrites.\n\nOnly do this for non-polled IO, as polled IO doesn't need this kind\nof deferral as it always completes within the task itself. This then\navoids a check for deferral in the polled IO completion handler.\n\nReviewed-by: Darrick J. Wong <djwong@kernel.org>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Dave Chinner <dchinner@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-08-01 17:32:45 -0600 io_uring/rw: add write support for IOCB_DIO_CALLER_COMP"
    },
    {
        "commit": "9c65505826395e1193495ad73087bcdaa4347813",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single tweak to a patch from last week, to avoid having idle\n  cqring waits be attributed as iowait\"\n\n* tag 'io_uring-6.5-2023-07-28' of git://git.kernel.dk/linux:\n  io_uring: gate iowait schedule on having pending requests",
        "kernel_version": "v6.5-rc4",
        "release_date": "2023-07-28 10:19:44 -0700 Merge tag 'io_uring-6.5-2023-07-28' of git://git.kernel.dk/linux"
    },
    {
        "commit": "53e7d08f6d6e214c40db1f51291bb2975c789dc2",
        "message": "In ublk_ctrl_start_dev(), if wait_for_completion_interruptible() is\ninterrupted by signal, queues aren't setup successfully yet, so we\nhave to fail UBLK_CMD_START_DEV, otherwise kernel oops can be triggered.\n\nReported by German when working on qemu-storage-deamon which requires\nsingle thread ublk daemon.\n\nFixes: 71f28f3136af (\"ublk_drv: add io_uring based userspace block driver\")\nReported-by: German Maglione <gmaglione@redhat.com>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20230726144502.566785-2-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc4",
        "release_date": "2023-07-27 07:17:36 -0600 ublk: fail to start device if queue setup is interrupted"
    },
    {
        "commit": "7b72d661f1f2f950ab8c12de7e2bc48bdac8ed69",
        "message": "A previous commit made all cqring waits marked as iowait, as a way to\nimprove performance for short schedules with pending IO. However, for\nuse cases that have a special reaper thread that does nothing but\nwait on events on the ring, this causes a cosmetic issue where we\nknow have one core marked as being \"busy\" with 100% iowait.\n\nWhile this isn't a grave issue, it is confusing to users. Rather than\nalways mark us as being in iowait, gate setting of current->in_iowait\nto 1 by whether or not the waiting task has pending requests.\n\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/io-uring/CAMEGJJ2RxopfNQ7GNLhr7X9=bHXKo+G5OOe0LUq=+UgLXsv1Xg@mail.gmail.com/\nLink: https://bugzilla.kernel.org/show_bug.cgi?id=217699\nLink: https://bugzilla.kernel.org/show_bug.cgi?id=217700\nReported-by: Oleksandr Natalenko <oleksandr@natalenko.name>\nReported-by: Phil Elwell <phil@raspberrypi.com>\nTested-by: Andres Freund <andres@anarazel.de>\nFixes: 8a796565cec3 (\"io_uring: Use io_schedule* in cqring wait\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc4",
        "release_date": "2023-07-24 11:44:35 -0600 io_uring: gate iowait schedule on having pending requests"
    },
    {
        "commit": "bdd1d82e7d02bd2764a68a5cc54533dfc2ba452a",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix for io-wq not always honoring REQ_F_NOWAIT, if it was set and\n   punted directly (eg via DRAIN) (me)\n\n - Capability check fix (Ondrej)\n\n - Regression fix for the mmap changes that went into 6.4, which\n   apparently broke IA64 (Helge)\n\n* tag 'io_uring-6.5-2023-07-21' of git://git.kernel.dk/linux:\n  ia64: mmap: Consider pgoff when searching for free mapping\n  io_uring: Fix io_uring mmap() by using architecture-provided get_unmapped_area()\n  io_uring: treat -EAGAIN for REQ_F_NOWAIT as final for io-wq\n  io_uring: don't audit the capability check in io_uring_create()",
        "kernel_version": "v6.5-rc3",
        "release_date": "2023-07-22 10:46:30 -0700 Merge tag 'io_uring-6.5-2023-07-21' of git://git.kernel.dk/linux"
    },
    {
        "commit": "32832a407a7178eec3215fad9b1a3298c14b0d69",
        "message": "The io_uring testcase is broken on IA-64 since commit d808459b2e31\n(\"io_uring: Adjust mapping wrt architecture aliasing requirements\").\n\nThe reason is, that this commit introduced an own architecture\nindependend get_unmapped_area() search algorithm which finds on IA-64 a\nmemory region which is outside of the regular memory region used for\nshared userspace mappings and which can't be used on that platform\ndue to aliasing.\n\nTo avoid similar problems on IA-64 and other platforms in the future,\nit's better to switch back to the architecture-provided\nget_unmapped_area() function and adjust the needed input parameters\nbefore the call. Beside fixing the issue, the function now becomes\neasier to understand and maintain.\n\nThis patch has been successfully tested with the io_uring testcase on\nphysical x86-64, ppc64le, IA-64 and PA-RISC machines. On PA-RISC the LTP\nmmmap testcases did not report any regressions.\n\nCc: stable@vger.kernel.org # 6.4\nSigned-off-by: Helge Deller <deller@gmx.de>\nReported-by: matoro <matoro_mailinglist_kernel@matoro.tk>\nFixes: d808459b2e31 (\"io_uring: Adjust mapping wrt architecture aliasing requirements\")\nLink: https://lore.kernel.org/r/20230721152432.196382-2-deller@gmx.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc3",
        "release_date": "2023-07-21 09:41:29 -0600 io_uring: Fix io_uring mmap() by using architecture-provided get_unmapped_area()"
    },
    {
        "commit": "a9be202269580ca611c6cebac90eaf1795497800",
        "message": "io-wq assumes that an issue is blocking, but it may not be if the\nrequest type has asked for a non-blocking attempt. If we get\n-EAGAIN for that case, then we need to treat it as a final result\nand not retry or arm poll for it.\n\nCc: stable@vger.kernel.org # 5.10+\nLink: https://github.com/axboe/liburing/issues/897\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc3",
        "release_date": "2023-07-20 13:16:53 -0600 io_uring: treat -EAGAIN for REQ_F_NOWAIT as final for io-wq"
    },
    {
        "commit": "6adc2272aaaf84f34b652cf77f770c6fcc4b8336",
        "message": "The check being unconditional may lead to unwanted denials reported by\nLSMs when a process has the capability granted by DAC, but denied by an\nLSM. In the case of SELinux such denials are a problem, since they can't\nbe effectively filtered out via the policy and when not silenced, they\nproduce noise that may hide a true problem or an attack.\n\nSince not having the capability merely means that the created io_uring\ncontext will be accounted against the current user's RLIMIT_MEMLOCK\nlimit, we can disable auditing of denials for this check by using\nns_capable_noaudit() instead of capable().\n\nFixes: 2b188cc1bb85 (\"Add io_uring IO interface\")\nLink: https://bugzilla.redhat.com/show_bug.cgi?id=2193317\nSigned-off-by: Ondrej Mosnacek <omosnace@redhat.com>\nReviewed-by: Jeff Moyer <jmoyer@redhat.com>\nLink: https://lore.kernel.org/r/20230718115607.65652-1-omosnace@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc3",
        "release_date": "2023-07-18 14:16:25 -0600 io_uring: don't audit the capability check in io_uring_create()"
    },
    {
        "commit": "f77569d22ad91dc25de294864fa5b24d37ddc149",
        "message": "Allow usage of IORING_ASYNC_CANCEL_OP through the sync cancelation\nAPI as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-07-17 10:05:48 -0600 io_uring/cancel: wire up IORING_ASYNC_CANCEL_OP for sync cancel"
    },
    {
        "commit": "d7b8b079a8f6bc007d06d9ee468659dae6053e13",
        "message": "Add IORING_ASYNC_CANCEL_OP flag for cancelation, which allows the\napplication to target cancelation based on the opcode of the original\nrequest.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-07-17 10:05:48 -0600 io_uring/cancel: support opcode based lookup and cancelation"
    },
    {
        "commit": "8165b566049b14152873011ea540eb22eae5111d",
        "message": "Add a flag to explicitly match on user_data in the request for\ncancelation purposes. This is the default behavior if none of the\nother match flags are set, but if we ALSO want to match on user_data,\nthen this flag can be set.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-07-17 10:05:48 -0600 io_uring/cancel: add IORING_ASYNC_CANCEL_USERDATA"
    },
    {
        "commit": "a30badf66de8516b5a5bca7a5d339f377ff983ea",
        "message": "Get rid of the request vs io_cancel_data checking and just use the\nexported helper for this.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-07-17 10:05:48 -0600 io_uring: use cancelation match helper for poll and timeout requests"
    },
    {
        "commit": "3a372b66923e4af966af2900da588e3b3de6fcd2",
        "message": "We always need to check/update the cancel sequence if\nIORING_ASYNC_CANCEL_ALL is set. Also kill the redundant check for\nIORING_ASYNC_CANCEL_ANY at the end, if we get here we know it's\nnot set as we would've matched it higher up.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-07-17 10:05:48 -0600 io_uring/cancel: fix sequence matching for IORING_ASYNC_CANCEL_ANY"
    },
    {
        "commit": "aa5cd116f3c25c05e4724d7b5e24dc9ed9020a12",
        "message": "We have different match code in a variety of spots. Start the cleanup of\nthis by abstracting out a helper that can be used to check if a given\nrequest matches the cancelation criteria outlined in io_cancel_data.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-07-17 10:05:48 -0600 io_uring/cancel: abstract out request match helper"
    },
    {
        "commit": "faa9c0ee3cab9c68b79183c9e0111ba967d9f402",
        "message": "In preparation for using a generic handler to match requests for\ncancelation purposes, ensure that ctx is set in io_cancel_data. The\ntimeout handlers don't check for this as it'll always match, but we'll\nneed it set going forward.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-07-17 10:05:48 -0600 io_uring/timeout: always set 'ctx' in io_cancel_data"
    },
    {
        "commit": "ad711c5d113f53d6f16096dd6ed9f4939a857149",
        "message": "This isn't strictly necessary for this callsite, as it uses it's\ninternal lookup for this cancelation purpose. But let's be consistent\nwith how it's used in general and set ctx as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.6-rc1",
        "release_date": "2023-07-17 10:05:48 -0600 io_uring/poll: always set 'ctx' in io_cancel_data"
    },
    {
        "commit": "ec17f16432058e1406c763a81acfc1394578bc8c",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single tweak for the wait logic in io_uring\"\n\n* tag 'io_uring-6.5-2023-07-14' of git://git.kernel.dk/linux:\n  io_uring: Use io_schedule* in cqring wait",
        "kernel_version": "v6.5-rc2",
        "release_date": "2023-07-14 19:46:54 -0700 Merge tag 'io_uring-6.5-2023-07-14' of git://git.kernel.dk/linux"
    },
    {
        "commit": "8a796565cec3601071cbbd27d6304e202019d014",
        "message": "I observed poor performance of io_uring compared to synchronous IO. That\nturns out to be caused by deeper CPU idle states entered with io_uring,\ndue to io_uring using plain schedule(), whereas synchronous IO uses\nio_schedule().\n\nThe losses due to this are substantial. On my cascade lake workstation,\nt/io_uring from the fio repository e.g. yields regressions between 20%\nand 40% with the following command:\n./t/io_uring -r 5 -X0 -d 1 -s 1 -c 1 -p 0 -S$use_sync -R 0 /mnt/t2/fio/write.0.0\n\nThis is repeatable with different filesystems, using raw block devices\nand using different block devices.\n\nUse io_schedule_prepare() / io_schedule_finish() in\nio_cqring_wait_schedule() to address the difference.\n\nAfter that using io_uring is on par or surpassing synchronous IO (using\nregistered files etc makes it reliably win, but arguably is a less fair\ncomparison).\n\nThere are other calls to schedule() in io_uring/, but none immediately\njump out to be similarly situated, so I did not touch them. Similarly,\nit's possible that mutex_lock_io() should be used, but it's not clear if\nthere are cases where that matters.\n\nCc: stable@vger.kernel.org # 5.10+\nCc: Pavel Begunkov <asml.silence@gmail.com>\nCc: io-uring@vger.kernel.org\nCc: linux-kernel@vger.kernel.org\nSigned-off-by: Andres Freund <andres@anarazel.de>\nLink: https://lore.kernel.org/r/20230707162007.194068-1-andres@anarazel.de\n[axboe: minor style fixup]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc2",
        "release_date": "2023-07-07 11:24:29 -0600 io_uring: Use io_schedule* in cqring wait"
    },
    {
        "commit": "4f52875366bfbd6ddc19c1045b603d853e0a889c",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"The fix for the msghdr->msg_inq assigned value being wrong, using -1\n  instead of -1U for the signed type.\n\n  Also a fix for ensuring when we're trying to run task_work on an\n  exiting task, that we wait for it. This is not really a correctness\n  thing as the work is being canceled, but it does help with ensuring\n  file descriptors are closed when the task has exited.\"\n\n* tag 'io_uring-6.5-2023-07-03' of git://git.kernel.dk/linux:\n  io_uring: flush offloaded and delayed task_work on exit\n  io_uring: remove io_fallback_tw() forward declaration\n  io_uring/net: use proper value for msg_inq",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-07-03 18:43:10 -0700 Merge tag 'io_uring-6.5-2023-07-03' of git://git.kernel.dk/linux"
    },
    {
        "commit": "3a8a670eeeaa40d87bd38a587438952741980c18",
        "message": "Pull networking changes from Jakub Kicinski:\n \"WiFi 7 and sendpage changes are the biggest pieces of work for this\n  release. The latter will definitely require fixes but I think that we\n  got it to a reasonable point.\n\n  Core:\n\n   - Rework the sendpage & splice implementations\n\n     Instead of feeding data into sockets page by page extend sendmsg\n     handlers to support taking a reference on the data, controlled by a\n     new flag called MSG_SPLICE_PAGES\n\n     Rework the handling of unexpected-end-of-file to invoke an\n     additional callback instead of trying to predict what the right\n     combination of MORE/NOTLAST flags is\n\n     Remove the MSG_SENDPAGE_NOTLAST flag completely\n\n   - Implement SCM_PIDFD, a new type of CMSG type analogous to\n     SCM_CREDENTIALS, but it contains pidfd instead of plain pid\n\n   - Enable socket busy polling with CONFIG_RT\n\n   - Improve reliability and efficiency of reporting for ref_tracker\n\n   - Auto-generate a user space C library for various Netlink families\n\n  Protocols:\n\n   - Allow TCP to shrink the advertised window when necessary, prevent\n     sk_rcvbuf auto-tuning from growing the window all the way up to\n     tcp_rmem[2]\n\n   - Use per-VMA locking for \"page-flipping\" TCP receive zerocopy\n\n   - Prepare TCP for device-to-device data transfers, by making sure\n     that payloads are always attached to skbs as page frags\n\n   - Make the backoff time for the first N TCP SYN retransmissions\n     linear. Exponential backoff is unnecessarily conservative\n\n   - Create a new MPTCP getsockopt to retrieve all info\n     (MPTCP_FULL_INFO)\n\n   - Avoid waking up applications using TLS sockets until we have a full\n     record\n\n   - Allow using kernel memory for protocol ioctl callbacks, paving the\n     way to issuing ioctls over io_uring\n\n   - Add nolocalbypass option to VxLAN, forcing packets to be fully\n     encapsulated even if they are destined for a local IP address\n\n   - Make TCPv4 use consistent hash in TIME_WAIT and SYN_RECV. Ensure\n     in-kernel ECMP implementation (e.g. Open vSwitch) select the same\n     link for all packets. Support L4 symmetric hashing in Open vSwitch\n\n   - PPPoE: make number of hash bits configurable\n\n   - Allow DNS to be overwritten by DHCPACK in the in-kernel DHCP client\n     (ipconfig)\n\n   - Add layer 2 miss indication and filtering, allowing higher layers\n     (e.g. ACL filters) to make forwarding decisions based on whether\n     packet matched forwarding state in lower devices (bridge)\n\n   - Support matching on Connectivity Fault Management (CFM) packets\n\n   - Hide the \"link becomes ready\" IPv6 messages by demoting their\n     printk level to debug\n\n   - HSR: don't enable promiscuous mode if device offloads the proto\n\n   - Support active scanning in IEEE 802.15.4\n\n   - Continue work on Multi-Link Operation for WiFi 7\n\n  BPF:\n\n   - Add precision propagation for subprogs and callbacks. This allows\n     maintaining verification efficiency when subprograms are used, or\n     in fact passing the verifier at all for complex programs,\n     especially those using open-coded iterators\n\n   - Improve BPF's {g,s}setsockopt() length handling. Previously BPF\n     assumed the length is always equal to the amount of written data.\n     But some protos allow passing a NULL buffer to discover what the\n     output buffer *should* be, without writing anything\n\n   - Accept dynptr memory as memory arguments passed to helpers\n\n   - Add routing table ID to bpf_fib_lookup BPF helper\n\n   - Support O_PATH FDs in BPF_OBJ_PIN and BPF_OBJ_GET commands\n\n   - Drop bpf_capable() check in BPF_MAP_FREEZE command (used to mark\n     maps as read-only)\n\n   - Show target_{obj,btf}_id in tracing link fdinfo\n\n   - Addition of several new kfuncs (most of the names are\n     self-explanatory):\n      - Add a set of new dynptr kfuncs: bpf_dynptr_adjust(),\n        bpf_dynptr_is_null(), bpf_dynptr_is_rdonly(), bpf_dynptr_size()\n        and bpf_dynptr_clone().\n      - bpf_task_under_cgroup()\n      - bpf_sock_destroy() - force closing sockets\n      - bpf_cpumask_first_and(), rework bpf_cpumask_any*() kfuncs\n\n  Netfilter:\n\n   - Relax set/map validation checks in nf_tables. Allow checking\n     presence of an entry in a map without using the value\n\n   - Increase ip_vs_conn_tab_bits range for 64BIT builds\n\n   - Allow updating size of a set\n\n   - Improve NAT tuple selection when connection is closing\n\n  Driver API:\n\n   - Integrate netdev with LED subsystem, to allow configuring HW\n     \"offloaded\" blinking of LEDs based on link state and activity\n     (i.e. packets coming in and out)\n\n   - Support configuring rate selection pins of SFP modules\n\n   - Factor Clause 73 auto-negotiation code out of the drivers, provide\n     common helper routines\n\n   - Add more fool-proof helpers for managing lifetime of MDIO devices\n     associated with the PCS layer\n\n   - Allow drivers to report advanced statistics related to Time Aware\n     scheduler offload (taprio)\n\n   - Allow opting out of VF statistics in link dump, to allow more VFs\n     to fit into the message\n\n   - Split devlink instance and devlink port operations\n\n  New hardware / drivers:\n\n   - Ethernet:\n      - Synopsys EMAC4 IP support (stmmac)\n      - Marvell 88E6361 8 port (5x1GE + 3x2.5GE) switches\n      - Marvell 88E6250 7 port switches\n      - Microchip LAN8650/1 Rev.B0 PHYs\n      - MediaTek MT7981/MT7988 built-in 1GE PHY driver\n\n   - WiFi:\n      - Realtek RTL8192FU, 2.4 GHz, b/g/n mode, 2T2R, 300 Mbps\n      - Realtek RTL8723DS (SDIO variant)\n      - Realtek RTL8851BE\n\n   - CAN:\n      - Fintek F81604\n\n  Drivers:\n\n   - Ethernet NICs:\n      - Intel (100G, ice):\n         - support dynamic interrupt allocation\n         - use meta data match instead of VF MAC addr on slow-path\n      - nVidia/Mellanox:\n         - extend link aggregation to handle 4, rather than just 2 ports\n         - spawn sub-functions without any features by default\n      - OcteonTX2:\n         - support HTB (Tx scheduling/QoS) offload\n         - make RSS hash generation configurable\n         - support selecting Rx queue using TC filters\n      - Wangxun (ngbe/txgbe):\n         - add basic Tx/Rx packet offloads\n         - add phylink support (SFP/PCS control)\n      - Freescale/NXP (enetc):\n         - report TAPRIO packet statistics\n      - Solarflare/AMD:\n         - support matching on IP ToS and UDP source port of outer\n           header\n         - VxLAN and GENEVE tunnel encapsulation over IPv4 or IPv6\n         - add devlink dev info support for EF10\n\n   - Virtual NICs:\n      - Microsoft vNIC:\n         - size the Rx indirection table based on requested\n           configuration\n         - support VLAN tagging\n      - Amazon vNIC:\n         - try to reuse Rx buffers if not fully consumed, useful for ARM\n           servers running with 16kB pages\n      - Google vNIC:\n         - support TCP segmentation of >64kB frames\n\n   - Ethernet embedded switches:\n      - Marvell (mv88e6xxx):\n         - enable USXGMII (88E6191X)\n      - Microchip:\n         - lan966x: add support for Egress Stage 0 ACL engine\n         - lan966x: support mapping packet priority to internal switch\n           priority (based on PCP or DSCP)\n\n   - Ethernet PHYs:\n      - Broadcom PHYs:\n         - support for Wake-on-LAN for BCM54210E/B50212E\n         - report LPI counter\n      - Microsemi PHYs: support RGMII delay configuration (VSC85xx)\n      - Micrel PHYs: receive timestamp in the frame (LAN8841)\n      - Realtek PHYs: support optional external PHY clock\n      - Altera TSE PCS: merge the driver into Lynx PCS which it is a\n        variant of\n\n   - CAN: Kvaser PCIEcan:\n      - support packet timestamping\n\n   - WiFi:\n      - Intel (iwlwifi):\n         - major update for new firmware and Multi-Link Operation (MLO)\n         - configuration rework to drop test devices and split the\n           different families\n         - support for segmented PNVM images and power tables\n         - new vendor entries for PPAG (platform antenna gain) feature\n      - Qualcomm 802.11ax (ath11k):\n         - Multiple Basic Service Set Identifier (MBSSID) and Enhanced\n           MBSSID Advertisement (EMA) support in AP mode\n         - support factory test mode\n      - RealTek (rtw89):\n         - add RSSI based antenna diversity\n         - support U-NII-4 channels on 5 GHz band\n      - RealTek (rtl8xxxu):\n         - AP mode support for 8188f\n         - support USB RX aggregation for the newer chips\"\n\n* tag 'net-next-6.5' of git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net-next: (1602 commits)\n  net: scm: introduce and use scm_recv_unix helper\n  af_unix: Skip SCM_PIDFD if scm->pid is NULL.\n  net: lan743x: Simplify comparison\n  netlink: Add __sock_i_ino() for __netlink_diag_dump().\n  net: dsa: avoid suspicious RCU usage for synced VLAN-aware MAC addresses\n  Revert \"af_unix: Call scm_recv() only after scm_set_cred().\"\n  phylink: ReST-ify the phylink_pcs_neg_mode() kdoc\n  libceph: Partially revert changes to support MSG_SPLICE_PAGES\n  net: phy: mscc: fix packet loss due to RGMII delays\n  net: mana: use vmalloc_array and vcalloc\n  net: enetc: use vmalloc_array and vcalloc\n  ionic: use vmalloc_array and vcalloc\n  pds_core: use vmalloc_array and vcalloc\n  gve: use vmalloc_array and vcalloc\n  octeon_ep: use vmalloc_array and vcalloc\n  net: usb: qmi_wwan: add u-blox 0x1312 composition\n  perf trace: fix MSG_SPLICE_PAGES build error\n  ipvlan: Fix return value of ipvlan_queue_xmit()\n  netfilter: nf_tables: fix underflow in chain reference counter\n  netfilter: nf_tables: unbind non-anonymous set if rule construction fails\n  ...",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-28 16:43:10 -0700 Merge tag 'net-next-6.5' of git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net-next"
    },
    {
        "commit": "9408d8a37e6cce8803681ab816383450a056c3a9",
        "message": "Drivers can poll requests directly, so use that. We just need to ensure\nthe driver's request was allocated from a polled hctx, so a special\ndriver flag is added to struct io_uring_cmd.\n\nThe allows unshared and multipath namespaces to use the same polling\ncallback, and multipath is guaranteed to get the same queue as the\ncommand was submitted on. Previously multipath polling might check a\ndifferent path and poll the wrong info.\n\nThe other bonus is we don't need a bio payload in order to poll,\nallowing commands like 'flush' and 'write zeroes' to be submitted on the\nsame high priority queue as read and write commands.\n\nFinally, using the request based polling skips the unnecessary bio\noverhead.\n\nSigned-off-by: Keith Busch <kbusch@kernel.org>\nReviewed-by: Sagi Grimberg <sagi@grimberg.me>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20230612190343.2087040-3-kbusch@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-28 16:09:41 -0600 nvme: improved uring polling"
    },
    {
        "commit": "dfbe5561ae9339516a3742a3fbd678609ad59fd0",
        "message": "io_uring offloads task_work for cancelation purposes when the task is\nexiting. This is conceptually fine, but we should be nicer and actually\nwait for that work to complete before returning.\n\nAdd an argument to io_fallback_tw() telling it to flush the deferred\nwork when it's all queued up, and have it flush a ctx behind whenever\nthe ctx changes.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-28 11:06:05 -0600 io_uring: flush offloaded and delayed task_work on exit"
    },
    {
        "commit": "10e1c0d59006c6492d380602aa0a6c4eb9441426",
        "message": "It's used just one function higher up, get rid of the declaration and\njust move it up a bit.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-27 16:07:24 -0600 io_uring: remove io_fallback_tw() forward declaration"
    },
    {
        "commit": "b65db9211ecb7f8383e02dcf71b8c1e9c7043a40",
        "message": "struct msghdr->msg_inq is a signed type, yet we attempt to store what\nis essentially an unsigned bitmask in there. We only really need to know\nif the field was stored or not, but let's use the proper type to avoid\nany misunderstandings on what is being attempted here.\n\nLink: https://lore.kernel.org/io-uring/CAHk-=wjKb24aSe6fE4zDH-eh8hr-FB9BbukObUVSMGOrsBHCRQ@mail.gmail.com/\nReported-by: Linus Torvalds <torvalds@linux-foundation.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-27 16:07:24 -0600 io_uring/net: use proper value for msg_inq"
    },
    {
        "commit": "99160af413b4ff1c3b4741e8a7583f8e7197f201",
        "message": "It is possible that the next available path we failover to, happens to\nbe frozen (for example if it is during connection establishment). If\nthe original I/O was set with NOWAIT, this cause the I/O to unnecessarily\nfail because the request queue cannot be entered, hence the I/O fails with\nEAGAIN.\n\nThe NOWAIT restriction that was originally set for the I/O is no longer\nrelevant or needed because this is the nvme requeue context. Hence we\nclear the REQ_NOWAIT flag when failing over I/O.\n\nThis fix a simple test case of nvme controller reset during I/O when the\nmultipath device that has only a single path and I/O fails with \"Resource\ntemporarily unavailable\" errno. Note that this reproduces with io_uring\nwhich by default sets IOCB_NOWAIT by default.\n\nSigned-off-by: Sagi Grimberg <sagi@grimberg.me>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Keith Busch <kbusch@kernel.org>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-27 08:16:26 -0700 nvme-mpath: fix I/O failure with EAGAIN when failing over I/O"
    },
    {
        "commit": "0aa69d53ac7c30f6184f88f2e310d808b32b35a5",
        "message": "Pull io_uring updates from Jens Axboe:\n \"Nothing major in this release, just a bunch of cleanups and some\n  optimizations around networking mostly.\n\n   - clean up file request flags handling (Christoph)\n\n   - clean up request freeing and CQ locking (Pavel)\n\n   - support for using pre-registering the io_uring fd at setup time\n     (Josh)\n\n   - Add support for user allocated ring memory, rather than having the\n     kernel allocate it. Mostly for packing rings into a huge page (me)\n\n   - avoid an unnecessary double retry on receive (me)\n\n   - maintain ordering for task_work, which also improves performance\n     (me)\n\n   - misc cleanups/fixes (Pavel, me)\"\n\n* tag 'for-6.5/io_uring-2023-06-23' of git://git.kernel.dk/linux: (39 commits)\n  io_uring: merge conditional unlock flush helpers\n  io_uring: make io_cq_unlock_post static\n  io_uring: inline __io_cq_unlock\n  io_uring: fix acquire/release annotations\n  io_uring: kill io_cq_unlock()\n  io_uring: remove IOU_F_TWQ_FORCE_NORMAL\n  io_uring: don't batch task put on reqs free\n  io_uring: move io_clean_op()\n  io_uring: inline io_dismantle_req()\n  io_uring: remove io_free_req_tw\n  io_uring: open code io_put_req_find_next\n  io_uring: add helpers to decode the fixed file file_ptr\n  io_uring: use io_file_from_index in io_msg_grab_file\n  io_uring: use io_file_from_index in __io_sync_cancel\n  io_uring: return REQ_F_ flags from io_file_get_flags\n  io_uring: remove io_req_ffs_set\n  io_uring: remove a confusing comment above io_file_get_flags\n  io_uring: remove the mode variable in io_file_get_flags\n  io_uring: remove __io_file_supports_nowait\n  io_uring: wait interruptibly for request completions on exit\n  ...",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-26 12:30:26 -0700 Merge tag 'for-6.5/io_uring-2023-06-23' of git://git.kernel.dk/linux"
    },
    {
        "commit": "c98c81a4ac37b651be7eb9d16f562fc4acc5f867",
        "message": "There is no reason not to use __io_cq_unlock_post_flush for intermediate\naux CQE flushing, all ->task_complete should apply there, i.e. if set it\nshould be the submitter task. Combine them, get rid of of\n__io_cq_unlock_post() and rename the left function.\n\nThis place was also taking a couple percents of CPU according to\nprofiles for max throughput net benchmarks due to multishot recv\nflooding it with completions.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bbed60734cbec2e833d9c7bdcf9741aada5d8aab.1687518903.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-23 08:19:40 -0600 io_uring: merge conditional unlock flush helpers"
    },
    {
        "commit": "0fdb9a196c6728b51e0e7a4f6fa292d9fd5793de",
        "message": "io_cq_unlock_post() is exclusively used in io_uring/io_uring.c, mark it\nstatic and don't expose to other files.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3dc8127dda4514e1dd24bb32035faac887c5fa37.1687518903.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-23 08:19:40 -0600 io_uring: make io_cq_unlock_post static"
    },
    {
        "commit": "ff12617728fa5c7fb5325e164503ca4e936b80bd",
        "message": "__io_cq_unlock is not very helpful, and users should be calling flush\nvariants anyway. Open code the function.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d875c4cfb69f38ccecb58a57111446c77a614caa.1687518903.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-23 08:19:40 -0600 io_uring: inline __io_cq_unlock"
    },
    {
        "commit": "55b6a69fed5df6f88ef0b2ace562b422162beb61",
        "message": "We do conditional locking, so __io_cq_lock() and friends not always\nactually grab/release the lock, so kill misleading annotations.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2a098f9144c24cab622f8bf90b39f44da5d0401e.1687518903.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-23 08:19:40 -0600 io_uring: fix acquire/release annotations"
    },
    {
        "commit": "f432b76bcc93f36edb3d371f7b8d7881261dd6e7",
        "message": "We're abusing ->completion_lock helpers. io_cq_unlock() neither\nlocking conditionally nor doing CQE flushing, which means that callers\nmust have some side reason of taking the lock and should do it directly.\n\nOpen code io_cq_unlock() into io_cqring_overflow_kill() and clean it up.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7dabb36856db2b562e78780480396c52c29b2bf4.1687518903.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-23 08:19:39 -0600 io_uring: kill io_cq_unlock()"
    },
    {
        "commit": "91c7884ac9a92ffbf78af7fc89603daf24f448a9",
        "message": "Extract a function for non-local task_work_add, and use it directly from\nio_move_task_work_from_local(). Now we don't use IOU_F_TWQ_FORCE_NORMAL\nand it can be killed.\n\nAs a small positive side effect we don't grab task->io_uring in\nio_req_normal_work_add anymore, which is not needed for\nio_req_local_work_add().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2e55571e8ff2927ae3cc12da606d204e2485525b.1687518903.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-23 08:19:39 -0600 io_uring: remove IOU_F_TWQ_FORCE_NORMAL"
    },
    {
        "commit": "2fdd6fb5ff958a0f6b403e3f3ffd645b60b2a2b2",
        "message": "We're trying to batch io_put_task() in io_free_batch_list(), but\nconsidering that the hot path is a simple inc, it's most cerainly and\nprobably faster to just do io_put_task() instead of task tracking.\n\nWe don't care about io_put_task_remote() as it's only for IOPOLL\nwhere polling/waiting is done by not the submitter task.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4a7ef7dce845fe2bd35507bf389d6bd2d5c1edf0.1687518903.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-23 08:19:39 -0600 io_uring: don't batch task put on reqs free"
    },
    {
        "commit": "5a754dea27fb91a418f7429e24479e4184dee2e3",
        "message": "Move io_clean_op() up in the source file and remove the forward\ndeclaration, as the function doesn't have tricky dependencies\nanymore.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1b7163b2ba7c3a8322d972c79c1b0a9301b3057e.1687518903.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-23 08:19:39 -0600 io_uring: move io_clean_op()"
    },
    {
        "commit": "3b7a612fd0dbd321e15a308b8ac1f8bbf81432bd",
        "message": "io_dismantle_req() is only used in __io_req_complete_post(), open code\nit there.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ba8f20cb2c914eefa2e7d120a104a198552050db.1687518903.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-23 08:19:39 -0600 io_uring: inline io_dismantle_req()"
    },
    {
        "commit": "6ec9afc7f4cba58ab740c59d4c964d9422e2ea82",
        "message": "Request completion is a very hot path in general, but there are 3 places\nthat can be doing it: io_free_batch_list(), io_req_complete_post() and\nio_free_req_tw().\n\nio_free_req_tw() is used rather marginally and we don't care about it.\nKilling it can help to clean up and optimise the left two, do that by\nreplacing it with io_req_task_complete().\n\nThere are two things to consider:\n1) io_free_req() is called when all refs are put, so we need to reinit\n   references. The easiest way to do that is to clear REQ_F_REFCOUNT.\n2) We also don't need a cqe from it, so silence it with REQ_F_CQE_SKIP.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/434a2be8f33d474ad888ce1c17fe5ea7bbcb2a55.1687518903.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-23 08:19:39 -0600 io_uring: remove io_free_req_tw"
    },
    {
        "commit": "247f97a5f19b642eba5f5c1cf95fc3169326b3fb",
        "message": "There is only one user of io_put_req_find_next() and it doesn't make\nmuch sense to have it. Open code the function.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/38b5c5e48e4adc8e6a0cd16fdd5c1531d7ff81a9.1687518903.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-23 08:19:39 -0600 io_uring: open code io_put_req_find_next"
    },
    {
        "commit": "c213de632f7ae293409051733d24c7c6afc15292",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A fix for a race condition with poll removal and linked timeouts, and\n  then a few followup fixes/tweaks for the msg_control patch from last\n  week.\n\n  Not super important, particularly the sparse fixup, as it was broken\n  before that recent commit. But let's get it sorted for real for this\n  release, rather than just have it broken a bit differently\"\n\n* tag 'io_uring-6.4-2023-06-21' of git://git.kernel.dk/linux:\n  io_uring/net: use the correct msghdr union member in io_sendmsg_copy_hdr\n  io_uring/net: disable partial retries for recvmsg with cmsg\n  io_uring/net: clear msg_controllen on partial sendmsg retry\n  io_uring/poll: serialize poll linked timer start with poll removal",
        "kernel_version": "v6.4",
        "release_date": "2023-06-22 17:32:34 -0700 Merge tag 'io_uring-6.4-2023-06-21' of git://git.kernel.dk/linux"
    },
    {
        "commit": "26fed83653d0154704cadb7afc418f315c7ac1f0",
        "message": "Rather than assign the user pointer to msghdr->msg_control, assign it\nto msghdr->msg_control_user to make sparse happy. They are in a union\nso the end result is the same, but let's avoid new sparse warnings and\nsquash this one.\n\nReported-by: kernel test robot <lkp@intel.com>\nCloses: https://lore.kernel.org/oe-kbuild-all/202306210654.mDMcyMuB-lkp@intel.com/\nFixes: cac9e4418f4c (\"io_uring/net: save msghdr->msg_control for retries\")\nReviewed-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4",
        "release_date": "2023-06-21 07:34:17 -0600 io_uring/net: use the correct msghdr union member in io_sendmsg_copy_hdr"
    },
    {
        "commit": "78d0d2063bab954d19a1696feae4c7706a626d48",
        "message": "We cannot sanely handle partial retries for recvmsg if we have cmsg\nattached. If we don't, then we'd just be overwriting the initial cmsg\nheader on retries. Alternatively we could increment and handle this\nappropriately, but it doesn't seem worth the complication.\n\nMove the MSG_WAITALL check into the non-multishot case while at it,\nsince MSG_WAITALL is explicitly disabled for multishot anyway.\n\nLink: https://lore.kernel.org/io-uring/0b0d4411-c8fd-4272-770b-e030af6919a0@kernel.dk/\nCc: stable@vger.kernel.org # 5.10+\nReported-by: Stefan Metzmacher <metze@samba.org>\nReviewed-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4",
        "release_date": "2023-06-21 07:34:07 -0600 io_uring/net: disable partial retries for recvmsg with cmsg"
    },
    {
        "commit": "b1dc492087db0f2e5a45f1072a743d04618dd6be",
        "message": "If we have cmsg attached AND we transferred partial data at least, clear\nmsg_controllen on retry so we don't attempt to send that again.\n\nCc: stable@vger.kernel.org # 5.10+\nFixes: cac9e4418f4c (\"io_uring/net: save msghdr->msg_control for retries\")\nReported-by: Stefan Metzmacher <metze@samba.org>\nReviewed-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4",
        "release_date": "2023-06-21 07:33:48 -0600 io_uring/net: clear msg_controllen on partial sendmsg retry"
    },
    {
        "commit": "4bfb0c9af832a182a54e549123a634e0070c8d4f",
        "message": "Remove all the open coded magic on slot->file_ptr by introducing two\nhelpers that return the file pointer and the flags instead.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20230620113235.920399-9-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-20 09:36:22 -0600 io_uring: add helpers to decode the fixed file file_ptr"
    },
    {
        "commit": "f432c8c8c12b84c5465b1ffddb6feb7d6b19c1ca",
        "message": "Use io_file_from_index instead of open coding it.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20230620113235.920399-8-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-20 09:36:22 -0600 io_uring: use io_file_from_index in io_msg_grab_file"
    },
    {
        "commit": "60a666f097a8d722a3907925d21e363add289c8c",
        "message": "Use io_file_from_index instead of open coding it.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20230620113235.920399-7-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-20 09:36:22 -0600 io_uring: use io_file_from_index in __io_sync_cancel"
    },
    {
        "commit": "8487f083c6ff6e02b2ec14f22ef2b0079a1b6425",
        "message": "Two of the three callers want them, so return the more usual format,\nand shift into the FFS_ form only for the fixed file table.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20230620113235.920399-6-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-20 09:36:22 -0600 io_uring: return REQ_F_ flags from io_file_get_flags"
    },
    {
        "commit": "3beed235d1a1d0a4ab093ab67ea6b2841e9d4fa2",
        "message": "Just checking the flag directly makes it a lot more obvious what is\ngoing on here.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20230620113235.920399-5-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-20 09:36:22 -0600 io_uring: remove io_req_ffs_set"
    },
    {
        "commit": "b57c7cd1c17616ae9db5614525ba703f384afd05",
        "message": "The SCM inflight mechanism has nothing to do with the fact that a file\nmight be a regular file or not and if it supports non-blocking\noperations.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20230620113235.920399-4-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-20 09:36:22 -0600 io_uring: remove a confusing comment above io_file_get_flags"
    },
    {
        "commit": "53cfd5cea7f36bac7f3d45de4fea77e0c8d57aee",
        "message": "The variable is only once now, so don't bother with it.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20230620113235.920399-3-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-20 09:36:22 -0600 io_uring: remove the mode variable in io_file_get_flags"
    },
    {
        "commit": "b9a6c9459a5aec7bfd9b763554d15148367f1806",
        "message": "Now that this only checks O_NONBLOCK and FMODE_NOWAIT, the helper is\ncomplete overkil\u013c, and the comments are confusing bordering to wrong.\nJust inline the check into the caller.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20230620113235.920399-2-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-20 09:36:21 -0600 io_uring: remove __io_file_supports_nowait"
    },
    {
        "commit": "ef7dfac51d8ed961b742218f526bd589f3900a59",
        "message": "We selectively grab the ctx->uring_lock for poll update/removal, but\nwe really should grab it from the start to fully synchronize with\nlinked timeouts. Normally this is indeed the case, but if requests\nare forced async by the application, we don't fully cover removal\nand timer disarm within the uring_lock.\n\nMake this simpler by having consistent locking state for poll removal.\n\nCc: stable@vger.kernel.org # 6.1+\nReported-by: Querijn Voet <querijnqyn@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4",
        "release_date": "2023-06-17 20:21:52 -0600 io_uring/poll: serialize poll linked timer start with poll removal"
    },
    {
        "commit": "3a12faba95f99d166a18588fa9b2b2df99eb1146",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A fix for sendmsg with CMSG, and the followup fix discussed for\n  avoiding touching task->worker_private after the worker has started\n  exiting\"\n\n* tag 'io_uring-6.4-2023-06-15' of git://git.kernel.dk/linux:\n  io_uring/io-wq: clear current->worker_private on exit\n  io_uring/net: save msghdr->msg_control for retries",
        "kernel_version": "v6.4-rc7",
        "release_date": "2023-06-16 12:27:20 -0700 Merge tag 'io_uring-6.4-2023-06-15' of git://git.kernel.dk/linux"
    },
    {
        "commit": "adeaa3f290ecf7f6a6a5c53219a4686cbdff5fbd",
        "message": "A recent fix stopped clearing PF_IO_WORKER from current->flags on exit,\nwhich meant that we can now call inc/dec running on the worker after it\nhas been removed if it ends up scheduling in/out as part of exit.\n\nIf this happens after an RCU grace period has passed, then the struct\npointed to by current->worker_private may have been freed, and we can\nnow be accessing memory that is freed.\n\nEnsure this doesn't happen by clearing the task worker_private field.\nBoth io_wq_worker_running() and io_wq_worker_sleeping() check this\nfield before going any further, and we don't need any accounting etc\ndone after this worker has exited.\n\nFixes: fd37b884003c (\"io_uring/io-wq: don't clear PF_IO_WORKER on exit\")\nReported-by: Zorro Lang <zlang@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc7",
        "release_date": "2023-06-14 12:54:55 -0600 io_uring/io-wq: clear current->worker_private on exit"
    },
    {
        "commit": "cac9e4418f4cbd548ccb065b3adcafe073f7f7d2",
        "message": "If the application sets ->msg_control and we have to later retry this\ncommand, or if it got queued with IOSQE_ASYNC to begin with, then we\nneed to retain the original msg_control value. This is due to the net\nstack overwriting this field with an in-kernel pointer, to copy it\nin. Hitting that path for the second time will now fail the copy from\nuser, as it's attempting to copy from a non-user address.\n\nCc: stable@vger.kernel.org # 5.10+\nLink: https://github.com/axboe/liburing/issues/880\nReported-and-tested-by: Marek Majkowski <marek@cloudflare.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc7",
        "release_date": "2023-06-13 19:26:42 -0600 io_uring/net: save msghdr->msg_control for retries"
    },
    {
        "commit": "38b57833de1d9716bf8134c6fefcc35d23d5b136",
        "message": "The f2fs uses generic_file_buffered_read(), which supports buffered async\nreads since commit 1a0a7853b901 (\"mm: support async buffered reads in\ngeneric_file_buffered_read()\").\n\nLet's enable it to match other file-systems. The read performance has been\ngreatly improved under io_uring:\n\n    167M/s -> 234M/s, Increase ratio by 40%\n\nTest w/:\n    ./fio --name=onessd --filename=/data/test/local/io_uring_test\n    --size=256M --rw=randread --bs=4k --direct=0 --overwrite=0\n    --numjobs=1 --iodepth=1 --time_based=0 --runtime=10\n    --ioengine=io_uring --registerfiles --fixedbufs\n    --gtod_reduce=1 --group_reporting --sqthread_poll=1\n\nSigned-off-by: Lu Hongfei <luhongfei@vivo.com>\nSigned-off-by: Yangtao Li <frank.li@vivo.com>\nReviewed-by: Chao Yu <chao@kernel.org>\nSigned-off-by: Jaegeuk Kim <jaegeuk@kernel.org>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-12 13:04:09 -0700 f2fs: flag as supporting buffered async reads"
    },
    {
        "commit": "fd37b884003c7e46a0337b6e9212326d3ee1f40d",
        "message": "A recent commit gated the core dumping task exit logic on current->flags\nremaining consistent in terms of PF_{IO,USER}_WORKER at task exit time.\nThis exposed a problem with the io-wq handling of that, which explicitly\nclears PF_IO_WORKER before calling do_exit().\n\nThe reasons for this manual clear of PF_IO_WORKER is historical, where\nio-wq used to potentially trigger a sleep on exit. As the io-wq thread\nis exiting, it should not participate any further accounting. But these\ndays we don't need to rely on current->flags anymore, so we can safely\nremove the PF_IO_WORKER clearing.\n\nReported-by: Zorro Lang <zlang@redhat.com>\nReported-by: Dave Chinner <david@fromorbit.com>\nLink: https://lore.kernel.org/all/ZIZSPyzReZkGBEFy@dread.disaster.area/\nFixes: f9010dbdce91 (\"fork, vhost: Use CLONE_THREAD to fix freezer/ps regression\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
        "kernel_version": "v6.4-rc7",
        "release_date": "2023-06-12 11:16:09 -0700 io_uring/io-wq: don't clear PF_IO_WORKER on exit"
    },
    {
        "commit": "4826c59453b3b4677d6bf72814e7ababdea86949",
        "message": "WHen the ring exits, cleanup is done and the final cancelation and\nwaiting on completions is done by io_ring_exit_work. That function is\ninvoked by kworker, which doesn't take any signals. Because of that, it\ndoesn't really matter if we wait for completions in TASK_INTERRUPTIBLE\nor TASK_UNINTERRUPTIBLE state. However, it does matter to the hung task\ndetection checker!\n\nNormally we expect cancelations and completions to happen rather\nquickly. Some test cases, however, will exit the ring and park the\nowning task stopped (eg via SIGSTOP). If the owning task needs to run\ntask_work to complete requests, then io_ring_exit_work won't make any\nprogress until the task is runnable again. Hence io_ring_exit_work can\ntrigger the hung task detection, which is particularly problematic if\npanic-on-hung-task is enabled.\n\nAs the ring exit doesn't take signals to begin with, have it wait\ninterruptibly rather than uninterruptibly. io_uring has a separate\nstuck-exit warning that triggers independently anyway, so we're not\nreally missing anything by making this switch.\n\nCc: stable@vger.kernel.org # 5.10+\nLink: https://lore.kernel.org/r/b0e4aaef-7088-56ce-244c-976edeac0e66@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-12 09:43:57 -0600 io_uring: wait interruptibly for request completions on exit"
    },
    {
        "commit": "89207c669bbf464c81e1561d8206f120a679aaf7",
        "message": "Similar to the COW selftests, also use io_uring fixed buffers to test if\nlong-term page pinning works as expected.\n\nLink: https://lkml.kernel.org/r/20230519102723.185721-4-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Jan Kara <jack@suse.cz>\nCc: Jason Gunthorpe <jgg@nvidia.com>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Shuah Khan <shuah@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-09 16:25:38 -0700 selftests/mm: gup_longterm: add liburing tests"
    },
    {
        "commit": "81b1e3f91d77564611ab10d2c61774cf6a46ec78",
        "message": "Patch series \"selftests/mm: new test for FOLL_LONGTERM on file mappings\".\n\nLet's add some selftests to make sure that:\n* R/O long-term pinning always works of file mappings\n* R/W long-term pinning always works in MAP_PRIVATE file mappings\n* R/W long-term pinning only works in MAP_SHARED mappings with special\n  filesystems (shmem, hugetlb) and fails with other filesystems (ext4, btrfs,\n  xfs).\n\nThe tests make use of the gup_test kernel module to trigger ordinary GUP\nand GUP-fast, and liburing (similar to our COW selftests).  Test with\nmemfd, memfd hugetlb, tmpfile() and mkstemp().  The latter usually gives\nus a \"real\" filesystem (ext4, btrfs, xfs) where long-term pinning is\nexpected to fail.\n\nNote that these selftests don't contain any actual reproducers for data\ncorruptions in case R/W long-term pinning on problematic filesystems\n\"would\" work.\n\nMaybe we can later come up with a racy !FOLL_LONGTERM reproducer that can\nreuse an existing interface to trigger short-term pinning (I'll look into\nthat next).\n\nOn current mm/mm-unstable:\n\t# ./gup_longterm\n\t# [INFO] detected hugetlb page size: 2048 KiB\n\t# [INFO] detected hugetlb page size: 1048576 KiB\n\tTAP version 13\n\t1..50\n\t# [RUN] R/W longterm GUP pin in MAP_SHARED file mapping ... with memfd\n\tok 1 Should have worked\n\t# [RUN] R/W longterm GUP pin in MAP_SHARED file mapping ... with tmpfile\n\tok 2 Should have worked\n\t# [RUN] R/W longterm GUP pin in MAP_SHARED file mapping ... with local tmpfile\n\tok 3 Should have failed\n\t# [RUN] R/W longterm GUP pin in MAP_SHARED file mapping ... with memfd hugetlb (2048 kB)\n\tok 4 Should have worked\n\t# [RUN] R/W longterm GUP pin in MAP_SHARED file mapping ... with memfd hugetlb (1048576 kB)\n\tok 5 Should have worked\n\t# [RUN] R/W longterm GUP-fast pin in MAP_SHARED file mapping ... with memfd\n\tok 6 Should have worked\n\t# [RUN] R/W longterm GUP-fast pin in MAP_SHARED file mapping ... with tmpfile\n\tok 7 Should have worked\n\t# [RUN] R/W longterm GUP-fast pin in MAP_SHARED file mapping ... with local tmpfile\n\tok 8 Should have failed\n\t# [RUN] R/W longterm GUP-fast pin in MAP_SHARED file mapping ... with memfd hugetlb (2048 kB)\n\tok 9 Should have worked\n\t# [RUN] R/W longterm GUP-fast pin in MAP_SHARED file mapping ... with memfd hugetlb (1048576 kB)\n\tok 10 Should have worked\n\t# [RUN] R/O longterm GUP pin in MAP_SHARED file mapping ... with memfd\n\tok 11 Should have worked\n\t# [RUN] R/O longterm GUP pin in MAP_SHARED file mapping ... with tmpfile\n\tok 12 Should have worked\n\t# [RUN] R/O longterm GUP pin in MAP_SHARED file mapping ... with local tmpfile\n\tok 13 Should have worked\n\t# [RUN] R/O longterm GUP pin in MAP_SHARED file mapping ... with memfd hugetlb (2048 kB)\n\tok 14 Should have worked\n\t# [RUN] R/O longterm GUP pin in MAP_SHARED file mapping ... with memfd hugetlb (1048576 kB)\n\tok 15 Should have worked\n\t# [RUN] R/O longterm GUP-fast pin in MAP_SHARED file mapping ... with memfd\n\tok 16 Should have worked\n\t# [RUN] R/O longterm GUP-fast pin in MAP_SHARED file mapping ... with tmpfile\n\tok 17 Should have worked\n\t# [RUN] R/O longterm GUP-fast pin in MAP_SHARED file mapping ... with local tmpfile\n\tok 18 Should have worked\n\t# [RUN] R/O longterm GUP-fast pin in MAP_SHARED file mapping ... with memfd hugetlb (2048 kB)\n\tok 19 Should have worked\n\t# [RUN] R/O longterm GUP-fast pin in MAP_SHARED file mapping ... with memfd hugetlb (1048576 kB)\n\tok 20 Should have worked\n\t# [RUN] R/W longterm GUP pin in MAP_PRIVATE file mapping ... with memfd\n\tok 21 Should have worked\n\t# [RUN] R/W longterm GUP pin in MAP_PRIVATE file mapping ... with tmpfile\n\tok 22 Should have worked\n\t# [RUN] R/W longterm GUP pin in MAP_PRIVATE file mapping ... with local tmpfile\n\tok 23 Should have worked\n\t# [RUN] R/W longterm GUP pin in MAP_PRIVATE file mapping ... with memfd hugetlb (2048 kB)\n\tok 24 Should have worked\n\t# [RUN] R/W longterm GUP pin in MAP_PRIVATE file mapping ... with memfd hugetlb (1048576 kB)\n\tok 25 Should have worked\n\t# [RUN] R/W longterm GUP-fast pin in MAP_PRIVATE file mapping ... with memfd\n\tok 26 Should have worked\n\t# [RUN] R/W longterm GUP-fast pin in MAP_PRIVATE file mapping ... with tmpfile\n\tok 27 Should have worked\n\t# [RUN] R/W longterm GUP-fast pin in MAP_PRIVATE file mapping ... with local tmpfile\n\tok 28 Should have worked\n\t# [RUN] R/W longterm GUP-fast pin in MAP_PRIVATE file mapping ... with memfd hugetlb (2048 kB)\n\tok 29 Should have worked\n\t# [RUN] R/W longterm GUP-fast pin in MAP_PRIVATE file mapping ... with memfd hugetlb (1048576 kB)\n\tok 30 Should have worked\n\t# [RUN] R/O longterm GUP pin in MAP_PRIVATE file mapping ... with memfd\n\tok 31 Should have worked\n\t# [RUN] R/O longterm GUP pin in MAP_PRIVATE file mapping ... with tmpfile\n\tok 32 Should have worked\n\t# [RUN] R/O longterm GUP pin in MAP_PRIVATE file mapping ... with local tmpfile\n\tok 33 Should have worked\n\t# [RUN] R/O longterm GUP pin in MAP_PRIVATE file mapping ... with memfd hugetlb (2048 kB)\n\tok 34 Should have worked\n\t# [RUN] R/O longterm GUP pin in MAP_PRIVATE file mapping ... with memfd hugetlb (1048576 kB)\n\tok 35 Should have worked\n\t# [RUN] R/O longterm GUP-fast pin in MAP_PRIVATE file mapping ... with memfd\n\tok 36 Should have worked\n\t# [RUN] R/O longterm GUP-fast pin in MAP_PRIVATE file mapping ... with tmpfile\n\tok 37 Should have worked\n\t# [RUN] R/O longterm GUP-fast pin in MAP_PRIVATE file mapping ... with local tmpfile\n\tok 38 Should have worked\n\t# [RUN] R/O longterm GUP-fast pin in MAP_PRIVATE file mapping ... with memfd hugetlb (2048 kB)\n\tok 39 Should have worked\n\t# [RUN] R/O longterm GUP-fast pin in MAP_PRIVATE file mapping ... with memfd hugetlb (1048576 kB)\n\tok 40 Should have worked\n\t# [RUN] io_uring fixed buffer with MAP_SHARED file mapping ... with memfd\n\tok 41 Should have worked\n\t# [RUN] io_uring fixed buffer with MAP_SHARED file mapping ... with tmpfile\n\tok 42 Should have worked\n\t# [RUN] io_uring fixed buffer with MAP_SHARED file mapping ... with local tmpfile\n\tok 43 Should have failed\n\t# [RUN] io_uring fixed buffer with MAP_SHARED file mapping ... with memfd hugetlb (2048 kB)\n\tok 44 Should have worked\n\t# [RUN] io_uring fixed buffer with MAP_SHARED file mapping ... with memfd hugetlb (1048576 kB)\n\tok 45 Should have worked\n\t# [RUN] io_uring fixed buffer with MAP_PRIVATE file mapping ... with memfd\n\tok 46 Should have worked\n\t# [RUN] io_uring fixed buffer with MAP_PRIVATE file mapping ... with tmpfile\n\tok 47 Should have worked\n\t# [RUN] io_uring fixed buffer with MAP_PRIVATE file mapping ... with local tmpfile\n\tok 48 Should have worked\n\t# [RUN] io_uring fixed buffer with MAP_PRIVATE file mapping ... with memfd hugetlb (2048 kB)\n\tok 49 Should have worked\n\t# [RUN] io_uring fixed buffer with MAP_PRIVATE file mapping ... with memfd hugetlb (1048576 kB)\n\tok 50 Should have worked\n\t# Totals: pass:50 fail:0 xfail:0 xpass:0 skip:0 error:0\n\n\nThis patch (of 3):\n\nLet's factor detection out into vm_util, to be reused by a new test.\n\nLink: https://lkml.kernel.org/r/20230519102723.185721-1-david@redhat.com\nLink: https://lkml.kernel.org/r/20230519102723.185721-2-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Jan Kara <jack@suse.cz>\nCc: Jason Gunthorpe <jgg@nvidia.com>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Shuah Khan <shuah@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-09 16:25:37 -0700 selftests/mm: factor out detection of hugetlb page sizes into vm_util"
    },
    {
        "commit": "34ed8d0dcd692378f1155fe27648f54f99adbfbf",
        "message": "Now that the GUP explicitly checks FOLL_LONGTERM pin_user_pages() for\nbroken file-backed mappings in \"mm/gup: disallow FOLL_LONGTERM GUP-nonfast\nwriting to file-backed mappings\", there is no need to explicitly check VMAs\nfor this condition, so simply remove this logic from io_uring altogether.\n\nLink: https://lkml.kernel.org/r/e4a4efbda9cd12df71e0ed81796dc630231a1ef2.1684350871.git.lstoakes@gmail.com\nSigned-off-by: Lorenzo Stoakes <lstoakes@gmail.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nReviewed-by: David Hildenbrand <david@redhat.com>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian K\u00f6nig <christian.koenig@amd.com>\nCc: Dennis Dalessandro <dennis.dalessandro@cornelisnetworks.com>\nCc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\nCc: Janosch Frank <frankja@linux.ibm.com>\nCc: Jarkko Sakkinen <jarkko@kernel.org>\nCc: Jason Gunthorpe <jgg@nvidia.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Sakari Ailus <sakari.ailus@linux.intel.com>\nCc: Sean Christopherson <seanjc@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-09 16:25:26 -0700 io_uring: rsrc: delegate VMA file-backed check to GUP"
    },
    {
        "commit": "54d020692b342f7bd02d7f5795fb5c401caecfcc",
        "message": "Patch series \"remove the vmas parameter from GUP APIs\", v6.\n\n(pin_/get)_user_pages[_remote]() each provide an optional output parameter\nfor an array of VMA objects associated with each page in the input range.\n\nThese provide the means for VMAs to be returned, as long as mm->mmap_lock\nis never released during the GUP operation (i.e.  the internal flag\nFOLL_UNLOCKABLE is not specified).\n\nIn addition, these VMAs can only be accessed with the mmap_lock held and\nbecome invalidated the moment it is released.\n\nThe vast majority of invocations do not use this functionality and of\nthose that do, all but one case retrieve a single VMA to perform checks\nupon.\n\nIt is not egregious in the single VMA cases to simply replace the\noperation with a vma_lookup().  In these cases we duplicate the (fast)\nlookup on a slow path already under the mmap_lock, abstracted to a new\nget_user_page_vma_remote() inline helper function which also performs\nerror checking and reference count maintenance.\n\nThe special case is io_uring, where io_pin_pages() specifically needs to\nassert that the VMAs underlying the range do not result in broken\nlong-term GUP file-backed mappings.\n\nAs GUP now internally asserts that FOLL_LONGTERM mappings are not\nfile-backed in a broken fashion (i.e.  requiring dirty tracking) - as\nimplemented in \"mm/gup: disallow FOLL_LONGTERM GUP-nonfast writing to\nfile-backed mappings\" - this logic is no longer required and so we can\nsimply remove it altogether from io_uring.\n\nEliminating the vmas parameter eliminates an entire class of danging\npointer errors that might have occured should the lock have been\nincorrectly released.\n\nIn addition, the API is simplified and now clearly expresses what it is\nintended for - applying the specified GUP flags and (if pinning) returning\npinned pages.\n\nThis change additionally opens the door to further potential improvements\nin GUP and the possible marrying of disparate code paths.\n\nI have run this series against gup_test with no issues.\n\nThanks to Matthew Wilcox for suggesting this refactoring!\n\n\nThis patch (of 6):\n\nNo invocation of get_user_pages() use the vmas parameter, so remove it.\n\nThe GUP API is confusing and caveated.  Recent changes have done much to\nimprove that, however there is more we can do.  Exporting vmas is a prime\ntarget as the caller has to be extremely careful to preclude their use\nafter the mmap_lock has expired or otherwise be left with dangling\npointers.\n\nRemoving the vmas parameter focuses the GUP functions upon their primary\npurpose - pinning (and outputting) pages as well as performing the actions\nimplied by the input flags.\n\nThis is part of a patch series aiming to remove the vmas parameter\naltogether.\n\nLink: https://lkml.kernel.org/r/cover.1684350871.git.lstoakes@gmail.com\nLink: https://lkml.kernel.org/r/589e0c64794668ffc799651e8d85e703262b1e9d.1684350871.git.lstoakes@gmail.com\nSigned-off-by: Lorenzo Stoakes <lstoakes@gmail.com>\nSuggested-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nAcked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\nAcked-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Jason Gunthorpe <jgg@nvidia.com>\nAcked-by: Christian K\u00f6nig <christian.koenig@amd.com> (for radeon parts)\nAcked-by: Jarkko Sakkinen <jarkko@kernel.org>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nAcked-by: Sean Christopherson <seanjc@google.com> (KVM)\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dennis Dalessandro <dennis.dalessandro@cornelisnetworks.com>\nCc: Janosch Frank <frankja@linux.ibm.com>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Sakari Ailus <sakari.ailus@linux.intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-09 16:25:25 -0700 mm/gup: remove unused vmas parameter from get_user_pages()"
    },
    {
        "commit": "003f242b0dc16b287e6d15833d1d7f4adfa346ff",
        "message": "Just use the ARRAY_SIZE directly, we don't use length for anything else\nin this function.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-07 15:00:07 -0600 io_uring: get rid of unnecessary 'length' variable"
    },
    {
        "commit": "d86eaed185e9c6052d1ee2ca538f1936ff255887",
        "message": "Everybody is passing in the request, so get rid of the io_ring_ctx and\nexplicit user_data pass-in. Both the ctx and user_data can be deduced\nfrom the request at hand.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-07 14:59:22 -0600 io_uring: cleanup io_aux_cqe() API"
    },
    {
        "commit": "899e373ef0b0d3f1c2712a849f019de4f46bfa02",
        "message": "Linux 6.4-rc4\n\n* tag 'v6.4-rc4': (606 commits)\n  Linux 6.4-rc4\n  cxl: Explicitly initialize resources when media is not ready\n  x86: re-introduce support for ERMS copies for user space accesses\n  NVMe: Add MAXIO 1602 to bogus nid list.\n  module: error out early on concurrent load of the same module file\n  x86/topology: Fix erroneous smp_num_siblings on Intel Hybrid platforms\n  cpufreq: amd-pstate: Update policy->cur in amd_pstate_adjust_perf()\n  io_uring: unlock sqd->lock before sq thread release CPU\n  MAINTAINERS: update arm64 Microchip entries\n  udplite: Fix NULL pointer dereference in __sk_mem_raise_allocated().\n  net: phy: mscc: enable VSC8501/2 RGMII RX clock\n  net: phy: mscc: remove unnecessary phydev locking\n  net: phy: mscc: add support for VSC8501\n  net: phy: mscc: add VSC8502 to MODULE_DEVICE_TABLE\n  net/handshake: Enable the SNI extension to work properly\n  net/handshake: Unpin sock->file if a handshake is cancelled\n  net/handshake: handshake_genl_notify() shouldn't ignore @flags\n  net/handshake: Fix uninitialized local variable\n  net/handshake: Fix handshake_dup() ref counting\n  net/handshake: Remove unneeded check from handshake_dup()\n  ...",
        "kernel_version": "v6.4-rc7",
        "release_date": "2023-06-04 13:50:38 +0100 Merge tag 'v6.4-rc4' into v4l_for_linus"
    },
    {
        "commit": "26d147799001edfe479c0b015ba1cb038def5ae7",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single revert in here, removing the warning on the epoll ctl\n  opcode.\n\n  We originally deprecated this a few releases ago, but I've since had\n  two people report that it's being used. Which isn't the biggest deal,\n  obviously this is why we out in the deprecation notice in the first\n  place, but it also means that we should just kill this warning again\n  and abandon the deprecation plans.\n\n  Since it's only a few handfuls of code to support epoll ctl, not worth\n  going any further with this imho\"\n\n* tag 'io_uring-6.4-2023-06-02' of git://git.kernel.dk/linux:\n  io_uring: undeprecate epoll_ctl support",
        "kernel_version": "v6.4-rc5",
        "release_date": "2023-06-02 13:08:27 -0400 Merge tag 'io_uring-6.4-2023-06-02' of git://git.kernel.dk/linux"
    },
    {
        "commit": "c92fcfc2bab54451c4f1481755ea244f413455cb",
        "message": "We use task_work for a variety of reasons, but doing completions or\ntriggering rety after poll are by far the hottest two. Use the indirect\nfuntion call wrappers to avoid the indirect function call if\nCONFIG_RETPOLINE is set.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-06-02 08:55:37 -0600 io_uring: avoid indirect function calls for the hottest task_work"
    },
    {
        "commit": "4ea0bf4b98d66a7a790abb285539f395596bae92",
        "message": "Libuv recently started using it so there is at least one consumer now.\n\nCc: stable@vger.kernel.org\nFixes: 61a2732af4b0 (\"io_uring: deprecate epoll_ctl support\")\nLink: https://github.com/libuv/libuv/pull/3979\nSigned-off-by: Ben Noordhuis <info@bnoordhuis.nl>\nLink: https://lore.kernel.org/r/20230506095502.13401-1-info@bnoordhuis.nl\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc5",
        "release_date": "2023-05-26 20:22:41 -0600 io_uring: undeprecate epoll_ctl support"
    },
    {
        "commit": "6fae9129b1c70bd6b7677b808c03bc96e83460fc",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single fix for the conditional schedule with the SQPOLL thread,\n  dropping the uring_lock if we do need to reschedule\"\n\n* tag 'io_uring-6.4-2023-05-26' of git://git.kernel.dk/linux:\n  io_uring: unlock sqd->lock before sq thread release CPU",
        "kernel_version": "v6.4-rc4",
        "release_date": "2023-05-26 15:00:04 -0700 Merge tag 'io_uring-6.4-2023-05-26' of git://git.kernel.dk/linux"
    },
    {
        "commit": "533ab73f5b5c95dcb4152b52d5482abcc824c690",
        "message": "The sq thread actively releases CPU resources by calling the\ncond_resched() and schedule() interfaces when it is idle. Therefore,\nmore resources are available for other threads to run.\n\nThere exists a problem in sq thread: it does not unlock sqd->lock before\nreleasing CPU resources every time. This makes other threads pending on\nsqd->lock for a long time. For example, the following interfaces all\nrequire sqd->lock: io_sq_offload_create(), io_register_iowq_max_workers()\nand io_ring_exit_work().\n\nBefore the sq thread releases CPU resources, unlocking sqd->lock will\nprovide the user a better experience because it can respond quickly to\nuser requests.\n\nSigned-off-by: Kanchan Joshi<joshi.k@samsung.com>\nSigned-off-by: Wenwen Chen<wenwen.chen@samsung.com>\nLink: https://lore.kernel.org/r/20230525082626.577862-1-wenwen.chen@samsung.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc4",
        "release_date": "2023-05-25 09:30:13 -0600 io_uring: unlock sqd->lock before sq thread release CPU"
    },
    {
        "commit": "f026be0e1e881e3395c3d5418ffc8c2a2203c3f3",
        "message": "Use IOU_F_TWQ_LAZY_WAKE via iou_cmd_exec_in_task_lazy() for passthrough\ncommands completion. It further delays the execution of task_work for\nDEFER_TASKRUN until there are enough of task_work items queued to meet\nthe waiting criteria, which reduces the number of wake ups we issue.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ecdfacd0967a22d88b7779e2efd09e040825d0f8.1684154817.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-25 08:54:06 -0600 nvme: optimise io_uring passthrough completion"
    },
    {
        "commit": "5f3139fc46993b2d653a7aa5cdfe66a91881fd06",
        "message": "We want to use IOU_F_TWQ_LAZY_WAKE in commands. First, introduce a new\ncmd tw helper accepting TWQ flags, and then add\nio_uring_cmd_do_in_task_laz() that will pass IOU_F_TWQ_LAZY_WAKE and\nimply the \"lazy\" semantics, i.e. it posts no more than 1 CQE and\ndelaying execution of this tw should not prevent forward progress.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5b9f6716006df7e817f18bd555aee2f8f9c8b0c3.1684154817.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-25 08:54:06 -0600 io_uring/cmd: add cmd lazy tw wake helper"
    },
    {
        "commit": "8b60e2189fcd8b10b592608256eb97aebfcff147",
        "message": "Niklas Cassel <nks@flawful.org> says:\n\nThis series adds support for Command Duration Limits.\nThe series is based on linux tag: v6.4-rc1\nThe series can also be found in git: https://github.com/floatious/linux/commits/cdl-v7\n\n=================\nCDL in ATA / SCSI\n=================\nCommand Duration Limits is defined in:\nT13 ATA Command Set - 5 (ACS-5) and\nT10 SCSI Primary Commands - 6 (SPC-6) respectively\n(a simpler version of CDL is defined in T10 SPC-5).\n\nCDL defines Duration Limits Descriptors (DLD).\n7 DLDs for read commands and 7 DLDs for write commands.\nSimply put, a DLD contains a limit and a policy.\n\nA command can specify that a certain limit should be applied by setting\nthe DLD index field (3 bits, so 0-7) in the command itself.\n\nThe DLD index points to one of the 7 DLDs.\nDLD index 0 means no descriptor, so no limit.\nDLD index 1-7 means DLD 1-7.\n\nA DLD can have a few different policies, but the two major ones are:\n-Policy 0xF (abort), command will be completed with command aborted error\n(ATA) or status CHECK CONDITION (SCSI), with sense data indicating that\nthe command timed out.\n-Policy 0xD (complete-unavailable), command will be completed without\nerror (ATA) or status GOOD (SCSI), with sense data indicating that the\ncommand timed out. Note that the command will not have transferred any\ndata to/from the device when the command timed out, even though the\ncommand returned success.\n\nRegardless of the CDL policy, in case of a CDL timeout, the I/O will\nresult in a -ETIME error to user-space.\n\nThe DLDs are defined in the CDL log page(s) and are readable and writable.\nReading and writing the CDL DLDs are outside the scope of the kernel.\nIf a user wants to read or write the descriptors, they can do so using a\nuser-space application that sends passthrough commands, such as cdl-tools:\nhttps://github.com/westerndigitalcorporation/cdl-tools\n\n================================\nThe introduction of ioprio hints\n================================\nWhat the kernel does provide, is a method to let I/O use one of the CDL DLDs\ndefined in the device. Note that the kernel will simply forward the DLD index\nto the device, so the kernel currently does not know, nor does it need to know,\nhow the DLDs are defined inside the device.\n\nThe way that the CDL DLD index is supplied to the kernel is by introducing a\nnew 10 bit \"ioprio hint\" field within the existing 16 bit ioprio definition.\n\nCurrently, only 6 out of the 16 ioprio bits are in use, the remaining 10 bits\nare unused, and are currently explicitly disallowed to be set by the kernel.\n\nFor now, we only add ioprio hints representing CDL DLD index 1-7. Additional\nioprio hints for other QoS features could be defined in the future.\n\nA theoretical future work could be to make an I/O scheduler aware of these\nhints. E.g. for CDL, an I/O scheduler could make use of the duration limit\nin each descriptor, and take that information into account while scheduling\ncommands. Right now, the ioprio hints will be ignored by the I/O schedulers.\n\n==============================\nHow to use CDL from user-space\n==============================\nSince CDL is mutually exclusive with NCQ priority\n(see ncq_prio_enable and sas_ncq_prio_enable in\nDocumentation/ABI/testing/sysfs-block-device),\nCDL has to be explicitly enabled using:\necho 1 > /sys/block/$bdev/device/cdl_enable\n\nSince the ioprio hints are supplied through the existing I/O priority API,\nit should be simple for an application to make use of the ioprio hints.\n\nIt simply has to reuse one of the new macros defined in\ninclude/uapi/linux/ioprio.h: IOPRIO_PRIO_HINT() or IOPRIO_PRIO_VALUE_HINT(),\nand supply one of the new hints defined in include/uapi/linux/ioprio.h:\nIOPRIO_HINT_DEV_DURATION_LIMIT_[1-7], which indicates that the I/O should\nuse the corresponding CDL DLD index 1-7.\n\nBy reusing the I/O priority API, the user can both define a DLD to use per\nAIO (io_uring sqe->ioprio or libaio iocb->aio_reqprio) or per-thread\n(ioprio_set()).\n\n=======\nTesting\n=======\nWith the following fio patches:\nhttps://github.com/floatious/fio/commits/cdl\n\nfio adds support for ioprio hints, such that CDL can be tested using e.g.:\nfio --ioengine=io_uring --cmdprio_percentage=10 --cmdprio_hint=DLD_index\n\nA simple way to test is to use a DLD with a very short duration limit,\nand send large reads. Regardless of the CDL policy, in case of a CDL\ntimeout, the I/O will result in a -ETIME error to user-space.\n\nWe also provide a CDL test suite located in the cdl-tools repo, see:\nhttps://github.com/westerndigitalcorporation/cdl-tools#testing-a-system-command-duration-limits-support\n\nWe have tested this patch series using:\n-real hardware\n-the following QEMU implementation:\nhttps://github.com/floatious/qemu/tree/cdl\n(NOTE: the QEMU implementation requires you to define the CDL policy at compile\ntime, so you currently need to recompile QEMU when switching between policies.)\n\n===================\nFurther information\n===================\nFor further information about CDL, see Damien's slides:\n\nPresented at SDC 2021:\nhttps://www.snia.org/sites/default/files/SDC/2021/pdfs/SNIA-SDC21-LeMoal-Be-On-Time-command-duration-limits-Feature-Support-in%20Linux.pdf\n\nPresented at Lund Linux Con 2022:\nhttps://drive.google.com/file/d/1I6ChFc0h4JY9qZdO1bY5oCAdYCSZVqWw/view?usp=sharing\n\n================\nChanges since V6\n================\n-Rebased series on v6.4-rc1.\n-Picked up Reviewed-by tags from Hannes (Thank you Hannes!)\n-Picked up Reviewed-by tag from Christoph (Thank you Christoph!)\n-Changed KernelVersion from 6.4 to 6.5 for new sysfs attributes.\n\nFor older change logs, see previous patch series versions:\nhttps://lore.kernel.org/linux-scsi/20230406113252.41211-1-nks@flawful.org/\nhttps://lore.kernel.org/linux-scsi/20230404182428.715140-1-nks@flawful.org/\nhttps://lore.kernel.org/linux-scsi/20230309215516.3800571-1-niklas.cassel@wdc.com/\nhttps://lore.kernel.org/linux-scsi/20230124190308.127318-1-niklas.cassel@wdc.com/\nhttps://lore.kernel.org/linux-scsi/20230112140412.667308-1-niklas.cassel@wdc.com/\nhttps://lore.kernel.org/linux-scsi/20221208105947.2399894-1-niklas.cassel@wdc.com/\n\nLink: https://lore.kernel.org/r/20230511011356.227789-1-nks@flawful.org\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-22 17:09:51 -0400 Merge patch series \"Add Command Duration Limits support\""
    },
    {
        "commit": "eca2040972b411ec27483bf75dc8b84e730e88ff",
        "message": "The I/O priority user interface defines the 16-bits ioprio values as the\ncombination of the upper 3-bits for an I/O priority class and the lower\n13-bits as priority data. However, the kernel only uses the lower 3-bits of\nthe priority data to define priority levels for the RT and BE priority\nclasses. The data part of an ioprio value is completely ignored for the\nIDLE and NONE classes. This is enforced by checks done in\nioprio_check_cap(), which is called for all paths that allow defining an\nI/O priority for I/Os: the per-context ioprio_set() system call, aio\ninterface and io_uring interface.\n\nClarify this fact in the uapi ioprio.h header file and introduce the\nIOPRIO_PRIO_LEVEL_MASK and IOPRIO_PRIO_LEVEL() macros for users to define\nand get priority levels in an ioprio value. The coarser macro\nIOPRIO_PRIO_DATA() is retained for backward compatibility with old\napplications already using it. There is no functional change introduced\nwith this.\n\nIn-kernel users of the IOPRIO_PRIO_DATA() macro which are explicitly\nhandling I/O priority data as a priority level are modified to use the new\nIOPRIO_PRIO_LEVEL() macro without any functional change. Since f2fs is the\nonly user of this macro not explicitly using that value as a priority\nlevel, it is left unchanged.\n\nSigned-off-by: Damien Le Moal <dlemoal@kernel.org>\nReviewed-by: Hannes Reinecke <hare@suse.de>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Niklas Cassel <niklas.cassel@wdc.com>\nLink: https://lore.kernel.org/r/20230511011356.227789-2-nks@flawful.org\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-22 17:05:18 -0400 scsi: block: ioprio: Clean up interface definition"
    },
    {
        "commit": "29dc5d06613f2438ec20a4ba5e0a5a740584d346",
        "message": "task_work_add() is used from early ublk development stage for handling\nrequest in batch. However, since commit 7d4a93176e01 (\"ublk_drv: don't\nforward io commands in reserve order\"), we can get similar batch\nprocessing with io_uring_cmd_complete_in_task(), and similar performance\ndata is observed between task_work_add() and\nio_uring_cmd_complete_in_task().\n\nMeantime we can kill one fast code path, which is actually seldom used\ngiven it is common to build ublk driver as module.\n\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20230519065030.351216-2-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-19 19:59:16 -0600 ublk: kill queuing request by task_work_add"
    },
    {
        "commit": "5498bf28d8f2bd63a46ad40f4427518615fb793f",
        "message": "It's racy to read ->cached_cq_tail without taking proper measures\n(usually grabbing ->completion_lock) as timeout requests with CQE\noffsets do, however they have never had a good semantics for from\nwhen they start counting. Annotate racy reads with data_race().\n\nReported-by: syzbot+cb265db2f3f3468ef436@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4de3685e185832a92a572df2be2c735d2e21a83d.1684506056.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-19 19:56:56 -0600 io_uring: annotate offset timeout races"
    },
    {
        "commit": "3af0356c162c299a8216576b644eb72715e97cb2",
        "message": "We use lockless lists for the local and deferred task_work, which means\nthat when we queue up events for processing, we ultimately process them\nin reverse order to how they were received. This usually doesn't matter,\nbut for some cases, it does seem to make a big difference. Do the right\nthing and reverse the list before processing it, so that we know it's\nprocessed in the same order in which it was received.\n\nThis makes a rather big difference for some medium load network tests,\nwhere consistency of performance was a bit all over the place. Here's\na case that has 4 connections each doing two sends and receives:\n\nio_uring port=10002: rps:161.13k Bps:  1.45M idle=256ms\nio_uring port=10002: rps:107.27k Bps:  0.97M idle=413ms\nio_uring port=10002: rps:136.98k Bps:  1.23M idle=321ms\nio_uring port=10002: rps:155.58k Bps:  1.40M idle=268ms\n\nand after the change:\n\nio_uring port=10002: rps:205.48k Bps:  1.85M idle=140ms user=40ms\nio_uring port=10002: rps:203.57k Bps:  1.83M idle=139ms user=20ms\nio_uring port=10002: rps:218.79k Bps:  1.97M idle=106ms user=30ms\nio_uring port=10002: rps:217.88k Bps:  1.96M idle=110ms user=20ms\nio_uring port=10002: rps:222.31k Bps:  2.00M idle=101ms user=0ms\nio_uring port=10002: rps:218.74k Bps:  1.97M idle=102ms user=20ms\nio_uring port=10002: rps:208.43k Bps:  1.88M idle=125ms user=40ms\n\nusing more of the time to actually process work rather than sitting\nidle.\n\nNo effects have been observed at the peak end of the spectrum, where\nperformance is still the same even with deep batch depths (and hence\nmore items to sort).\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-19 13:49:51 -0600 io_uring: maintain ordering for DEFER_TASKRUN tw list"
    },
    {
        "commit": "ac5902f84bb546c64aea02c439c2579cbf40318f",
        "message": "When handling UBLK_IO_FETCH_REQ, ctx->uring_lock is grabbed first, then\nub->mutex is acquired.\n\nWhen handling UBLK_CMD_STOP_DEV or UBLK_CMD_DEL_DEV, ub->mutex is\ngrabbed first, then calling io_uring_cmd_done() for canceling uring\ncommand, in which ctx->uring_lock may be required.\n\nReal deadlock only happens when all the above commands are issued from\nsame uring context, and in reality different uring contexts are often used\nfor handing control command and IO command.\n\nFix the issue by using io_uring_cmd_complete_in_task() to cancel command\nin ublk_cancel_dev(ublk_cancel_queue).\n\nReported-by: Shinichiro Kawasaki <shinichiro.kawasaki@wdc.com>\nCloses: https://lore.kernel.org/linux-block/becol2g7sawl4rsjq2dztsbc7mqypfqko6wzsyoyazqydoasml@rcxarzwidrhk\nCc: Ziyang Zhang <ZiyangZhang@linux.alibaba.com>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nTested-by: Shinichiro Kawasaki <shinichiro.kawasaki@wdc.com>\nLink: https://lore.kernel.org/r/20230517133408.210944-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc3",
        "release_date": "2023-05-18 07:59:08 -0600 ublk: fix AB-BA lockdep warning"
    },
    {
        "commit": "a2741c58ac677e5de35bba7dec6376579dd513cd",
        "message": "If we're doing multishot receives, then we always end up doing two trips\nthrough sock_recvmsg(). For protocols that sanely set msghdr->msg_inq,\nthen we don't need to waste time picking a new buffer and attempting a\nnew receive if there's nothing there.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-17 13:14:11 -0600 io_uring/net: don't retry recvmsg() unnecessarily"
    },
    {
        "commit": "7d41bcb7f32fbeac05d6fab553821a228af18bee",
        "message": "Rather than have this logic in both io_recv() and io_recvmsg_multishot(),\npush it into the handler they both call when finishing a receive\noperation.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-17 12:20:44 -0600 io_uring/net: push IORING_CQE_F_SOCK_NONEMPTY into io_recv_finish()"
    },
    {
        "commit": "88fc8b8463b024df556d5c4245f2c273f22d83a1",
        "message": "We can't currently tell if ->msg_inq was set when we ask for msg_get_inq,\ninitialize it to -1U so we can tell apart if it was set and there's\nno data left, or if it just wasn't set at all by the protocol.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-17 12:18:13 -0600 io_uring/net: initalize msghdr->msg_inq to known value"
    },
    {
        "commit": "bf34e697931f64b21c82232e98b3d1f566214e40",
        "message": "We only need to clear the input fields on the first invocation, not\nwhen potentially doing a retry.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-17 12:15:00 -0600 io_uring/net: initialize struct msghdr more sanely for io_recv()"
    },
    {
        "commit": "81cf1ade0bb3a42023dd1694ed080c4adb7dafd3",
        "message": "Merge branch 'tcp-io_uring-zc-opts'\n\nPavel Begunkov says:\n\n====================\nminor tcp io_uring zc optimisations\n\nPatch 1 is a simple cleanup, patch 2 gives removes 2 atomics from the\nio_uring zc TCP submission path, which yielded extra 0.5% for my\nthroughput CPU bound tests based on liburing/examples/send-zerocopy.c\n====================\n\nReviewed-by: David Ahern <dsahern@kernel.org>\nReviewed-by: Eric Dumazet <edumazet@google.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-17 08:38:42 +0100 Merge branch 'tcp-io_uring-zc-opts'"
    },
    {
        "commit": "a7533584728d366f11b210d0e8a2fad03109c669",
        "message": "io_uring keeps a reference to ubuf_info during submission, so if\ntcp_sendmsg_locked() sees msghdr::msg_ubuf in can be sure the buffer\nwill be kept alive and doesn't need to additionally pin it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Eric Dumazet <edumazet@google.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-17 08:37:55 +0100 net/tcp: optimise io_uring zc ubuf refcounting"
    },
    {
        "commit": "eea96a3e2c909a055005ac65dde356b36cabc4ed",
        "message": "Move tcp_write_queue_tail() to SOCK_ZEROCOPY specific flag as zerocopy\nsetup for msghdr->ubuf_info doesn't need to peek into the last request.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: David Ahern <dsahern@kernel.org>\nReviewed-by: Eric Dumazet <edumazet@google.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-17 08:37:54 +0100 net/tcp: don't peek at tail for io_uring zc"
    },
    {
        "commit": "6e76ac595855db27bbdaef337173294a6fd6eb2c",
        "message": "With IORING_REGISTER_USE_REGISTERED_RING, an application can register\nthe ring fd and use it via registered index rather than installed fd.\nThis allows using a registered ring for everything *except* the initial\nmmap.\n\nWith IORING_SETUP_NO_MMAP, io_uring_setup uses buffers allocated by the\nuser, rather than requiring a subsequent mmap.\n\nThe combination of the two allows a user to operate *entirely* via a\nregistered ring fd, making it unnecessary to ever install the fd in the\nfirst place. So, add a flag IORING_SETUP_REGISTERED_FD_ONLY to make\nio_uring_setup register the fd and return a registered index, without\ninstalling the fd.\n\nThis allows an application to avoid touching the fd table at all, and\nallows a library to never even momentarily install a file descriptor.\n\nThis splits out an io_ring_add_registered_file helper from\nio_ring_add_registered_fd, for use by io_uring_setup.\n\nSigned-off-by: Josh Triplett <josh@joshtriplett.org>\nLink: https://lore.kernel.org/r/bc8f431bada371c183b95a83399628b605e978a3.1682699803.git.josh@joshtriplett.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-16 08:06:00 -0600 io_uring: Add io_uring_setup flag to pre-register ring fd and never install it"
    },
    {
        "commit": "03d89a2de25bbc5c77e61a0cf77663978c4b6ea7",
        "message": "Currently io_uring applications must call mmap(2) twice to map the rings\nthemselves, and the sqes array. This works fine, but it does not support\nusing huge pages to back the rings/sqes.\n\nProvide a way for the application to pass in pre-allocated memory for\nthe rings/sqes, which can then suitably be allocated from shmfs or\nvia mmap to get huge page support.\n\nParticularly for larger rings, this reduces the TLBs needed.\n\nIf an application wishes to take advantage of that, it must pre-allocate\nthe memory needed for the sq/cq ring, and the sqes. The former must\nbe passed in via the io_uring_params->cq_off.user_data field, while the\nlatter is passed in via the io_uring_params->sq_off.user_data field. Then\nit must set IORING_SETUP_NO_MMAP in the io_uring_params->flags field,\nand io_uring will then map the existing memory into the kernel for shared\nuse. The application must not call mmap(2) to map rings as it otherwise\nwould have, that will now fail with -EINVAL if this setup flag was used.\n\nThe pages used for the rings and sqes must be contigious. The intent here\nis clearly that huge pages should be used, otherwise the normal setup\nprocedure works fine as-is. The application may use one huge page for\nboth the rings and sqes.\n\nOutside of those initialization changes, everything works like it did\nbefore.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-16 08:04:55 -0600 io_uring: support for user allocated memory for rings/sqes"
    },
    {
        "commit": "9c189eee73af1825ea9c895fafad469de5f82641",
        "message": "We do rings and sqes separately, move them into a helper that does both\nthe freeing and clearing of the memory.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-16 08:04:49 -0600 io_uring: add ring freeing helper"
    },
    {
        "commit": "e27cef86a0edd4ef7f8b4670f508a03b509cbbb2",
        "message": "In preparation for having more than one time of ring allocator, make the\nexisting one return valid/error-pointer rather than just NULL.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-16 08:04:42 -0600 io_uring: return error pointer from io_mem_alloc()"
    },
    {
        "commit": "9b1b58cacc65ecee29bd85988c9ff957a84b43f4",
        "message": "We only have two reserved members we're not clearing, do so manually\ninstead. This is in preparation for using one of these members for\na new feature.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-16 08:04:37 -0600 io_uring: remove sq/cq_off memset"
    },
    {
        "commit": "caec5ebe77f97d948dcf46f07d622bda7f1f6dfd",
        "message": "Now that we have both sockets and block devices setting FMODE_NOWAIT\nappropriately, we can get rid of all the odd special casing in\n__io_file_supports_nowait() and rely soley on FMODE_NOWAIT and\nO_NONBLOCK rather than special case sockets and (in particular) bdevs.\n\nLink: https://lore.kernel.org/r/20230509151910.183637-4-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-15 10:12:27 -0600 io_uring: rely solely on FMODE_NOWAIT"
    },
    {
        "commit": "fe34db062b8036f72e97c2b9eaa7e9fbb725ead2",
        "message": "The socket read/write functions deal with O_NONBLOCK and IOCB_NOWAIT\njust fine, so we can flag them as being FMODE_NOWAIT compliant. With\nthis, we can remove socket special casing in io_uring when checking\nif a file type is sane for nonblocking IO, and it's also the defined\nway to flag file types as such in the kernel.\n\nCc: \"David S. Miller\" <davem@davemloft.net>\nCc: Eric Dumazet <edumazet@google.com>\nCc: Jakub Kicinski <kuba@kernel.org>\nCc: netdev@vger.kernel.org\nReviewed-by: Paolo Abeni <pabeni@redhat.com>\nLink: https://lore.kernel.org/r/20230509151910.183637-2-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.5-rc1",
        "release_date": "2023-05-15 10:12:27 -0600 net: set FMODE_NOWAIT for sockets"
    },
    {
        "commit": "584dc5dbcbcca71cc7ccce9077a1456842c26179",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single fix making io_uring_sqe_cmd() available regardless of\n  CONFIG_IO_URING, fixing a regression introduced during the merge\n  window if nvme was selected but io_uring was not\"\n\n* tag 'io_uring-6.4-2023-05-12' of git://git.kernel.dk/linux:\n  io_uring: make io_uring_sqe_cmd() unconditionally available",
        "kernel_version": "v6.4-rc2",
        "release_date": "2023-05-12 16:39:05 -0500 Merge tag 'io_uring-6.4-2023-05-12' of git://git.kernel.dk/linux"
    },
    {
        "commit": "e14cadfd80d76f01bfaa1a8d745b1db19b57d6be",
        "message": "Now sk->sk_shutdown is no longer a bitfield, we can add\nstandard READ_ONCE()/WRITE_ONCE() annotations to silence\nKCSAN reports like the following:\n\nBUG: KCSAN: data-race in tcp_disconnect / tcp_poll\n\nwrite to 0xffff88814588582c of 1 bytes by task 3404 on cpu 1:\ntcp_disconnect+0x4d6/0xdb0 net/ipv4/tcp.c:3121\n__inet_stream_connect+0x5dd/0x6e0 net/ipv4/af_inet.c:715\ninet_stream_connect+0x48/0x70 net/ipv4/af_inet.c:727\n__sys_connect_file net/socket.c:2001 [inline]\n__sys_connect+0x19b/0x1b0 net/socket.c:2018\n__do_sys_connect net/socket.c:2028 [inline]\n__se_sys_connect net/socket.c:2025 [inline]\n__x64_sys_connect+0x41/0x50 net/socket.c:2025\ndo_syscall_x64 arch/x86/entry/common.c:50 [inline]\ndo_syscall_64+0x41/0xc0 arch/x86/entry/common.c:80\nentry_SYSCALL_64_after_hwframe+0x63/0xcd\n\nread to 0xffff88814588582c of 1 bytes by task 3374 on cpu 0:\ntcp_poll+0x2e6/0x7d0 net/ipv4/tcp.c:562\nsock_poll+0x253/0x270 net/socket.c:1383\nvfs_poll include/linux/poll.h:88 [inline]\nio_poll_check_events io_uring/poll.c:281 [inline]\nio_poll_task_func+0x15a/0x820 io_uring/poll.c:333\nhandle_tw_list io_uring/io_uring.c:1184 [inline]\ntctx_task_work+0x1fe/0x4d0 io_uring/io_uring.c:1246\ntask_work_run+0x123/0x160 kernel/task_work.c:179\nget_signal+0xe64/0xff0 kernel/signal.c:2635\narch_do_signal_or_restart+0x89/0x2a0 arch/x86/kernel/signal.c:306\nexit_to_user_mode_loop+0x6f/0xe0 kernel/entry/common.c:168\nexit_to_user_mode_prepare+0x6c/0xb0 kernel/entry/common.c:204\n__syscall_exit_to_user_mode_work kernel/entry/common.c:286 [inline]\nsyscall_exit_to_user_mode+0x26/0x140 kernel/entry/common.c:297\ndo_syscall_64+0x4d/0xc0 arch/x86/entry/common.c:86\nentry_SYSCALL_64_after_hwframe+0x63/0xcd\n\nvalue changed: 0x03 -> 0x00\n\nFixes: 1da177e4c3f4 (\"Linux-2.6.12-rc2\")\nReported-by: syzbot <syzkaller@googlegroups.com>\nSigned-off-by: Eric Dumazet <edumazet@google.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>",
        "kernel_version": "v6.4-rc2",
        "release_date": "2023-05-10 10:27:31 +0100 tcp: add annotations around sk->sk_shutdown accesses"
    },
    {
        "commit": "293007b033418c8c9d1b35d68dec49a500750fde",
        "message": "If CONFIG_IO_URING isn't set, then io_uring_sqe_cmd() is not defined.\nAs the nvme driver uses this helper, it causes a compilation issue:\n\n drivers/nvme/host/ioctl.c: In function 'nvme_uring_cmd_io':\n drivers/nvme/host/ioctl.c:555:44: error: implicit declaration of function 'io_uring_sqe_cmd'; did you mean 'io_uring_free'? [-Werror=implicit-function-declaration]\n   555 |         const struct nvme_uring_cmd *cmd = io_uring_sqe_cmd(ioucmd->sqe);\n       |                                            ^~~~~~~~~~~~~~~~\n       |                                            io_uring_free\n\nFix it by just making io_uring_sqe_cmd() generally available - the types\nare known, and there's no reason to hide it under CONFIG_IO_URING.\n\nFixes: fd9b8547bc5c (\"io_uring: Pass whole sqe to commands\")\nReported-by: kernel test robot <lkp@intel.com>\nReported-by: Chen-Yu Tsai <wenst@chromium.org>\nTested-by: Chen-Yu Tsai <wenst@chromium.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc2",
        "release_date": "2023-05-09 07:59:54 -0600 io_uring: make io_uring_sqe_cmd() unconditionally available"
    },
    {
        "commit": "03e5cb7b50feb687508946a702febaba24c77f0b",
        "message": "Pull more io_uring updates from Jens Axboe:\n \"Nothing major in here, just two different parts:\n\n   - A small series from Breno that enables passing the full SQE down\n     for ->uring_cmd().\n\n     This is a prerequisite for enabling full network socket operations.\n     Queued up a bit late because of some stylistic concerns that got\n     resolved, would be nice to have this in 6.4-rc1 so the dependent\n     work will be easier to handle for 6.5.\n\n   - Fix for the huge page coalescing, which was a regression introduced\n     in the 6.3 kernel release (Tobias)\"\n\n* tag 'for-6.4/io_uring-2023-05-07' of git://git.kernel.dk/linux:\n  io_uring: Remove unnecessary BUILD_BUG_ON\n  io_uring: Pass whole sqe to commands\n  io_uring: Create a helper to return the SQE size\n  io_uring/rsrc: check for nonconsecutive pages",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-05-07 10:00:09 -0700 Merge tag 'for-6.4/io_uring-2023-05-07' of git://git.kernel.dk/linux"
    },
    {
        "commit": "7644c8231987288e7aae378d2ff3c56a980d1988",
        "message": "Pull nonblocking pipe io_uring support from Jens Axboe:\n \"Here's the revised edition of the FMODE_NOWAIT support for pipes, in\n  which we just flag it as such supporting FMODE_NOWAIT unconditionally,\n  but clear it if we ever end up using splice/vmsplice on the pipe.\n\n  The pipe read/write side is perfectly fine for nonblocking IO, however\n  splice and vmsplice can potentially wait for IO with the pipe lock\n  held\"\n\n* tag 'pipe-nonblock-2023-05-06' of git://git.kernel.dk/linux:\n  pipe: set FMODE_NOWAIT on pipes\n  splice: clear FMODE_NOWAIT on file if splice/vmsplice is used",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-05-06 08:15:20 -0700 Merge tag 'pipe-nonblock-2023-05-06' of git://git.kernel.dk/linux"
    },
    {
        "commit": "d2b7fa6174bc4260e496cbf84375c73636914641",
        "message": "In the io_uring_cmd_prep_async() there is an unnecessary compilation time\ncheck to check if cmd is correctly placed at field 48 of the SQE.\n\nThis is unnecessary, since this check is already in place at\nio_uring_init():\n\n          BUILD_BUG_SQE_ELEM(48, __u64,  addr3);\n\nRemove it and the uring_cmd_pdu_size() function, which is not used\nanymore.\n\nKeith started a discussion about this topic in the following thread:\nLink: https://lore.kernel.org/lkml/ZDBmQOhbyU0iLhMw@kbusch-mbp.dhcp.thefacebook.com/\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20230504121856.904491-4-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-05-04 08:19:05 -0600 io_uring: Remove unnecessary BUILD_BUG_ON"
    },
    {
        "commit": "fd9b8547bc5c34186dc42ea05fb4380d21695374",
        "message": "Currently uring CMD operation relies on having large SQEs, but future\noperations might want to use normal SQE.\n\nThe io_uring_cmd currently only saves the payload (cmd) part of the SQE,\nbut, for commands that use normal SQE size, it might be necessary to\naccess the initial SQE fields outside of the payload/cmd block.  So,\nsaves the whole SQE other than just the pdu.\n\nThis changes slightly how the io_uring_cmd works, since the cmd\nstructures and callbacks are not opaque to io_uring anymore. I.e, the\ncallbacks can look at the SQE entries, not only, in the cmd structure.\n\nThe main advantage is that we don't need to create custom structures for\nsimple commands.\n\nCreates io_uring_sqe_cmd() that returns the cmd private data as a null\npointer and avoids casting in the callee side.\nAlso, make most of ublk_drv's sqe->cmd priv structure into const, and use\nio_uring_sqe_cmd() to get the private structure, removing the unwanted\ncast. (There is one case where the cast is still needed since the\nheader->{len,addr} is updated in the private structure)\n\nSuggested-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Breno Leitao <leitao@debian.org>\nReviewed-by: Keith Busch <kbusch@kernel.org>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20230504121856.904491-3-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-05-04 08:19:05 -0600 io_uring: Pass whole sqe to commands"
    },
    {
        "commit": "96c7d4f81db0fea05c0792f7563ae0cb4ad5f022",
        "message": "Create a simple helper that returns the size of the SQE. The SQE could\nhave two size, depending of the flags.\n\nIf IO_URING_SETUP_SQE128 flag is set, then return a double SQE,\notherwise returns the sizeof of io_uring_sqe (64 bytes).\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20230504121856.904491-2-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-05-04 08:19:05 -0600 io_uring: Create a helper to return the SQE size"
    },
    {
        "commit": "776617db78c6d208780e7c69d4d68d1fa82913de",
        "message": "Pages that are from the same folio do not necessarily need to be\nconsecutive. In that case, we cannot consolidate them into a single bvec\nentry. Before applying the huge page optimization from commit 57bebf807e2a\n(\"io_uring/rsrc: optimise registered huge pages\"), check that the memory\nis actually consecutive.\n\nCc: stable@vger.kernel.org\nFixes: 57bebf807e2a (\"io_uring/rsrc: optimise registered huge pages\")\nSigned-off-by: Tobias Holl <tobias@tholl.xyz>\n[axboe: formatting]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-05-03 09:00:22 -0600 io_uring/rsrc: check for nonconsecutive pages"
    },
    {
        "commit": "22b8cc3e78f5448b4c5df00303817a9137cd663f",
        "message": "Pull x86 LAM (Linear Address Masking) support from Dave Hansen:\n \"Add support for the new Linear Address Masking CPU feature.\n\n  This is similar to ARM's Top Byte Ignore and allows userspace to store\n  metadata in some bits of pointers without masking it out before use\"\n\n* tag 'x86_mm_for_6.4' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:\n  x86/mm/iommu/sva: Do not allow to set FORCE_TAGGED_SVA bit from outside\n  x86/mm/iommu/sva: Fix error code for LAM enabling failure due to SVA\n  selftests/x86/lam: Add test cases for LAM vs thread creation\n  selftests/x86/lam: Add ARCH_FORCE_TAGGED_SVA test cases for linear-address masking\n  selftests/x86/lam: Add inherit test cases for linear-address masking\n  selftests/x86/lam: Add io_uring test cases for linear-address masking\n  selftests/x86/lam: Add mmap and SYSCALL test cases for linear-address masking\n  selftests/x86/lam: Add malloc and tag-bits test cases for linear-address masking\n  x86/mm/iommu/sva: Make LAM and SVA mutually exclusive\n  iommu/sva: Replace pasid_valid() helper with mm_valid_pasid()\n  mm: Expose untagging mask in /proc/$PID/status\n  x86/mm: Provide arch_prctl() interface for LAM\n  x86/mm: Reduce untagged_addr() overhead for systems without LAM\n  x86/uaccess: Provide untagged_addr() and remove tags before address check\n  mm: Introduce untagged_addr_remote()\n  x86/mm: Handle LAM on context switch\n  x86: CPUID and CR3/CR4 flags for Linear Address Masking\n  x86: Allow atomic MM_CONTEXT flags setting\n  x86/mm: Rework address range check in get_user() and put_user()",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-28 09:43:49 -0700 Merge tag 'x86_mm_for_6.4' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip"
    },
    {
        "commit": "6d2ed653185baa5ba601306cbd6cd7192642045d",
        "message": "Fix one kernel-doc warning, but invesigating that led to other\nkernel-doc movement (lsm_hooks.h to security.c) that needs to be\nfixed also.\n\ninclude/linux/lsm_hooks.h:1: warning: no structured comments found\n\nFixes: e261301c851a (\"lsm: move the remaining LSM hook comments to security/security.c\")\nFixes: 1cd2aca64a5d (\"lsm: move the io_uring hook comments to security/security.c\")\nFixes: 452b670c7222 (\"lsm: move the perf hook comments to security/security.c\")\nFixes: 55e853201a9e (\"lsm: move the bpf hook comments to security/security.c\")\nFixes: b14faf9c94a6 (\"lsm: move the audit hook comments to security/security.c\")\nFixes: 1427ddbe5cc1 (\"lsm: move the binder hook comments to security/security.c\")\nFixes: 43fad2821876 (\"lsm: move the sysv hook comments to security/security.c\")\nFixes: ecc419a44535 (\"lsm: move the key hook comments to security/security.c\")\nFixes: 742b99456e86 (\"lsm: move the xfrm hook comments to security/security.c\")\nFixes: ac318aed5498 (\"lsm: move the Infiniband hook comments to security/security.c\")\nFixes: 4a49f592e931 (\"lsm: move the SCTP hook comments to security/security.c\")\nFixes: 6b6bbe8c02a1 (\"lsm: move the socket hook comments to security/security.c\")\nFixes: 2c2442fd46cd (\"lsm: move the AF_UNIX hook comments to security/security.c\")\nFixes: 2bcf51bf2f03 (\"lsm: move the netlink hook comments to security/security.c\")\nFixes: 130c53bfee4b (\"lsm: move the task hook comments to security/security.c\")\nFixes: a0fd6480de48 (\"lsm: move the file hook comments to security/security.c\")\nFixes: 9348944b775d (\"lsm: move the kernfs hook comments to security/security.c\")\nFixes: 916e32584dfa (\"lsm: move the inode hook comments to security/security.c\")\nFixes: 08526a902cc4 (\"lsm: move the filesystem hook comments to security/security.c\")\nFixes: 36819f185590 (\"lsm: move the fs_context hook comments to security/security.c\")\nFixes: 1661372c912d (\"lsm: move the program execution hook comments to security/security.c\")\nSigned-off-by: Randy Dunlap <rdunlap@infradead.org>\nCc: Paul Moore <paul@paul-moore.com>\nCc: James Morris <jmorris@namei.org>\nCc: \"Serge E. Hallyn\" <serge@hallyn.com>\nCc: linux-security-module@vger.kernel.org\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: linux-doc@vger.kernel.org\nCc: KP Singh <kpsingh@kernel.org>\nCc: bpf@vger.kernel.org\nSigned-off-by: Paul Moore <paul@paul-moore.com>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-28 11:58:34 -0400 lsm: move hook comments docs to security/security.c"
    },
    {
        "commit": "5b9a7bb72fddbc5247f56ede55d485fab7abdf92",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - Cleanup of the io-wq per-node mapping, notably getting rid of it so\n   we just have a single io_wq entry per ring (Breno)\n\n - Followup to the above, move accounting to io_wq as well and\n   completely drop struct io_wqe (Gabriel)\n\n - Enable KASAN for the internal io_uring caches (Breno)\n\n - Add support for multishot timeouts. Some applications use timeouts to\n   wake someone waiting on completion entries, and this makes it a bit\n   easier to just have a recurring timer rather than needing to rearm it\n   every time (David)\n\n - Support archs that have shared cache coloring between userspace and\n   the kernel, and hence have strict address requirements for mmap'ing\n   the ring into userspace. This should only be parisc/hppa. (Helge, me)\n\n - XFS has supported O_DIRECT writes without needing to lock the inode\n   exclusively for a long time, and ext4 now supports it as well. This\n   is true for the common cases of not extending the file size. Flag the\n   fs as having that feature, and utilize that to avoid serializing\n   those writes in io_uring (me)\n\n - Enable completion batching for uring commands (me)\n\n - Revert patch adding io_uring restriction to what can be GUP mapped or\n   not. This does not belong in io_uring, as io_uring isn't really\n   special in this regard. Since this is also getting in the way of\n   cleanups and improvements to the GUP code, get rid of if (me)\n\n - A few series greatly reducing the complexity of registered resources,\n   like buffers or files. Not only does this clean up the code a lot,\n   the simplified code is also a LOT more efficient (Pavel)\n\n - Series optimizing how we wait for events and run task_work related to\n   it (Pavel)\n\n - Fixes for file/buffer unregistration with DEFER_TASKRUN (Pavel)\n\n - Misc cleanups and improvements (Pavel, me)\n\n* tag 'for-6.4/io_uring-2023-04-21' of git://git.kernel.dk/linux: (71 commits)\n  Revert \"io_uring/rsrc: disallow multi-source reg buffers\"\n  io_uring: add support for multishot timeouts\n  io_uring/rsrc: disassociate nodes and rsrc_data\n  io_uring/rsrc: devirtualise rsrc put callbacks\n  io_uring/rsrc: pass node to io_rsrc_put_work()\n  io_uring/rsrc: inline io_rsrc_put_work()\n  io_uring/rsrc: add empty flag in rsrc_node\n  io_uring/rsrc: merge nodes and io_rsrc_put\n  io_uring/rsrc: infer node from ctx on io_queue_rsrc_removal\n  io_uring/rsrc: remove unused io_rsrc_node::llist\n  io_uring/rsrc: refactor io_queue_rsrc_removal\n  io_uring/rsrc: simplify single file node switching\n  io_uring/rsrc: clean up __io_sqe_buffers_update()\n  io_uring/rsrc: inline switch_start fast path\n  io_uring/rsrc: remove rsrc_data refs\n  io_uring/rsrc: fix DEFER_TASKRUN rsrc quiesce\n  io_uring/rsrc: use wq for quiescing\n  io_uring/rsrc: refactor io_rsrc_ref_quiesce\n  io_uring/rsrc: remove io_rsrc_node::done\n  io_uring/rsrc: use nospec'ed indexes\n  ...",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-26 12:40:31 -0700 Merge tag 'for-6.4/io_uring-2023-04-21' of git://git.kernel.dk/linux"
    },
    {
        "commit": "3323ddce085cdb33331c2c1bb7a88233023566a9",
        "message": "Pull user work thread updates from Christian Brauner:\n \"This contains the work generalizing the ability to create a kernel\n  worker from a userspace process.\n\n  Such user workers will run with the same credentials as the userspace\n  process they were created from providing stronger security and\n  accounting guarantees than the traditional override_creds() approach\n  ever could've hoped for.\n\n  The original work was heavily based and optimzed for the needs of\n  io_uring which was the first user. However, as it quickly turned out\n  the ability to create user workers inherting properties from a\n  userspace process is generally useful.\n\n  The vhost subsystem currently creates workers using the kthread api.\n  The consequences of using the kthread api are that RLIMITs don't work\n  correctly as they are inherited from khtreadd. This leads to bugs\n  where more workers are created than would be allowed by the RLIMITs of\n  the userspace process in lieu of which workers are created.\n\n  Problems like this disappear with user workers created from the\n  userspace processes for which they perform the work. In addition,\n  providing this api allows vhost to remove additional complexity. For\n  example, cgroup and mm sharing will just work out of the box with user\n  workers based on the relevant userspace process instead of manually\n  ensuring the correct cgroup and mm contexts are used.\n\n  So the vhost subsystem should simply be made to use the same mechanism\n  as io_uring. To this end the original mechanism used for\n  create_io_thread() is generalized into user workers:\n\n   - Introduce PF_USER_WORKER as a generic indicator that a given task\n     is a user worker, i.e., a kernel task that was created from a\n     userspace process. Now a PF_IO_WORKER thread is just a specialized\n     version of PF_USER_WORKER. So io_uring io workers raise both flags.\n\n   - Make copy_process() available to core kernel code\n\n   - Extend struct kernel_clone_args with the following bitfields\n     allowing to indicate to copy_process():\n       - to create a user worker (raise PF_USER_WORKER)\n       - to not inherit any files from the userspace process\n       - to ignore signals\n\n  After all generic changes are in place the vhost subsystem implements\n  a new dedicated vhost api based on user workers. Finally, vhost is\n  switched to rely on the new api moving it off of kthreads.\n\n  Thanks to Mike for sticking it out and making it through this rather\n  arduous journey\"\n\n* tag 'v6.4/kernel.user_worker' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux:\n  vhost: use vhost_tasks for worker threads\n  vhost: move worker thread fields to new struct\n  vhost_task: Allow vhost layer to use copy_process\n  fork: allow kernel code to call copy_process\n  fork: Add kernel_clone_args flag to ignore signals\n  fork: add kernel_clone_args flag to not dup/clone files\n  fork/vm: Move common PF_IO_WORKER behavior to new flag\n  kernel: Make io_thread and kthread bit fields\n  kthread: Pass in the thread's name during creation\n  kernel: Allow a kernel thread's name to be set in copy_process\n  csky: Remove kernel_thread declaration",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-24 12:52:35 -0700 Merge tag 'v6.4/kernel.user_worker' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux"
    },
    {
        "commit": "08e30833f86ba25945e416b9f372791aacfef153",
        "message": "Pull lsm updates from Paul Moore:\n\n - Move the LSM hook comment blocks into security/security.c\n\n   For many years the LSM hook comment blocks were located in a very odd\n   place, include/linux/lsm_hooks.h, where they lived on their own,\n   disconnected from both the function prototypes and definitions.\n\n   In keeping with current kernel conventions, this moves all of these\n   comment blocks to the top of the function definitions, transforming\n   them into the kdoc format in the process. This should make it much\n   easier to maintain these comments, which are the main source of LSM\n   hook documentation.\n\n   For the most part the comment contents were left as-is, although some\n   glaring errors were corrected. Expect additional edits in the future\n   as we slowly update and correct the comment blocks.\n\n   This is the bulk of the diffstat.\n\n - Introduce LSM_ORDER_LAST\n\n   Similar to how LSM_ORDER_FIRST is used to specify LSMs which should\n   be ordered before \"normal\" LSMs, the LSM_ORDER_LAST is used to\n   specify LSMs which should be ordered after \"normal\" LSMs.\n\n   This is one of the prerequisites for transitioning IMA/EVM to a\n   proper LSM.\n\n - Remove the security_old_inode_init_security() hook\n\n   The security_old_inode_init_security() LSM hook only allows for a\n   single xattr which is problematic both for LSM stacking and the\n   IMA/EVM-as-a-LSM effort. This finishes the conversion over to the\n   security_inode_init_security() hook and removes the single-xattr LSM\n   hook.\n\n - Fix a reiserfs problem with security xattrs\n\n   During the security_old_inode_init_security() removal work it became\n   clear that reiserfs wasn't handling security xattrs properly so we\n   fixed it.\n\n* tag 'lsm-pr-20230420' of git://git.kernel.org/pub/scm/linux/kernel/git/pcmoore/lsm: (32 commits)\n  reiserfs: Add security prefix to xattr name in reiserfs_security_write()\n  security: Remove security_old_inode_init_security()\n  ocfs2: Switch to security_inode_init_security()\n  reiserfs: Switch to security_inode_init_security()\n  security: Remove integrity from the LSM list in Kconfig\n  Revert \"integrity: double check iint_cache was initialized\"\n  security: Introduce LSM_ORDER_LAST and set it for the integrity LSM\n  device_cgroup: Fix typo in devcgroup_css_alloc description\n  lsm: fix a badly named parameter in security_get_getsecurity()\n  lsm: fix doc warnings in the LSM hook comments\n  lsm: styling fixes to security/security.c\n  lsm: move the remaining LSM hook comments to security/security.c\n  lsm: move the io_uring hook comments to security/security.c\n  lsm: move the perf hook comments to security/security.c\n  lsm: move the bpf hook comments to security/security.c\n  lsm: move the audit hook comments to security/security.c\n  lsm: move the binder hook comments to security/security.c\n  lsm: move the sysv hook comments to security/security.c\n  lsm: move the key hook comments to security/security.c\n  lsm: move the xfrm hook comments to security/security.c\n  ...",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-24 11:21:50 -0700 Merge tag 'lsm-pr-20230420' of git://git.kernel.org/pub/scm/linux/kernel/git/pcmoore/lsm"
    },
    {
        "commit": "3c85cc43c8e7855d202da184baf00c7b8eeacf71",
        "message": "This reverts commit edd478269640b360c6f301f2baa04abdda563ef3.\n\nThere's really no specific need to disallow multiple sources of buffers,\nand io_uring really should not be mandating this by itself. We should\nbe able to solely rely on GUP making these decisions.\n\nAs this also stands in the way of a cleanup where io_uring is the odd\none out, kill it.\n\nLink: https://lore.kernel.org/all/61ded378-51a8-1dcb-b631-fda1903248a9@gmail.com/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-20 06:51:48 -0600 Revert \"io_uring/rsrc: disallow multi-source reg buffers\""
    },
    {
        "commit": "ea97f6c8558e83cb457c3b5f53351e4fd8519ab1",
        "message": "A multishot timeout submission will repeatedly generate completions with\nthe IORING_CQE_F_MORE cflag set. Depending on the value of the `off'\nfield in the submission, these timeouts can either repeat indefinitely\nuntil cancelled (`off' = 0) or for a fixed number of times (`off' > 0).\n\nOnly noseq timeouts (i.e. not dependent on the number of I/O\ncompletions) are supported.\n\nAn indefinite timer will be cancelled if the CQ ever overflows.\n\nSigned-off-by: David Wei <davidhwei@meta.com>\nLink: https://lore.kernel.org/r/20230418225817.1905027-1-davidhwei@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-18 19:38:36 -0600 io_uring: add support for multishot timeouts"
    },
    {
        "commit": "2236b3905b4d4e9cd4d149ab35767858c02bb79b",
        "message": "Make rsrc nodes independent from rsrd_data, for that we keep ctx and\nrsrc type in nodes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4f259abe9cd4eea6a3b4ed83508635218acd3c3f.1681822823.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-18 19:38:26 -0600 io_uring/rsrc: disassociate nodes and rsrc_data"
    },
    {
        "commit": "fc7f3a8d3a78503c4f3e108155fb9a233dc307a4",
        "message": "We only have two rsrc types, buffers and files, replace virtual\ncallbacks for putting resources down with a switch..case.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/02ca727bf8e5f7f820c2f404e95ae88c8f472930.1681822823.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-18 19:38:26 -0600 io_uring/rsrc: devirtualise rsrc put callbacks"
    },
    {
        "commit": "29b26c556e7439b1370ac6a59fce83a9d1521de1",
        "message": "Instead of passing rsrc_data and a resource to io_rsrc_put_work() just\nforward node, that's all the function needs.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/791e8edd28d78797240b74d34e99facbaad62f3b.1681822823.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-18 19:38:26 -0600 io_uring/rsrc: pass node to io_rsrc_put_work()"
    },
    {
        "commit": "4130b49991d6b8ca0ea44cb256e710c4e48d7f01",
        "message": "io_rsrc_put_work() is simple enough to be open coded into its only\ncaller.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1b36dd46766ced39a9b160767babfa2fce07b8f8.1681822823.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-18 19:38:26 -0600 io_uring/rsrc: inline io_rsrc_put_work()"
    },
    {
        "commit": "26147da37f3e52041d9deba189d39f27ce78a84f",
        "message": "Unless a node was flushed by io_rsrc_ref_quiesce(), it'll carry a\nresource. Replace ->inline_items with an empty flag, which is\ninitialised to false and only raised in io_rsrc_ref_quiesce().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/75d384c9d2252e12af73b9cf8a44e1699106aeb1.1681822823.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-18 19:38:26 -0600 io_uring/rsrc: add empty flag in rsrc_node"
    },
    {
        "commit": "c376644fb915fbdea8c4a04f859d032a8be352fd",
        "message": "struct io_rsrc_node carries a number of resources represented by struct\nio_rsrc_put. That was handy before for sync overhead ammortisation, but\nall complexity is gone and nodes are simple and lightweight. Let's\nallocate a separate node for each resource.\n\nNodes and io_rsrc_put and not much different in size, and former are\ncached, so node allocation should work better. That also removes some\noverhead for nested iteration in io_rsrc_node_ref_zero() /\n__io_rsrc_put_work().\n\nAnother reason for the patch is that it greatly reduces complexity\nby moving io_rsrc_node_switch[_start]() inside io_queue_rsrc_removal(),\nso users don't have to care about it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c7d3a45b30cc14cd93700a710dd112edc703db98.1681822823.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-18 19:38:26 -0600 io_uring/rsrc: merge nodes and io_rsrc_put"
    },
    {
        "commit": "63fea89027ff4fd4f350b471ad5b9220d373eec5",
        "message": "For io_queue_rsrc_removal() we should always use the current active rsrc\nnode, don't pass it directly but let the function grab it from the\ncontext.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d15939b4afea730978b4925685c2577538b823bb.1681822823.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-18 19:38:26 -0600 io_uring/rsrc: infer node from ctx on io_queue_rsrc_removal"
    },
    {
        "commit": "2e6f45ac0e640bbd49296adfa0982c84f85fa342",
        "message": "->llist was needed for rsrc node destruction offload, which is removed\nnow. Get rid of the unused field.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8e7d764c3f947489fde88d0927c3060d2e1bb599.1681822823.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-18 19:38:26 -0600 io_uring/rsrc: remove unused io_rsrc_node::llist"
    },
    {
        "commit": "c899a5d7d0eca054546b63e95c94b1e609516f84",
        "message": "We can queue up a rsrc into a list in io_queue_rsrc_removal() while\nallocating io_rsrc_put and so simplify the function.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/36bd708ee25c0e2e7992dc19b17db166eea9ac40.1681395792.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-15 14:45:55 -0600 io_uring/rsrc: refactor io_queue_rsrc_removal"
    },
    {
        "commit": "c87fd583f3b5ef770af33893394ea37c7a10b5b8",
        "message": "At maximum io_install_fixed_file() removes only one file, so no need to\nkeep needs_switch state and we can call io_rsrc_node_switch() right after\nremoval.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/37cfb46f46160f81dced79f646e97db608994574.1681395792.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-15 14:45:49 -0600 io_uring/rsrc: simplify single file node switching"
    },
    {
        "commit": "9a57fffedc0ee078418a7793ab29cd3864205340",
        "message": "Inline offset variable, so we don't use it without subjecting it to\narray_index_nospec() first.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/77936d9ed23755588810c5eafcea7e1c3b90e3cd.1681395792.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-15 14:44:57 -0600 io_uring/rsrc: clean up __io_sqe_buffers_update()"
    },
    {
        "commit": "2f2af35f8e5a1ed552ed02e47277d50092a2b9f6",
        "message": "Inline the part of io_rsrc_node_switch_start() that checks whether the\ncache is empty or not, as most of the times it will have some number of\nentries in there.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9619c1717a0e01f22c5fce2f1ba2735f804da0f2.1681395792.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-15 14:44:57 -0600 io_uring/rsrc: inline switch_start fast path"
    },
    {
        "commit": "0b222eeb6514ba6c3457b667fa4f3645032e1fc9",
        "message": "Instead of waiting for rsrc_data->refs to be downed to zero, check\nwhether there are rsrc nodes queued for completion, that's easier then\nmaintaining references.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8e33fd143d83e11af3e386aea28eb6d6c6a1be10.1681395792.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-15 14:44:57 -0600 io_uring/rsrc: remove rsrc_data refs"
    },
    {
        "commit": "7d481e0356334eb2de254414769b4bed4b2a8827",
        "message": "For io_rsrc_ref_quiesce() to progress it should execute all task_work\nitems, including deferred ones. However, currently nobody would wake us,\nand so let's set ctx->cq_wait_nr, so io_req_local_work_add() would wake\nus up.\n\nFixes: c0e0d6ba25f18 (\"io_uring: add IORING_SETUP_DEFER_TASKRUN\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f1a90d1bc5ebf096475b018fed52e54f3b89d4af.1681395792.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-15 14:44:57 -0600 io_uring/rsrc: fix DEFER_TASKRUN rsrc quiesce"
    },
    {
        "commit": "4ea15b56f0810f0d8795d475db1bb74b3a7c1b2f",
        "message": "Replace completions with waitqueues for rsrc data quiesce, the main\nwakeup condition is when data refs hit zero. Note that data refs are\nonly changes under ->uring_lock, so we prepare before mutex_unlock()\nreacquire it after taking the lock back. This change will be needed\nin the next patch.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1d0dbc74b3b4fd67c8f01819e680c5e0da252956.1681395792.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-15 14:44:57 -0600 io_uring/rsrc: use wq for quiescing"
    },
    {
        "commit": "eef81fcaa61e1bc6b7735be65f41bbf1a8efd133",
        "message": "Refactor io_rsrc_ref_quiesce() by moving the first mutex_unlock(),\nso we don't have to have a second mutex_unlock() further in the loop.\nIt prepares us to the next patch.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/65bc876271fb16bf550a53a4c76c91aacd94e52e.1681395792.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-15 14:44:57 -0600 io_uring/rsrc: refactor io_rsrc_ref_quiesce"
    },
    {
        "commit": "c732ea242d565c8281c4b017929fc62a246d81b9",
        "message": "Kill io_rsrc_node::node and check refs instead, it's set when the nodes\nrefcount hits zero, and it won't change afterwards.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bbde361f4010f7e8bf196f1ecca27a763b79926f.1681395792.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-15 14:44:57 -0600 io_uring/rsrc: remove io_rsrc_node::done"
    },
    {
        "commit": "953c37e066f05a3dca2d74643574b8dfe8a83983",
        "message": "We use array_index_nospec() for registered buffer indexes, but don't use\nit while poking into rsrc tags, fix that.\n\nFixes: 634d00df5e1cf (\"io_uring: add full-fledged dynamic buffers support\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f02fafc5a9c0dd69be2b0618c38831c078232ff0.1681395792.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-15 14:44:57 -0600 io_uring/rsrc: use nospec'ed indexes"
    },
    {
        "commit": "519760df251bf2dcafb0af23df0229096537e78a",
        "message": "Add a constant IO_NOTIF_UBUF_FLAGS for struct ubuf_info flags that\nnotifications use. That should minimise merge conflicts for planned\nchanges touching both io_uring and net at the same time.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-15 14:21:04 -0600 io_uring/notif: add constant for ubuf_info flags"
    },
    {
        "commit": "1c6492d64646246834414964cfba9f826e7330b4",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a small tweak to when task_work needs redirection, marked for\n  stable as well\"\n\n* tag 'io_uring-6.3-2023-04-14' of git://git.kernel.dk/linux:\n  io_uring: complete request via task work in case of DEFER_TASKRUN",
        "kernel_version": "v6.3-rc7",
        "release_date": "2023-04-15 10:29:53 -0700 Merge tag 'io_uring-6.3-2023-04-14' of git://git.kernel.dk/linux"
    },
    {
        "commit": "860e1c7f8b0b43fbf91b4d689adfaa13adb89452",
        "message": "So far io_req_complete_post() only covers DEFER_TASKRUN by completing\nrequest via task work when the request is completed from IOWQ.\n\nHowever, uring command could be completed from any context, and if io\nuring is setup with DEFER_TASKRUN, the command is required to be\ncompleted from current context, otherwise wait on IORING_ENTER_GETEVENTS\ncan't be wakeup, and may hang forever.\n\nThe issue can be observed on removing ublk device, but turns out it is\none generic issue for uring command & DEFER_TASKRUN, so solve it in\nio_uring core code.\n\nFixes: e6aeb2721d3b (\"io_uring: complete all requests in task context\")\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/linux-block/b3fc9991-4c53-9218-a8cc-5b4dd3952108@kernel.dk/\nReported-by: Jens Axboe <axboe@kernel.dk>\nCc: Kanchan Joshi <joshi.k@samsung.com>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc7",
        "release_date": "2023-04-14 06:38:23 -0600 io_uring: complete request via task work in case of DEFER_TASKRUN"
    },
    {
        "commit": "d581076b6a85c6f8308a4ba2bdcd82651f5183df",
        "message": "SCM file accounting is a slow path and is only used for UNIX files.\nExtract a helper out of io_rsrc_file_put() that does the SCM\nunaccounting.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/58cc7bffc2ee96bec8c2b89274a51febcbfa5556.1681210788.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-12 12:09:41 -0600 io_uring/rsrc: extract SCM file put helper"
    },
    {
        "commit": "2933ae6eaa05e8db6ad33a3ca12af18d2a25358c",
        "message": "We use io_rsrc_node_switch() coupled with io_rsrc_node_switch_start()\nfor a bunch of cases including initialising ctx->rsrc_node, i.e. by\npassing NULL instead of rsrc_data. Leave it to only deal with actual\nnode changing.\n\nFor that, first remove it from io_uring_create() and add a function\nallocating the first node. Then also remove all calls to\nio_rsrc_node_switch() from files/buffers register as we already have a\nnode installed and it does essentially nothing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d146fe306ff98b1a5a60c997c252534f03d423d7.1681210788.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-12 12:09:41 -0600 io_uring/rsrc: refactor io_rsrc_node_switch"
    },
    {
        "commit": "13c223962eac16f161cf9b6355209774c609af28",
        "message": "struct io_rsrc_node::rsrc_data field is initialised on rsrc removal and\nshouldn't be used before that, still let's play safe and zero the field\non alloc.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/09bd03cedc8da8a7974c5e6e4bf0489fd16593ab.1681210788.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-12 12:09:41 -0600 io_uring/rsrc: zero node's rsrc data on alloc"
    },
    {
        "commit": "528407b1e0ea51260fff2cc8b669c632a65d7a09",
        "message": "We store one pre-allocated rsrc node in ->rsrc_backup_node, merge it\nwith ->rsrc_node_cache.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6d5410e51ccd29be7a716be045b51d6b371baef6.1681210788.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-12 12:09:41 -0600 io_uring/rsrc: consolidate node caching"
    },
    {
        "commit": "786788a8cfe03056e9c7b1c6e418c1db92a0ce80",
        "message": "Add a lockdep chek to make sure that file and buffer updates hold\n->uring_lock.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/961bbe6e433ec9bc0375127f23468b37b729df99.1681210788.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-12 12:09:41 -0600 io_uring/rsrc: add lockdep checks"
    },
    {
        "commit": "8ce4269eeedc5b31f5817f610b42cba8be8fa9de",
        "message": "We don't post CQEs from the IRQ context, add a check catching that.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f23f7a24dbe8027b3d37873fece2b6488f878b31.1681210788.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-12 12:09:41 -0600 io_uring: add irq lockdep checks"
    },
    {
        "commit": "ceac766a5581e4e671ec8e5236b8fdaed8e4c8c9",
        "message": "The kernel test robot complains about __io_remove_buffers().\n\nio_uring/kbuf.c:221 __io_remove_buffers() warn: variable dereferenced\nbefore check 'bl->buf_ring' (see line 219)\n\nThat check is not needed as ->buf_ring will always be set, so we can\nremove it and so silence the warning.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9a632bbf749d9d911e605255652ce08d18e7d2c6.1681210788.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-12 12:09:41 -0600 io_uring/kbuf: remove extra ->buf_ring null check"
    },
    {
        "commit": "8b1df11f97333d6d8647f1c6c0554eb2d9774396",
        "message": "io_uring/io_uring.c:432 io_prep_async_work() error: we previously\nassumed 'req->file' could be null (see line 425).\n\nEven though it's a false positive as there will not be REQ_F_ISREG set\nwithout a file, let's add a simple check to make the kernel test robot\nhappy. We don't care about performance here, but assumingly it'll be\noptimised out by the compiler.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a6cfbe92c74b789c0b4f046f7f98d19b1ca2e5b7.1681210788.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-12 12:09:41 -0600 io_uring: shut io_prep_async_work warning"
    },
    {
        "commit": "27a67079c0e548d5c3232c40951517cfa630fe51",
        "message": "We know now what the completion context is for the uring_cmd completion\nhandling, so use that to have io_req_task_complete() decide what the\nbest way to complete the request is. This allows batching of the posted\ncompletions if we have multiple pending, rather than always doing them\none-by-one.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-12 12:07:36 -0600 io_uring/uring_cmd: take advantage of completion batching"
    },
    {
        "commit": "d3f05a4c428565163f26b5d34f60f02ee4ea4009",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Just two minor fixes for provided buffers - one where we could\n  potentially leak a buffer, and one where the returned values was\n  off-by-one in some cases\"\n\n* tag 'io_uring-6.3-2023-04-06' of git://git.kernel.dk/linux:\n  io_uring: fix memory leak when removing provided buffers\n  io_uring: fix return value when removing provided buffers",
        "kernel_version": "v6.3-rc6",
        "release_date": "2023-04-08 11:34:17 -0700 Merge tag 'io_uring-6.3-2023-04-06' of git://git.kernel.dk/linux"
    },
    {
        "commit": "360cd42c4e95ff06d8d7b0a54e42236c7e7c187f",
        "message": "Chains of memory accesses are never good for performance.\nThe req->task->io_uring->in_cancel in io_req_local_work_add() is there\nso that when a task is exiting via io_uring_try_cancel_requests() and\nstarts waiting for completions, it gets woken up by every new task_work\nitem queued.\n\nDo a little trick by announcing waiting in io_uring_try_cancel_requests(),\nmaking io_req_local_work_add() wake us up. We also need to check for\ndeferred tw items after prepare_to_wait(TASK_INTERRUPTIBLE);\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/fb11597e9bbcb365901824f8c5c2cf0d6ee100d0.1680782017.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-06 16:24:36 -0600 io_uring: optimise io_req_local_work_add"
    },
    {
        "commit": "c66ae3ec38f946edb1776d25c1c8cd63803b8ec3",
        "message": "Separate ->task_complete path in __io_cq_unlock_post_flush().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/baa9b8d822f024e4ee01c40209dbbe38d9c8c11d.1680782017.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-06 16:23:28 -0600 io_uring: refactor __io_cq_unlock_post_flush()"
    },
    {
        "commit": "8751d15426a31baaf40f7570263c27c3e5d1dc44",
        "message": "Every task_work will try to wake the task to be executed, which causes\nexcessive scheduling and additional overhead. For some tw it's\njustified, but others won't do much but post a single CQE.\n\nWhen a task waits for multiple cqes, every such task_work will wake it\nup. Instead, the task may give a hint about how many cqes it waits for,\nio_req_local_work_add() will compare against it and skip wake ups\nif #cqes + #tw is not enough to satisfy the waiting condition. Task_work\nthat uses the optimisation should be simple enough and never post more\nthan one CQE. It's also ignored for non DEFER_TASKRUN rings.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d2b77e99d1e86624d8a69f7037d764b739dcd225.1680782017.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-06 16:23:28 -0600 io_uring: reduce scheduling due to tw"
    },
    {
        "commit": "5150940079a3ce94d7474f6f5b0d6276569dc1de",
        "message": "We'll need to grab some information from the previous request in the tw\nlist, inline llist_add(), it'll be used in the following patch.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f0165493af7b379943c792114b972f331e7d7d10.1680782017.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-06 16:23:28 -0600 io_uring: inline llist_add()"
    },
    {
        "commit": "8501fe70ae9855076ffb03a3670e02a7b3437304",
        "message": "We pass 'allow_local' into io_req_task_work_add() but will need more\nflags. Replace it with a flags bit field and name this allow_local\nflag.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4c0f01e7ef4e6feebfb199093cc995af7a19befa.1680782017.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-06 16:23:28 -0600 io_uring: add tw add flags"
    },
    {
        "commit": "6e7248adf8f7adb5e36ec1e91efcc85a83bf8aeb",
        "message": "Instead of smp_mb() + __io_cqring_wake() in __io_cq_unlock_post_flush()\nuse equivalent io_cqring_wake(). With that we can clean it up further\nand remove __io_cqring_wake().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/662ee5d898168ac206be06038525e97b64072a46.1680782017.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-06 16:23:28 -0600 io_uring: refactor io_cqring_wake()"
    },
    {
        "commit": "d73a572df24661851465c821d33c03e70e4b68e5",
        "message": "We currently pin the ctx for io_req_local_work_add() with\npercpu_ref_get/put, which implies two rcu_read_lock/unlock pairs and some\nextra overhead on top in the fast path. Replace it with a pure rcu read\nand let io_ring_exit_work() synchronise against it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/cbdfcb6b232627f30e9e50ef91f13c4f05910247.1680782017.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-06 16:23:10 -0600 io_uring: optimize local tw add ctx pinning"
    },
    {
        "commit": "ab1c590f5c9b96d8d8843d351aed72469f8f2ef0",
        "message": "Move ctx pinning from io_req_local_work_add() to the caller, looks\nbetter and makes working with the code a bit easier.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/49c0dbed390b0d6d04cb942dd3592879fd5bfb1b.1680782017.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-06 16:22:07 -0600 io_uring: move pinning out of io_req_local_work_add"
    },
    {
        "commit": "1d1665279a845d16c93687389e364386e3fe0f38",
        "message": "block size is one very key setting for block layer, and bad block size\ncould panic kernel easily.\n\nMake sure that block size is set correctly.\n\nMeantime if ublk_validate_params() fails, clear ub->params so that disk\nis prevented from being added.\n\nFixes: 71f28f3136af (\"ublk_drv: add io_uring based userspace block driver\")\nReported-and-tested-by: Breno Leitao <leitao@debian.org>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc6",
        "release_date": "2023-04-06 08:12:08 -0600 block: ublk: make sure that block size is set correctly"
    },
    {
        "commit": "e07fec475cc86ce6ded82908df1d511edc3303b7",
        "message": "iov_iter for ep_read_iter can be ITER_UBUF with io_uring.\nIn that case dup_iter() does not have to allocate iov and it can\nreturn NULL. Fix the assumption by checking for iter_is_ubuf()\nother wise ep_read_iter can treat this as failure and return -ENOMEM.\n\nFixes: 1e23db450cff (\"io_uring: use iter_ubuf for single range imports\")\nSigned-off-by: Sandeep Dhavale <dhavale@google.com>\nAcked-by: Jens Axboe <axboe@kernel.dk>\nLink: https://lore.kernel.org/r/20230401060509.3608259-3-dhavale@google.com\nSigned-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>",
        "kernel_version": "v6.3-rc6",
        "release_date": "2023-04-05 19:24:01 +0200 usb: gadgetfs: Fix ep_read_iter to handle ITER_UBUF"
    },
    {
        "commit": "d356b3cdd00cae4508be566a47c0cfb74e14862a",
        "message": "iov_iter for ffs_epfile_read_iter can be ITER_UBUF with io_uring.\nIn that case dup_iter() does not have to allocate anything and it\ncan return NULL. ffs_epfile_read_iter treats this as a failure and\nreturns -ENOMEM. Fix it by checking if iter_is_ubuf().\n\nFixes: 1e23db450cff (\"io_uring: use iter_ubuf for single range imports\")\nSigned-off-by: Sandeep Dhavale <dhavale@google.com>\nAcked-by: Jens Axboe <axboe@kernel.dk>\nLink: https://lore.kernel.org/r/20230401060509.3608259-2-dhavale@google.com\nSigned-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>",
        "kernel_version": "v6.3-rc6",
        "release_date": "2023-04-05 19:24:01 +0200 usb: gadget: f_fs: Fix ffs_epfile_read_iter to handle ITER_UBUF"
    },
    {
        "commit": "758d5d64b619ddbbf96a5605d8d5a919aafaafab",
        "message": "Rather than check this in the fast path issue, it makes more sense to\njust assign the copy of the data when we're setting it up anyway. This\nmakes the code a bit cleaner, and removes the need for this check in\nthe issue path.\n\nReviewed-by: Gabriel Krisman Bertazi <krisman@suse.de>\nReviewed-by: Keith Busch <kbusch@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-05 09:30:18 -0600 io_uring/uring_cmd: assign ioucmd->cmd at async prep time"
    },
    {
        "commit": "69bbc6ade9d9d4e3c556cb83e77b6f3cd9ad3d18",
        "message": "The number of entries in the rsrc node cache is limited to 512, which\nstill seems unnecessarily large. Add per cache thresholds and set to\nto 32 for the rsrc node cache.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d0cd538b944dac0bf878e276fc0199f21e6bccea.1680576071.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-04 09:30:39 -0600 io_uring/rsrc: add custom limit for node caching"
    },
    {
        "commit": "757ef4682b6aa29fdf752ad47f0d63eb48b261cf",
        "message": "Every struct io_rsrc_node takes a struct io_rsrc_data reference, which\nmeans all rsrc updates do 2 extra atomics. Replace atomics refcounting\nwith a int as it's all done under ->uring_lock.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e73c3d6820cf679532696d790b5b8fae23537213.1680576071.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-04 09:30:39 -0600 io_uring/rsrc: optimise io_rsrc_data refcounting"
    },
    {
        "commit": "1f2c8f610aa6c6a3dc3523f93eaf28c25051df6f",
        "message": "We should hold ->uring_lock while putting nodes with io_put_rsrc_node(),\nadd a lockdep check for that.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b50d5f156ac41450029796738c1dfd22a521df7a.1680576071.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-04 09:30:39 -0600 io_uring/rsrc: add lockdep sanity checks"
    },
    {
        "commit": "9eae8655f9cd2eeed99fb7a0d2bb22816c17e497",
        "message": "Add allocation cache for struct io_rsrc_node, it's always allocated and\nput under ->uring_lock, so it doesn't need any extra synchronisation\naround caches.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/252a9d9ef9654e6467af30fdc02f57c0118fb76e.1680576071.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-04 09:30:39 -0600 io_uring/rsrc: cache struct io_rsrc_node"
    },
    {
        "commit": "36b9818a5a84cb7c977fb723babca1c8d74f288f",
        "message": "struct delayed_work rsrc_put_work was previously used to offload node\nfreeing because io_rsrc_node_ref_zero() was previously called by RCU in\nthe IRQ context. Now, as percpu refcounting is gone, we can do it\neagerly at the spot without pushing it to a worker.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/13fb1aac1e8d068ad8fd4a0c6d0d157ab61b90c0.1680576071.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-04 09:30:39 -0600 io_uring/rsrc: don't offload node free"
    },
    {
        "commit": "ff7c75ecaa9e6b251f76c24e289d4bfe413ffe31",
        "message": "Every io_rsrc_node keeps a list of items to put, and all entries are\nkmalloc()'ed. However, it's quite often to queue up only one entry per\nnode, so let's add an inline entry there to avoid extra allocations.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c482c1c652c45c85ac52e67c974bc758a50fed5f.1680576071.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-04 09:30:39 -0600 io_uring/rsrc: optimise io_rsrc_put allocation"
    },
    {
        "commit": "c824986c113f15e2ef2c00da9a226c09ecaac74c",
        "message": "We have too many \"rsrc\" around which makes the name of struct\nio_rsrc_node::rsrc_list confusing. The field is responsible for keeping\na list of files or buffers, so call it item_list and add comments\naround.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3e34d4dfc1fdbb6b520f904ee6187c2ccf680efe.1680576071.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-04 09:30:39 -0600 io_uring/rsrc: rename rsrc_list"
    },
    {
        "commit": "0a4813b1abdf06e44ce60cdebfd374cfd27c46bf",
        "message": "We use ->rsrc_ref_lock spinlock to protect ->rsrc_ref_list in\nio_rsrc_node_ref_zero(). Now we removed pcpu refcounting, which means\nio_rsrc_node_ref_zero() is not executed from the irq context as an RCU\ncallback anymore, and we also put it under ->uring_lock.\nio_rsrc_node_switch(), which queues up nodes into the list, is also\nprotected by ->uring_lock, so we can safely get rid of ->rsrc_ref_lock.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6b60af883c263551190b526a55ff2c9d5ae07141.1680576071.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-04 09:30:39 -0600 io_uring/rsrc: kill rsrc_ref_lock"
    },
    {
        "commit": "ef8ae64ffa9578c12e44de42604004c2cc3e9c27",
        "message": "Currently, for nodes we have an atomic counter and some cached\n(non-atomic) refs protected by uring_lock. Let's put all ref\nmanipulations under uring_lock and get rid of the atomic part.\nIt's free as in all cases we care about we already hold the lock.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/25b142feed7d831008257d90c8b17c0115d4fc15.1680576071.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-04 09:30:39 -0600 io_uring/rsrc: protect node refs with uring_lock"
    },
    {
        "commit": "03adabe81abb20221079b48343783b4327bd1186",
        "message": "io_free_req() is not often used but nevertheless problematic as there is\nno way to know the current context, it may be used from the submission\npath or even by an irq handler. Push it to a fresh context using\ntask_work.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3a92fe80bb068757e51aaa0b105cfbe8f5dfee9e.1680576071.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-04 09:30:39 -0600 io_uring: io_free_req() via tw"
    },
    {
        "commit": "2ad4c6d08018e4eec130c29992028dc356ab2181",
        "message": "io_req_put_rsrc() doesn't need any locking, so move it out of\na spinlock section in __io_req_complete_post() and adjust helpers.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d5b87a5f31270dade6805f7acafc4cc34b84b241.1680576071.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-04 09:30:39 -0600 io_uring: don't put nodes under spinlocks"
    },
    {
        "commit": "8e15c0e71b8ae64fb7163532860f8d608165281f",
        "message": "We cache refs of the current node (i.e. ctx->rsrc_node) in\nctx->rsrc_cached_refs. We'll be moving away from atomics, so move the\ncached refs in struct io_rsrc_node for now. It's a prep patch and\nshouldn't change anything in practise.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9edc3669c1d71b06c2dca78b2b2b8bb9292738b9.1680576071.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-04 09:30:39 -0600 io_uring/rsrc: keep cached refs per node"
    },
    {
        "commit": "b8fb5b4fdd67f9d18109c5d21d44a8bd4ddb608b",
        "message": "One problem with the current rsrc infra is that often updates will\ngenerates lots of rsrc nodes, each carry pcpu refs. That takes quite a\nlot of memory, especially if there is a stall, and takes lots of CPU\ncycles. Only pcpu allocations takes >50 of CPU with a naive benchmark\nupdating files in a loop.\n\nReplace pcpu refs with normal refcounting. There is already a hot path\navoiding atomics / refs, but following patches will further improve it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e9ed8a9457b331a26555ff9443afc64cdaab7247.1680576071.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-04 09:30:39 -0600 io_uring/rsrc: use non-pcpu refcounts for nodes"
    },
    {
        "commit": "e3ef728ff07b42668e7e12f49cd2f9055e064ec1",
        "message": "We already do this manually for the !SQPOLL case, do it in general and\nwe can also dump the ugly min3() in io_submit_sqes().\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:16:15 -0600 io_uring: cap io_sqring_entries() at SQ ring size"
    },
    {
        "commit": "2ad57931db641f3de627023afb8147a8ec0b41dc",
        "message": "It has nothing to do with the SQE at this point, it's a request\nsubmission. While in there, get rid of the 'force_nonblock' argument\nwhich is also dead, as we only pass in true.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:16:15 -0600 io_uring: rename trace_io_uring_submit_sqe() tracepoint"
    },
    {
        "commit": "a282967c848fb1d92c28334430c472da9c334e54",
        "message": "For task works we're passing around a bool pointer for whether the\ncurrent ring is locked or not, let's wrap it in a structure, that\nwill make it more opaque preventing abuse and will also help us\nto pass more info in the future if needed.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1ecec9483d58696e248d1bfd52cf62b04442df1d.1679931367.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:16:15 -0600 io_uring: encapsulate task_work state"
    },
    {
        "commit": "13bfa6f15d0b39254937076ab0557da6875bb455",
        "message": "Before cond_resched()'ing in handle_tw_list() we also drop the current\nring context, and so the next loop iteration will need to pick/pin a new\ncontext and do trylock.\n\nThe chunk removed by this patch was intended to be an optimisation\ncovering exactly this case, i.e. retaking the lock after reschedule, but\nin reality it's skipped for the first iteration after resched as\ndescribed and will keep hammering the lock if it's contended.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1ecec9483d58696e248d1bfd52cf62b04442df1d.1679931367.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:16:15 -0600 io_uring: remove extra tw trylocks"
    },
    {
        "commit": "07d99096e1635805fb7c60382dc12554886a39b8",
        "message": "Since the move to PF_IO_WORKER, we don't juggle memory context manually\nanymore. Remove that outdated part of the comment for __io_worker_idle().\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:16:15 -0600 io_uring/io-wq: drop outdated comment"
    },
    {
        "commit": "d322818ef4c752d79cd667474418691237aa9ccf",
        "message": "There are two leftover structures from the notification registration\nmechanism that has never been released, kill them.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f05f65aebaf8b1b5bf28519a8fdb350e3e7c9ad0.1679924536.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:16:15 -0600 io_uring: kill unused notif declarations"
    },
    {
        "commit": "eb47943f2238bf3a002128d897f18abb143612d3",
        "message": "Since commit 0654b05e7e65 (\"io_uring: One wqe per wq\"), we have just a\nsingle io_wqe instance embedded per io_wq.  Drop the extra structure in\nfavor of accessing struct io_wq directly, cleaning up quite a bit of\ndereferences and backpointers.\n\nNo functional changes intended.  Tested with liburing's testsuite\nand mmtests performance microbenchmarks.  I didn't observe any\nperformance regressions.\n\nSigned-off-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20230322011628.23359-2-krisman@suse.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:16:15 -0600 io-wq: Drop struct io_wqe"
    },
    {
        "commit": "fcb46c0ccc7c07af54f818fd498e461353ea50e7",
        "message": "On at least parisc, we have strict requirements on how we virtually map\nan address that is shared between the application and the kernel. On\nthese platforms, IOU_PBUF_RING_MMAP should be used when setting up a\nshared ring buffer for provided buffers. If the application is mapping\nthese pages and asking the kernel to pin+map them as well, then we have\nno control over what virtual address we get in the kernel.\n\nFor that case, do a sanity check if SHM_COLOUR is defined, and disallow\nthe mapping request. The application must fall back to using\nIOU_PBUF_RING_MMAP for this case, and liburing will do that transparently\nwith the set of helpers that it has.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:16:14 -0600 io_uring/kbuf: disallow mapping a badly aligned provided ring buffer"
    },
    {
        "commit": "e1fe7ee885dc0712e982ee465d9f8b96254c30c1",
        "message": "Add support for KASAN in the alloc_caches (apoll and netmsg_cache).\nThus, if something touches the unused caches, it will raise a KASAN\nwarning/exception.\n\nIt poisons the object when the object is put to the cache, and unpoisons\nit when the object is gotten or freed.\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nReviewed-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20230223164353.2839177-2-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:16:14 -0600 io_uring: Add KASAN support for alloc_caches"
    },
    {
        "commit": "efba1a9e653e107577a48157b5424878c46f2285",
        "message": "Having cache entries linked using the hlist format brings no benefit, and\nalso requires an unnecessary extra pointer address per cache entry.\n\nUse the internal io_wq_work_node single-linked list for the internal\nalloc caches (async_msghdr and async_poll)\n\nThis is required to be able to use KASAN on cache entries, since we do\nnot need to touch unused (and poisoned) cache entries when adding more\nentries to the list.\n\nSuggested-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Breno Leitao <leitao@debian.org>\nLink: https://lore.kernel.org/r/20230223164353.2839177-2-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:16:12 -0600 io_uring: Move from hlist to io_wq_work_node"
    },
    {
        "commit": "da64d6db3bd304d44d7ac1eb7f319a1cc7efd611",
        "message": "Right now io_wq allocates one io_wqe per NUMA node.  As io_wq is now\nbound to a task, the task basically uses only the NUMA local io_wqe, and\nalmost never changes NUMA nodes, thus, the other wqes are mostly\nunused.\n\nAllocate just one io_wqe embedded into io_wq, and uses all possible cpus\n(cpu_possible_mask) in the io_wqe->cpumask.\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nLink: https://lore.kernel.org/r/20230310201107.4020580-1-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:14:21 -0600 io_uring: One wqe per wq"
    },
    {
        "commit": "c56e022c0a27142b7b59ae6bdf45f86bf4b298a1",
        "message": "The ring mapped provided buffer rings rely on the application allocating\nthe memory for the ring, and then the kernel will map it. This generally\nworks fine, but runs into issues on some architectures where we need\nto be able to ensure that the kernel and application virtual address for\nthe ring play nicely together. This at least impacts architectures that\nset SHM_COLOUR, but potentially also anyone setting SHMLBA.\n\nTo use this variant of ring provided buffers, the application need not\nallocate any memory for the ring. Instead the kernel will do so, and\nthe allocation must subsequently call mmap(2) on the ring with the\noffset set to:\n\n\tIORING_OFF_PBUF_RING | (bgid << IORING_OFF_PBUF_SHIFT)\n\nto get a virtual address for the buffer ring. Normally the application\nwould allocate a suitable piece of memory (and correctly aligned) and\nsimply pass that in via io_uring_buf_reg.ring_addr and the kernel would\nmap it.\n\nOutside of the setup differences, the kernel allocate + user mapped\nprovided buffer ring works exactly the same.\n\nAcked-by: Helge Deller <deller@gmx.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:14:21 -0600 io_uring: add support for user mapped provided buffer ring"
    },
    {
        "commit": "81cf17cd3ab3e5441e876a8e9e9c38ae9920cecb",
        "message": "In preparation for allowing flags to be set for registration, rename\nthe padding and use it for that.\n\nAcked-by: Helge Deller <deller@gmx.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:14:21 -0600 io_uring/kbuf: rename struct io_uring_buf_reg 'pad' to'flags'"
    },
    {
        "commit": "25a2c188a0a00b3d9f2057798aa86fe6b04377bf",
        "message": "Rather than rely on checking buffer_list->buf_pages or ->buf_nr_pages,\nadd a separate member that tracks if this is a ring mapped provided\nbuffer list or not.\n\nAcked-by: Helge Deller <deller@gmx.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:14:20 -0600 io_uring/kbuf: add buffer_list->is_mapped member"
    },
    {
        "commit": "ba56b63242d12df088ed9a701cad320e6b306dfe",
        "message": "In preparation for allowing the kernel to allocate the provided buffer\nrings and have the application mmap it instead, abstract out the\ncurrent method of pinning and mapping the user allocated ring.\n\nNo functional changes intended in this patch.\n\nAcked-by: Helge Deller <deller@gmx.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:14:20 -0600 io_uring/kbuf: move pinning of provided buffer ring into helper"
    },
    {
        "commit": "d808459b2e31bd5123a14258a7a529995db974c8",
        "message": "Some architectures have memory cache aliasing requirements (e.g. parisc)\nif memory is shared between userspace and kernel. This patch fixes the\nkernel to return an aliased address when asked by userspace via mmap().\n\nSigned-off-by: Helge Deller <deller@gmx.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:14:20 -0600 io_uring: Adjust mapping wrt architecture aliasing requirements"
    },
    {
        "commit": "d4755e15386c38e4ae532ace5acc29fbfaee42e7",
        "message": "io_uring hashes writes to a given file/inode so that it can serialize\nthem. This is useful if the file system needs exclusive access to the\nfile to perform the write, as otherwise we end up with a ton of io-wq\nthreads trying to lock the inode at the same time. This can cause\nexcessive system time.\n\nBut if the file system has flagged that it supports parallel O_DIRECT\nwrites, then there's no need to serialize the writes. Check for that\nthrough FMODE_DIO_PARALLEL_WRITE and don't hash it if we don't need to.\n\nIn a basic test of 8 threads writing to a file on XFS on a gen2 Optane,\nwith each thread writing in 4k chunks, it improves performance from\n~1350K IOPS (or ~5290MiB/sec) to ~1410K IOPS (or ~5500MiB/sec).\n\nReviewed-by: Darrick J. Wong <djwong@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:14:20 -0600 io_uring: avoid hashing O_DIRECT writes if the filesystem doesn't need it"
    },
    {
        "commit": "d8aeb44a9ae324c4b823689fabb30b6621d93c88",
        "message": "Some filesystems support multiple threads writing to the same file with\nO_DIRECT without requiring exclusive access to it. io_uring can use this\nhint to avoid serializing dio writes to this inode, instead allowing them\nto run in parallel.\n\nXFS and ext4 both fall into this category, so set the flag for both of\nthem.\n\nReviewed-by: Darrick J. Wong <djwong@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-03 07:14:20 -0600 fs: add FMODE_DIO_PARALLEL_WRITE flag"
    },
    {
        "commit": "dc70c9615c067dbc34a1af736477f7d2b7f75319",
        "message": "John Garry <john.g.garry@oracle.com> says:\n\nIt's easy to get scsi_debug to error on throughput testing when we have\nmultiple shosts:\n\n$ lsscsi\n[7:0:0:0]       disk    Linux   scsi_debug      0191\n[0:0:0:0]       disk    Linux   scsi_debug      0191\n\n$ fio --filename=/dev/sda --filename=/dev/sdb --direct=1 --rw=read\n--bs=4k --iodepth=256 --runtime=60 --numjobs=40 --time_based --name=jpg\n--eta-newline=1 --readonly --ioengine=io_uring --hipri --exitall_on_error\njpg: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256\n...\nfio-3.28\nStarting 40 processes\n[   27.521809] hrtimer: interrupt took 33067 ns\n[   27.904660] sd 7:0:0:0: [sdb] tag#171 FAILED Result: hostbyte=DID_ABORT driverbyte=DRIVER_OK cmd_age=0s\n[   27.904660] sd 0:0:0:0: [sda] tag#58 FAILED Result: hostbyte=DID_ABORT driverbyte=DRIVER_OK cmd_age=0s\nfio: io_u error [   27.904667] sd 0:0:0:0: [sda] tag#58 CDB: Read(10) 28 00 00 00 27 00 00 01 18 00\non file /dev/sda[   27.904670] sd 0:0:0:0: [sda] tag#62 FAILED Result: hostbyte=DID_ABORT driverbyte=DRIVER_OK cmd_age=0s\n\nThe issue is related to how the driver manages submit queues and tags. A\nsingle array of submit queues - sdebug_q_arr - with its own set of tags is\nshared among all shosts. As such, for occasions when we have more than one\nhost it is possible to overload the submit queues and run out of tags.\n\nAnother separate issue that we may reduce the shost submit queue depth,\nsdebug_max_queue, dynamically causing the shost to be overloaded. How many\nIOs which the shost may be sent is fixed at can_queue at init time, which\nis the same initial value for sdebug_max_queue. So reducing\nsdebug_max_queue means that the shost may be sent more IOs than it is\nconfigured to handle, causing overloading.\n\nThis series removes the scsi_debug submit queue concept and uses\npre-existing APIs to manage and examine tags, like scsi_block_requests()\nand blk_mq_tagset_busy_iter(). Using standard APIs makes the driver more\nmaintainable and extensible in future.\n\nA restriction is also added to allow sdebug_max_queue only be modified when\nno shosts are present, i.e. we need to remove shosts, modify\nsdebug_max_queue, and then re-add the shosts.\n\nLink: https://lore.kernel.org/r/20230327074310.1862889-1-john.g.garry@oracle.com\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-02 22:10:40 -0400 Merge patch series \"Fix shost command overloading issues\""
    },
    {
        "commit": "f1437cd1e535c5d5cc9f6e5bfdfc9b1cd3141bc4",
        "message": "It's easy to get scsi_debug to error on throughput testing when we have\nmultiple shosts:\n\n$ lsscsi\n[7:0:0:0]       disk    Linux   scsi_debug      0191\n[0:0:0:0]       disk    Linux   scsi_debug      0191\n\n$ fio --filename=/dev/sda --filename=/dev/sdb --direct=1 --rw=read --bs=4k\n--iodepth=256 --runtime=60 --numjobs=40 --time_based --name=jpg\n--eta-newline=1 --readonly --ioengine=io_uring --hipri --exitall_on_error\njpg: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256\n...\nfio-3.28\nStarting 40 processes\n[   27.521809] hrtimer: interrupt took 33067 ns\n[   27.904660] sd 7:0:0:0: [sdb] tag#171 FAILED Result: hostbyte=DID_ABORT driverbyte=DRIVER_OK cmd_age=0s\n[   27.904660] sd 0:0:0:0: [sda] tag#58 FAILED Result: hostbyte=DID_ABORT driverbyte=DRIVER_OK cmd_age=0s\nfio: io_u error [   27.904667] sd 0:0:0:0: [sda] tag#58 CDB: Read(10) 28 00 00 00 27 00 00 01 18 00\non file /dev/sda[   27.904670] sd 0:0:0:0: [sda] tag#62 FAILED Result: hostbyte=DID_ABORT driverbyte=DRIVER_OK cmd_age=0s\n\nThe issue is related to how the driver manages submit queues and tags. A\nsingle array of submit queues - sdebug_q_arr - with its own set of tags is\nshared among all shosts. As such, for occasions when we have more than one\nshost it is possible to overload the submit queues and run out of tags.\n\nThe struct sdebug_queue is to manage tags and hold the associated\nqueued command entry pointer (for that tag).\n\nSince the tagset iters are now used for functions like\nsdebug_blk_mq_poll(), there is no need to manage these queues. Indeed,\nblk-mq already provides what we need for managing tags and queues.\n\nDrop sdebug_queue and all its usage in the driver.\n\nSigned-off-by: John Garry <john.g.garry@oracle.com>\nLink: https://lore.kernel.org/r/20230327074310.1862889-12-john.g.garry@oracle.com\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-02 22:09:22 -0400 scsi: scsi_debug: Drop sdebug_queue"
    },
    {
        "commit": "57f7225a4fe25425c29402adad990c7409958c40",
        "message": "The shost->can_queue value is initially used to set per-HW queue context\ntag depth in the block layer. This ensures that the shost is not sent too\nmany commands which it can deal with. However lowering sdebug_max_queue\nseparately means that we can easily overload the shost, as in the following\nexample:\n\n$ cat /sys/bus/pseudo/drivers/scsi_debug/max_queue\n192\n$ cat /sys/class/scsi_host/host0/can_queue\n192\n$ echo 100 > /sys/bus/pseudo/drivers/scsi_debug/max_queue\n$ cat /sys/class/scsi_host/host0/can_queue\n192\n$ fio --filename=/dev/sda --direct=1 --rw=read --bs=4k --iodepth=256\n--runtime=1200 --numjobs=10 --time_based --group_reporting\n--name=iops-test-job --eta-newline=1 --readonly    --ioengine=io_uring\n--hipri --exitall_on_error\niops-test-job: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256\n...\nfio-3.28\nStarting 10 processes\n[  111.269885] scsi_io_completion_action: 400 callbacks suppressed\n[  111.269885] blk_print_req_error: 400 callbacks suppressed\n[  111.269889] I/O error, dev sda, sector 440 op 0x0:(READ) flags 0x1200000 phys_seg 1 prio class 2\n[  111.269892] sd 0:0:0:0: [sda] tag#132 FAILED Result: hostbyte=DID_ABORT driverbyte=DRIVER_OK cmd_age=0s\n[  111.269897] sd 0:0:0:0: [sda] tag#132 CDB: Read(10) 28 00 00 00 01 68 00 00 08 00\n[  111.277058] I/O error, dev sda, sector 360 op 0x0:(READ) flags 0x1200000 phys_seg 1 prio class 2\n\n[...]\n\nEnsure that this cannot happen by allowing sdebug_max_queue be modified\nonly when we have no shosts. As such, any shost->can_queue value will match\nsdebug_max_queue, and sdebug_max_queue cannot be modified separately.\n\nSince retired_max_queue is no longer set, remove support.\n\nContinue to apply the restriction that sdebug_host_max_queue cannot be\nmodified when sdebug_host_max_queue is set. Adding support for that would\nmean extra code, and no one has complained about this restriction\npreviously.\n\nA command like the following may be used to remove a shost:\necho -1 > /sys/bus/pseudo/drivers/scsi_debug/add_host\n\nSigned-off-by: John Garry <john.g.garry@oracle.com>\nLink: https://lore.kernel.org/r/20230327074310.1862889-11-john.g.garry@oracle.com\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-04-02 22:09:22 -0400 scsi: scsi_debug: Only allow sdebug_max_queue be modified when no shosts"
    },
    {
        "commit": "b4a72c0589fdea6259720375426179888969d6a2",
        "message": "When removing provided buffers, io_buffer structs are not being disposed\nof, leading to a memory leak. They can't be freed individually, because\nthey are allocated in page-sized groups. They need to be added to some\nfree list instead, such as io_buffers_cache. All callers already hold\nthe lock protecting it, apart from when destroying buffers, so had to\nextend the lock there.\n\nFixes: cc3cec8367cb (\"io_uring: speedup provided buffer handling\")\nSigned-off-by: Wojciech Lukowicz <wlukowicz01@gmail.com>\nLink: https://lore.kernel.org/r/20230401195039.404909-2-wlukowicz01@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc6",
        "release_date": "2023-04-01 16:52:12 -0600 io_uring: fix memory leak when removing provided buffers"
    },
    {
        "commit": "c0921e51dab767ef5adf6175c4a0ba3c6e1074a3",
        "message": "When a request to remove buffers is submitted, and the given number to be\nremoved is larger than available in the specified buffer group, the\nresulting CQE result will be the number of removed buffers + 1, which is\n1 more than it should be.\n\nPreviously, the head was part of the list and it got removed after the\nloop, so the increment was needed. Now, the head is not an element of\nthe list, so the increment shouldn't be there anymore.\n\nFixes: dbc7d452e7cf (\"io_uring: manage provided buffers strictly ordered\")\nSigned-off-by: Wojciech Lukowicz <wlukowicz01@gmail.com>\nLink: https://lore.kernel.org/r/20230401195039.404909-2-wlukowicz01@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc6",
        "release_date": "2023-04-01 16:52:12 -0600 io_uring: fix return value when removing provided buffers"
    },
    {
        "commit": "f3fa7f026e5faf10f730b0655b2f96f86d3c7dd8",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix a regression with the poll retry, introduced in this merge window\n   (me)\n\n - Fix a regression with the alloc cache not decrementing the member\n   count on removal. Also a regression from this merge window (Pavel)\n\n - Fix race around rsrc node grabbing (Pavel)\n\n* tag 'io_uring-6.3-2023-03-30' of git://git.kernel.dk/linux:\n  io_uring: fix poll/netmsg alloc caches\n  io_uring/rsrc: fix rogue rsrc node grabbing\n  io_uring/poll: clear single/double poll flags on poll arming",
        "kernel_version": "v6.3-rc5",
        "release_date": "2023-03-31 12:30:13 -0700 Merge tag 'io_uring-6.3-2023-03-30' of git://git.kernel.dk/linux"
    },
    {
        "commit": "fd30d1cdcc4ff405fc54765edf2e11b03f2ed4f3",
        "message": "We increase cache->nr_cached when we free into the cache but don't\ndecrease when we take from it, so in some time we'll get an empty\ncache with cache->nr_cached larger than IO_ALLOC_CACHE_MAX, that fails\nio_alloc_cache_put() and effectively disables caching.\n\nFixes: 9b797a37c4bd8 (\"io_uring: add abstraction around apoll cache\")\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc5",
        "release_date": "2023-03-30 06:53:42 -0600 io_uring: fix poll/netmsg alloc caches"
    },
    {
        "commit": "4ff0b50de8cabba055efe50bbcb7506c41a69835",
        "message": "We should not be looking at ctx->rsrc_node and anyhow modifying the node\nwithout holding uring_lock, grabbing references in such a way is not\nsafe either.\n\nCc: stable@vger.kernel.org\nFixes: 5106dd6e74ab6 (\"io_uring: propagate issue_flags state down to file assignment\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1202ede2d7bb90136e3482b2b84aad9ed483e5d6.1680098433.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc5",
        "release_date": "2023-03-29 09:23:46 -0600 io_uring/rsrc: fix rogue rsrc node grabbing"
    },
    {
        "commit": "005308f7bdacf5685ed1a431244a183dbbb9e0e8",
        "message": "Unless we have at least one entry queued, then don't call into\nio_poll_remove_entries(). Normally this isn't possible, but if we\nretry poll then we can have ->nr_entries cleared again as we're\nsetting it up. If this happens for a poll retry, then we'll still have\nat least REQ_F_SINGLE_POLL set. io_poll_remove_entries() then thinks\nit has entries to remove.\n\nClear REQ_F_SINGLE_POLL and REQ_F_DOUBLE_POLL unconditionally when\narming a poll request.\n\nFixes: c16bda37594f (\"io_uring/poll: allow some retries for poll triggering spuriously\")\nCc: stable@vger.kernel.org\nReported-by: Pengfei Xu <pengfei.xu@intel.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc5",
        "release_date": "2023-03-28 07:09:01 -0600 io_uring/poll: clear single/double poll flags on poll arming"
    },
    {
        "commit": "83511470af1a734ec47c37957612d2eecf1a2352",
        "message": "Pull block fixes from Jens Axboe:\n\n - NVMe pull request via Christoph:\n     - Send Identify with CNS 06h only to I/O controllers (Martin\n       George)\n     - Fix nvme_tcp_term_pdu to match spec (Caleb Sander)\n\n - Pass in issue_flags for uring_cmd, so the end_io handlers don't need\n   to assume what the right context is (me)\n\n - Fix for ublk, marking it as LIVE before adding it to avoid races on\n   the initial IO (Ming)\n\n* tag 'block-6.3-2023-03-24' of git://git.kernel.dk/linux:\n  nvme-tcp: fix nvme_tcp_term_pdu to match spec\n  nvme: send Identify with CNS 06h only to I/O controllers\n  block/io_uring: pass in issue_flags for uring_cmd task_work handling\n  block: ublk_drv: mark device as LIVE before adding disk",
        "kernel_version": "v6.3-rc4",
        "release_date": "2023-03-24 14:10:39 -0700 Merge tag 'block-6.3-2023-03-24' of git://git.kernel.dk/linux"
    },
    {
        "commit": "e344eb7be2a25c66a8bac5d4388f1a4583450a72",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix an issue with repeated -ECONNREFUSED on a socket (me)\n\n - Fix a NULL pointer deference due to a stale lookup cache for\n   allocating direct descriptors (Savino)\n\n* tag 'io_uring-6.3-2023-03-24' of git://git.kernel.dk/linux:\n  io_uring/rsrc: fix null-ptr-deref in io_file_bitmap_get()\n  io_uring/net: avoid sending -ECONNABORTED on repeated connection requests",
        "kernel_version": "v6.3-rc4",
        "release_date": "2023-03-24 14:01:01 -0700 Merge tag 'io_uring-6.3-2023-03-24' of git://git.kernel.dk/linux"
    },
    {
        "commit": "02a4d923e4400a36d340ea12d8058f69ebf3a383",
        "message": "When fixed files are unregistered, file_alloc_end and alloc_hint\nare not cleared. This can later cause a NULL pointer dereference in\nio_file_bitmap_get() if auto index selection is enabled via\nIORING_FILE_INDEX_ALLOC:\n\n[    6.519129] BUG: kernel NULL pointer dereference, address: 0000000000000000\n[...]\n[    6.541468] RIP: 0010:_find_next_zero_bit+0x1a/0x70\n[...]\n[    6.560906] Call Trace:\n[    6.561322]  <TASK>\n[    6.561672]  io_file_bitmap_get+0x38/0x60\n[    6.562281]  io_fixed_fd_install+0x63/0xb0\n[    6.562851]  ? __pfx_io_socket+0x10/0x10\n[    6.563396]  io_socket+0x93/0xf0\n[    6.563855]  ? __pfx_io_socket+0x10/0x10\n[    6.564411]  io_issue_sqe+0x5b/0x3d0\n[    6.564914]  io_submit_sqes+0x1de/0x650\n[    6.565452]  __do_sys_io_uring_enter+0x4fc/0xb20\n[    6.566083]  ? __do_sys_io_uring_register+0x11e/0xd80\n[    6.566779]  do_syscall_64+0x3c/0x90\n[    6.567247]  entry_SYSCALL_64_after_hwframe+0x72/0xdc\n[...]\n\nTo fix the issue, set file alloc range and alloc_hint to zero after\nfile tables are freed.\n\nCc: stable@vger.kernel.org\nFixes: 4278a0deb1f6 (\"io_uring: defer alloc_hint update to io_file_bitmap_set()\")\nSigned-off-by: Savino Dicanosa <sd7.dev@pm.me>\n[axboe: add explicit bitmap == NULL check as well]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc4",
        "release_date": "2023-03-22 11:04:55 -0600 io_uring/rsrc: fix null-ptr-deref in io_file_bitmap_get()"
    },
    {
        "commit": "74e2e17ee1f8d8a0928b90434ad7e2df70f8483e",
        "message": "Since io_uring does nonblocking connect requests, if we do two repeated\nones without having a listener, the second will get -ECONNABORTED rather\nthan the expected -ECONNREFUSED. Treat -ECONNABORTED like a normal retry\ncondition if we're nonblocking, if we haven't already seen it.\n\nCc: stable@vger.kernel.org\nFixes: 3fb1bd688172 (\"io_uring/net: handle -EINPROGRESS correct for IORING_OP_CONNECT\")\nLink: https://github.com/axboe/liburing/issues/828\nReported-by: Hui, Chunyang <sanqian.hcy@antgroup.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc4",
        "release_date": "2023-03-20 20:44:45 -0600 io_uring/net: avoid sending -ECONNABORTED on repeated connection requests"
    },
    {
        "commit": "9d2789ac9d60c049d26ef6d3005d9c94c5a559e9",
        "message": "io_uring_cmd_done() currently assumes that the uring_lock is held\nwhen invoked, and while it generally is, this is not guaranteed.\nPass in the issue_flags associated with it, so that we have\nIO_URING_F_UNLOCKED available to be able to lock the CQ ring\nappropriately when completing events.\n\nCc: stable@vger.kernel.org\nFixes: ee692a21e9bf (\"fs,io_uring: add infrastructure for uring-cmd\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc4",
        "release_date": "2023-03-20 20:01:25 -0600 block/io_uring: pass in issue_flags for uring_cmd task_work handling"
    },
    {
        "commit": "54bdd67d0f88489ac88f7664b56cb7c93799d84d",
        "message": "io_uring provides the only way user space can poll completions, and that\nalways sets BLK_POLL_NOSLEEP. This effectively makes hybrid polling dead\ncode, so remove it and everything supporting it.\n\nHybrid polling was effectively killed off with 9650b453a3d4b1, \"block:\nignore RWF_HIPRI hint for sync dio\", but still potentially reachable\nthrough io_uring until d729cf9acb93119, \"io_uring: don't sleep when\npolling for I/O\", but hybrid polling probably should not have been\nreachable through that async interface from the beginning.\n\nFixes: 9650b453a3d4 (\"block: ignore RWF_HIPRI hint for sync dio\")\nFixes: d729cf9acb93 (\"io_uring: don't sleep when polling for I/O\")\nSigned-off-by: Keith Busch <kbusch@kernel.org>\nLink: https://lore.kernel.org/r/20230320194926.3353144-1-kbusch@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-03-20 15:30:03 -0600 blk-mq: remove hybrid polling"
    },
    {
        "commit": "4985e7b2c002eb4c5c794a1d3acd91b82c89a0fd",
        "message": "IO can be started before add_disk() returns, such as reading parititon table,\nthen the monitor work should work for making forward progress.\n\nSo mark device as LIVE before adding disk, meantime change to\nDEAD if add_disk() fails.\n\nFixed: 71f28f3136af (\"ublk_drv: add io_uring based userspace block driver\")\nReviewed-by: Ziyang Zhang <ZiyangZhang@linux.alibaba.com>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20230318141231.55562-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc4",
        "release_date": "2023-03-18 08:32:46 -0600 block: ublk_drv: mark device as LIVE before adding disk"
    },
    {
        "commit": "b7966a5a5cd009a682ccb0823e89f1e9fb719f27",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - When PF_NO_SETAFFINITY was removed for io-wq threads, we kind of\n   forgot about the SQPOLL thread. Remove it there as well, there's even\n   less of a reason to set it there (Michal)\n\n - Fixup a confusing 'ret' setting (Li)\n\n - When MSG_RING is used to send a direct descriptor to another ring,\n   it's possible to have it allocate it on the target ring rather than\n   provide a specific index for it. If this is done, return the chosen\n   value in the CQE, like we would've done locally (Pavel)\n\n - Fix a regression in this series on huge page bvec collapsing (Pavel)\n\n* tag 'io_uring-6.3-2023-03-16' of git://git.kernel.dk/linux:\n  io_uring/rsrc: fix folio accounting\n  io_uring/msg_ring: let target know allocated index\n  io_uring: rsrc: Optimize return value variable 'ret'\n  io_uring/sqpoll: Do not set PF_NO_SETAFFINITY on sqpoll threads",
        "kernel_version": "v6.3-rc3",
        "release_date": "2023-03-17 11:12:07 -0700 Merge tag 'io_uring-6.3-2023-03-16' of git://git.kernel.dk/linux"
    },
    {
        "commit": "72fd6d738c991225c1053ee5003dd45e9c04e0e6",
        "message": "LAM should be supported in kernel thread, using io_uring to verify LAM feature.\nThe test cases implement read a file through io_uring, the test cases choose an\niovec array as receiving buffer, which used to receive data, according to LAM\nmode, set metadata in high bits of these buffer.\n\nio_uring can deal with these buffers that pointed to pointers with the metadata\nin high bits.\n\nSigned-off-by: Weihong Zhang <weihong.zhang@intel.com>\nSigned-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nSigned-off-by: Dave Hansen <dave.hansen@linux.intel.com>\nAcked-by: Peter Zijlstra (Intel) <peterz@infradead.org>\nLink: https://lore.kernel.org/all/20230312112612.31869-15-kirill.shutemov%40linux.intel.com",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-03-16 13:08:40 -0700 selftests/x86/lam: Add io_uring test cases for linear-address masking"
    },
    {
        "commit": "d2acf789088bb562cea342b6a24e646df4d47839",
        "message": "| BUG: Bad page state in process kworker/u8:0  pfn:5c001\n| page:00000000bfda61c8 refcount:0 mapcount:0 mapping:0000000000000000 index:0x20001 pfn:0x5c001\n| head:0000000011409842 order:9 entire_mapcount:0 nr_pages_mapped:0 pincount:1\n| anon flags: 0x3fffc00000b0004(uptodate|head|mappedtodisk|swapbacked|node=0|zone=0|lastcpupid=0xffff)\n| raw: 03fffc0000000000 fffffc0000700001 ffffffff00700903 0000000100000000\n| raw: 0000000000000200 0000000000000000 00000000ffffffff 0000000000000000\n| head: 03fffc00000b0004 dead000000000100 dead000000000122 ffff00000a809dc1\n| head: 0000000000020000 0000000000000000 00000000ffffffff 0000000000000000\n| page dumped because: nonzero pincount\n| CPU: 3 PID: 9 Comm: kworker/u8:0 Not tainted 6.3.0-rc2-00001-gc6811bf0cd87 #1\n| Hardware name: linux,dummy-virt (DT)\n| Workqueue: events_unbound io_ring_exit_work\n| Call trace:\n|  dump_backtrace+0x13c/0x208\n|  show_stack+0x34/0x58\n|  dump_stack_lvl+0x150/0x1a8\n|  dump_stack+0x20/0x30\n|  bad_page+0xec/0x238\n|  free_tail_pages_check+0x280/0x350\n|  free_pcp_prepare+0x60c/0x830\n|  free_unref_page+0x50/0x498\n|  free_compound_page+0xcc/0x100\n|  free_transhuge_page+0x1f0/0x2b8\n|  destroy_large_folio+0x80/0xc8\n|  __folio_put+0xc4/0xf8\n|  gup_put_folio+0xd0/0x250\n|  unpin_user_page+0xcc/0x128\n|  io_buffer_unmap+0xec/0x2c0\n|  __io_sqe_buffers_unregister+0xa4/0x1e0\n|  io_ring_exit_work+0x68c/0x1188\n|  process_one_work+0x91c/0x1a58\n|  worker_thread+0x48c/0xe30\n|  kthread+0x278/0x2f0\n|  ret_from_fork+0x10/0x20\n\nMark reports an issue with the recent patches coalescing compound pages\nwhile registering them in io_uring. The reason is that we try to drop\nexcessive references with folio_put_refs(), but pages were acquired\nwith pin_user_pages(), which has extra accounting and so should be put\ndown with matching unpin_user_pages() or at least gup_put_folio().\n\nAs a fix unpin_user_pages() all but first page instead, and let's figure\nout a better API after.\n\nFixes: 57bebf807e2abcf8 (\"io_uring/rsrc: optimise registered huge pages\")\nReported-by: Mark Rutland <mark.rutland@arm.com>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nTested-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/10efd5507d6d1f05ea0f3c601830e08767e189bd.1678980230.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc3",
        "release_date": "2023-03-16 09:32:18 -0600 io_uring/rsrc: fix folio accounting"
    },
    {
        "commit": "5da28edd7bd5518f97175ecea77615bb729a7a28",
        "message": "msg_ring requests transferring files support auto index selection via\nIORING_FILE_INDEX_ALLOC, however they don't return the selected index\nto the target ring and there is no other good way for the userspace to\nknow where is the receieved file.\n\nReturn the index for allocated slots and 0 otherwise, which is\nconsistent with other fixed file installing requests.\n\nCc: stable@vger.kernel.org # v6.0+\nFixes: e6130eba8a848 (\"io_uring: add support for passing fixed file descriptors\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://github.com/axboe/liburing/issues/809\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc3",
        "release_date": "2023-03-16 07:16:56 -0600 io_uring/msg_ring: let target know allocated index"
    },
    {
        "commit": "6acd352dfee558194643adbed7e849fe80fd1b93",
        "message": "The initialization assignment of the variable ret is changed to 0, only\nin 'goto fail;' Use the ret variable as the function return value.\n\nSigned-off-by: Li zeming <zeming@nfschina.com>\nLink: https://lore.kernel.org/r/20230317182538.3027-1-zeming@nfschina.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc3",
        "release_date": "2023-03-15 19:59:11 -0600 io_uring: rsrc: Optimize return value variable 'ret'"
    },
    {
        "commit": "a5fc1441af7719e93dc7a638a960befb694ade89",
        "message": "Users may specify a CPU where the sqpoll thread would run. This may\nconflict with cpuset operations because of strict PF_NO_SETAFFINITY\nrequirement. That flag is unnecessary for polling \"kernel\" threads, see\nthe reasoning in commit 01e68ce08a30 (\"io_uring/io-wq: stop setting\nPF_NO_SETAFFINITY on io-wq workers\"). Drop the flag on poll threads too.\n\nFixes: 01e68ce08a30 (\"io_uring/io-wq: stop setting PF_NO_SETAFFINITY on io-wq workers\")\nLink: https://lore.kernel.org/all/20230314162559.pnyxdllzgw7jozgx@blackpad/\nSigned-off-by: Michal Koutn\u00fd <mkoutny@suse.com>\nLink: https://lore.kernel.org/r/20230314183332.25834-1-mkoutny@suse.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc3",
        "release_date": "2023-03-15 06:50:59 -0600 io_uring/sqpoll: Do not set PF_NO_SETAFFINITY on sqpoll threads"
    },
    {
        "commit": "f331c5de7960d69fc767d2dc08f5f5859ce70061",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Stop setting PF_NO_SETAFFINITY on io-wq workers.\n\n   This has been reported in the past as it confuses some applications,\n   as some of their threads will fail with -1/EINVAL if attempted\n   affinitized. Most recent report was on cpusets, where enabling that\n   with io-wq workers active will fail.\n\n   Just deal with the mask changing by checking when a worker times out,\n   and then exit if we have no work pending.\n\n - Fix an issue with passthrough support where we don't properly check\n   if the file type has pollable uring_cmd support.\n\n - Fix a reported W=1 warning on a variable being set and unused. Add a\n   special helper for iterating these lists that doesn't save the\n   previous list element, if that iterator never ends up using it.\n\n* tag 'io_uring-6.3-2023-03-09' of git://git.kernel.dk/linux:\n  io_uring: silence variable \u2018prev\u2019 set but not used warning\n  io_uring/uring_cmd: ensure that device supports IOPOLL\n  io_uring/io-wq: stop setting PF_NO_SETAFFINITY on io-wq workers",
        "kernel_version": "v6.3-rc2",
        "release_date": "2023-03-10 08:31:29 -0800 Merge tag 'io_uring-6.3-2023-03-09' of git://git.kernel.dk/linux"
    },
    {
        "commit": "99e0cd4d552a152c4e453511997441d1d0dde3cc",
        "message": "chenxiang <chenxiang66@hisilicon.com> says:\n\nTo support IO_URING IOPOLL support for hisi_sas, we need to:\n\n - Add and fill mq_poll interface to poll queue;\n\n - Ensure internal I/Os (including internal abort I/Os) are delivered and\n   completed through non-iopoll queue (queue 0);\n\nSending internal abort commands to non-poll queue actually requires to\nsending the abort command to every queue. This carries a a risk. Make iopoll\nsupport module parameter \"experimental\".\n\nI have tested performance on v3 hw with different modes as follows.  4K\nREADs and 4K WRITEs both see an improvement when enabling poll mode:\n\n\t\t\t4K READ\t    4K RANDREAD\t    4K WRITE\t4K RANDWRITE\ninterrupt + libaio\t1770k\t    1316k\t    1197k\t831k\ninterrupt + io_uring\t1848k\t    1390k\t    1238k\t857k\niopoll + io_uring\t2117k\t    1364k\t    1874k\t849k\n\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-03-09 21:50:53 -0500 Merge patch series \"Add poll support for hisi_sas v3 hw\""
    },
    {
        "commit": "b711ef5e176bf47c10c0d24e21c1486b5331d33f",
        "message": "Currently we sync irq to avoid freeing task before using task in I/O\ncompletion. After adding io_uring support, we need to do something similar\nfor poll queues.  As the process of CQ entries on poll queue are protected\nby spinlock cq->lock, we can use spin_lock() + spin_unlock() on cq->lock to\nmake sure that CQ entries are processed to completion and then the complete\nqueue is synced.\n\nSigned-off-by: Xiang Chen <chenxiang66@hisilicon.com>\nLink: https://lore.kernel.org/r/1678169355-76215-4-git-send-email-chenxiang66@hisilicon.com\nReviewed-by: John Garry <john.garry@huawei.com>\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-03-09 21:50:02 -0500 scsi: hisi_sas: Sync complete queue for poll queue"
    },
    {
        "commit": "fa780334a8c392d959ae05eb19f2410b3a1e6cb0",
        "message": "If io_uring.o is built with W=1, it triggers a warning:\n\nio_uring/io_uring.c: In function \u2018__io_submit_flush_completions\u2019:\nio_uring/io_uring.c:1502:40: warning: variable \u2018prev\u2019 set but not used [-Wunused-but-set-variable]\n 1502 |         struct io_wq_work_node *node, *prev;\n      |                                        ^~~~\n\nwhich is due to the wq_list_for_each() iterator always keeping a 'prev'\nvariable. Most users need this to remove an entry from a list, for\nexample, but __io_submit_flush_completions() never does that.\n\nAdd a basic helper that doesn't track prev instead, and use that in\nthat function.\n\nReported-by: Vincenzo Palazzo <vincenzopalazzodev@gmail.com>\nReviewed-by: Vincenzo Palazzo <vincenzopalazzodev@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc2",
        "release_date": "2023-03-09 10:10:58 -0700 io_uring: silence variable \u2018prev\u2019 set but not used warning"
    },
    {
        "commit": "03b3d6be73e81ddb7c2930d942cdd17f4cfd5ba5",
        "message": "It's possible for a file type to support uring commands, but not\npollable ones. Hence before issuing one of those, we should check\nthat it is supported and error out upfront if it isn't.\n\nCc: stable@vger.kernel.org\nFixes: 5756a3a7e713 (\"io_uring: add iopoll infrastructure for io_uring_cmd\")\nLink: https://github.com/axboe/liburing/issues/816\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc2",
        "release_date": "2023-03-09 09:23:59 -0700 io_uring/uring_cmd: ensure that device supports IOPOLL"
    },
    {
        "commit": "01e68ce08a30db3d842ce7a55f7f6e0474a55f9a",
        "message": "Every now and then reports come in that are puzzled on why changing\naffinity on the io-wq workers fails with EINVAL. This happens because they\nset PF_NO_SETAFFINITY as part of their creation, as io-wq organizes\nworkers into groups based on what CPU they are running on.\n\nHowever, this is purely an optimization and not a functional requirement.\nWe can allow setting affinity, and just lazily update our worker to wqe\nmappings. If a given io-wq thread times out, it normally exits if there's\nno more work to do. The exception is if it's the last worker available.\nFor the timeout case, check the affinity of the worker against group mask\nand exit even if it's the last worker. New workers should be created with\nthe right mask and in the right location.\n\nReported-by:Daniel Dao <dqminh@cloudflare.com>\nLink: https://lore.kernel.org/io-uring/CA+wXwBQwgxB3_UphSny-yAP5b26meeOu1W4TwYVcD_+5gOhvPw@mail.gmail.com/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc2",
        "release_date": "2023-03-08 08:48:13 -0700 io_uring/io-wq: stop setting PF_NO_SETAFFINITY on io-wq workers"
    },
    {
        "commit": "10369080454d87ee5b2db211ce947cb3118f0e13",
        "message": "Commit 0091bfc81741 (\"io_uring/af_unix: defer registered\nfiles gc to io_uring release\") added one bit to struct sk_buff.\n\nThis structure is critical for networking, and we try very hard\nto not add bloat on it, unless absolutely required.\n\nFor instance, we can use a specific destructor as a wrapper\naround unix_destruct_scm(), to identify skbs that unix_gc()\nhas to special case.\n\nSigned-off-by: Eric Dumazet <edumazet@google.com>\nCc: Pavel Begunkov <asml.silence@gmail.com>\nCc: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>\nCc: Jens Axboe <axboe@kernel.dk>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-03-08 13:21:47 +0000 net: reclaim skb->scm_io_uring bit"
    },
    {
        "commit": "1cd2aca64a5dc4edb65539dc26f24e162ab0e11c",
        "message": "This patch relocates the LSM hook function comments to the function\ndefinitions, in keeping with the current kernel conventions.  This\nshould make the hook descriptions more easily discoverable and easier\nto maintain.\n\nWhile formatting changes have been done to better fit the kernel-doc\nstyle, content changes have been kept to a minimum and limited to\ntext which was obviously incorrect and/or outdated.  It is expected\nthe future patches will improve the quality of the function header\ncomments.\n\nAcked-by: Casey Schaufler <casey@schaufler-ca.com>\nSigned-off-by: Paul Moore <paul@paul-moore.com>",
        "kernel_version": "v6.4-rc1",
        "release_date": "2023-03-06 13:41:07 -0500 lsm: move the io_uring hook comments to security/security.c"
    },
    {
        "commit": "53ae7e117637ff201fdf038b68e76a7202112dea",
        "message": "Pull more io_uring updates from Jens Axboe:\n \"Here's a set of fixes/changes that didn't make the first cut, either\n  because they got queued before I sent the early merge request, or\n  fixes that came in afterwards. In detail:\n\n   - Don't set MSG_NOSIGNAL on recv/recvmsg opcodes, as AF_PACKET will\n     error out (David)\n\n   - Fix for spurious poll wakeups (me)\n\n   - Fix for a file leak for buffered reads in certain conditions\n     (Joseph)\n\n   - Don't allow registered buffers of mixed types (Pavel)\n\n   - Improve handling of huge pages for registered buffers (Pavel)\n\n   - Provided buffer ring size calculation fix (Wojciech)\n\n   - Minor cleanups (me)\"\n\n* tag 'io_uring-6.3-2023-03-03' of git://git.kernel.dk/linux:\n  io_uring/poll: don't pass in wake func to io_init_poll_iocb()\n  io_uring: fix fget leak when fs don't support nowait buffered read\n  io_uring/poll: allow some retries for poll triggering spuriously\n  io_uring: remove MSG_NOSIGNAL from recvmsg\n  io_uring/rsrc: always initialize 'folio' to NULL\n  io_uring/rsrc: optimise registered huge pages\n  io_uring/rsrc: optimise single entry advance\n  io_uring/rsrc: disallow multi-source reg buffers\n  io_uring: remove unused wq_list_merge\n  io_uring: fix size calculation when registering buf ring\n  io_uring/rsrc: fix a comment in io_import_fixed()\n  io_uring: rename 'in_idle' to 'in_cancel'\n  io_uring: consolidate the put_ref-and-return section of adding work",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-03-03 10:25:29 -0800 Merge tag 'io_uring-6.3-2023-03-03' of git://git.kernel.dk/linux"
    },
    {
        "commit": "1947ddf9b3d5b886ba227bbfd3d6f501af08b5b0",
        "message": "We only use one, and it's io_poll_wake(). Hardwire that in the initial\ninit, as well as in __io_queue_proc() if we're setting up for double\npoll.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-03-01 10:06:53 -0700 io_uring/poll: don't pass in wake func to io_init_poll_iocb()"
    },
    {
        "commit": "54aa7f2330b82884f4a1afce0220add6e8312f8b",
        "message": "Heming reported a BUG when using io_uring doing link-cp on ocfs2. [1]\n\nDo the following steps can reproduce this BUG:\nmount -t ocfs2 /dev/vdc /mnt/ocfs2\ncp testfile /mnt/ocfs2/\n./link-cp /mnt/ocfs2/testfile /mnt/ocfs2/testfile.1\numount /mnt/ocfs2\n\nThen umount will fail, and it outputs:\numount: /mnt/ocfs2: target is busy.\n\nWhile tracing umount, it blames mnt_get_count() not return as expected.\nDo a deep investigation for fget()/fput() on related code flow, I've\nfinally found that fget() leaks since ocfs2 doesn't support nowait\nbuffered read.\n\nio_issue_sqe\n|-io_assign_file  // do fget() first\n  |-io_read\n  |-io_iter_do_read\n    |-ocfs2_file_read_iter  // return -EOPNOTSUPP\n  |-kiocb_done\n    |-io_rw_done\n      |-__io_complete_rw_common  // set REQ_F_REISSUE\n    |-io_resubmit_prep\n      |-io_req_prep_async  // override req->file, leak happens\n\nThis was introduced by commit a196c78b5443 in v5.18. Fix it by don't\nre-assign req->file if it has already been assigned.\n\n[1] https://lore.kernel.org/ocfs2-devel/ab580a75-91c8-d68a-3455-40361be1bfa8@linux.alibaba.com/T/#t\n\nFixes: a196c78b5443 (\"io_uring: assign non-fixed early for async work\")\nCc: <stable@vger.kernel.org>\nReported-by: Heming Zhao <heming.zhao@suse.com>\nSigned-off-by: Joseph Qi <joseph.qi@linux.alibaba.com>\nCc: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20230228045459.13524-1-joseph.qi@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-28 05:58:42 -0700 io_uring: fix fget leak when fs don't support nowait buffered read"
    },
    {
        "commit": "c16bda37594f83147b167d381d54c010024efecf",
        "message": "If we get woken spuriously when polling and fail the operation with\n-EAGAIN again, then we generally only allow polling again if data\nhad been transferred at some point. This is indicated with\nREQ_F_PARTIAL_IO. However, if the spurious poll triggers when the socket\nwas originally empty, then we haven't transferred data yet and we will\nfail the poll re-arm. This either punts the socket to io-wq if it's\nblocking, or it fails the request with -EAGAIN if not. Neither condition\nis desirable, as the former will slow things down, while the latter\nwill make the application confused.\n\nWe want to ensure that a repeated poll trigger doesn't lead to infinite\nwork making no progress, that's what the REQ_F_PARTIAL_IO check was\nfor. But it doesn't protect against a loop post the first receive, and\nit's unnecessarily strict if we started out with an empty socket.\n\nAdd a somewhat random retry count, just to put an upper limit on the\npotential number of retries that will be done. This should be high enough\nthat we won't really hit it in practice, unless something needs to be\naborted anyway.\n\nCc: stable@vger.kernel.org # v5.10+\nLink: https://github.com/axboe/liburing/issues/364\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-25 20:10:13 -0700 io_uring/poll: allow some retries for poll triggering spuriously"
    },
    {
        "commit": "310726c33ad76cebdee312dbfafc12c1b44bf977",
        "message": "Wei reports a crash with an application using polled IO:\n\nPGD 14265e067 P4D 14265e067 PUD 47ec50067 PMD 0\nOops: 0000 [#1] SMP\nCPU: 0 PID: 21915 Comm: iocore_0 Kdump: loaded Tainted: G S                5.12.0-0_fbk12_clang_7346_g1bb6f2e7058f #1\nHardware name: Wiwynn Delta Lake MP T8/Delta Lake-Class2, BIOS Y3DLM08 04/10/2022\nRIP: 0010:bio_poll+0x25/0x200\nCode: 0f 1f 44 00 00 0f 1f 44 00 00 55 41 57 41 56 41 55 41 54 53 48 83 ec 28 65 48 8b 04 25 28 00 00 00 48 89 44 24 20 48 8b 47 08 <48> 8b 80 70 02 00 00 4c 8b 70 50 8b 6f 34 31 db 83 fd ff 75 25 65\nRSP: 0018:ffffc90005fafdf8 EFLAGS: 00010292\nRAX: 0000000000000000 RBX: 0000000000000000 RCX: 74b43cd65dd66600\nRDX: 0000000000000003 RSI: ffffc90005fafe78 RDI: ffff8884b614e140\nRBP: ffff88849964df78 R08: 0000000000000000 R09: 0000000000000008\nR10: 0000000000000000 R11: 0000000000000000 R12: ffff88849964df00\nR13: ffffc90005fafe78 R14: ffff888137d3c378 R15: 0000000000000001\nFS:  00007fd195000640(0000) GS:ffff88903f400000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 0000000000000270 CR3: 0000000466121001 CR4: 00000000007706f0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\nPKRU: 55555554\nCall Trace:\n iocb_bio_iopoll+0x1d/0x30\n io_do_iopoll+0xac/0x250\n __se_sys_io_uring_enter+0x3c5/0x5a0\n ? __x64_sys_write+0x89/0xd0\n do_syscall_64+0x2d/0x40\n entry_SYSCALL_64_after_hwframe+0x44/0xae\nRIP: 0033:0x94f225d\nCode: 24 cc 00 00 00 41 8b 84 24 d0 00 00 00 c1 e0 04 83 e0 10 41 09 c2 8b 33 8b 53 04 4c 8b 43 18 4c 63 4b 0c b8 aa 01 00 00 0f 05 <85> c0 0f 88 85 00 00 00 29 03 45 84 f6 0f 84 88 00 00 00 41 f6 c7\nRSP: 002b:00007fd194ffcd88 EFLAGS: 00000202 ORIG_RAX: 00000000000001aa\nRAX: ffffffffffffffda RBX: 00007fd194ffcdc0 RCX: 00000000094f225d\nRDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000007\nRBP: 00007fd194ffcdb0 R08: 0000000000000000 R09: 0000000000000008\nR10: 0000000000000001 R11: 0000000000000202 R12: 00007fd269d68030\nR13: 0000000000000000 R14: 0000000000000001 R15: 0000000000000000\n\nwhich is due to bio->bi_bdev being NULL. This can happen if we have two\ntasks doing polled IO, and task B ends up completing IO from task A if\nthey are sharing a poll queue. If task B completes the IO and puts the\nbio into our cache, then it can allocate that bio again before task A\nis done polling for it. As that would necessitate a preempt between the\ntwo tasks, it's enough to just be a bit more careful in checking for\nwhether or not bio->bi_bdev is NULL.\n\nReported-and-tested-by: Wei Zhang <wzhang@meta.com>\nCc: stable@vger.kernel.org\nFixes: be4d234d7aeb (\"bio: add allocation cache abstraction\")\nReviewed-by: Keith Busch <kbusch@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-24 13:19:59 -0700 block: be a bit more careful in checking for NULL bdev while polling"
    },
    {
        "commit": "7605c43d67face310b4b87dee1a28bc0c8cd8c0f",
        "message": "MSG_NOSIGNAL is not applicable for the receiving side, SIGPIPE is\ngenerated when trying to write to a \"broken pipe\".  AF_PACKET's\npacket_recvmsg() does enforce this, giving back EINVAL when MSG_NOSIGNAL\nis set - making it unuseable in io_uring's recvmsg.\n\nRemove MSG_NOSIGNAL from io_recvmsg_prep().\n\nCc: stable@vger.kernel.org # v5.10+\nSigned-off-by: David Lamparter <equinox@diac24.net>\nCc: Eric Dumazet <edumazet@google.com>\nCc: Jens Axboe <axboe@kernel.dk>\nReviewed-by: Eric Dumazet <edumazet@google.com>\nLink: https://lore.kernel.org/r/20230224150123.128346-1-equinox@diac24.net\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-24 12:59:02 -0700 io_uring: remove MSG_NOSIGNAL from recvmsg"
    },
    {
        "commit": "977bc87356107fb946fb4ff24f1e4c241b5043ec",
        "message": "Smatch complains that:\n\nsmatch warnings:\nio_uring/rsrc.c:1262 io_sqe_buffer_register() error: uninitialized symbol 'folio'.\n\n'folio' may be used uninitialized, which can happen if we end up with a\nsingle page mapped. Ensure that we clear folio to NULL at the top so\nit's always set.\n\nReported-by: kernel test robot <lkp@intel.com>\nReported-by: Dan Carpenter <error27@gmail.com>\nLink: https://lore.kernel.org/r/202302241432.YML1CD5C-lkp@intel.com/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-24 12:58:31 -0700 io_uring/rsrc: always initialize 'folio' to NULL"
    },
    {
        "commit": "57bebf807e2abcf87d96b9de1266104ee2d8fc2f",
        "message": "When registering huge pages, internally io_uring will split them into\nmany PAGE_SIZE bvec entries. That's bad for performance as drivers need\nto eventually dma-map the data and will do it individually for each bvec\nentry. Coalesce huge pages into one large bvec.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-22 09:57:24 -0700 io_uring/rsrc: optimise registered huge pages"
    },
    {
        "commit": "b000ae0ec2d709046ac1a3c5722fea417f8a067e",
        "message": "Iterating within the first bvec entry should be essentially free, but we\nuse iov_iter_advance() for that, which shows up in benchmark profiles\ntaking up to 0.5% of CPU. Replace it with a hand coded version.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-22 09:57:23 -0700 io_uring/rsrc: optimise single entry advance"
    },
    {
        "commit": "edd478269640b360c6f301f2baa04abdda563ef3",
        "message": "If two or more mappings go back to back to each other they can be passed\ninto io_uring to be registered as a single registered buffer. That would\neven work if mappings came from different sources, e.g. it's possible to\nmix in this way anon pages and pages from shmem or hugetlb. That is not\na problem but it'd rather be less prone if we forbid such mixing.\n\nCc: <stable@vger.kernel.org>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-22 09:57:23 -0700 io_uring/rsrc: disallow multi-source reg buffers"
    },
    {
        "commit": "9a1563d1720680bdc1d702486b7b73f51c079b32",
        "message": "There are no users of wq_list_merge, kill it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5f9ad0301949213230ad9000a8359d591aae615a.1677002255.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-22 09:57:23 -0700 io_uring: remove unused wq_list_merge"
    },
    {
        "commit": "48ba08374e779421ca34bd14b4834aae19fc3e6a",
        "message": "Using struct_size() to calculate the size of io_uring_buf_ring will sum\nthe size of the struct and of the bufs array. However, the struct's fields\nare overlaid with the array making the calculated size larger than it\nshould be.\n\nWhen registering a ring with N * PAGE_SIZE / sizeof(struct io_uring_buf)\nentries, i.e. with fully filled pages, the calculated size will span one\nmore page than it should and io_uring will try to pin the following page.\nDepending on how the application allocated the ring, it might succeed\nusing an unrelated page or fail returning EFAULT.\n\nThe size of the ring should be the product of ring_entries and the size\nof io_uring_buf, i.e. the size of the bufs array only.\n\nFixes: c7fb19428d67 (\"io_uring: add support for ring mapped supplied buffers\")\nSigned-off-by: Wojciech Lukowicz <wlukowicz01@gmail.com>\nReviewed-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/20230218184141.70891-1-wlukowicz01@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-22 09:57:23 -0700 io_uring: fix size calculation when registering buf ring"
    },
    {
        "commit": "6bf65a1b3668b04bb6c8126494d00303104eb9e5",
        "message": "io_import_fixed() supports offsets, but \"may not\" means the opposite.\nReplace it with \"might not\" so the comments rather speaks about\npossible cases.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Gabriel Krisman Bertazi <krisman@suse.de>\nLink: https://lore.kernel.org/r/5b5f79958456caa6dc532f6205f75f224b232c81.1676902343.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-22 09:57:23 -0700 io_uring/rsrc: fix a comment in io_import_fixed()"
    },
    {
        "commit": "8d664282a03fec09682f10252d3c785c2513691d",
        "message": "This better describes what it does - it's incremented when the task is\ncurrently undergoing a cancelation operation, due to exiting or exec'ing.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-22 09:57:23 -0700 io_uring: rename 'in_idle' to 'in_cancel'"
    },
    {
        "commit": "ce8e04f6e5d3b2d14cd00cc4c0b1cc8cbdcf4d12",
        "message": "We've got a few cases of this, move them to one section and just use\ngotos to get there. Reduces the text section on both arm64 and x86-64,\nusing gcc-12.2.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-22 09:57:23 -0700 io_uring: consolidate the put_ref-and-return section of adding work"
    },
    {
        "commit": "4a7d37e824f57dbace61abf62f53843800bd245c",
        "message": "Pull hardening updates from Kees Cook:\n \"Beyond some specific LoadPin, UBSAN, and fortify features, there are\n  other fixes scattered around in various subsystems where maintainers\n  were okay with me carrying them in my tree or were non-responsive but\n  the patches were reviewed by others:\n\n   - Replace 0-length and 1-element arrays with flexible arrays in\n     various subsystems (Paulo Miguel Almeida, Stephen Rothwell, Kees\n     Cook)\n\n   - randstruct: Disable Clang 15 support (Eric Biggers)\n\n   - GCC plugins: Drop -std=gnu++11 flag (Sam James)\n\n   - strpbrk(): Refactor to use strchr() (Andy Shevchenko)\n\n   - LoadPin LSM: Allow root filesystem switching when non-enforcing\n\n   - fortify: Use dynamic object size hints when available\n\n   - ext4: Fix CFI function prototype mismatch\n\n   - Nouveau: Fix DP buffer size arguments\n\n   - hisilicon: Wipe entire crypto DMA pool on error\n\n   - coda: Fully allocate sig_inputArgs\n\n   - UBSAN: Improve arm64 trap code reporting\n\n   - copy_struct_from_user(): Add minimum bounds check on kernel buffer\n     size\"\n\n* tag 'hardening-v6.3-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:\n  randstruct: disable Clang 15 support\n  uaccess: Add minimum bounds check on kernel buffer size\n  arm64: Support Clang UBSAN trap codes for better reporting\n  coda: Avoid partial allocation of sig_inputArgs\n  gcc-plugins: drop -std=gnu++11 to fix GCC 13 build\n  lib/string: Use strchr() in strpbrk()\n  crypto: hisilicon: Wipe entire pool on error\n  net/i40e: Replace 0-length array with flexible array\n  io_uring: Replace 0-length array with flexible array\n  ext4: Fix function prototype mismatch for ext4_feat_ktype\n  i915/gvt: Replace one-element array with flexible-array member\n  drm/nouveau/disp: Fix nvif_outp_acquire_dp() argument size\n  LoadPin: Allow filesystem switch when not enforcing\n  LoadPin: Move pin reporting cleanly out of locking\n  LoadPin: Refactor sysctl initialization\n  LoadPin: Refactor read-only check into a helper\n  ARM: ixp4xx: Replace 0-length arrays with flexible arrays\n  fortify: Use __builtin_dynamic_object_size() when available\n  rxrpc: replace zero-lenth array with DECLARE_FLEX_ARRAY() helper",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-21 11:07:23 -0800 Merge tag 'hardening-v6.3-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux"
    },
    {
        "commit": "9c7c4bc986932218fd0df9d2a100509772028fb1",
        "message": "sizeof(struct ublksrv_io_cmd) is 16bytes, which can be held in 64byte SQE,\nso not necessary to check IO_URING_F_SQE128.\n\nWith this change, we get chance to save half SQ ring memory.\n\nFixed: 71f28f3136af (\"ublk_drv: add io_uring based userspace block driver\")\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20230220041413.1524335-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-21 09:27:23 -0700 ublk: remove check IO_URING_F_SQE128 in ublk_ch_uring_cmd"
    },
    {
        "commit": "07b679f70d73483930e8d3c293942416d9cd5c13",
        "message": "This patch adds completion batching to the IRQ path. It reuses batch\ncompletion code of virtblk_poll(). It collects requests to io_comp_batch\nand processes them all at once. It can boost up the performance by 2%.\n\nTo validate the performance improvement and stabilty, I did fio test with\n4 vCPU VM and 12 vCPU VM respectively. Both VMs have 8GB ram and the same\nnumber of HW queues as vCPU.\nThe fio cammad is as follows and I ran the fio 5 times and got IOPS average.\n(io_uring, randread, direct=1, bs=512, iodepth=64 numjobs=2,4)\n\nTest result shows about 2% improvement.\n\n           4 vcpu VM       |   numjobs=2   |   numjobs=4\n      -----------------------------------------------------------\n        fio without patch  |  367.2K IOPS  |   397.6K IOPS\n      -----------------------------------------------------------\n        fio with patch     |  372.8K IOPS  |   407.7K IOPS\n\n           12 vcpu VM      |   numjobs=2   |   numjobs=4\n      -----------------------------------------------------------\n        fio without patch  |  363.6K IOPS  |   374.8K IOPS\n      -----------------------------------------------------------\n        fio with patch     |  373.8K IOPS  |   385.3K IOPS\n\nSigned-off-by: Suwan Kim <suwan.kim027@gmail.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nMessage-Id: <20221221145456.281218-3-suwan.kim027@gmail.com>\nSigned-off-by: Michael S. Tsirkin <mst@redhat.com>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-20 19:26:57 -0500 virtio-blk: support completion batching for the IRQ path"
    },
    {
        "commit": "c1ef5003079531b5aae12467a350379496752334",
        "message": "Pull io_uring ITER_UBUF conversion from Jens Axboe:\n \"Since we now have ITER_UBUF available, switch to using it for single\n  ranges as it's more efficient than ITER_IOVEC for that\"\n\n* tag 'for-6.3/iter-ubuf-2023-02-16' of git://git.kernel.dk/linux:\n  block: use iter_ubuf for single range\n  iov_iter: move iter_ubuf check inside restore WARN\n  io_uring: use iter_ubuf for single range imports\n  io_uring: switch network send/recv to ITER_UBUF\n  iov: add import_ubuf()",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-20 14:03:57 -0800 Merge tag 'for-6.3/iter-ubuf-2023-02-16' of git://git.kernel.dk/linux"
    },
    {
        "commit": "cce5fe5eda0581363a9c585dabf8a5923f15a708",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - Cleanup series making the async prep and handling of\n   REQ_F_FORCE_ASYNC easier to follow and verify (Dylan)\n\n - Enable specifying specific flags for OP_MSG_RING (Breno)\n\n - Enable use of KASAN with the internal request cache (Breno)\n\n - Split the opcode definition structs into a hot and cold part (Breno)\n\n - OP_MSG_RING fixes (Pavel, me)\n\n - Fix an issue with IOPOLL cancelation and PREEMPT_NONE (me)\n\n - Handle TIF_NOTIFY_RESUME for the io-wq threads that never return to\n   userspace (me)\n\n - Add support for using io_uring_register() with a registered ring fd\n   (Josh)\n\n - Improve handling of poll on the ring fd (Pavel)\n\n - Series improving the task_work handling (Pavel)\n\n - Misc cleanups, fixes, improvements (Dmitrii, Quanfa, Richard, Pavel,\n   me)\n\n* tag 'for-6.3/io_uring-2023-02-16' of git://git.kernel.dk/linux: (51 commits)\n  io_uring: Support calling io_uring_register with a registered ring fd\n  io_uring,audit: don't log IORING_OP_MADVISE\n  io_uring: mark task TASK_RUNNING before handling resume/task work\n  io_uring: always go async for unsupported open flags\n  io_uring: always go async for unsupported fadvise flags\n  io_uring: for requests that require async, force it\n  io_uring: if a linked request has REQ_F_FORCE_ASYNC then run it async\n  io_uring: add reschedule point to handle_tw_list()\n  io_uring: add a conditional reschedule to the IOPOLL cancelation loop\n  io_uring: return normal tw run linking optimisation\n  io_uring: refactor tctx_task_work\n  io_uring: refactor io_put_task helpers\n  io_uring: refactor req allocation\n  io_uring: improve io_get_sqe\n  io_uring: kill outdated comment about overflow flush\n  io_uring: use user visible tail in io_uring_poll()\n  io_uring: pass in io_issue_def to io_assign_file()\n  io_uring: Enable KASAN for request cache\n  io_uring: handle TIF_NOTIFY_RESUME when checking for task_work\n  io_uring/msg-ring: ensure flags passing works for task_work completions\n  ...",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-20 13:53:39 -0800 Merge tag 'for-6.3/io_uring-2023-02-16' of git://git.kernel.dk/linux"
    },
    {
        "commit": "67205f80be9910207481406c47f7d85e703fb2e9",
        "message": "By default, non-mq drivers do not support nowait. This causes io_uring\nto use a slower path as the driver cannot be trust not to block. brd\ncan safely set the nowait flag, as worst case all it does is a NOIO\nallocation.\n\nFor io_uring, this makes a substantial difference. Before:\n\nsubmitter=0, tid=453, file=/dev/ram0, node=-1\npolled=0, fixedbufs=1/0, register_files=1, buffered=0, QD=128\nEngine=io_uring, sq_ring=128, cq_ring=128\nIOPS=440.03K, BW=1718MiB/s, IOS/call=32/31\nIOPS=428.96K, BW=1675MiB/s, IOS/call=32/32\nIOPS=442.59K, BW=1728MiB/s, IOS/call=32/31\nIOPS=419.65K, BW=1639MiB/s, IOS/call=32/32\nIOPS=426.82K, BW=1667MiB/s, IOS/call=32/31\n\nand after:\n\nsubmitter=0, tid=354, file=/dev/ram0, node=-1\npolled=0, fixedbufs=1/0, register_files=1, buffered=0, QD=128\nEngine=io_uring, sq_ring=128, cq_ring=128\nIOPS=3.37M, BW=13.15GiB/s, IOS/call=32/31\nIOPS=3.45M, BW=13.46GiB/s, IOS/call=32/31\nIOPS=3.43M, BW=13.42GiB/s, IOS/call=32/32\nIOPS=3.43M, BW=13.39GiB/s, IOS/call=32/31\nIOPS=3.43M, BW=13.38GiB/s, IOS/call=32/31\n\nor about an 8x in difference. Now that brd is prepared to deal with\nREQ_NOWAIT reads/writes, mark it as supporting that.\n\nCc: stable@vger.kernel.org # 5.10+\nLink: https://lore.kernel.org/linux-block/20230203103005.31290-1-p.raghav@samsung.com/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-16 10:02:55 -0700 brd: mark as nowait compatible"
    },
    {
        "commit": "7d3fd88d61a41016da01889f076fd1c60c7298fc",
        "message": "Add a new flag IORING_REGISTER_USE_REGISTERED_RING (set via the high bit\nof the opcode) to treat the fd as a registered index rather than a file\ndescriptor.\n\nThis makes it possible for a library to open an io_uring, register the\nring fd, close the ring fd, and subsequently use the ring entirely via\nregistered index.\n\nSigned-off-by: Josh Triplett <josh@joshtriplett.org>\nLink: https://lore.kernel.org/r/f2396369e638284586b069dbddffb8c992afba95.1676419314.git.josh@joshtriplett.org\n[axboe: remove extra high bit clear]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-16 06:09:30 -0700 io_uring: Support calling io_uring_register with a registered ring fd"
    },
    {
        "commit": "fbe870a72fd1ddc5e08c23764e23e5766f54aa87",
        "message": "fadvise and madvise both provide hints for caching or access pattern for\nfile and memory respectively.  Skip them.\n\nFixes: 5bd2182d58e9 (\"audit,io_uring,io-wq: add some basic audit support to io_uring\")\nSigned-off-by: Richard Guy Briggs <rgb@redhat.com>\nLink: https://lore.kernel.org/r/b5dfdcd541115c86dbc774aa9dd502c964849c5f.1675282642.git.rgb@redhat.com\nAcked-by: Paul Moore <paul@paul-moore.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-10 16:00:30 -0700 io_uring,audit: don't log IORING_OP_MADVISE"
    },
    {
        "commit": "2f2bb1ffc9983e227424d0787289da5483b0c74f",
        "message": "Just like for task_work, set the task mode to TASK_RUNNING before doing\nany potential resume work. We're not holding any locks at this point,\nbut we may have already set the task state to TASK_INTERRUPTIBLE in\npreparation for going to sleep waiting for events. Ensure that we set it\nback to TASK_RUNNING if we have work to process, to avoid warnings on\ncalling blocking operations with !TASK_RUNNING.\n\nFixes: b5d3ae202fbf (\"io_uring: handle TIF_NOTIFY_RESUME when checking for task_work\")\nReported-by: kernel test robot <oliver.sang@intel.com>\nLink: https://lore.kernel.org/oe-lkp/202302062208.24d3e563-oliver.sang@intel.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-06 08:23:21 -0700 io_uring: mark task TASK_RUNNING before handling resume/task work"
    },
    {
        "commit": "cc342a21930f0e3862c5fd0871cd5a65c5b59e27",
        "message": "Use the bvec_set_page helper to initialize a bvec.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Chaitanya Kulkarni <kch@nvidia.com>\nLink: https://lore.kernel.org/r/20230203150634.3199647-19-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-03 10:17:42 -0700 io_uring: use bvec_set_page to initialize a bvec"
    },
    {
        "commit": "7d28631786b2333c5d48ad25172eb159aaa2945f",
        "message": "Patch series \"remove ->rw_page\".\n\nThis series removes the ->rw_page block_device_operation, which is an old\nand clumsy attempt at a simple read/write fast path for the block layer. \nIt isn't actually used by the fastest block layer operations that we\nsupport (polled I/O through io_uring), but only used by the mpage buffered\nI/O helpers which are some of the slowest I/O we have and do not make any\ndifference there at all, and zram which is a block device abused to\nduplicate the zram functionality.\n\nGiven that zram is heavily used we need to make sure there is a good\nreplacement for synchronous I/O, so this series adds a new flag for\ndrivers that complete I/O synchronously and uses that flag to use on-stack\nbios and synchronous submission for them in the swap code.\n\n\nThis patch (of 7):\n\nThese are micro-optimizations for synchronous I/O, which do not matter\ncompared to all the other inefficiencies in the legacy buffer_head based\nmpage code.\n\nLink: https://lkml.kernel.org/r/20230125133436.447864-1-hch@lst.de\nLink: https://lkml.kernel.org/r/20230125133436.447864-2-hch@lst.de\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Dan Williams <dan.j.williams@intel.com>\nCc: Keith Busch <kbusch@kernel.org>\nCc: Dave Jiang <dave.jiang@intel.com>\nCc: Ira Weiny <ira.weiny@intel.com>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Minchan Kim <minchan@kernel.org>\nCc: Sergey Senozhatsky <senozhatsky@chromium.org>\nCc: Vishal Verma <vishal.l.verma@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-02-02 22:33:32 -0800 mpage: stop using bdev_{read,write}_page"
    },
    {
        "commit": "29baef789c838bd5c02f50c88adbbc6b955aaf61",
        "message": "When validating drafted SPDK ublk target, in a case that\nassigning large queue depth to multiqueue ublk device,\nublk target would run into a weird incorrect state. During\nrounds of review and debug, An overflow bug was found\nin ublk driver.\n\nIn ublk_cmd.h, UBLK_MAX_QUEUE_DEPTH is 4096 which means\neach ublk queue depth can be set as large as 4096. But\nwhen setting qd for a ublk device,\nsizeof(struct ublk_queue) + depth * sizeof(struct ublk_io)\nwill be larger than 65535 if qd is larger than 2728.\nThen queue_size is overflowed, and ublk_get_queue()\nreferences a wrong pointer position. The wrong content of\nublk_queue elements will lead to out-of-bounds memory\naccess.\n\nExtend queue_size in ublk_device as \"unsigned int\".\n\nSigned-off-by: Liu Xiaodong <xiaodong.liu@intel.com>\nFixes: 71f28f3136af (\"ublk_drv: add io_uring based userspace block driver\")\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20230131070552.115067-1-xiaodong.liu@intel.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc7",
        "release_date": "2023-01-31 07:58:53 -0700 block: ublk: extending queue_size to fix overflow"
    },
    {
        "commit": "73a166d9749230d598320fdae3b687cdc0e2e205",
        "message": "If any ubq daemon is unprivileged, the ublk char device is allowed\nfor unprivileged user actually, and we can't trust the current user,\nso not probe partitions.\n\nFixes: 71f28f3136af (\"ublk_drv: add io_uring based userspace block driver\")\nReviewed-by: ZiyangZhang <ZiyangZhang@linux.alibaba.com>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20230106041711.914434-3-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:18:34 -0700 ublk_drv: don't probe partitions if the ubq daemon isn't trusted"
    },
    {
        "commit": "0ffae640ad83de46865c6b8dc3fda370823e4f1d",
        "message": "No point in issuing -> return -EAGAIN -> go async, when it can be done upfront.\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20230127135227.3646353-5-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:18:26 -0700 io_uring: always go async for unsupported open flags"
    },
    {
        "commit": "c31cc60fddd11134031e7f9eb76812353cfaac84",
        "message": "No point in issuing -> return -EAGAIN -> go async, when it can be done upfront.\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20230127135227.3646353-4-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:18:26 -0700 io_uring: always go async for unsupported fadvise flags"
    },
    {
        "commit": "aebb224fd4fc7352cd839ad90414c548387142fd",
        "message": "Some requests require being run async as they do not support\nnon-blocking. Instead of trying to issue these requests, getting -EAGAIN\nand then queueing them for async issue, rather just force async upfront.\n\nAdd WARN_ON_ONCE to make sure surprising code paths do not come up,\nhowever in those cases the bug would end up being a blocking\nio_uring_enter(2) which should not be critical.\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20230127135227.3646353-3-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:18:26 -0700 io_uring: for requests that require async, force it"
    },
    {
        "commit": "6bb30855560e6343e7b88595d7c3159d0f848a04",
        "message": "REQ_F_FORCE_ASYNC was being ignored for re-queueing linked\nrequests. Instead obey that flag.\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20230127135227.3646353-2-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:18:26 -0700 io_uring: if a linked request has REQ_F_FORCE_ASYNC then run it async"
    },
    {
        "commit": "f58680085478dd292435727210122960d38e8014",
        "message": "If CONFIG_PREEMPT_NONE is set and the task_work chains are long, we\ncould be running into issues blocking others for too long. Add a\nreschedule check in handle_tw_list(), and flush the ctx if we need to\nreschedule.\n\nCc: stable@vger.kernel.org # 5.10+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: add reschedule point to handle_tw_list()"
    },
    {
        "commit": "fcc926bb857949dbfa51a7d95f3f5ebc657f198c",
        "message": "If the kernel is configured with CONFIG_PREEMPT_NONE, we could be\nsitting in a tight loop reaping events but not giving them a chance to\nfinish. This results in a trace ala:\n\nrcu: INFO: rcu_sched self-detected stall on CPU\nrcu: \t2-...!: (5249 ticks this GP) idle=935c/1/0x4000000000000000 softirq=4265/4274 fqs=1\n\t(t=5251 jiffies g=465 q=4135 ncpus=4)\nrcu: rcu_sched kthread starved for 5249 jiffies! g465 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x0 ->cpu=0\nrcu: \tUnless rcu_sched kthread gets sufficient CPU time, OOM is now expected behavior.\nrcu: RCU grace-period kthread stack dump:\ntask:rcu_sched       state:R  running task     stack:0     pid:12    ppid:2      flags:0x00000008\nCall trace:\n __switch_to+0xb0/0xc8\n __schedule+0x43c/0x520\n schedule+0x4c/0x98\n schedule_timeout+0xbc/0xdc\n rcu_gp_fqs_loop+0x308/0x344\n rcu_gp_kthread+0xd8/0xf0\n kthread+0xb8/0xc8\n ret_from_fork+0x10/0x20\nrcu: Stack dump where RCU GP kthread last ran:\nTask dump for CPU 0:\ntask:kworker/u8:10   state:R  running task     stack:0     pid:89    ppid:2      flags:0x0000000a\nWorkqueue: events_unbound io_ring_exit_work\nCall trace:\n __switch_to+0xb0/0xc8\n 0xffff0000c8fefd28\nCPU: 2 PID: 95 Comm: kworker/u8:13 Not tainted 6.2.0-rc5-00042-g40316e337c80-dirty #2759\nHardware name: linux,dummy-virt (DT)\nWorkqueue: events_unbound io_ring_exit_work\npstate: 61400005 (nZCv daif +PAN -UAO -TCO +DIT -SSBS BTYPE=--)\npc : io_do_iopoll+0x344/0x360\nlr : io_do_iopoll+0xb8/0x360\nsp : ffff800009bebc60\nx29: ffff800009bebc60 x28: 0000000000000000 x27: 0000000000000000\nx26: ffff0000c0f67d48 x25: ffff0000c0f67840 x24: ffff800008950024\nx23: 0000000000000001 x22: 0000000000000000 x21: ffff0000c27d3200\nx20: ffff0000c0f67840 x19: ffff0000c0f67800 x18: 0000000000000000\nx17: 0000000000000000 x16: 0000000000000000 x15: 0000000000000000\nx14: 0000000000000001 x13: 0000000000000001 x12: 0000000000000000\nx11: 0000000000000179 x10: 0000000000000870 x9 : ffff800009bebd60\nx8 : ffff0000c27d3ad0 x7 : fefefefefefefeff x6 : 0000646e756f626e\nx5 : ffff0000c0f67840 x4 : 0000000000000000 x3 : ffff0000c2398000\nx2 : 0000000000000000 x1 : 0000000000000000 x0 : 0000000000000000\nCall trace:\n io_do_iopoll+0x344/0x360\n io_uring_try_cancel_requests+0x21c/0x334\n io_ring_exit_work+0x90/0x40c\n process_one_work+0x1a4/0x254\n worker_thread+0x1ec/0x258\n kthread+0xb8/0xc8\n ret_from_fork+0x10/0x20\n\nAdd a cond_resched() in the cancelation IOPOLL loop to fix this.\n\nCc: stable@vger.kernel.org # 5.10+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: add a conditional reschedule to the IOPOLL cancelation loop"
    },
    {
        "commit": "50470fc5723ae0adb2f429a8b27ff6bf1a41913e",
        "message": "io_submit_flush_completions() may produce new task_work items, so it's a\ngood idea to recheck the task_work list after flushing completions. The\noptimisation is not new and was accidentially removed by\nf88262e60bb9 (\"io_uring: lockless task list\")\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a7ed5ede84de190832cc33ebbcdd6e91cd90f5b6.1674484266.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: return normal tw run linking optimisation"
    },
    {
        "commit": "cb6bf7f285c270d7808807128b5bce414e3f254a",
        "message": "Merge almost identical sections of tctx_task_work(), this will make code\nmodifications later easier and also inlines handle_tw_list().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d06592d91e3e7559e7a4dbb8907d110863008dc7.1674484266.git.asml.silence@gmail.com\n[axboe: fold in setting count to zero patch from Tom Rix]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: refactor tctx_task_work"
    },
    {
        "commit": "5afa4650713918e60865ed42d9439e82f6d24773",
        "message": "Add a helper for putting refs from the target task context, rename\n__io_put_task() and add a couple of comments around. Use the remote\nversion for __io_req_complete_post(), the local is only needed for\n__io_submit_flush_completions().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3bf92ebd594769d8a5d648472a8e335f2031d542.1674484266.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: refactor io_put_task helpers"
    },
    {
        "commit": "c8576f3e612d3c5b01d434ae296b9b76d7907708",
        "message": "Follow the io_get_sqe pattern returning the result via a pointer\nand hide request cache refill inside io_alloc_req().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8c37c2e8a3cb5e4cd6a8ae3b91371227a92708a6.1674484266.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: refactor req allocation"
    },
    {
        "commit": "b5083dfa36676e7b5d72bf3d70f429a0d08c5075",
        "message": "Return an SQE from io_get_sqe() as a parameter and use the return value\nto determine if it failed or not. This enables the compiler to compile out\nthe sqe NULL check when we know that the return SQE is valid.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9cceb11329240ea097dffef6bf0a675bca14cf42.1674484266.git.asml.silence@gmail.com\n[axboe: remove bogus const modifier on return value]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: improve io_get_sqe"
    },
    {
        "commit": "b2aa66aff60c841b2c93242752c25abf4c82a28c",
        "message": "__io_cqring_overflow_flush() doesn't return anything anymore, remove\noutdate comment.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4ce2bcbb17eac80cdf883fd1459d5ee6586e238c.1674484266.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: kill outdated comment about overflow flush"
    },
    {
        "commit": "c10bb64684813a326174c3eebcafb3ee5af52ca3",
        "message": "We return POLLIN from io_uring_poll() depending on whether there are\nCQEs for the userspace, and so we should use the user visible tail\npointer instead of a transient cached value.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/228ffcbf30ba98856f66ffdb9a6a60ead1dd96c0.1674484266.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: use user visible tail in io_uring_poll()"
    },
    {
        "commit": "f499254474a83bf60191c86de82c1fec1d8eb9f9",
        "message": "This generates better code for me, avoiding an extra load on arm64, and\nboth call sites already have this variable available for easy passing.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: pass in io_issue_def to io_assign_file()"
    },
    {
        "commit": "c1755c25a7190494b45861284b4a30bd9cd813ff",
        "message": "Every io_uring request is represented by struct io_kiocb, which is\ncached locally by io_uring (not SLAB/SLUB) in the list called\nsubmit_state.freelist. This patch simply enabled KASAN for this free\nlist.\n\nThis list is initially created by KMEM_CACHE, but later, managed by\nio_uring. This patch basically poisons the objects that are not used\n(i.e., they are the free list), and unpoisons it when the object is\nallocated/removed from the list.\n\nTouching these poisoned objects while in the freelist will cause a KASAN\nwarning.\n\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Breno Leitao <leitao@debian.org>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: Enable KASAN for request cache"
    },
    {
        "commit": "b5d3ae202fbfe055aa2a8ae8524531ee1dcab717",
        "message": "If TIF_NOTIFY_RESUME is set, then we need to call resume_user_mode_work()\nfor PF_IO_WORKER threads. They never return to usermode, hence never get\na chance to process any items that are marked by this flag. Most notably\nthis includes the final put of files, but also any throttling markers set\nby block cgroups.\n\nCc: stable@vger.kernel.org # 5.10+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: handle TIF_NOTIFY_RESUME when checking for task_work"
    },
    {
        "commit": "8572df941cbef2b295282535b013828e7df39471",
        "message": "If the target ring is using IORING_SETUP_SINGLE_ISSUER and we're posting\na message from a different thread, then we need to ensure that the\nfallback task_work that posts the CQE knwos about the flags passing as\nwell. If not we'll always be posting 0 as the flags.\n\nFixes: 3563d7ed58a5 (\"io_uring/msg_ring: Pass custom flags to the cqe\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring/msg-ring: ensure flags passing works for task_work completions"
    },
    {
        "commit": "f30bd4d03824fb437bf080c2b2f926cfee3f09d0",
        "message": "This patch removes some \"cold\" fields from `struct io_issue_def`.\n\nThe plan is to keep only highly used fields into `struct io_issue_def`, so,\nit may be hot in the cache. The hot fields are basically all the bitfields\nand the callback functions for .issue and .prep.\n\nThe other less frequently used fields are now located in a secondary and\ncold struct, called `io_cold_def`.\n\nThis is the size for the structs:\n\nBefore: io_issue_def = 56 bytes\nAfter: io_issue_def = 24 bytes; io_cold_def = 40 bytes\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20230112144411.2624698-2-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: Split io_issue_def struct"
    },
    {
        "commit": "a7dd27828b00be8c0c7520c53baf0b360f4d8bea",
        "message": "The current io_op_def struct is becoming huge and the name is a bit\ngeneric.\n\nThe goal of this patch is to rename this struct to `io_issue_def`. This\nstruct will contain the hot functions associated with the issue code\npath.\n\nFor now, this patch only renames the structure, and an upcoming patch\nwill break up the structure in two, moving the non-issue fields to a\nsecondary struct.\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20230112144411.2624698-1-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: Rename struct io_op_def"
    },
    {
        "commit": "68a2cc1bba98144340f6ed66f3aed57e914766d0",
        "message": "Keep parts of __io_req_complete_post() relying on req->flags together so\nthe value can be cached.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2b4fbb42f404a0e75c4d9f0a5b16f314a839d0a9.1673887636.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: refactor __io_req_complete_post"
    },
    {
        "commit": "632ffe0956740ba56ae3f83778f3bf97edd57c69",
        "message": "There may be different cost for reeading just one byte or more, so it's\nbenificial to keep ctx flag bits that we access together in a single\nbyte. That affected code generation of __io_cq_unlock_post_flush() and\nremoved one memory load.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bbe8ca4705704690319d65e45845f9fc9d35f420.1673887636.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: optimise ctx flags layout"
    },
    {
        "commit": "31f084b7b0288fd51740b1e1efdb0ff61fb81e48",
        "message": "Lock the ring with uring_lock in io_fallback_req_func(), which should\nmake it a bit safer and easier. With that we also don't need refs\npinning as io_ring_exit_work() will wait until uring_lock is freed.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/56170e6a0cbfc8edee2794c6613e8f6f1d76d276.1673887636.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: simplify fallback execution"
    },
    {
        "commit": "89800a2dd570919bfe01ced90c80e3b472d1c723",
        "message": "io_put_task() is only used in uring.c so enclose it there together with\n__io_put_task().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/43c7f9227e2ab215f1a6069dadbc5382bed346fe.1673887636.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: don't export io_put_task()"
    },
    {
        "commit": "b0b7a7d24b66109a940d09d8d2dcf513e4eaf3a1",
        "message": "io_submit_flush_completions() may queue new requests for tw execution,\nespecially true for linked requests. Recheck the tw list for emptiness\nafter flushing completions.\n\nNote that this doesn't really fix the commit referenced below, but it\ndoes reinstate an optimization that existed before that got merged.\n\nFixes: f88262e60bb9 (\"io_uring: lockless task list\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6328acdbb5e60efc762b18003382de077e6e1367.1673887636.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:41 -0700 io_uring: return back links tw run optimisation"
    },
    {
        "commit": "88b80534f60f5ddf5f42218b280e0370f95eae78",
        "message": "Change the return type to void since it always return 0, and no need\nto do the checking in syscall io_uring_enter.\n\nSigned-off-by: Quanfa Fu <quanfafu@gmail.com>\nLink: https://lore.kernel.org/r/20230115071519.554282-1-quanfafu@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: make io_sqpoll_wait_sq return void"
    },
    {
        "commit": "c3f4d39ee4bc06f975ff45012398342ff276eb69",
        "message": "We needed fake nodes in __io_run_local_work() and to avoid unecessary wake\nups while the task already running task_works, but we don't need them\nanymore since wake ups are protected by cq_waiting, which is always\ncleared by the time we're executing deferred task_work items.\n\nNote that because of loose sync around cq_waiting clearing\nio_req_local_work_add() may wake the task more than once, but that's\nfine and should be rare to not hurt perf.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8839534891f0a2f1076e78554a31ea7e099f7de5.1673274244.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: optimise deferred tw execution"
    },
    {
        "commit": "d80c0f00d04766972c95e72b7535a842d6f4680d",
        "message": "Don't wake the master task after queueing a deferred tw unless it's\ncurrently waiting in io_cqring_wait.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/717702d772825a6647e6c315b4690277ba84c3fc.1673274244.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: add io_req_local_work_add wake fast path"
    },
    {
        "commit": "130bd686d9be918e4cc8c03abf5794ba2d860502",
        "message": "With DEFER_TASKRUN only ctx->submitter_task might be waiting for CQEs,\nwe can use this to optimise io_cqring_wait(). Replace ->cq_wait\nwaitqueue with waking the task directly.\n\nIt works but misses an important optimisation covered by the following\npatch, so this patch without follow ups might hurt performance.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/103d174d35d919d4cb0922d8a9c93a8f0c35f74a.1673274244.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: waitqueue-less cq waiting"
    },
    {
        "commit": "3181e22fb79910c7071e84a43af93ac89e8a7106",
        "message": "Flush completions is done either from the submit syscall or by the\ntask_work, both are in the context of the submitter task, and when it\ngoes for a single threaded rings like implied by ->task_complete, there\nwon't be any waiters on ->cq_wait but the master task. That means that\nthere can be no tasks sleeping on cq_wait while we run\n__io_submit_flush_completions() and so waking up can be skipped.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/60ad9768ec74435a0ddaa6eec0ffa7729474f69f.1673274244.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: wake up optimisations"
    },
    {
        "commit": "bca39f39058567643487cd654970717705784ba3",
        "message": "Even though io_poll_wq_wake()'s waitqueue_active reuses a barrier we do\nfor another waitqueue, it's not going to be the case in the future and\nso we want to have a fast path for it when the ring has never been\npolled.\n\nMove poll_wq wake ups into __io_commit_cqring_flush() using a new flag\ncalled ->poll_activated. The idea behind the flag is to set it when the\nring was polled for the first time. This requires additional sync to not\nmiss events, which is done here by using task_work for ->task_complete\nrings, and by default enabling the flag for all other types of rings.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/060785e8e9137a920b232c0c7f575b131af19cac.1673274244.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: add lazy poll_wq activation"
    },
    {
        "commit": "7b235dd82ad32c1626e51303d94ec5ef4d7bc994",
        "message": "Don't use ->cq_wait for ring polling but add a separate wait queue for\nit. We need it for following patches.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/dea0be0bf990503443c5c6c337fc66824af7d590.1673274244.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: separate wq for ring polling"
    },
    {
        "commit": "360173ab9e1a8a50bc9092ae8c741f0a05d499b7",
        "message": "io_run_local_work_locked() is only used in io_uring.c, move it there.\nWith that we can also make __io_run_local_work() static.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/91757bcb33e5774e49fed6f2b6e058630608119b.1673274244.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: move io_run_local_work_locked"
    },
    {
        "commit": "3e5655552a8299492d54117a15b6ddf5a2e7512c",
        "message": "io_run_local_work is enclosed in io_uring.c, we don't need to export it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b477fb81f5e77044f724a06fe245d5c078659364.1673274244.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: mark io_run_local_work static"
    },
    {
        "commit": "2f413956cc8a72fc11c2779228112559bbca8279",
        "message": "The CQ waiting loop sets TASK_RUNNING before trying to execute\ntask_work, no need to repeat it in io_run_local_work().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9d9422c429ef3f9457b4f4b8288bf4789564f33b.1673274244.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: don't set TASK_RUNNING in local tw runner"
    },
    {
        "commit": "bd550173acc2dc782d9c57852f6b6e71f5d9a159",
        "message": "Remove a local variable ctx in io_wake_function(), we don't need it if\nio_should_wake() triggers it to wake up.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e60eb1008aebe286aab7d34c772ed01c447bddb1.1673274244.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: refactor io_wake_function"
    },
    {
        "commit": "dde40322ae20f2d6b0bcb781a9eedcfc7ca3aa73",
        "message": "->submitter_task is used somewhat more frequent now than before, i.e.\nfor local tw enqueue and run, let's move it from the end of ctx, which\nis full of cold data, to the first cacheline with mostly constants.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/415ca91dc5ad1dec612b892e489cda98e1069542.1673274244.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: move submitter_task out of cold cacheline"
    },
    {
        "commit": "81594e7e7a146888f0bac4fa782b0b5d3c37fdff",
        "message": "The IS_ERR function uses the IS_ERR_VALUE macro under the hood which\nalready wraps the condition into unlikely.\n\nSigned-off-by: Dmitrii Bundin <dmitrii.bundin.a@gmail.com>\nLink: https://lore.kernel.org/r/20230109185854.25698-1-dmitrii.bundin.a@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: remove excessive unlikely on IS_ERR"
    },
    {
        "commit": "cbeb47a7b5f003429ded32b1fb3a7108ce5c1b54",
        "message": "This patch adds a new flag (IORING_MSG_RING_FLAGS_PASS) in the message\nring operations (IORING_OP_MSG_RING). This new flag enables the sender\nto specify custom flags, which will be copied over to cqe->flags in the\nreceiving ring.  These custom flags should be specified using the\nsqe->file_index field.\n\nThis mechanism provides additional flexibility when sending messages\nbetween rings.\n\nSigned-off-by: Breno Leitao <leitao@debian.org>\nLink: https://lore.kernel.org/r/20230103160507.617416-1-leitao@debian.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring/msg_ring: Pass custom flags to the cqe"
    },
    {
        "commit": "d33a39e577687e12d4468e9dd999375b9973d700",
        "message": "Move waiting timeout into io_wait_queue\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e4b48a9e26a3b1cf97c80121e62d4b5ab873d28d.1672916894.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: keep timeout in io_wait_queue"
    },
    {
        "commit": "46ae7eef44f6dfd825a3bcfa43392d3ad9836ada",
        "message": "Unlike the jiffy scheduling version, schedule_hrtimeout() jumps a few\nfunctions before getting into schedule() even if there is no actual\ntimeout needed. Some tests showed that it takes up to 1% of CPU cycles.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/89f880574eceee6f4899783377ead234df7b3d04.1672916894.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: optimise non-timeout waiting"
    },
    {
        "commit": "326a9e482e2134d7a44b7f8f9a721b38c6bbb146",
        "message": "Instead of constantly watching that the state of the task is running\nbefore executing tw or taking locks in io_cqring_wait(), switch it back\nto TASK_RUNNING immediately.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/246dddee247d89fd52023f785ed17cc34962a008.1672916894.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: set TASK_RUNNING right after schedule"
    },
    {
        "commit": "490c00eb4fa5e5e25e0127240f6d6c1b499da95b",
        "message": "->work_llist should never be non-empty for a non DEFER_TASKRUN ring, so\nwe can safely skip checking the flag.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/26af9f73c09a56c9a035f94db56127358688f3aa.1672916894.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: simplify io_has_work"
    },
    {
        "commit": "846072f16eed3b3fb4e59b677f3ed8afb8509b89",
        "message": "io_cqring_wait_schedule() is called after we started waiting on the cq\nwq and set the state to TASK_INTERRUPTIBLE, for that reason we have to\nconstantly worry whether we has returned the state back to running or\nnot. Leave only quick checks in io_cqring_wait_schedule() and move the\nrest including running task work to the callers. Note, we run tw in the\nloop after the sched checks because of the fast path in the beginning of\nthe function.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2814fabe75e2e019e7ca43ea07daa94564349805.1672916894.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:40 -0700 io_uring: mimimise io_cqring_wait_schedule"
    },
    {
        "commit": "3fcf19d592d5cb63eb209400b22055651e3c27d0",
        "message": "We already avoid flushing overflows in io_cqring_wait_schedule() but\nonly return an error for the outer loop to handle it. Minimise it even\nfurther by moving all ->check_cq parsing there.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9dfcec3121013f98208dbf79368d636d74e1231a.1672916894.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:39 -0700 io_uring: parse check_cq out of wq waiting"
    },
    {
        "commit": "140102ae9a9f2f83f0592b98b3c5c6119d9a9b32",
        "message": "Most places that want to run local tw explicitly and in advance check if\nthey are allowed to do so. Don't rely on a similar check in\n__io_run_local_work(), leave it as a just-in-case warning and make sure\ncallers checks capabilities themselves.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/990fe0e8e70fd4d57e43625e5ce8fba584821d1a.1672916894.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:39 -0700 io_uring: move defer tw task checks"
    },
    {
        "commit": "1414d62985848d095af5a400a4ca074a4888b77f",
        "message": "There is only one user of io_run_task_work_ctx(), inline it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/40953c65f7c88fb00cdc4d870ca5d5319fb3d7ea.1672916894.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:39 -0700 io_uring: kill io_run_task_work_ctx"
    },
    {
        "commit": "f36ba6cf1ab6b05a538aae9cca896917db14ba27",
        "message": "Task work runners keep running until all queues tw items are exhausted.\nIt's also rare for defer tw to queue normal tw and vise versa. Taking it\ninto account, there is only a dim chance that further iterating the\nio_cqring_wait() fast path will get us anything and so we can remove\nthe loop there.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1f9565726661266abaa5d921e97433c831759ecf.1672916894.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:39 -0700 io_uring: don't iterate cq wait fast path"
    },
    {
        "commit": "0c4fe008c9cb2215b3f838769886857ae986014b",
        "message": "There should be nothing in the ->work_llist for non DEFER_TASKRUN rings,\nso we can skip flag checks and test the list emptiness directly. Also\nmove it out of io_run_local_work() for inlining.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/331d63fd15ca79b35b95c82a82d9246110686392.1672916894.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-29 15:17:39 -0700 io_uring: rearrange defer list checks"
    },
    {
        "commit": "f851453bf19554a42eb480b65436b9500c3cf392",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two small fixes for this release:\n\n   - Sanitize how async prep is done for drain requests, so we ensure\n     that it always gets done (Dylan)\n\n   - A ring provided buffer recycling fix for multishot receive (me)\"\n\n* tag 'io_uring-6.2-2023-01-27' of git://git.kernel.dk/linux:\n  io_uring: always prep_async for drain requests\n  io_uring/net: cache provided buffer group value for multishot receives",
        "kernel_version": "v6.2-rc6",
        "release_date": "2023-01-27 16:15:06 -0800 Merge tag 'io_uring-6.2-2023-01-27' of git://git.kernel.dk/linux"
    },
    {
        "commit": "36632d062975a9ff4410c90dd6d37922b68d0920",
        "message": "Zero-length arrays are deprecated[1]. Replace struct io_uring_buf_ring's\n\"bufs\" with a flexible array member. (How is the size of this array\nverified?) Detected with GCC 13, using -fstrict-flex-arrays=3:\n\nIn function 'io_ring_buffer_select',\n    inlined from 'io_buffer_select' at io_uring/kbuf.c:183:10:\nio_uring/kbuf.c:141:23: warning: array subscript 255 is outside the bounds of an interior zero-length array 'struct io_uring_buf[0]' [-Wzero-length-bounds]\n  141 |                 buf = &br->bufs[head];\n      |                       ^~~~~~~~~~~~~~~\nIn file included from include/linux/io_uring.h:7,\n                 from io_uring/kbuf.c:10:\ninclude/uapi/linux/io_uring.h: In function 'io_buffer_select':\ninclude/uapi/linux/io_uring.h:628:41: note: while referencing 'bufs'\n  628 |                 struct io_uring_buf     bufs[0];\n      |                                         ^~~~\n\n[1] https://www.kernel.org/doc/html/latest/process/deprecated.html#zero-length-and-one-element-arrays\n\nFixes: c7fb19428d67 (\"io_uring: add support for ring mapped supplied buffers\")\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Pavel Begunkov <asml.silence@gmail.com>\nCc: \"Gustavo A. R. Silva\" <gustavoars@kernel.org>\nCc: stable@vger.kernel.org\nCc: io-uring@vger.kernel.org\nSigned-off-by: Kees Cook <keescook@chromium.org>\nReviewed-by: Gustavo A. R. Silva <gustavoars@kernel.org>\nLink: https://lore.kernel.org/r/20230105190507.gonna.131-kees@kernel.org",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-27 11:42:57 -0800 io_uring: Replace 0-length array with flexible array"
    },
    {
        "commit": "ef5c600adb1d985513d2b612cc90403a148ff287",
        "message": "Drain requests all go through io_drain_req, which has a quick exit in case\nthere is nothing pending (ie the drain is not useful). In that case it can\nrun the issue the request immediately.\n\nHowever for safety it queues it through task work.\nThe problem is that in this case the request is run asynchronously, but\nthe async work has not been prepared through io_req_prep_async.\n\nThis has not been a problem up to now, as the task work always would run\nbefore returning to userspace, and so the user would not have a chance to\nrace with it.\n\nHowever - with IORING_SETUP_DEFER_TASKRUN - this is no longer the case and\nthe work might be defered, giving userspace a chance to change data being\nreferred to in the request.\n\nInstead _always_ prep_async for drain requests, which is simpler anyway\nand removes this issue.\n\nCc: stable@vger.kernel.org\nFixes: c0e0d6ba25f1 (\"io_uring: add IORING_SETUP_DEFER_TASKRUN\")\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20230127105911.2420061-1-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc6",
        "release_date": "2023-01-27 06:29:29 -0700 io_uring: always prep_async for drain requests"
    },
    {
        "commit": "8e4ff684762b6503db45e8906e258faee080c336",
        "message": "The 'ublk_chr_class' is needed when deleting ublk char devices in\nublk_exit(), so move it after devices(idle) are removed.\n\nFixes the following warning reported by Harris, James R:\n\n[  859.178950] sysfs group 'power' not found for kobject 'ublkc0'\n[  859.178962] WARNING: CPU: 3 PID: 1109 at fs/sysfs/group.c:278 sysfs_remove_group+0x9c/0xb0\n\nReported-by: \"Harris, James R\" <james.r.harris@intel.com>\nFixes: 71f28f3136af (\"ublk_drv: add io_uring based userspace block driver\")\nLink: https://lore.kernel.org/linux-block/Y9JlFmSgDl3+zy3N@T590/T/#t\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nTested-by: Jim Harris <james.r.harris@intel.com>\nLink: https://lore.kernel.org/r/20230126115346.263344-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc6",
        "release_date": "2023-01-26 07:55:21 -0700 block: ublk: move ublk_chr_class destroying after devices are removed"
    },
    {
        "commit": "8ef0ca4a177d2dbe6680fb4388173897900a4ed1",
        "message": "* thermal: (734 commits)\n  thermal: core: call put_device() only after device_register() fails\n  Linux 6.2-rc4\n  kbuild: Fix CFI hash randomization with KASAN\n  firmware: coreboot: Check size of table entry and use flex-array\n  kallsyms: Fix scheduling with interrupts disabled in self-test\n  ata: pata_cs5535: Don't build on UML\n  lockref: stop doing cpu_relax in the cmpxchg loop\n  x86/pci: Treat EfiMemoryMappedIO as reservation of ECAM space\n  efi: tpm: Avoid READ_ONCE() for accessing the event log\n  io_uring: lock overflowing for IOPOLL\n  ALSA: pcm: Move rwsem lock inside snd_ctl_elem_read to prevent UAF\n  iommu/mediatek-v1: Fix an error handling path in mtk_iommu_v1_probe()\n  iommu/iova: Fix alloc iova overflows issue\n  iommu: Fix refcount leak in iommu_device_claim_dma_owner\n  iommu/arm-smmu-v3: Don't unregister on shutdown\n  iommu/arm-smmu: Don't unregister on shutdown\n  iommu/arm-smmu: Report IOMMU_CAP_CACHE_COHERENCY even betterer\n  platform/x86: thinkpad_acpi: Fix profile mode display in AMT mode\n  ALSA: usb-audio: Fix possible NULL pointer dereference in snd_usb_pcm_has_fixed_rate()\n  platform/x86: int3472/discrete: Ensure the clk/power enable pins are in output mode\n  ...",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-24 21:12:49 +0100 Merge back other thermal control material for 6.3."
    },
    {
        "commit": "b00c51ef8f72ced0965d021a291b98ff822c5337",
        "message": "If we're using ring provided buffers with multishot receive, and we end\nup doing an io-wq based issue at some points that also needs to select\na buffer, we'll lose the initially assigned buffer group as\nio_ring_buffer_select() correctly clears the buffer group list as the\nissue isn't serialized by the ctx uring_lock. This is fine for normal\nreceives as the request puts the buffer and finishes, but for multishot,\nwe will re-arm and do further receives. On the next trigger for this\nmultishot receive, the receive will try and pick from a buffer group\nwhose value is the same as the buffer ID of the las receive. That is\nobviously incorrect, and will result in a premature -ENOUFS error for\nthe receive even if we had available buffers in the correct group.\n\nCache the buffer group value at prep time, so we can restore it for\nfuture receives. This only needs doing for the above mentioned case, but\njust do it by default to keep it easier to read.\n\nCc: stable@vger.kernel.org\nFixes: b3fdea6ecb55 (\"io_uring: multishot recv\")\nFixes: 9bb66906f23e (\"io_uring: support multishot in recvmsg\")\nCc: Dylan Yudaken <dylany@meta.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc6",
        "release_date": "2023-01-23 07:08:08 -0700 io_uring/net: cache provided buffer group value for multishot receives"
    },
    {
        "commit": "95f184d0e1e14e6fd4368a804db5f870e5f841d2",
        "message": "Pull another io_uring fix from Jens Axboe:\n \"Just a single fix for a regression that happened in this release due\n  to a poll change. Normally I would've just deferred it to next week,\n  but since the original fix got picked up by stable, I think it's\n  better to just send this one off separately.\n\n  The issue is around the poll race fix, and how it mistakenly also got\n  applied to multishot polling. Those don't need the race fix, and we\n  should not be doing any reissues for that case. Exhaustive test cases\n  were written and committed to the liburing regression suite for the\n  reported issue, and additions for similar issues\"\n\n* tag 'io_uring-6.2-2023-01-21' of git://git.kernel.dk/linux:\n  io_uring/poll: don't reissue in case of poll race on multishot request",
        "kernel_version": "v6.2-rc5",
        "release_date": "2023-01-21 16:21:56 -0800 Merge tag 'io_uring-6.2-2023-01-21' of git://git.kernel.dk/linux"
    },
    {
        "commit": "8caa03f10bf92cb8657408a6ece6a8a73f96ce13",
        "message": "A previous commit fixed a poll race that can occur, but it's only\napplicable for multishot requests. For a multishot request, we can safely\nignore a spurious wakeup, as we never leave the waitqueue to begin with.\n\nA blunt reissue of a multishot armed request can cause us to leak a\nbuffer, if they are ring provided. While this seems like a bug in itself,\nit's not really defined behavior to reissue a multishot request directly.\nIt's less efficient to do so as well, and not required to rearm anything\nlike it is for singleshot poll requests.\n\nCc: stable@vger.kernel.org\nFixes: 6e5aedb9324a (\"io_uring/poll: attempt request issue after racy poll wakeup\")\nReported-and-tested-by: Olivier Langlois <olivier@trillion01.com>\nLink: https://github.com/axboe/liburing/issues/778\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc5",
        "release_date": "2023-01-20 15:11:54 -0700 io_uring/poll: don't reissue in case of poll race on multishot request"
    },
    {
        "commit": "9c38747f0cdb20516de3d708f39720762786750a",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Fixes for the MSG_RING opcode. Nothing really major:\n\n   - Fix an overflow missing serialization around posting CQEs to the\n     target ring (me)\n\n   - Disable MSG_RING on a ring that isn't enabled yet. There's nothing\n     really wrong with allowing it, but 1) it's somewhat odd as nobody\n     can receive them yet, and 2) it means that using the right delivery\n     mechanism might change. As nobody should be sending CQEs to a ring\n     that isn't enabled yet, let's just disable it (Pavel)\n\n   - Tweak to when we decide to post remotely or not for MSG_RING\n     (Pavel)\"\n\n* tag 'io_uring-6.2-2023-01-20' of git://git.kernel.dk/linux:\n  io_uring/msg_ring: fix remote queue to disabled ring\n  io_uring/msg_ring: fix flagging remote execution\n  io_uring/msg_ring: fix missing lock on overflow for IOPOLL\n  io_uring/msg_ring: move double lock/unlock helpers higher up",
        "kernel_version": "v6.2-rc5",
        "release_date": "2023-01-20 12:39:45 -0800 Merge tag 'io_uring-6.2-2023-01-20' of git://git.kernel.dk/linux"
    },
    {
        "commit": "8579538c89e33ce78be2feb41e07489c8cbf8f31",
        "message": "IORING_SETUP_R_DISABLED rings don't have the submitter task set, so\nit's not always safe to use ->submitter_task. Disallow posting msg_ring\nmessaged to disabled rings. Also add task NULL check for loosy sync\naround testing for IORING_SETUP_R_DISABLED.\n\nCc: stable@vger.kernel.org\nFixes: 6d043ee1164ca (\"io_uring: do msg_ring in target task via tw\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc5",
        "release_date": "2023-01-20 09:49:34 -0700 io_uring/msg_ring: fix remote queue to disabled ring"
    },
    {
        "commit": "56d8e3180c065c9b78ed77afcd0cf99677a4e22f",
        "message": "There is a couple of problems with queueing a tw in io_msg_ring_data()\nfor remote execution. First, once we queue it the target ring can\ngo away and so setting IORING_SQ_TASKRUN there is not safe. Secondly,\nthe userspace might not expect IORING_SQ_TASKRUN.\n\nExtract a helper and uniformly use TWA_SIGNAL without TWA_SIGNAL_NO_IPI\ntricks for now, just as it was done in the original patch.\n\nCc: stable@vger.kernel.org\nFixes: 6d043ee1164ca (\"io_uring: do msg_ring in target task via tw\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc5",
        "release_date": "2023-01-20 09:49:29 -0700 io_uring/msg_ring: fix flagging remote execution"
    },
    {
        "commit": "e12d7a46f65ae4b7d58a5e0c1cbfa825cf8d830d",
        "message": "If the target ring is configured with IOPOLL, then we always need to hold\nthe target ring uring_lock before posting CQEs. We could just grab it\nunconditionally, but since we don't expect many target rings to be of this\ntype, make grabbing the uring_lock conditional on the ring type.\n\nLink: https://lore.kernel.org/io-uring/Y8krlYa52%2F0YGqkg@ip-172-31-85-199.ec2.internal/\nReported-by: Xingyuan Mo <hdthky0@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc5",
        "release_date": "2023-01-19 10:43:59 -0700 io_uring/msg_ring: fix missing lock on overflow for IOPOLL"
    },
    {
        "commit": "423d5081d0451faa59a707e57373801da5b40141",
        "message": "In preparation for needing them somewhere else, move them and get rid of\nthe unused 'issue_flags' for the unlock side.\n\nNo functional changes in this patch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc5",
        "release_date": "2023-01-19 09:01:27 -0700 io_uring/msg_ring: move double lock/unlock helpers higher up"
    },
    {
        "commit": "6a6dcae8f486c3f3298d0767d34505121c7b0b81",
        "message": "The default queue mapping builder of blk_mq_map_queues doesn't take NUMA\ntopo into account, so the built mapping is pretty bad, since CPUs\nbelonging to different NUMA node are assigned to same queue. It is\nobserved that IOPS drops by ~30% when running two jobs on same hctx\nof null_blk from two CPUs belonging to two NUMA nodes compared with\nfrom same NUMA node.\n\nAddress the issue by reusing group_cpus_evenly() for building queue mapping\nsince group_cpus_evenly() does group cpus according to CPU/NUMA locality.\n\nAlso performance data becomes more stable with this given correct queue\nmapping is applied wrt. numa locality viewpoint, for example, on one two\nnodes arm64 machine with 160 cpus, node 0(cpu 0~79), node 1(cpu 80~159):\n\n1) modprobe null_blk nr_devices=1 submit_queues=2\n\n2) run 'fio(t/io_uring -p 0 -n 4 -r 20 /dev/nullb0)', and observe that\nIOPS becomes much stable on multiple tests:\n\n - unpatched: IOPS is 2.5M ~ 4.5M\n - patched:   IOPS is 4.3M ~ 5.0M\n\nLots of drivers may benefit from the change, such as nvme pci poll,\nnvme tcp, ...\n\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nSigned-off-by: Thomas Gleixner <tglx@linutronix.de>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: John Garry <john.g.garry@oracle.com>\nReviewed-by: Jens Axboe <axboe@kernel.dk>                                                                                                                                                                                                    \nLink: https://lore.kernel.org/r/20221227022905.352674-7-ming.lei@redhat.com",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-17 18:50:06 +0100 blk-mq: Build default queue map via group_cpus_evenly()"
    },
    {
        "commit": "7746564793978fe2f43b18a302b22dca0ad3a0e8",
        "message": "When there are no read queues read requests will be assigned a\ndefault queue on allocation. However, blk_mq_get_cached_request() is not\nprepared for that and will fail all attempts to grab read requests from\nthe cache. Worst case it doubles the number of requests allocated,\nroughly half of which will be returned by blk_mq_free_plug_rqs().\n\nIt only affects batched allocations and so is io_uring specific.\nFor reference, QD8 t/io_uring benchmark improves by 20-35%.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/80d4511011d7d4751b4cf6375c4e38f237d935e3.1673955390.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc5",
        "release_date": "2023-01-17 09:56:52 -0700 block: fix hctx checks for batch allocation"
    },
    {
        "commit": "c1917514107982794fd448ee2596878f396ee6a8",
        "message": "The details of the iov_iter types are appropriately abstracted, so\nthere's no need to check for specific type fields. Just let the\nabstractions handle it.\n\nThis is preparing for io_uring/net's io_send to utilize the more\nefficient ITER_UBUF.\n\nSigned-off-by: Keith Busch <kbusch@kernel.org>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nLink: https://lore.kernel.org/r/20230111184245.3784393-1-kbusch@meta.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-13 20:44:20 -0800 caif: don't assume iov_iter type"
    },
    {
        "commit": "2ce7592df99f7356cc8697ad10849987237abca4",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A fix for a regression that happened last week, rest is fixes that\n  will be headed to stable as well. In detail:\n\n   - Fix for a regression added with the leak fix from last week (me)\n\n   - In writing a test case for that leak, inadvertently discovered a\n     case where we a poll request can race. So fix that up and mark it\n     for stable, and also ensure that fdinfo covers both the poll tables\n     that we have. The latter was an oversight when the split poll table\n     were added (me)\n\n   - Fix for a lockdep reported issue with IOPOLL (Pavel)\"\n\n* tag 'io_uring-6.2-2023-01-13' of git://git.kernel.dk/linux:\n  io_uring: lock overflowing for IOPOLL\n  io_uring/poll: attempt request issue after racy poll wakeup\n  io_uring/fdinfo: include locked hash table in fdinfo output\n  io_uring/poll: add hash if ready poll request can't complete inline\n  io_uring/io-wq: only free worker if it was allocated for creation",
        "kernel_version": "v6.2-rc4",
        "release_date": "2023-01-13 17:37:09 -0600 Merge tag 'io_uring-6.2-2023-01-13' of git://git.kernel.dk/linux"
    },
    {
        "commit": "544d163d659d45a206d8929370d5a2984e546cb7",
        "message": "syzbot reports an issue with overflow filling for IOPOLL:\n\nWARNING: CPU: 0 PID: 28 at io_uring/io_uring.c:734 io_cqring_event_overflow+0x1c0/0x230 io_uring/io_uring.c:734\nCPU: 0 PID: 28 Comm: kworker/u4:1 Not tainted 6.2.0-rc3-syzkaller-16369-g358a161a6a9e #0\nWorkqueue: events_unbound io_ring_exit_work\nCall trace:\n\u00a0io_cqring_event_overflow+0x1c0/0x230 io_uring/io_uring.c:734\n\u00a0io_req_cqe_overflow+0x5c/0x70 io_uring/io_uring.c:773\n\u00a0io_fill_cqe_req io_uring/io_uring.h:168 [inline]\n\u00a0io_do_iopoll+0x474/0x62c io_uring/rw.c:1065\n\u00a0io_iopoll_try_reap_events+0x6c/0x108 io_uring/io_uring.c:1513\n\u00a0io_uring_try_cancel_requests+0x13c/0x258 io_uring/io_uring.c:3056\n\u00a0io_ring_exit_work+0xec/0x390 io_uring/io_uring.c:2869\n\u00a0process_one_work+0x2d8/0x504 kernel/workqueue.c:2289\n\u00a0worker_thread+0x340/0x610 kernel/workqueue.c:2436\n\u00a0kthread+0x12c/0x158 kernel/kthread.c:376\n\u00a0ret_from_fork+0x10/0x20 arch/arm64/kernel/entry.S:863\n\nThere is no real problem for normal IOPOLL as flush is also called with\nuring_lock taken, but it's getting more complicated for IOPOLL|SQPOLL,\nfor which __io_cqring_overflow_flush() happens from the CQ waiting path.\n\nReported-and-tested-by: syzbot+6805087452d72929404e@syzkaller.appspotmail.com\nCc: stable@vger.kernel.org # 5.10+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc4",
        "release_date": "2023-01-13 07:32:46 -0700 io_uring: lock overflowing for IOPOLL"
    },
    {
        "commit": "6e5aedb9324aab1c14a23fae3d8eeb64a679c20e",
        "message": "If we have multiple requests waiting on the same target poll waitqueue,\nthen it's quite possible to get a request triggered and get disappointed\nin not being able to make any progress with it. If we race in doing so,\nwe'll potentially leave the poll request on the internal tables, but\nremoved from the waitqueue. That means that any subsequent trigger of\nthe poll waitqueue will not kick that request into action, causing an\napplication to potentially wait for completion of a request that will\nnever happen.\n\nFix this by adding a new poll return state, IOU_POLL_REISSUE. Rather\nthan have complicated logic for how to re-arm a given type of request,\njust punt it for a reissue.\n\nWhile in there, move the 'ret' variable to the only section where it\ngets used. This avoids confusion the scope of it.\n\nCc: stable@vger.kernel.org\nFixes: eb0089d629ba (\"io_uring: single shot poll removal optimisation\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc4",
        "release_date": "2023-01-12 10:35:51 -0700 io_uring/poll: attempt request issue after racy poll wakeup"
    },
    {
        "commit": "ea97cbebaf861d99c3e892275147e6fca6d2c1ca",
        "message": "A previous commit split the hash table for polled requests into two\nparts, but didn't get the fdinfo output updated. This means that it's\nless useful for debugging, as we may think a given request is not pending\npoll.\n\nFix this up by dumping the locked hash table contents too.\n\nFixes: 9ca9fb24d5fe (\"io_uring: mutex locked poll hashing\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc4",
        "release_date": "2023-01-10 10:24:52 -0700 io_uring/fdinfo: include locked hash table in fdinfo output"
    },
    {
        "commit": "febb985c06cb6f5fac63598c0bffd4fd823d110d",
        "message": "If we don't, then we may lose access to it completely, leading to a\nrequest leak. This will eventually stall the ring exit process as\nwell.\n\nCc: stable@vger.kernel.org\nFixes: 49f1c68e048f (\"io_uring: optimise submission side poll_refs\")\nReported-and-tested-by: syzbot+6c95df01470a47fc3af4@syzkaller.appspotmail.com\nLink: https://lore.kernel.org/io-uring/0000000000009f829805f1ce87b2@google.com/\nSuggested-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc4",
        "release_date": "2023-01-09 15:46:57 -0700 io_uring/poll: add hash if ready poll request can't complete inline"
    },
    {
        "commit": "4397a17c1dc53f436285f372432dd1aea44e7953",
        "message": "io_uring is using iter_ubuf types for single vector requests. We expect\nstate restore may happen for this type now, and it is already handled\ncorrectly, so suppress the warning.\n\nSigned-off-by: Keith Busch <kbusch@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nReviewed-by: Christoph Hellwig <hch@lst.de>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-08 20:59:17 -0700 iov_iter: move iter_ubuf check inside restore WARN"
    },
    {
        "commit": "1e23db450cff5f0410480137041181d1514bda2a",
        "message": "This is more efficient than iter_iov.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\n[merge to 6.2, minor fixes]\nSigned-off-by: Keith Busch <kbusch@kernel.org>\nReviewed-by: Christoph Hellwig <hch@lst.de>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-08 20:59:17 -0700 io_uring: use iter_ubuf for single range imports"
    },
    {
        "commit": "4b61152e107a95bc0a73d84072dbd75cb97689e3",
        "message": "This is more efficient than iter_iov.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\n[merged to 6.2]\nSigned-off-by: Keith Busch <kbusch@kernel.org>\nReviewed-by: Christoph Hellwig <hch@lst.de>",
        "kernel_version": "v6.3-rc1",
        "release_date": "2023-01-08 20:59:17 -0700 io_uring: switch network send/recv to ITER_UBUF"
    },
    {
        "commit": "e6db6f9398dadcbc06318a133d4c44a2d3844e61",
        "message": "We have two types of task_work based creation, one is using an existing\nworker to setup a new one (eg when going to sleep and we have no free\nworkers), and the other is allocating a new worker. Only the latter\nshould be freed when we cancel task_work creation for a new worker.\n\nFixes: af82425c6a2d (\"io_uring/io-wq: free worker if task_work creation is canceled\")\nReported-by: syzbot+d56ec896af3637bdb7e4@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc4",
        "release_date": "2023-01-08 10:39:17 -0700 io_uring/io-wq: only free worker if it was allocated for creation"
    },
    {
        "commit": "ef1a4a770994b97a86988fd86f5a2784b87449f7",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A few minor fixes that should go into the 6.2 release:\n\n   - Fix for a memory leak in io-wq worker creation, if we ultimately\n     end up canceling the worker creation before it gets created (me)\n\n   - lockdep annotations for the CQ locking (Pavel)\n\n   - A regression fix for CQ timeout handling (Pavel)\n\n   - Ring pinning around deferred task_work fix (Pavel)\n\n   - A trivial member move in struct io_ring_ctx, saving us some memory\n     (me)\"\n\n* tag 'io_uring-2023-01-06' of git://git.kernel.dk/linux:\n  io_uring: fix CQ waiting timeout handling\n  io_uring: move 'poll_multi_queue' bool in io_ring_ctx\n  io_uring: lockdep annotate CQ locking\n  io_uring: pin context while queueing deferred tw\n  io_uring/io-wq: free worker if task_work creation is canceled",
        "kernel_version": "v6.2-rc3",
        "release_date": "2023-01-06 13:05:13 -0800 Merge tag 'io_uring-2023-01-06' of git://git.kernel.dk/linux"
    },
    {
        "commit": "12521a5d5cb7ff0ad43eadfc9c135d86e1131fa8",
        "message": "Jiffy to ktime CQ waiting conversion broke how we treat timeouts, in\nparticular we rearm it anew every time we get into\nio_cqring_wait_schedule() without adjusting the timeout. Waiting for 2\nCQEs and getting a task_work in the middle may double the timeout value,\nor even worse in some cases task may wait indefinitely.\n\nCc: stable@vger.kernel.org\nFixes: 228339662b398 (\"io_uring: don't convert to jiffies for waiting on timeouts\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f7bffddd71b08f28a877d44d37ac953ddb01590d.1672915663.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc3",
        "release_date": "2023-01-05 08:04:47 -0700 io_uring: fix CQ waiting timeout handling"
    },
    {
        "commit": "59b745bb4e0bd445366c45b8df6b51b69134f4f5",
        "message": "The cacheline section holding this variable has two gaps, where one is\ncaused by this bool not packing well with structs. This causes it to\nblow into the next cacheline. Move the variable, shrinking io_ring_ctx\nby a full cacheline in size.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc3",
        "release_date": "2023-01-04 13:49:54 -0700 io_uring: move 'poll_multi_queue' bool in io_ring_ctx"
    },
    {
        "commit": "fa8e442e832a3647cdd90f3e606c473a51bc1b26",
        "message": "Most of control command handlers may sleep, so return -EAGAIN in case\nof IO_URING_F_NONBLOCK to defer the handling into io wq context.\n\nFixes: 71f28f3136af (\"ublk_drv: add io_uring based userspace block driver\")\nReported-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20230104133235.836536-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc3",
        "release_date": "2023-01-04 13:24:53 -0700 ublk: honor IO_URING_F_NONBLOCK for handling control command"
    },
    {
        "commit": "f26cc9593581bd734c846bf827401350b36dc3c9",
        "message": "Locking around CQE posting is complex and depends on options the ring is\ncreated with, add more thorough lockdep annotations checking all\ninvariants.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/aa3770b4eacae3915d782cc2ab2f395a99b4b232.1672795976.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc3",
        "release_date": "2023-01-03 19:05:41 -0700 io_uring: lockdep annotate CQ locking"
    },
    {
        "commit": "9ffa13ff78a0a55df968a72d6f0ebffccee5c9f4",
        "message": "Unlike normal tw, nothing prevents deferred tw to be executed right\nafter an tw item added to ->work_llist in io_req_local_work_add(). For\ninstance, the waiting task may get waken up by CQ posting or a normal\ntw. Thus we need to pin the ring for the rest of io_req_local_work_add()\n\nCc: stable@vger.kernel.org\nFixes: c0e0d6ba25f18 (\"io_uring: add IORING_SETUP_DEFER_TASKRUN\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1a79362b9c10b8523ef70b061d96523650a23344.1672795998.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc3",
        "release_date": "2023-01-03 19:03:28 -0700 io_uring: pin context while queueing deferred tw"
    },
    {
        "commit": "af82425c6a2d2f347c79b63ce74fca6dc6be157f",
        "message": "If we cancel the task_work, the worker will never come into existance.\nAs this is the last reference to it, ensure that we get it freed\nappropriately.\n\nCc: stable@vger.kernel.org\nReported-by: \uc9c4\ud638 <wnwlsgh98@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc3",
        "release_date": "2023-01-02 16:49:46 -0700 io_uring/io-wq: free worker if task_work creation is canceled"
    },
    {
        "commit": "ac787ffa5a246e53675ae93294420ea948600818",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Two fixes for mutex grabbing when the task state is != TASK_RUNNING\n   (me)\n\n - Check for invalid opcode in io_uring_register() a bit earlier, to\n   avoid going through the quiesce machinery just to return -EINVAL\n   later in the process (me)\n\n - Fix for the uapi io_uring header, skipping including time_types.h\n   when necessary (Stefan)\n\n* tag 'io_uring-6.2-2022-12-29' of git://git.kernel.dk/linux:\n  uapi:io_uring.h: allow linux/time_types.h to be skipped\n  io_uring: check for valid register opcode earlier\n  io_uring/cancel: re-grab ctx mutex after finishing wait\n  io_uring: finish waiting before flushing overflow entries",
        "kernel_version": "v6.2-rc2",
        "release_date": "2022-12-29 16:48:21 -0800 Merge tag 'io_uring-6.2-2022-12-29' of git://git.kernel.dk/linux"
    },
    {
        "commit": "9eb803402a2a83400c6c6afd900e3b7c87c06816",
        "message": "include/uapi/linux/io_uring.h is synced 1:1 into\nliburing:src/include/liburing/io_uring.h.\n\nliburing has a configure check to detect the need for\nlinux/time_types.h. It can opt-out by defining\nUAPI_LINUX_IO_URING_H_SKIP_LINUX_TIME_TYPES_H\n\nFixes: 78a861b94959 (\"io_uring: add sync cancelation API through io_uring_register()\")\nLink: https://github.com/axboe/liburing/issues/708\nLink: https://github.com/axboe/liburing/pull/709\nLink: https://lore.kernel.org/io-uring/20221115212614.1308132-1-ammar.faizi@intel.com/T/#m9f5dd571cd4f6a5dee84452dbbca3b92ba7a4091\nCC: Jens Axboe <axboe@kernel.dk>\nCc: Ammar Faizi <ammarfaizi2@gnuweeb.org>\nSigned-off-by: Stefan Metzmacher <metze@samba.org>\nReviewed-by: Ammar Faizi <ammarfaizi2@gnuweeb.org>\nLink: https://lore.kernel.org/r/7071a0a1d751221538b20b63f9160094fc7e06f4.1668630247.git.metze@samba.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc2",
        "release_date": "2022-12-27 07:32:51 -0700 uapi:io_uring.h: allow linux/time_types.h to be skipped"
    },
    {
        "commit": "343190841a1f22b96996d9f8cfab902a4d1bfd0e",
        "message": "We only check the register opcode value inside the restricted ring\nsection, move it into the main io_uring_register() function instead\nand check it up front.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc2",
        "release_date": "2022-12-23 06:40:32 -0700 io_uring: check for valid register opcode earlier"
    },
    {
        "commit": "5d4740fc787db767811c4ac625665493314b382c",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Improve the locking for timeouts. This was originally queued up for\n   the initial pull, but I messed up and it got missed. (Pavel)\n\n - Fix an issue with running task_work from the wait path, causing some\n   inefficiencies (me)\n\n - Add a clear of ->free_iov upfront in the 32-bit compat data\n   importing, so we ensure that it's always sane at completion time (me)\n\n - Use call_rcu_hurry() for the eventfd signaling (Dylan)\n\n - Ordering fix for multishot recv completions (Pavel)\n\n - Add the io_uring trace header to the MAINTAINERS entry (Ammar)\n\n* tag 'io_uring-6.2-2022-12-19' of git://git.kernel.dk/linux:\n  MAINTAINERS: io_uring: Add include/trace/events/io_uring.h\n  io_uring/net: fix cleanup after recycle\n  io_uring/net: ensure compat import handlers clear free_iov\n  io_uring: include task_work run after scheduling in wait for events\n  io_uring: don't use TIF_NOTIFY_SIGNAL to test for availability of task_work\n  io_uring: use call_rcu_hurry if signaling an eventfd\n  io_uring: fix overflow handling regression\n  io_uring: ease timeout flush locking requirements\n  io_uring: revise completion_lock locking\n  io_uring: protect cq_timeouts with timeout_lock",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-21 16:28:25 -0800 Merge tag 'io_uring-6.2-2022-12-19' of git://git.kernel.dk/linux"
    },
    {
        "commit": "23fffb2f09ce1145cbd751801d45ba74acaa6542",
        "message": "If we have a signal pending during cancelations, it'll cause the\ntask_work run to return an error. Since we didn't run task_work, the\ncurrent task is left in TASK_INTERRUPTIBLE state when we need to\nre-grab the ctx mutex, and the kernel will rightfully complain about\nthat.\n\nMove the lock grabbing for the error cases outside the loop to avoid\nthat issue.\n\nReported-by: syzbot+7df055631cd1be4586fd@syzkaller.appspotmail.com\nLink: https://lore.kernel.org/io-uring/0000000000003a14a905f05050b0@google.com/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc2",
        "release_date": "2022-12-21 13:31:40 -0700 io_uring/cancel: re-grab ctx mutex after finishing wait"
    },
    {
        "commit": "52ea806ad983490b3132a9e526e11a10dc2fd10c",
        "message": "If we have overflow entries being generated after we've done the\ninitial flush in io_cqring_wait(), then we could be flushing them in the\nmain wait loop as well. If that's done after having added ourselves\nto the cq_wait waitqueue, then the task state can be != TASK_RUNNING\nwhen we enter the overflow flush.\n\nCheck for the need to overflow flush, and finish our wait cycle first\nif we have to do so.\n\nReported-and-tested-by: syzbot+cf6ea1d6bb30a4ce10b2@syzkaller.appspotmail.com\nLink: https://lore.kernel.org/io-uring/000000000000cb143a05f04eee15@google.com/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc2",
        "release_date": "2022-12-21 08:43:53 -0700 io_uring: finish waiting before flushing overflow entries"
    },
    {
        "commit": "5ad70eb27d2b87ec722fedd23638354be37ea0b0",
        "message": "This header file was introduced in commit c826bd7a743f (\"io_uring: add\nset of tracing events\"). It didn't get added to the io_uring\nmaintainers section. Add this header file to the io_uring maintainers\nsection.\n\nSigned-off-by: Ammar Faizi <ammarfaizi2@gnuweeb.org>\nLink: https://lore.kernel.org/r/20221219164521.2481728-1-ammar.faizi@intel.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-19 09:56:09 -0700 MAINTAINERS: io_uring: Add include/trace/events/io_uring.h"
    },
    {
        "commit": "6c3e8955d4bd9811a6e1761eea412a14fb51a2e6",
        "message": "Don't access io_async_msghdr io_netmsg_recycle(), it may be reallocated.\n\nCc: stable@vger.kernel.org\nFixes: 9bb66906f23e5 (\"io_uring: support multishot in recvmsg\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9e326f4ad4046ddadf15bf34bf3fa58c6372f6b5.1671461985.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-19 08:28:28 -0700 io_uring/net: fix cleanup after recycle"
    },
    {
        "commit": "990a4de57e44f4f4cfc33c90d2ec5d285b7c8342",
        "message": "If we're not allocating the vectors because the count is below\nUIO_FASTIOV, we still do need to properly clear ->free_iov to prevent\nan erronous free of on-stack data.\n\nReported-by: Jiri Slaby <jirislaby@gmail.com>\nFixes: 4c17a496a7a0 (\"io_uring/net: fix cleanup double free free_iov init\")\nCc: stable@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-19 07:35:16 -0700 io_uring/net: ensure compat import handlers clear free_iov"
    },
    {
        "commit": "35d90f95cfa773b7e3b1f57ba15ce06a470f354c",
        "message": "It's quite possible that we got woken up because task_work was queued,\nand we need to process this task_work to generate the events waited for.\nIf we return to the wait loop without running task_work, we'll end up\nadding the task to the waitqueue again, only to call\nio_cqring_wait_schedule() again which will run the task_work. This is\nless efficient than it could be, as it requires adding to the cq_wait\nqueue again. It also triggers the wakeup path for completions as\ncq_wait is now non-empty with the task itself, and it'll require another\nlock grab and deletion to remove ourselves from the waitqueue.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-17 20:35:54 -0700 io_uring: include task_work run after scheduling in wait for events"
    },
    {
        "commit": "6434ec0186b80c734aa7a2acf95f75f5c6dd943b",
        "message": "Use task_work_pending() as a better test for whether we have task_work\nor not, TIF_NOTIFY_SIGNAL is only valid if the any of the task_work\nitems had been queued with TWA_SIGNAL as the notification mechanism.\nHence task_work_pending() is a more reliable check.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-17 13:40:17 -0700 io_uring: don't use TIF_NOTIFY_SIGNAL to test for availability of task_work"
    },
    {
        "commit": "44a84da45272b3f4beb90025a64cfbde18f1aef0",
        "message": "io_uring uses call_rcu in the case it needs to signal an eventfd as a\nresult of an eventfd signal, since recursing eventfd signals are not\nallowed. This should be calling the new call_rcu_hurry API to not delay\nthe signal.\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\n\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Paul E. McKenney <paulmck@kernel.org>\nAcked-by: Paul E. McKenney <paulmck@kernel.org>\nReviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>\nLink: https://lore.kernel.org/r/20221215184138.795576-1-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-15 11:59:29 -0700 io_uring: use call_rcu_hurry if signaling an eventfd"
    },
    {
        "commit": "a8cf95f93610eb8282f8b6d0117ba78b74588d6b",
        "message": "Because the single task locking series got reordered ahead of the\ntimeout and completion lock changes, two hunks inadvertently ended up\nusing __io_fill_cqe_req() rather than io_fill_cqe_req(). This meant\nthat we dropped overflow handling in those two spots. Reinstate the\ncorrect CQE filling helper.\n\nFixes: f66f73421f0a (\"io_uring: skip spinlocking for ->task_complete\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-15 08:20:10 -0700 io_uring: fix overflow handling regression"
    },
    {
        "commit": "e5f30f6fb29a0b8fa7ca784e44571a610b949b04",
        "message": "We don't need completion_lock for timeout flushing, don't take it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1e3dc657975ac445b80e7bdc40050db783a5935a.1670002973.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-14 08:53:35 -0700 io_uring: ease timeout flush locking requirements"
    },
    {
        "commit": "6971253f078766543c716db708ba2c787826690d",
        "message": "io_kill_timeouts() doesn't post any events but queues everything to\ntask_work. Locking there is needed for protecting linked requests\ntraversing, we should grab completion_lock directly instead of using\nio_cq_[un]lock helpers. Same goes for __io_req_find_next_prep().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/88e75d481a65dc295cb59722bb1cf76402d1c06b.1670002973.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-14 08:53:04 -0700 io_uring: revise completion_lock locking"
    },
    {
        "commit": "ea011ee10231f5fa6cbb415007048ca0bb948baf",
        "message": "Read cq_timeouts in io_flush_timeouts() only after taking the\ntimeout_lock, as it's protected by it. There are many places where we\nalso grab ->completion_lock, but for instance io_timeout_fn() doesn't\nand still modifies cq_timeouts.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9c79544dd6cf5c4018cb1bab99cf481a93ea46ef.1670002973.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-14 08:51:28 -0700 io_uring: protect cq_timeouts with timeout_lock"
    },
    {
        "commit": "96f7e448b9f4546ffd0356ffceb2b9586777f316",
        "message": "Pull io_uring updates part two from Jens Axboe:\n\n - Misc fixes (me, Lin)\n\n - Series from Pavel extending the single task exclusive ring mode,\n   yielding nice improvements for the common case of having a single\n   ring per thread (Pavel)\n\n - Cleanup for MSG_RING, removing our IOPOLL hack (Pavel)\n\n - Further poll cleanups and fixes (Pavel)\n\n - Misc cleanups and fixes (Pavel)\n\n* tag 'for-6.2/io_uring-next-2022-12-08' of git://git.kernel.dk/linux: (22 commits)\n  io_uring/msg_ring: flag target ring as having task_work, if needed\n  io_uring: skip spinlocking for ->task_complete\n  io_uring: do msg_ring in target task via tw\n  io_uring: extract a io_msg_install_complete helper\n  io_uring: get rid of double locking\n  io_uring: never run tw and fallback in parallel\n  io_uring: use tw for putting rsrc\n  io_uring: force multishot CQEs into task context\n  io_uring: complete all requests in task context\n  io_uring: don't check overflow flush failures\n  io_uring: skip overflow CQE posting for dying ring\n  io_uring: improve io_double_lock_ctx fail handling\n  io_uring: dont remove file from msg_ring reqs\n  io_uring: reshuffle issue_flags\n  io_uring: don't reinstall quiesce node for each tw\n  io_uring: improve rsrc quiesce refs checks\n  io_uring: don't raw spin unlock to match cq_lock\n  io_uring: combine poll tw handlers\n  io_uring: improve poll warning handling\n  io_uring: remove ctx variable in io_poll_check_events\n  ...",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-13 10:40:31 -0800 Merge tag 'for-6.2/io_uring-next-2022-12-08' of git://git.kernel.dk/linux"
    },
    {
        "commit": "54e60e505d6144a22c787b5be1fdce996a27be1b",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - Always ensure proper ordering in case of CQ ring overflow, which then\n   means we can remove some work-arounds for that (Dylan)\n\n - Support completion batching for multishot, greatly increasing the\n   efficiency for those (Dylan)\n\n - Flag epoll/eventfd wakeups done from io_uring, so that we can easily\n   tell if we're recursing into io_uring again.\n\n   Previously, this would have resulted in repeated multishot\n   notifications if we had a dependency there. That could happen if an\n   eventfd was registered as the ring eventfd, and we multishot polled\n   for events on it. Or if an io_uring fd was added to epoll, and\n   io_uring had a multishot request for the epoll fd.\n\n   Test cases here:\n\thttps://git.kernel.dk/cgit/liburing/commit/?id=919755a7d0096fda08fb6d65ac54ad8d0fe027cd\n\n   Previously these got terminated when the CQ ring eventually\n   overflowed, now it's handled gracefully (me).\n\n - Tightening of the IOPOLL based completions (Pavel)\n\n - Optimizations of the networking zero-copy paths (Pavel)\n\n - Various tweaks and fixes (Dylan, Pavel)\n\n* tag 'for-6.2/io_uring-2022-12-08' of git://git.kernel.dk/linux: (41 commits)\n  io_uring: keep unlock_post inlined in hot path\n  io_uring: don't use complete_post in kbuf\n  io_uring: spelling fix\n  io_uring: remove io_req_complete_post_tw\n  io_uring: allow multishot polled reqs to defer completion\n  io_uring: remove overflow param from io_post_aux_cqe\n  io_uring: add lockdep assertion in io_fill_cqe_aux\n  io_uring: make io_fill_cqe_aux static\n  io_uring: add io_aux_cqe which allows deferred completion\n  io_uring: allow defer completion for aux posted cqes\n  io_uring: defer all io_req_complete_failed\n  io_uring: always lock in io_apoll_task_func\n  io_uring: remove iopoll spinlock\n  io_uring: iopoll protect complete_post\n  io_uring: inline __io_req_complete_put()\n  io_uring: remove io_req_tw_post_queue\n  io_uring: use io_req_task_complete() in timeout\n  io_uring: hold locks for io_req_complete_failed\n  io_uring: add completion locking for iopoll\n  io_uring: kill io_cqring_ev_posted() and __io_cq_unlock_post()\n  ...",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-13 10:33:08 -0800 Merge tag 'for-6.2/io_uring-2022-12-08' of git://git.kernel.dk/linux"
    },
    {
        "commit": "fcd0ccd836ffad73d98a66f6fea7b16f735ea920",
        "message": "For dax pud, pud_huge() returns true on x86. So the function works as long\nas hugetlb is configured. However, dax doesn't depend on hugetlb.\nCommit 414fd080d125 (\"mm/gup: fix gup_pmd_range() for dax\") fixed\ndevmap-backed huge PMDs, but missed devmap-backed huge PUDs. Fix this as\nwell.\n\nThis fixes the below kernel panic:\n\ngeneral protection fault, probably for non-canonical address 0x69e7c000cc478: 0000 [#1] SMP\n\t< snip >\nCall Trace:\n<TASK>\nget_user_pages_fast+0x1f/0x40\niov_iter_get_pages+0xc6/0x3b0\n? mempool_alloc+0x5d/0x170\nbio_iov_iter_get_pages+0x82/0x4e0\n? bvec_alloc+0x91/0xc0\n? bio_alloc_bioset+0x19a/0x2a0\nblkdev_direct_IO+0x282/0x480\n? __io_complete_rw_common+0xc0/0xc0\n? filemap_range_has_page+0x82/0xc0\ngeneric_file_direct_write+0x9d/0x1a0\n? inode_update_time+0x24/0x30\n__generic_file_write_iter+0xbd/0x1e0\nblkdev_write_iter+0xb4/0x150\n? io_import_iovec+0x8d/0x340\nio_write+0xf9/0x300\nio_issue_sqe+0x3c3/0x1d30\n? sysvec_reschedule_ipi+0x6c/0x80\n__io_queue_sqe+0x33/0x240\n? fget+0x76/0xa0\nio_submit_sqes+0xe6a/0x18d0\n? __fget_light+0xd1/0x100\n__x64_sys_io_uring_enter+0x199/0x880\n? __context_tracking_enter+0x1f/0x70\n? irqentry_exit_to_user_mode+0x24/0x30\n? irqentry_exit+0x1d/0x30\n? __context_tracking_exit+0xe/0x70\ndo_syscall_64+0x3b/0x90\nentry_SYSCALL_64_after_hwframe+0x61/0xcb\nRIP: 0033:0x7fc97c11a7be\n\t< snip >\n</TASK>\n---[ end trace 48b2e0e67debcaeb ]---\nRIP: 0010:internal_get_user_pages_fast+0x340/0x990\n\t< snip >\nKernel panic - not syncing: Fatal exception\nKernel Offset: disabled\n\nLink: https://lkml.kernel.org/r/1670392853-28252-1-git-send-email-ssengar@linux.microsoft.com\nFixes: 414fd080d125 (\"mm/gup: fix gup_pmd_range() for dax\")\nSigned-off-by: John Starks <jostarks@microsoft.com>\nSigned-off-by: Saurabh Sengar <ssengar@linux.microsoft.com>\nCc: Jan Kara <jack@suse.cz>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Jason Gunthorpe <jgg@nvidia.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Dan Williams <dan.j.williams@intel.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: <stable@vger.kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v6.1",
        "release_date": "2022-12-09 18:41:17 -0800 mm/gup: fix gup_pud_range() for dax"
    },
    {
        "commit": "af145500afa53fce55c9ee98e405fd0d65f018d0",
        "message": "Pull io_uring fix from Jens Axboe:\n \"A single small fix for an issue related to ordering between\n  cancelation and current->io_uring teardown\"\n\n* tag 'io_uring-6.1-2022-12-08' of git://git.kernel.dk/linux:\n  io_uring: Fix a null-ptr-deref in io_tctx_exit_cb()",
        "kernel_version": "v6.1",
        "release_date": "2022-12-08 15:44:09 -0800 Merge tag 'io_uring-6.1-2022-12-08' of git://git.kernel.dk/linux"
    },
    {
        "commit": "761c61c15903db41343532882b0443addb8c2faf",
        "message": "Before the recent change, we didn't even wake the targeted task when\nposting the cqe remotely. Now we do wake it, but we do want to ensure\nthat the recipient knows there's potential work there that needs to\nget processed to get the CQE posted.\n\nOR in IORING_SQ_TASKRUN for that purpose.\n\nLink: https://lore.kernel.org/io-uring/2843c6b4-ba9a-b67d-e0f4-957f42098489@kernel.dk/\nFixes: 6d043ee1164c (\"io_uring: do msg_ring in target task via tw\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-08 09:36:02 -0700 io_uring/msg_ring: flag target ring as having task_work, if needed"
    },
    {
        "commit": "f66f73421f0a929734bb41dde575e6d7859e548f",
        "message": "->task_complete was added to serialised CQE posting by doing it from\nthe task context only (or fallback wq when the task is dead), and now we\ncan use that to avoid taking ->completion_lock while filling CQ entries.\nThe patch skips spinlocking only in two spots,\n__io_submit_flush_completions() and flushing in io_aux_cqe, it's safer\nand covers all cases we care about. Extra care is taken to force taking\nthe lock while queueing overflow entries.\n\nIt fundamentally relies on SINGLE_ISSUER to have only one task posting\nevents. It also need to take into account overflowed CQEs, flushing of\nwhich happens in the cq wait path, and so this implementation also needs\nDEFER_TASKRUN to limit waiters. For the same reason we disable it for\nSQPOLL, and for IOPOLL as it won't benefit from it in any case.\nDEFER_TASKRUN, SQPOLL and IOPOLL requirement may be relaxed in the\nfuture.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2a8c91fd82cfcdcc1d2e5bac7051fe2c183bda73.1670384893.git.asml.silence@gmail.com\n[axboe: modify to apply]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-07 08:51:08 -0700 io_uring: skip spinlocking for ->task_complete"
    },
    {
        "commit": "6d043ee1164ca3305738131f170e560587070fa9",
        "message": "While executing in a context of one io_uring instance msg_ring\nmanipulates another ring. We're trying to keep CQEs posting contained in\nthe context of the ring-owner task, use task_work to send the request to\nthe target ring's task when we're modifying its CQ or trying to install\na file. Note, we can't safely use io_uring task_work infra and have to\nuse task_work directly.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4d76c7b28ed5d71b520de4482fbb7f660f21cd80.1670384893.git.asml.silence@gmail.com\n[axboe: use TWA_SIGNAL_NO_IPI]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-07 08:50:57 -0700 io_uring: do msg_ring in target task via tw"
    },
    {
        "commit": "172113101641cf1f9628c528ec790cb809f2b704",
        "message": "Extract a helper called io_msg_install_complete() from io_msg_send_fd(),\nwill be used later.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1500ca1054cc4286a3ee1c60aacead57fcdfa02a.1670384893.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-07 06:47:13 -0700 io_uring: extract a io_msg_install_complete helper"
    },
    {
        "commit": "11373026f2960390d5e330df4e92735c4265c440",
        "message": "We don't need to take both uring_locks at once, msg_ring can be split in\ntwo parts, first getting a file from the filetable of the first ring and\nthen installing it into the second one.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a80ecc2bc99c3b3f2cf20015d618b7c51419a797.1670384893.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-07 06:47:13 -0700 io_uring: get rid of double locking"
    },
    {
        "commit": "77e443ab294ca5b88896e8ddab41884948d5519a",
        "message": "Once we fallback a tw we want all requests to that task to be given to\nthe fallback wq so we dont run it in parallel with the last, i.e. post\nPF_EXITING, tw run of the task.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/96f4987265c4312f376f206511c6af3e77aaf5ac.1670384893.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-07 06:47:13 -0700 io_uring: never run tw and fallback in parallel"
    },
    {
        "commit": "d34b1b0b6779d4f5ee877b53cad90eef0f1cbe34",
        "message": "Use task_work for completing rsrc removals, it'll be needed later for\nspinlock optimisations.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/cbba5d53a11ee6fc2194dacea262c1d733c8b529.1670384893.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-07 06:47:13 -0700 io_uring: use tw for putting rsrc"
    },
    {
        "commit": "17add5cea2bbafea0d481f1a3ea9dea019a98ee9",
        "message": "Multishot are posting CQEs outside of the normal request completion\npath, which is usually done from within a task work handler. However, it\nmight be not the case when it's yet to be polled but has been punted to\nio-wq. Make it abide ->task_complete and push it to the polling path\nwhen executed by io-wq.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d7714aaff583096769a0f26e8e747759e556feb1.1670384893.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-07 06:47:13 -0700 io_uring: force multishot CQEs into task context"
    },
    {
        "commit": "e6aeb2721d3bad8379c43644d0380908e93b0187",
        "message": "This patch adds ctx->task_complete flag. If set, we'll complete all\nrequests in the context of the original task. Note, this extends to\ncompletion CQE posting only but not io_kiocb cleanup / free, e.g. io-wq\nmay free the requests in the free calllback. This flag will be used\nlater for optimisations purposes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/21ece72953f76bb2e77659a72a14326227ab6460.1670384893.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-07 06:47:13 -0700 io_uring: complete all requests in task context"
    },
    {
        "commit": "1b346e4aa8e79227391ffd6b7c6ee5acf0fa8bfc",
        "message": "The only way to fail overflowed CQEs flush is for CQ to be fully packed.\nThere is one place checking for flush failures, i.e. io_cqring_wait(),\nbut we limit the number to be waited for by the CQ size, so getting a\nfailure automatically means that we're done with waiting.\n\nDon't check for failures, rarely but they might spuriously fail CQ\nwaiting with -EBUSY.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6b720a45c03345655517f8202cbd0bece2848fb2.1670384893.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-07 06:47:13 -0700 io_uring: don't check overflow flush failures"
    },
    {
        "commit": "a85381d8326d75417ae177bddf44be533d1d21be",
        "message": "After io_ring_ctx_wait_and_kill() is called there should be no users\npoking into rings and so there is no need to post CQEs. So, instead of\ntrying to post overflowed CQEs into the CQ, drop them. Also, do it\nin io_ring_exit_work() in a loop to reduce the number of contexts it\ncan be executed from and even when it struggles to quiesce the ring we\nwon't be leaving memory allocated for longer than needed.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/26d13751155a735a3029e24f8d9ca992f810419d.1670384893.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-07 06:47:13 -0700 io_uring: skip overflow CQE posting for dying ring"
    },
    {
        "commit": "4c979eaefa4356d385b7c7d2877dc04d7fe88969",
        "message": "msg_ring will fail the request if it can't lock rings, instead punt it\nto io-wq as was originally intended.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4697f05afcc37df5c8f89e2fe6d9c7c19f0241f9.1670384893.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-07 06:47:13 -0700 io_uring: improve io_double_lock_ctx fail handling"
    },
    {
        "commit": "ef0ec1ad03119b8b46b035dad42bca7d6da7c2e5",
        "message": "We should not be messing with req->file outside of core paths. Clearing\nit makes msg_ring non reentrant, i.e. luckily io_msg_send_fd() fails the\nrequest on failed io_double_lock_ctx() but clearly was originally\nintended to do retries instead.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e5ac9edadb574fe33f6d727cb8f14ce68262a684.1670384893.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-12-07 06:47:13 -0700 io_uring: dont remove file from msg_ring reqs"
    },
    {
        "commit": "998b30c3948e4d0b1097e639918c5cff332acac5",
        "message": "Syzkaller reports a NULL deref bug as follows:\n\n BUG: KASAN: null-ptr-deref in io_tctx_exit_cb+0x53/0xd3\n Read of size 4 at addr 0000000000000138 by task file1/1955\n\n CPU: 1 PID: 1955 Comm: file1 Not tainted 6.1.0-rc7-00103-gef4d3ea40565 #75\n Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.11.0-2.el7 04/01/2014\n Call Trace:\n  <TASK>\n  dump_stack_lvl+0xcd/0x134\n  ? io_tctx_exit_cb+0x53/0xd3\n  kasan_report+0xbb/0x1f0\n  ? io_tctx_exit_cb+0x53/0xd3\n  kasan_check_range+0x140/0x190\n  io_tctx_exit_cb+0x53/0xd3\n  task_work_run+0x164/0x250\n  ? task_work_cancel+0x30/0x30\n  get_signal+0x1c3/0x2440\n  ? lock_downgrade+0x6e0/0x6e0\n  ? lock_downgrade+0x6e0/0x6e0\n  ? exit_signals+0x8b0/0x8b0\n  ? do_raw_read_unlock+0x3b/0x70\n  ? do_raw_spin_unlock+0x50/0x230\n  arch_do_signal_or_restart+0x82/0x2470\n  ? kmem_cache_free+0x260/0x4b0\n  ? putname+0xfe/0x140\n  ? get_sigframe_size+0x10/0x10\n  ? do_execveat_common.isra.0+0x226/0x710\n  ? lockdep_hardirqs_on+0x79/0x100\n  ? putname+0xfe/0x140\n  ? do_execveat_common.isra.0+0x238/0x710\n  exit_to_user_mode_prepare+0x15f/0x250\n  syscall_exit_to_user_mode+0x19/0x50\n  do_syscall_64+0x42/0xb0\n  entry_SYSCALL_64_after_hwframe+0x63/0xcd\n RIP: 0023:0x0\n Code: Unable to access opcode bytes at 0xffffffffffffffd6.\n RSP: 002b:00000000fffb7790 EFLAGS: 00000200 ORIG_RAX: 000000000000000b\n RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000000000000\n RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000\n RBP: 0000000000000000 R08: 0000000000000000 R09: 0000000000000000\n R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000000\n R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000\n  </TASK>\n Kernel panic - not syncing: panic_on_warn set ...\n\nThis happens because the adding of task_work from io_ring_exit_work()\nisn't synchronized with canceling all work items from eg exec. The\nexecution of the two are ordered in that they are both run by the task\nitself, but if io_tctx_exit_cb() is queued while we're canceling all\nwork items off exec AND gets executed when the task exits to userspace\nrather than in the main loop in io_uring_cancel_generic(), then we can\nfind current->io_uring == NULL and hit the above crash.\n\nIt's safe to add this NULL check here, because the execution of the two\npaths are done by the task itself.\n\nCc: stable@vger.kernel.org\nFixes: d56d938b4bef (\"io_uring: do ctx initiated file note removal\")\nReported-by: syzkaller <syzkaller@googlegroups.com>\nSigned-off-by: Harshit Mogalapalli <harshit.m.mogalapalli@oracle.com>\nLink: https://lore.kernel.org/r/20221206093833.3812138-1-harshit.m.mogalapalli@oracle.com\n[axboe: add code comment and also put an explanation in the commit msg]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1",
        "release_date": "2022-12-07 06:45:20 -0700 io_uring: Fix a null-ptr-deref in io_tctx_exit_cb()"
    },
    {
        "commit": "ce5a23c835aa0f0a931b5bcde1e7811f951b0146",
        "message": "Following the pattern of io_uring, perf, skb, and bpf, iommfd will use\nuser->locked_vm for accounting pinned pages. Ensure the value is included\nin the struct and export free_uid() as iommufd is modular.\n\nuser->locked_vm is the good accounting to use for ulimit because it is\nper-user, and the security sandboxing of locked pages is not supposed to\nbe per-process. Other places (vfio, vdpa and infiniband) have used\nmm->pinned_vm and/or mm->locked_vm for accounting pinned pages, but this\nis only per-process and inconsistent with the new FOLL_LONGTERM users in\nthe kernel.\n\nConcurrent work is underway to try to put this in a cgroup, so everything\ncan be consistent and the kernel can provide a FOLL_LONGTERM limit that\nactually provides security.\n\nLink: https://lore.kernel.org/r/7-v6-a196d26f289e+11787-iommufd_jgg@nvidia.com\nReviewed-by: Kevin Tian <kevin.tian@intel.com>\nReviewed-by: Eric Auger <eric.auger@redhat.com>\nTested-by: Nicolin Chen <nicolinc@nvidia.com>\nTested-by: Yi Liu <yi.l.liu@intel.com>\nTested-by: Lixiao Yang <lixiao.yang@intel.com>\nTested-by: Matthew Rosato <mjrosato@linux.ibm.com>\nSigned-off-by: Jason Gunthorpe <jgg@nvidia.com>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-30 20:16:49 -0400 kernel/user: Allow user_struct::locked_vm to be usable for iommufd"
    },
    {
        "commit": "7500194a630b11236761df35fef300009d7d3f6f",
        "message": "Reshuffle issue flags to keep normal flags separate from the uring_cmd\nctx-setup like flags. Shift the second type to the second byte so it's\neasier to add new ones in the future.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d6e4696c883943082d248716f4cd568f37b17a74.1669821213.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-30 10:29:07 -0700 io_uring: reshuffle issue_flags"
    },
    {
        "commit": "77e3202a21967e7de5b4412c0534f2e34e175227",
        "message": "There is no need to reinit data and install a new rsrc node every time\nwe get a task_work, it's detrimental, just execute it and conitnue\nwaiting.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3895d3344164cd9b3a0bbb24a6e357e20a13434b.1669821213.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-30 10:29:07 -0700 io_uring: don't reinstall quiesce node for each tw"
    },
    {
        "commit": "0ced756f6412123b01cd72e5741d9dd6ae5f1dd5",
        "message": "Do a little bit of refactoring of io_rsrc_ref_quiesce(), flatten the\ndata refs checks and so get rid of a conditional weird unlock-else-break\nconstruct.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d21283e9f88a77612c746ed526d86fe3bfb58a70.1669821213.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-30 10:29:07 -0700 io_uring: improve rsrc quiesce refs checks"
    },
    {
        "commit": "618d653a345a477aaae307a0455900eb8789e952",
        "message": "There is one newly added place when we lock ring with io_cq_lock() but\nunlocking is hand coded calling spin_unlock directly. It's ugly and\ntroublesome in the long run. Make it consistent with the other completion\nlocking.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4ca4f0564492b90214a190cd5b2a6c76522de138.1669821213.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-30 10:28:49 -0700 io_uring: don't raw spin unlock to match cq_lock"
    },
    {
        "commit": "443e57550670234f1bd34983b3c577edcf2eeef5",
        "message": "Merge apoll and regular poll tw handlers, it will help with inlining.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/482e59edb9fc81bd275fdbf486837330fb27120a.1669821213.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-30 10:27:34 -0700 io_uring: combine poll tw handlers"
    },
    {
        "commit": "c3bfb57ea7011e0c04e4b7f28cb357a551b1efb9",
        "message": "Don't try to complete requests if their refs are broken and we've got\na warning, it's much better to drop them and potentially leaking than\ndouble freeing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/31edf9f96f05d03ab62c114508a231a2dce434cb.1669821213.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-30 10:27:22 -0700 io_uring: improve poll warning handling"
    },
    {
        "commit": "047b6aef0966f9863e1940b57c256ebbb465a6b5",
        "message": "ctx is only used by io_poll_check_events() for multishot poll CQE\nposting, don't save it on stack in advance.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/552c1771f8a0e7688afdb4f538ead245f53e80e7.1669821213.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-30 10:26:57 -0700 io_uring: remove ctx variable in io_poll_check_events"
    },
    {
        "commit": "9805fa2d94993e16efd0e1adbd2b54d8d1fe2f9f",
        "message": "The fast path in io_poll_check_events() is when we have only one\n(i.e. master) reference. Move all verification, cancellations\nchecks, edge case handling and so on under a common if.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8c21c5d5e027e32dc553705e88796dec79ff6f93.1669821213.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-30 10:26:57 -0700 io_uring: carve io_poll_check_events fast path"
    },
    {
        "commit": "f6f7f903e78dddcb1e1552b896e0e3e9c14c17ae",
        "message": "We don't need to worry about checking PF_EXITING in io_poll_issue().\ntask works using the function should take care of it and never try to\nresubmit / retry if the task is dying.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2e9dc998dc07507c759a0c9cb5d2fbea0710d58c.1669821213.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-30 10:26:57 -0700 io_uring: kill io_poll_issue's PF_EXITING check"
    },
    {
        "commit": "b2cf789f6cb6d449f2b457ee3fb055b7f431481f",
        "message": "* for-6.2/io_uring: (41 commits)\n  io_uring: keep unlock_post inlined in hot path\n  io_uring: don't use complete_post in kbuf\n  io_uring: spelling fix\n  io_uring: remove io_req_complete_post_tw\n  io_uring: allow multishot polled reqs to defer completion\n  io_uring: remove overflow param from io_post_aux_cqe\n  io_uring: add lockdep assertion in io_fill_cqe_aux\n  io_uring: make io_fill_cqe_aux static\n  io_uring: add io_aux_cqe which allows deferred completion\n  io_uring: allow defer completion for aux posted cqes\n  io_uring: defer all io_req_complete_failed\n  io_uring: always lock in io_apoll_task_func\n  io_uring: remove iopoll spinlock\n  io_uring: iopoll protect complete_post\n  io_uring: inline __io_req_complete_put()\n  io_uring: remove io_req_tw_post_queue\n  io_uring: use io_req_task_complete() in timeout\n  io_uring: hold locks for io_req_complete_failed\n  io_uring: add completion locking for iopoll\n  io_uring: kill io_cqring_ev_posted() and __io_cq_unlock_post()\n  ...",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-29 12:08:37 -0700 Merge branch 'for-6.2/io_uring' into for-6.2/io_uring-next"
    },
    {
        "commit": "364eb618348c1aaebe6ccc102ca15d92c2bf6033",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - A few poll related fixes. One fixing a race condition between poll\n   cancelation and trigger, and one making the overflow handling a bit\n   more robust (Lin, Pavel)\n\n - Fix an fput() for error handling in the direct file table (Lin)\n\n - Fix for a regression introduced in this cycle, where we don't always\n   get TIF_NOTIFY_SIGNAL cleared appropriately (me)\n\n* tag 'io_uring-6.1-2022-11-25' of git://git.kernel.dk/linux:\n  io_uring: clear TIF_NOTIFY_SIGNAL if set and task_work not available\n  io_uring/poll: fix poll_refs race with cancelation\n  io_uring/filetable: fix file reference underflow\n  io_uring: make poll refs more robust\n  io_uring: cmpxchg for poll arm refs release",
        "kernel_version": "v6.1-rc7",
        "release_date": "2022-11-25 17:46:04 -0800 Merge tag 'io_uring-6.1-2022-11-25' of git://git.kernel.dk/linux"
    },
    {
        "commit": "7cfe7a09489c1cefee7181e07b5f2bcbaebd9f41",
        "message": "With how task_work is added and signaled, we can have TIF_NOTIFY_SIGNAL\nset and no task_work pending as it got run in a previous loop. Treat\nTIF_NOTIFY_SIGNAL like get_signal(), always clear it if set regardless\nof whether or not task_work is pending to run.\n\nCc: stable@vger.kernel.org\nFixes: 46a525e199e4 (\"io_uring: don't gate task_work run on TIF_NOTIFY_SIGNAL\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc7",
        "release_date": "2022-11-25 10:55:08 -0700 io_uring: clear TIF_NOTIFY_SIGNAL if set and task_work not available"
    },
    {
        "commit": "12ad3d2d6c5b0131a6052de91360849e3e154846",
        "message": "There is an interesting race condition of poll_refs which could result\nin a NULL pointer dereference. The crash trace is like:\n\nKASAN: null-ptr-deref in range [0x0000000000000008-0x000000000000000f]\nCPU: 0 PID: 30781 Comm: syz-executor.2 Not tainted 6.0.0-g493ffd6605b2 #1\nHardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS\n1.13.0-1ubuntu1.1 04/01/2014\nRIP: 0010:io_poll_remove_entry io_uring/poll.c:154 [inline]\nRIP: 0010:io_poll_remove_entries+0x171/0x5b4 io_uring/poll.c:190\nCode: ...\nRSP: 0018:ffff88810dfefba0 EFLAGS: 00010202\nRAX: 0000000000000001 RBX: 0000000000000000 RCX: 0000000000040000\nRDX: ffffc900030c4000 RSI: 000000000003ffff RDI: 0000000000040000\nRBP: 0000000000000008 R08: ffffffff9764d3dd R09: fffffbfff3836781\nR10: fffffbfff3836781 R11: 0000000000000000 R12: 1ffff11003422d60\nR13: ffff88801a116b04 R14: ffff88801a116ac0 R15: dffffc0000000000\nFS:  00007f9c07497700(0000) GS:ffff88811a600000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00007ffb5c00ea98 CR3: 0000000105680005 CR4: 0000000000770ef0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\nPKRU: 55555554\nCall Trace:\n <TASK>\n io_apoll_task_func+0x3f/0xa0 io_uring/poll.c:299\n handle_tw_list io_uring/io_uring.c:1037 [inline]\n tctx_task_work+0x37e/0x4f0 io_uring/io_uring.c:1090\n task_work_run+0x13a/0x1b0 kernel/task_work.c:177\n get_signal+0x2402/0x25a0 kernel/signal.c:2635\n arch_do_signal_or_restart+0x3b/0x660 arch/x86/kernel/signal.c:869\n exit_to_user_mode_loop kernel/entry/common.c:166 [inline]\n exit_to_user_mode_prepare+0xc2/0x160 kernel/entry/common.c:201\n __syscall_exit_to_user_mode_work kernel/entry/common.c:283 [inline]\n syscall_exit_to_user_mode+0x58/0x160 kernel/entry/common.c:294\n entry_SYSCALL_64_after_hwframe+0x63/0xcd\n\nThe root cause for this is a tiny overlooking in\nio_poll_check_events() when cocurrently run with poll cancel routine\nio_poll_cancel_req().\n\nThe interleaving to trigger use-after-free:\n\nCPU0                                       |  CPU1\n                                           |\nio_apoll_task_func()                       |  io_poll_cancel_req()\n io_poll_check_events()                    |\n  // do while first loop                   |\n  v = atomic_read(...)                     |\n  // v = poll_refs = 1                     |\n  ...                                      |  io_poll_mark_cancelled()\n                                           |   atomic_or()\n                                           |   // poll_refs =\nIO_POLL_CANCEL_FLAG | 1\n                                           |\n  atomic_sub_return(...)                   |\n  // poll_refs = IO_POLL_CANCEL_FLAG       |\n  // loop continue                         |\n                                           |\n                                           |  io_poll_execute()\n                                           |   io_poll_get_ownership()\n                                           |   // poll_refs =\nIO_POLL_CANCEL_FLAG | 1\n                                           |   // gets the ownership\n  v = atomic_read(...)                     |\n  // poll_refs not change                  |\n                                           |\n  if (v & IO_POLL_CANCEL_FLAG)             |\n   return -ECANCELED;                      |\n  // io_poll_check_events return           |\n  // will go into                          |\n  // io_req_complete_failed() free req     |\n                                           |\n                                           |  io_apoll_task_func()\n                                           |  // also go into\nio_req_complete_failed()\n\nAnd the interleaving to trigger the kernel WARNING:\n\nCPU0                                       |  CPU1\n                                           |\nio_apoll_task_func()                       |  io_poll_cancel_req()\n io_poll_check_events()                    |\n  // do while first loop                   |\n  v = atomic_read(...)                     |\n  // v = poll_refs = 1                     |\n  ...                                      |  io_poll_mark_cancelled()\n                                           |   atomic_or()\n                                           |   // poll_refs =\nIO_POLL_CANCEL_FLAG | 1\n                                           |\n  atomic_sub_return(...)                   |\n  // poll_refs = IO_POLL_CANCEL_FLAG       |\n  // loop continue                         |\n                                           |\n  v = atomic_read(...)                     |\n  // v = IO_POLL_CANCEL_FLAG               |\n                                           |  io_poll_execute()\n                                           |   io_poll_get_ownership()\n                                           |   // poll_refs =\nIO_POLL_CANCEL_FLAG | 1\n                                           |   // gets the ownership\n                                           |\n  WARN_ON_ONCE(!(v & IO_POLL_REF_MASK)))   |\n  // v & IO_POLL_REF_MASK = 0 WARN         |\n                                           |\n                                           |  io_apoll_task_func()\n                                           |  // also go into\nio_req_complete_failed()\n\nBy looking up the source code and communicating with Pavel, the\nimplementation of this atomic poll refs should continue the loop of\nio_poll_check_events() just to avoid somewhere else to grab the\nownership. Therefore, this patch simply adds another AND operation to\nmake sure the loop will stop if it finds the poll_refs is exactly equal\nto IO_POLL_CANCEL_FLAG. Since io_poll_cancel_req() grabs ownership and\nwill finally make its way to io_req_complete_failed(), the req will\nbe reclaimed as expected.\n\nFixes: aa43477b0402 (\"io_uring: poll rework\")\nSigned-off-by: Lin Ma <linma@zju.edu.cn>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\n[axboe: tweak description and code style]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc7",
        "release_date": "2022-11-25 07:17:33 -0700 io_uring/poll: fix poll_refs race with cancelation"
    },
    {
        "commit": "9d94c04c0db024922e886c9fd429659f22f48ea4",
        "message": "There is an interesting reference bug when -ENOMEM occurs in calling of\nio_install_fixed_file(). KASan report like below:\n\n[   14.057131] ==================================================================\n[   14.059161] BUG: KASAN: use-after-free in unix_get_socket+0x10/0x90\n[   14.060975] Read of size 8 at addr ffff88800b09cf20 by task kworker/u8:2/45\n[   14.062684]\n[   14.062768] CPU: 2 PID: 45 Comm: kworker/u8:2 Not tainted 6.1.0-rc4 #1\n[   14.063099] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.16.0-0-gd239552ce722-prebuilt.qemu.org 04/01/2014\n[   14.063666] Workqueue: events_unbound io_ring_exit_work\n[   14.063936] Call Trace:\n[   14.064065]  <TASK>\n[   14.064175]  dump_stack_lvl+0x34/0x48\n[   14.064360]  print_report+0x172/0x475\n[   14.064547]  ? _raw_spin_lock_irq+0x83/0xe0\n[   14.064758]  ? __virt_addr_valid+0xef/0x170\n[   14.064975]  ? unix_get_socket+0x10/0x90\n[   14.065167]  kasan_report+0xad/0x130\n[   14.065353]  ? unix_get_socket+0x10/0x90\n[   14.065553]  unix_get_socket+0x10/0x90\n[   14.065744]  __io_sqe_files_unregister+0x87/0x1e0\n[   14.065989]  ? io_rsrc_refs_drop+0x1c/0xd0\n[   14.066199]  io_ring_exit_work+0x388/0x6a5\n[   14.066410]  ? io_uring_try_cancel_requests+0x5bf/0x5bf\n[   14.066674]  ? try_to_wake_up+0xdb/0x910\n[   14.066873]  ? virt_to_head_page+0xbe/0xbe\n[   14.067080]  ? __schedule+0x574/0xd20\n[   14.067273]  ? read_word_at_a_time+0xe/0x20\n[   14.067492]  ? strscpy+0xb5/0x190\n[   14.067665]  process_one_work+0x423/0x710\n[   14.067879]  worker_thread+0x2a2/0x6f0\n[   14.068073]  ? process_one_work+0x710/0x710\n[   14.068284]  kthread+0x163/0x1a0\n[   14.068454]  ? kthread_complete_and_exit+0x20/0x20\n[   14.068697]  ret_from_fork+0x22/0x30\n[   14.068886]  </TASK>\n[   14.069000]\n[   14.069088] Allocated by task 289:\n[   14.069269]  kasan_save_stack+0x1e/0x40\n[   14.069463]  kasan_set_track+0x21/0x30\n[   14.069652]  __kasan_slab_alloc+0x58/0x70\n[   14.069899]  kmem_cache_alloc+0xc5/0x200\n[   14.070100]  __alloc_file+0x20/0x160\n[   14.070283]  alloc_empty_file+0x3b/0xc0\n[   14.070479]  path_openat+0xc3/0x1770\n[   14.070689]  do_filp_open+0x150/0x270\n[   14.070888]  do_sys_openat2+0x113/0x270\n[   14.071081]  __x64_sys_openat+0xc8/0x140\n[   14.071283]  do_syscall_64+0x3b/0x90\n[   14.071466]  entry_SYSCALL_64_after_hwframe+0x63/0xcd\n[   14.071791]\n[   14.071874] Freed by task 0:\n[   14.072027]  kasan_save_stack+0x1e/0x40\n[   14.072224]  kasan_set_track+0x21/0x30\n[   14.072415]  kasan_save_free_info+0x2a/0x50\n[   14.072627]  __kasan_slab_free+0x106/0x190\n[   14.072858]  kmem_cache_free+0x98/0x340\n[   14.073075]  rcu_core+0x427/0xe50\n[   14.073249]  __do_softirq+0x110/0x3cd\n[   14.073440]\n[   14.073523] Last potentially related work creation:\n[   14.073801]  kasan_save_stack+0x1e/0x40\n[   14.074017]  __kasan_record_aux_stack+0x97/0xb0\n[   14.074264]  call_rcu+0x41/0x550\n[   14.074436]  task_work_run+0xf4/0x170\n[   14.074619]  exit_to_user_mode_prepare+0x113/0x120\n[   14.074858]  syscall_exit_to_user_mode+0x1d/0x40\n[   14.075092]  do_syscall_64+0x48/0x90\n[   14.075272]  entry_SYSCALL_64_after_hwframe+0x63/0xcd\n[   14.075529]\n[   14.075612] Second to last potentially related work creation:\n[   14.075900]  kasan_save_stack+0x1e/0x40\n[   14.076098]  __kasan_record_aux_stack+0x97/0xb0\n[   14.076325]  task_work_add+0x72/0x1b0\n[   14.076512]  fput+0x65/0xc0\n[   14.076657]  filp_close+0x8e/0xa0\n[   14.076825]  __x64_sys_close+0x15/0x50\n[   14.077019]  do_syscall_64+0x3b/0x90\n[   14.077199]  entry_SYSCALL_64_after_hwframe+0x63/0xcd\n[   14.077448]\n[   14.077530] The buggy address belongs to the object at ffff88800b09cf00\n[   14.077530]  which belongs to the cache filp of size 232\n[   14.078105] The buggy address is located 32 bytes inside of\n[   14.078105]  232-byte region [ffff88800b09cf00, ffff88800b09cfe8)\n[   14.078685]\n[   14.078771] The buggy address belongs to the physical page:\n[   14.079046] page:000000001bd520e7 refcount:1 mapcount:0 mapping:0000000000000000 index:0xffff88800b09de00 pfn:0xb09c\n[   14.079575] head:000000001bd520e7 order:1 compound_mapcount:0 compound_pincount:0\n[   14.079946] flags: 0x100000000010200(slab|head|node=0|zone=1)\n[   14.080244] raw: 0100000000010200 0000000000000000 dead000000000001 ffff88800493cc80\n[   14.080629] raw: ffff88800b09de00 0000000080190018 00000001ffffffff 0000000000000000\n[   14.081016] page dumped because: kasan: bad access detected\n[   14.081293]\n[   14.081376] Memory state around the buggy address:\n[   14.081618]  ffff88800b09ce00: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n[   14.081974]  ffff88800b09ce80: 00 00 00 00 00 fc fc fc fc fc fc fc fc fc fc fc\n[   14.082336] >ffff88800b09cf00: fa fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb\n[   14.082690]                                ^\n[   14.082909]  ffff88800b09cf80: fb fb fb fb fb fb fb fb fb fb fb fb fb fc fc fc\n[   14.083266]  ffff88800b09d000: fc fc fc fc fc fc fc fc fa fb fb fb fb fb fb fb\n[   14.083622] ==================================================================\n\nThe actual tracing of this bug is shown below:\n\ncommit 8c71fe750215 (\"io_uring: ensure fput() called correspondingly\nwhen direct install fails\") adds an additional fput() in\nio_fixed_fd_install() when io_file_bitmap_get() returns error values. In\nthat case, the routine will never make it to io_install_fixed_file() due\nto an early return.\n\nstatic int io_fixed_fd_install(...)\n{\n  if (alloc_slot) {\n    ...\n    ret = io_file_bitmap_get(ctx);\n    if (unlikely(ret < 0)) {\n      io_ring_submit_unlock(ctx, issue_flags);\n      fput(file);\n      return ret;\n    }\n    ...\n  }\n  ...\n  ret = io_install_fixed_file(req, file, issue_flags, file_slot);\n  ...\n}\n\nIn the above scenario, the reference is okay as io_fixed_fd_install()\nensures the fput() is called when something bad happens, either via\nbitmap or via inner io_install_fixed_file().\n\nHowever, the commit 61c1b44a21d7 (\"io_uring: fix deadlock on iowq file\nslot alloc\") breaks the balance because it places fput() into the common\npath for both io_file_bitmap_get() and io_install_fixed_file(). Since\nio_install_fixed_file() handles the fput() itself, the reference\nunderflow come across then.\n\nThere are some extra commits make the current code into\nio_fixed_fd_install() -> __io_fixed_fd_install() ->\nio_install_fixed_file()\n\nHowever, the fact that there is an extra fput() is called if\nio_install_fixed_file() calls fput(). Traversing through the code, I\nfind that the existing two callers to __io_fixed_fd_install():\nio_fixed_fd_install() and io_msg_send_fd() have fput() when handling\nerror return, this patch simply removes the fput() in\nio_install_fixed_file() to fix the bug.\n\nFixes: 61c1b44a21d7 (\"io_uring: fix deadlock on iowq file slot alloc\")\nSigned-off-by: Lin Ma <linma@zju.edu.cn>\nLink: https://lore.kernel.org/r/be4ba4b.5d44.184a0a406a4.Coremail.linma@zju.edu.cn\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc7",
        "release_date": "2022-11-25 06:54:46 -0700 io_uring/filetable: fix file reference underflow"
    },
    {
        "commit": "a26a35e9019fd70bf3cf647dcfdae87abc7bacea",
        "message": "poll_refs carry two functions, the first is ownership over the request.\nThe second is notifying the io_poll_check_events() that there was an\nevent but wake up couldn't grab the ownership, so io_poll_check_events()\nshould retry.\n\nWe want to make poll_refs more robust against overflows. Instead of\nalways incrementing it, which covers two purposes with one atomic, check\nif poll_refs is elevated enough and if so set a retry flag without\nattempts to grab ownership. The gap between the bias check and following\natomics may seem racy, but we don't need it to be strict. Moreover there\nmight only be maximum 4 parallel updates: by the first and the second\npoll entries, __io_arm_poll_handler() and cancellation. From those four,\nonly poll wake ups may be executed multiple times, but they're protected\nby a spin.\n\nCc: stable@vger.kernel.org\nReported-by: Lin Ma <linma@zju.edu.cn>\nFixes: aa43477b04025 (\"io_uring: poll rework\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c762bc31f8683b3270f3587691348a7119ef9c9d.1668963050.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc7",
        "release_date": "2022-11-25 06:54:46 -0700 io_uring: make poll refs more robust"
    },
    {
        "commit": "2f3893437a4ebf2e892ca172e9e122841319d675",
        "message": "Replace atomically substracting the ownership reference at the end of\narming a poll with a cmpxchg. We try to release ownership by setting 0\nassuming that poll_refs didn't change while we were arming. If it did\nchange, we keep the ownership and use it to queue a tw, which is fully\ncapable to process all events and (even tolerates spurious wake ups).\n\nIt's a bit more elegant as we reduce races b/w setting the cancellation\nflag and getting refs with this release, and with that we don't have to\nworry about any kinds of underflows. It's not the fastest path for\npolling. The performance difference b/w cmpxchg and atomic dec is\nusually negligible and it's not the fastest path.\n\nCc: stable@vger.kernel.org\nFixes: aa43477b04025 (\"io_uring: poll rework\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0c95251624397ea6def568ff040cad2d7926fd51.1668963050.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc7",
        "release_date": "2022-11-25 06:54:16 -0700 io_uring: cmpxchg for poll arm refs release"
    },
    {
        "commit": "5d772916855f593672de55c437925daccc8ecd73",
        "message": "This partially reverts\n\n6c16fe3c16bdc (\"io_uring: kill io_cqring_ev_posted() and __io_cq_unlock_post()\")\n\nThe redundancy of __io_cq_unlock_post() was always to keep it inlined\ninto __io_submit_flush_completions(). Inline it back and rename with\nhope of clarifying the intention behind it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/372a16c485fca44c069be2e92fc5e7332a1d7fd7.1669310258.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-25 06:11:15 -0700 io_uring: keep unlock_post inlined in hot path"
    },
    {
        "commit": "c3b490930dbe6a6c98d3820f445757ddec1efb08",
        "message": "Now we're handling IOPOLL completions more generically, get rid uses of\n_post() and send requests through the normal path. It may have some\nextra mertis performance wise, but we don't care much as there is a\nbetter interface for selected buffers.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4deded706587f55b006dc33adf0c13cfc3b2319f.1669310258.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-25 06:11:15 -0700 io_uring: don't use complete_post in kbuf"
    },
    {
        "commit": "10d8bc35416d9e83ffe9644478756281c7bd4f52",
        "message": "s/pushs/pushes/\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20221125103412.1425305-3-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-25 06:10:46 -0700 io_uring: spelling fix"
    },
    {
        "commit": "27f35fe9096b183d45ff6f22ad277ddf107d8428",
        "message": "It's only used in one place. Inline it.\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20221125103412.1425305-2-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-25 06:10:46 -0700 io_uring: remove io_req_complete_post_tw"
    },
    {
        "commit": "9a6924519e5e882631a7fff429facca838207e45",
        "message": "Until now there was no reason for multishot polled requests to defer\ncompletions as there was no functional difference. However now this will\nactually defer the completions, for a performance win.\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20221124093559.3780686-10-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-25 06:10:04 -0700 io_uring: allow multishot polled reqs to defer completion"
    },
    {
        "commit": "b529c96a896b7bea8464a58d350836cc106d70bd",
        "message": "The only call sites which would not allow overflow are also call sites\nwhich would use the io_aux_cqe as they care about ordering.\n\nSo remove this parameter from io_post_aux_cqe.\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20221124093559.3780686-9-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-25 06:10:04 -0700 io_uring: remove overflow param from io_post_aux_cqe"
    },
    {
        "commit": "2e2ef4a1dab980d88a1ab45bf0e28c8851999e33",
        "message": "Add an assertion for the completion lock to io_fill_cqe_aux\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20221124093559.3780686-8-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-25 06:10:04 -0700 io_uring: add lockdep assertion in io_fill_cqe_aux"
    },
    {
        "commit": "a77ab745f28d5ab2ce51d0e44e85af942bb77d47",
        "message": "This is only used in io_uring.c\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20221124093559.3780686-7-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-25 06:10:04 -0700 io_uring: make io_fill_cqe_aux static"
    },
    {
        "commit": "9b8c54755a2b16d4f23c0ea184b75e2edf77d906",
        "message": "Use the just introduced deferred post cqe completion state when possible\nin io_aux_cqe. If not possible fallback to io_post_aux_cqe.\n\nThis introduces a complication because of allow_overflow. For deferred\ncompletions we cannot know without locking the completion_lock if it will\noverflow (and even if we locked it, another post could sneak in and cause\nthis cqe to be in overflow).\nHowever since overflow protection is mostly a best effort defence in depth\nto prevent infinite loops of CQEs for poll, just checking the overflow bit\nis going to be good enough and will result in at most 16 (array size of\ndeferred cqes) overflows.\n\nSuggested-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20221124093559.3780686-6-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-25 06:10:04 -0700 io_uring: add io_aux_cqe which allows deferred completion"
    },
    {
        "commit": "931147ddfa6e9ffd814272f1c0370c4740acbe17",
        "message": "Multishot ops cannot use the compl_reqs list as the request must stay in\nthe poll list, but that means they need to run each completion without\nbenefiting from batching.\n\nHere introduce batching infrastructure for only small (ie 16 byte)\nCQEs. This restriction is ok because there are no use cases posting 32\nbyte CQEs.\n\nIn the ring keep a batch of up to 16 posted results, and flush in the same\nway as compl_reqs.\n\n16 was chosen through experimentation on a microbenchmark ([1]), as well\nas trying not to increase the size of the ring too much. This increases\nthe size to 1472 bytes from 1216.\n\n[1]: https://github.com/DylanZA/liburing/commit/9ac66b36bcf4477bfafeff1c5f107896b7ae31cf\nRun with $ make -j && ./benchmark/reg.b -s 1 -t 2000 -r 10\nGives results:\nbaseline\t8309 k/s\n8\t\t18807 k/s\n16\t\t19338 k/s\n32\t\t20134 k/s\n\nSuggested-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20221124093559.3780686-5-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-25 06:10:04 -0700 io_uring: allow defer completion for aux posted cqes"
    },
    {
        "commit": "973fc83f3a94bdffcacf482641db38f57c7c8609",
        "message": "All failures happen under lock now, and can be deferred. To be consistent\nwhen the failure has happened after some multishot cqe has been\ndeferred (and keep ordering), always defer failures.\n\nTo make this obvious at the caller (and to help prevent a future bug)\nrename io_req_complete_failed to io_req_defer_failed.\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20221124093559.3780686-4-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-25 06:10:04 -0700 io_uring: defer all io_req_complete_failed"
    },
    {
        "commit": "c06c6c5d276707e04cedbcc55625e984922118aa",
        "message": "This is required for the failure case (io_req_complete_failed) and is\nmissing.\n\nThe alternative would be to only lock in the failure path, however all of\nthe non-error paths in io_poll_check_events that do not do not return\nIOU_POLL_NO_ACTION end up locking anyway. The only extraneous lock would\nbe for the multishot poll overflowing the CQE ring, however multishot poll\nwould probably benefit from being locked as it will allow completions to\nbe batched.\n\nSo it seems reasonable to lock always.\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20221124093559.3780686-3-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-25 06:10:04 -0700 io_uring: always lock in io_apoll_task_func"
    },
    {
        "commit": "7d4a93176e0142ce16d23c849a47d5b00b856296",
        "message": "Either ublk_can_use_task_work() is true or not, io commands are\nforwarded to ublk server in reverse order, since llist_add() is\nalways to add one element to the head of the list.\n\nEven though block layer doesn't guarantee request dispatch order,\nrequests should be sent to hardware in the sequence order generated\nfrom io scheduler, which usually considers the request's LBA, and\norder is often important for HDD.\n\nSo forward io commands in the sequence made from io scheduler by\naligning task work with current io_uring command's batch handling,\nand it has been observed that both can get similar performance data\nif IORING_SETUP_COOP_TASKRUN is set from ublk server.\n\nReported-by: Andreas Hindborg <andreas.hindborg@wdc.com>\nCc: Damien Le Moal <damien.lemoal@opensource.wdc.com>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nReviewed-by: Damien Le Moal <damien.lemoal@opensource.wdc.com>\nReviewed-by: ZiyangZhang <ZiyangZhang@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20221121155645.396272-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc7",
        "release_date": "2022-11-23 20:36:57 -0700 ublk_drv: don't forward io commands in reserve order"
    },
    {
        "commit": "2dac1a159216b39ced8d78dba590c5d2f4249586",
        "message": "This reverts commit 2ccc92f4effcfa1c51c4fcf1e34d769099d3cad4\n\nio_req_complete_post() should now behave well even in case of IOPOLL, we\ncan remove completion_lock locking.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7e171c8b530656b14a671c59100ca260e46e7f2a.1669203009.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-23 10:47:07 -0700 io_uring: remove iopoll spinlock"
    },
    {
        "commit": "1bec951c3809051f64a6957fe86d1b4786cc0313",
        "message": "io_req_complete_post() may be used by iopoll enabled rings, grab locks\nin this case. That requires to pass issue_flags to propagate the locking\nstate.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/cc6d854065c57c838ca8e8806f707a226b70fd2d.1669203009.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-23 10:45:31 -0700 io_uring: iopoll protect complete_post"
    },
    {
        "commit": "fa18fa2272c7469e470dcb7bf838ea50a25494ca",
        "message": "Inline __io_req_complete_put() into io_req_complete_post(), there are no\nother users.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1923a4dfe80fa877f859a22ed3df2d5fc8ecf02b.1669203009.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-23 10:44:01 -0700 io_uring: inline __io_req_complete_put()"
    },
    {
        "commit": "833b5dfffc26c81835ce38e2a5df9ac5fa142735",
        "message": "Remove io_req_tw_post() and io_req_tw_post_queue(), we can use\nio_req_task_complete() instead.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b9b73c08022c7f1457023ac841f35c0100e70345.1669203009.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-23 10:44:00 -0700 io_uring: remove io_req_tw_post_queue"
    },
    {
        "commit": "624fd779fd869bdcb2c0ccca0f09456eed71ed52",
        "message": "Use a more generic io_req_task_complete() in timeout completion\ntask_work instead of io_req_complete_post().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bda1710b58c07bf06107421c2a65c529ea9cdcac.1669203009.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-23 10:44:00 -0700 io_uring: use io_req_task_complete() in timeout"
    },
    {
        "commit": "e276ae344a770f91912a81c6a338d92efd319be2",
        "message": "A preparation patch, make sure we always hold uring_lock around\nio_req_complete_failed(). The only place deviating from the rule\nis io_cancel_defer_files(), queue a tw instead.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/70760344eadaecf2939287084b9d4ba5c05a6984.1669203009.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-23 10:44:00 -0700 io_uring: hold locks for io_req_complete_failed"
    },
    {
        "commit": "2ccc92f4effcfa1c51c4fcf1e34d769099d3cad4",
        "message": "There are pieces of code that may allow iopoll to race filling cqes,\ntemporarily add spinlocking around posting events.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/84d86b5c117feda075471c5c9e65208e0dccf5d0.1669203009.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-23 10:44:00 -0700 io_uring: add completion locking for iopoll"
    },
    {
        "commit": "6c16fe3c16bdc420719768f7ea97b82bd6303eec",
        "message": "__io_cq_unlock_post() is identical to io_cq_unlock_post(), and\nio_cqring_ev_posted() has a single caller so migth as well just inline\nit there.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-22 06:09:30 -0700 io_uring: kill io_cqring_ev_posted() and __io_cq_unlock_post()"
    },
    {
        "commit": "4061f0ef730cca5171351b7018b34a45b76df9c2",
        "message": "This reverts commit 7fdbc5f014c3f71bc44673a2d6c5bb2d12d45f25.\n\nThis patch dealt with a subset of the real problem, which is a potential\ncircular dependency on the wakup path for io_uring itself. Outside of\nio_uring, eventfd can also trigger this (see details in 03e02acda8e2)\nand so can epoll (see details in caf1aeaffc3b). Now that we have a\ngeneric solution to this problem, get rid of the io_uring specific\nwork-around.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-22 06:08:31 -0700 Revert \"io_uring: disallow self-propelled ring polling\""
    },
    {
        "commit": "4464853277d0ccdb9914608dd1332f0fa2f9846f",
        "message": "Pass in EPOLL_URING_WAKE when signaling eventfd or doing poll related\nwakups, so that we can check for a circular event dependency between\neventfd and epoll. If this flag is set when our wakeup handlers are\ncalled, then we know we have a dependency that needs to terminate\nmultishot requests.\n\neventfd and epoll are the only such possible dependencies.\n\nCc: stable@vger.kernel.org # 6.0\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-22 06:08:31 -0700 io_uring: pass in EPOLL_URING_WAKE for eventfd signaling and wakeups"
    },
    {
        "commit": "03e02acda8e267a8183e1e0ed289ff1ef9cd7ed8",
        "message": "This is identical to eventfd_signal(), but it allows the caller to pass\nin a mask to be used for the poll wakeup key. The use case is avoiding\nrepeated multishot triggers if we have a dependency between eventfd and\nio_uring.\n\nIf we setup an eventfd context and register that as the io_uring eventfd,\nand at the same time queue a multishot poll request for the eventfd\ncontext, then any CQE posted will repeatedly trigger the multishot request\nuntil it terminates when the CQ ring overflows.\n\nIn preparation for io_uring detecting this circular dependency, add the\nmentioned helper so that io_uring can pass in EPOLL_URING as part of the\npoll wakeup key.\n\nCc: stable@vger.kernel.org # 6.0\n[axboe: fold in !CONFIG_EVENTFD fix from Zhang Qilong]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-22 06:07:55 -0700 eventfd: provide a eventfd_signal_mask() helper"
    },
    {
        "commit": "caf1aeaffc3b09649a56769e559333ae2c4f1802",
        "message": "We can have dependencies between epoll and io_uring. Consider an epoll\ncontext, identified by the epfd file descriptor, and an io_uring file\ndescriptor identified by iofd. If we add iofd to the epfd context, and\narm a multishot poll request for epfd with iofd, then the multishot\npoll request will repeatedly trigger and generate events until terminated\nby CQ ring overflow. This isn't a desired behavior.\n\nAdd EPOLL_URING so that io_uring can pass it in as part of the poll wakeup\nkey, and io_uring can check for that to detect a potential recursive\ninvocation.\n\nCc: stable@vger.kernel.org # 6.0\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:45:29 -0700 eventpoll: add EPOLL_URING_WAKE poll wakeup flag"
    },
    {
        "commit": "f9d567c75ec216447f36da6e855500023504fa04",
        "message": "There is only one user of __io_req_complete_post(), inline it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ef4c9059950a3da5cf68df00f977f1fd13bd9306.1668597569.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:45:19 -0700 io_uring: inline __io_req_complete_post()"
    },
    {
        "commit": "d75936062049522172a107c994242b76c89777f9",
        "message": "When the target process is dying and so task_work_add() is not allowed\nwe push all task_work item to the fallback workqueue. Move the part\nresponsible for moving tw items out of __io_req_task_work_add() into\na separate function. Makes it a bit cleaner and gives the compiler a bit\nof extra info.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e503dab9d7af95470ca6b214c6de17715ae4e748.1668162751.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:44:21 -0700 io_uring: split tw fallback into a function"
    },
    {
        "commit": "e52d2e583e4ad1d5d0b804d79c2b8752eb0e5ceb",
        "message": "__io_req_task_work_add() is huge but marked inline, that makes compilers\nto generate lots of garbage. Inline the wrapper caller\nio_req_task_work_add() instead.\n\nbefore and after:\n   text    data     bss     dec     hex filename\n  47347   16248       8   63603    f873 io_uring/io_uring.o\n   text    data     bss     dec     hex filename\n  45303   16248       8   61559    f077 io_uring/io_uring.o\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/26dc8c28ca0160e3269ef3e55c5a8b917c4d4450.1668162751.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:44:18 -0700 io_uring: inline io_req_task_work_add()"
    },
    {
        "commit": "23a6c9ac4dbd7cccf5b909e78aa84192b65f2833",
        "message": "Previous commit ebc11b6c6b87 (\"io_uring: clean io-wq callbacks\") rename\nio_free_work() into io_wq_free_work() for consistency. This patch also\nupdates relevant comment to avoid misunderstanding.\n\nFixes: ebc11b6c6b87 (\"io_uring: clean io-wq callbacks\")\nSigned-off-by: Lin Ma <linma@zju.edu.cn>\nLink: https://lore.kernel.org/r/20221110122103.20120-1-linma@zju.edu.cn\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:44:16 -0700 io_uring: update outdated comment of callbacks"
    },
    {
        "commit": "cd42a53d25d489317b9ae5213da721cde8cb7071",
        "message": "Previous commit 13a99017ff19 (\"io_uring: remove events caching\natavisms\") entirely removes the events caching optimization introduced\nby commit 81459350d581 (\"io_uring: cache req->apoll->events in\nreq->cflags\"). Hence the related comment should also be removed to avoid\nmisunderstanding.\n\nFixes: 13a99017ff19 (\"io_uring: remove events caching atavisms\")\nSigned-off-by: Lin Ma <linma@zju.edu.cn>\nLink: https://lore.kernel.org/r/20221110060313.16303-1-linma@zju.edu.cn\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:44:14 -0700 io_uring/poll: remove outdated comments of caching"
    },
    {
        "commit": "e2ad599d1ed38fe743106f10d58a0cbfc00b51e2",
        "message": "With commit aa1df3a360a0 (\"io_uring: fix CQE reordering\"), there are\nstronger guarantees for overflow ordering. Specifically ensuring that\nuserspace will not receive out of order receive CQEs. Therefore this is\nnot needed any more for recv/recvmsg.\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20221107125236.260132-4-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:44:09 -0700 io_uring: allow multishot recv CQEs to overflow"
    },
    {
        "commit": "515e26961295bee9da5e26916c27739dca6c10e1",
        "message": "This is no longer needed after commit aa1df3a360a0 (\"io_uring: fix CQE\nreordering\"), since all reordering is now taken care of.\n\nThis reverts commit cbd25748545c (\"io_uring: fix multishot accept\nordering\").\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20221107125236.260132-2-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:43:42 -0700 io_uring: revert \"io_uring fix multishot accept ordering\""
    },
    {
        "commit": "ef67fcb41de6d3d5bbb16aaa66d4c706c4cacf54",
        "message": "Running task work when not needed can unnecessarily delay\noperations. Specifically IORING_SETUP_DEFER_TASKRUN tries to avoid running\ntask work until the user requests it. Therefore do not run it in\nio_uring_register any more.\n\nThe one catch is that io_rsrc_ref_quiesce expects it to have run in order\nto process all outstanding references, and so reorder it's loop to do this.\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20221107123349.4106213-1-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:38:54 -0700 io_uring: do not always force run task_work in io_uring_register"
    },
    {
        "commit": "df730ec21f7ba395b1b22e7f93a3a85b1d1b7882",
        "message": "Fixes two errors:\n\n\"ERROR: do not use assignment in if condition\n130: FILE: io_uring/net.c:130:\n+       if (!(issue_flags & IO_URING_F_UNLOCKED) &&\n\nERROR: do not use assignment in if condition\n599: FILE: io_uring/poll.c:599:\n+       } else if (!(issue_flags & IO_URING_F_UNLOCKED) &&\"\nreported by checkpatch.pl in net.c and poll.c .\n\nSigned-off-by: Xinghui Li <korantli@tencent.com>\nReported-by: kernel test robot <lkp@intel.com>\nLink: https://lore.kernel.org/r/20221102082503.32236-1-korantwork@gmail.com\n[axboe: style tweaks]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:38:31 -0700 io_uring: fix two assignments in if conditions"
    },
    {
        "commit": "42385b02baad0df55474b7f36dc13e0d4ffd0cc0",
        "message": "We can also move mm accounting to the extended callbacks. It removes a\nfew cycles from the hot path including skipping one function call and\nsetting io_req_task_complete as a callback directly. For user backed I/O\nit shouldn't make any difference taking into considering atomic mm\naccounting and page pinning.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1062f270273ad11c1b7b45ec59a6a317533d5e64.1667557923.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:38:31 -0700 io_uring/net: move mm accounting to a slower path"
    },
    {
        "commit": "40725d1b960f19a11a1ebd1ab537844ebf39347c",
        "message": "Add custom tw and notif callbacks on top of usual bits also handling zc\nreporting. That moves it from the hot path.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/40de4a6409042478e1f35adc4912e23226cb1b5c.1667557923.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:38:31 -0700 io_uring: move zc reporting from the hot path"
    },
    {
        "commit": "bedd20bcf3b08c5d2f03f30a83a10022bde5e596",
        "message": "io_notif_flush() is pretty simple, we can inline it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/332359e7bd124138dfe51340bbec829c9b265c18.1667557923.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:38:31 -0700 io_uring/net: inline io_notif_flush()"
    },
    {
        "commit": "7fa8e84192fd8dbc97b4c8c1acfd10017c3dd7b6",
        "message": "Just a simple renaming patch, io_uring_tx_zerocopy_callback() is too\nbulky and doesn't follow usual naming style.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/24d78325403ca6dcb1ec4bced1e33cacc9b832a5.1667557923.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:38:31 -0700 io_uring/net: rename io_uring_tx_zerocopy_callback"
    },
    {
        "commit": "fc1dd0d4fa523916529ddf7c56d7b866312c4262",
        "message": "We're going to have multiple notification tw functions. In preparation\nfor future changes default the tw callback in advance so later we can\nreplace it with other versions.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7acdbea5e20eadd844513320cd454af14ba50f64.1667557923.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:38:31 -0700 io_uring/net: preset notif tw handler"
    },
    {
        "commit": "5bc8e8884b4e9579ca57e33d42d60090b7288050",
        "message": "io_send_zc_prep() sets up notification's rsrc_node when needed, don't\nunconditionally install it on notif alloc.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/dbe4875ac33e180b9799d8537a5e27935e82aac4.1667557923.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:38:31 -0700 io_uring/net: remove extra notif rsrc setup"
    },
    {
        "commit": "3671163beb633fbe3297b8e30369b640ce4bd690",
        "message": "There are multiple users of io_req_task_complete() including zc\nnotifications, but only read requests use selected buffers. As we\nalready have an rw specific tw function, move io_put_kbuf() in there.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/94374c7649aaefc3a17808dc4701f25ccd457e25.1667557923.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:38:31 -0700 io_uring: move kbuf put out of generic tw complete"
    },
    {
        "commit": "e307e6698165ca6508ed42c69cb1be76c8eb6a3c",
        "message": "It might be useful for applications to detect if a zero copy transfer with\nSEND[MSG]_ZC was actually possible or not. The application can fallback to\nplain SEND[MSG] in order to avoid the overhead of two cqes per request. Or\nit can generate a log message that could indicate to an administrator that\nno zero copy was possible and could explain degraded performance.\n\nCc: stable@vger.kernel.org # 6.1\nLink: https://lore.kernel.org/io-uring/fb6a7599-8a9b-15e5-9b64-6cd9d01c6ff4@gmail.com/T/#m2b0d9df94ce43b0e69e6c089bdff0ce6babbdfaa\nSigned-off-by: Stefan Metzmacher <metze@samba.org>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8945b01756d902f5d5b0667f20b957ad3f742e5e.1666895626.git.metze@samba.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-21 07:38:31 -0700 io_uring/net: introduce IORING_SEND_ZC_REPORT_USAGE flag"
    },
    {
        "commit": "e428e9613531d1ef6bd0d91352899712b29134fb",
        "message": "If a kernel thread is created by a user thread, it may carry FPU/SIMD\nthread info flags (TIF_USEDFPU, TIF_USEDSIMD, etc.). Then it will be\nconsidered as a fpu owner and kernel try to save its FPU/SIMD context\nand cause such errors:\n\n[   41.518931] do_fpu invoked from kernel context![#1]:\n[   41.523933] CPU: 1 PID: 395 Comm: iou-wrk-394 Not tainted 6.1.0-rc5+ #217\n[   41.530757] Hardware name: Loongson Loongson-3A5000-7A1000-1w-CRB/Loongson-LS3A5000-7A1000-1w-CRB, BIOS vUDK2018-LoongArch-V2.0.pre-beta8 08/18/2022\n[   41.544064] $ 0   : 0000000000000000 90000000011e9468 9000000106c7c000 9000000106c7fcf0\n[   41.552101] $ 4   : 9000000106305d40 9000000106689800 9000000106c7fd08 0000000003995818\n[   41.560138] $ 8   : 0000000000000001 90000000009a72e4 0000000000000020 fffffffffffffffc\n[   41.568174] $12   : 0000000000000000 0000000000000000 0000000000000020 00000009aab7e130\n[   41.576211] $16   : 00000000000001ff 0000000000000407 0000000000000001 0000000000000000\n[   41.584247] $20   : 0000000000000000 0000000000000001 9000000106c7fd70 90000001002f0400\n[   41.592284] $24   : 0000000000000000 900000000178f740 90000000011e9834 90000001063057c0\n[   41.600320] $28   : 0000000000000000 0000000000000001 9000000006826b40 9000000106305140\n[   41.608356] era   : 9000000000228848 _save_fp+0x0/0xd8\n[   41.613542] ra    : 90000000011e9468 __schedule+0x568/0x8d0\n[   41.619160] CSR crmd: 000000b0\n[   41.619163] CSR prmd: 00000000\n[   41.622359] CSR euen: 00000000\n[   41.625558] CSR ecfg: 00071c1c\n[   41.628756] CSR estat: 000f0000\n[   41.635239] ExcCode : f (SubCode 0)\n[   41.638783] PrId  : 0014c010 (Loongson-64bit)\n[   41.643191] Modules linked in: acpi_ipmi vfat fat ipmi_si ipmi_devintf cfg80211 ipmi_msghandler rfkill fuse efivarfs\n[   41.653734] Process iou-wrk-394 (pid: 395, threadinfo=0000000004ebe913, task=00000000636fa1be)\n[   41.662375] Stack : 00000000ffff0875 9000000006800ec0 9000000006800ec0 90000000002d57e0\n[   41.670412]         0000000000000001 0000000000000000 9000000106535880 0000000000000001\n[   41.678450]         9000000105291800 0000000000000000 9000000105291838 900000000178e000\n[   41.686487]         9000000106c7fd90 9000000106305140 0000000000000001 90000000011e9834\n[   41.694523]         00000000ffff0875 90000000011f034c 9000000105291838 9000000105291830\n[   41.702561]         0000000000000000 9000000006801440 00000000ffff0875 90000000002d48c0\n[   41.710597]         9000000128800001 9000000106305140 9000000105291838 9000000105291838\n[   41.718634]         9000000105291830 9000000107811740 9000000105291848 90000000009bf1e0\n[   41.726672]         9000000105291830 9000000107811748 2d6b72772d756f69 0000000000343933\n[   41.734708]         0000000000000000 0000000000000000 0000000000000000 0000000000000000\n[   41.742745]         ...\n[   41.745252] Call Trace:\n[   42.197868] [<9000000000228848>] _save_fp+0x0/0xd8\n[   42.205214] [<90000000011ed468>] __schedule+0x568/0x8d0\n[   42.210485] [<90000000011ed834>] schedule+0x64/0xd4\n[   42.215411] [<90000000011f434c>] schedule_timeout+0x88/0x188\n[   42.221115] [<90000000009c36d0>] io_wqe_worker+0x184/0x350\n[   42.226645] [<9000000000221cf0>] ret_from_kernel_thread+0xc/0x9c\n\nThis can be easily triggered by ltp testcase syscalls/io_uring02 and it\ncan also be easily fixed by clearing the FPU/SIMD thread info flags for\nkernel threads in copy_thread().\n\nCc: stable@vger.kernel.org\nReported-by: Qi Hu <huqi@loongson.cn>\nSigned-off-by: Huacai Chen <chenhuacai@loongson.cn>",
        "kernel_version": "v6.1-rc7",
        "release_date": "2022-11-21 19:02:57 +0800 LoongArch: Clear FPU/SIMD thread info flags for kernel thread"
    },
    {
        "commit": "a66e4cbf7a29fe555ebb995b130b2e059fc26d89",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"This is mostly fixing issues around the poll rework, but also two\n  tweaks for the multishot handling for accept and receive.\n\n  All stable material\"\n\n* tag 'io_uring-6.1-2022-11-18' of git://git.kernel.dk/linux:\n  io_uring: disallow self-propelled ring polling\n  io_uring: fix multishot recv request leaks\n  io_uring: fix multishot accept request leaks\n  io_uring: fix tw losing poll events\n  io_uring: update res mask in io_poll_check_events",
        "kernel_version": "v6.1-rc6",
        "release_date": "2022-11-18 14:59:53 -0800 Merge tag 'io_uring-6.1-2022-11-18' of git://git.kernel.dk/linux"
    },
    {
        "commit": "7fdbc5f014c3f71bc44673a2d6c5bb2d12d45f25",
        "message": "When we post a CQE we wake all ring pollers as it normally should be.\nHowever, if a CQE was generated by a multishot poll request targeting\nits own ring, it'll wake that request up, which will make it to post\na new CQE, which will wake the request and so on until it exhausts all\nCQ entries.\n\nDon't allow multishot polling io_uring files but downgrade them to\noneshots, which was always stated as a correct behaviour that the\nuserspace should check for.\n\nCc: stable@vger.kernel.org\nFixes: aa43477b04025 (\"io_uring: poll rework\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3124038c0e7474d427538c2d915335ec28c92d21.1668785722.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc6",
        "release_date": "2022-11-18 09:29:31 -0700 io_uring: disallow self-propelled ring polling"
    },
    {
        "commit": "100d6b17c06ee4c2b42fdddf0fe4ab77c86eb77e",
        "message": "Having REQ_F_POLLED set doesn't guarantee that the request is\nexecuted as a multishot from the polling path. Fortunately for us, if\nthe code thinks it's multishot issue when it's not, it can only ask to\nskip completion so leaking the request. Use issue_flags to mark\nmultipoll issues.\n\nCc: stable@vger.kernel.org\nFixes: 1300ebb20286b (\"io_uring: multishot recv\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/37762040ba9c52b81b92a2f5ebfd4ee484088951.1668710222.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc6",
        "release_date": "2022-11-17 12:33:33 -0700 io_uring: fix multishot recv request leaks"
    },
    {
        "commit": "91482864768a874c4290ef93b84a78f4f1dac51b",
        "message": "Having REQ_F_POLLED set doesn't guarantee that the request is\nexecuted as a multishot from the polling path. Fortunately for us, if\nthe code thinks it's multishot issue when it's not, it can only ask to\nskip completion so leaking the request. Use issue_flags to mark\nmultipoll issues.\n\nCc: stable@vger.kernel.org\nFixes: 390ed29b5e425 (\"io_uring: add IORING_ACCEPT_MULTISHOT for accept\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7700ac57653f2823e30b34dc74da68678c0c5f13.1668710222.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc6",
        "release_date": "2022-11-17 12:33:33 -0700 io_uring: fix multishot accept request leaks"
    },
    {
        "commit": "539bcb57da2f58886d7d5c17134236b0ec9cd15d",
        "message": "We may never try to process a poll wake and its mask if there was\nmultiple wake ups racing for queueing up a tw. Force\nio_poll_check_events() to update the mask by vfs_poll().\n\nCc: stable@vger.kernel.org\nFixes: aa43477b04025 (\"io_uring: poll rework\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/00344d60f8b18907171178d7cf598de71d127b0b.1668710222.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc6",
        "release_date": "2022-11-17 12:33:33 -0700 io_uring: fix tw losing poll events"
    },
    {
        "commit": "b98186aee22fa593bc8c6b2c5d839c2ee518bc8c",
        "message": "When io_poll_check_events() collides with someone attempting to queue a\ntask work, it'll spin for one more time. However, it'll continue to use\nthe mask from the first iteration instead of updating it. For example,\nif the first wake up was a EPOLLIN and the second EPOLLOUT, the\nuserspace will not get EPOLLOUT in time.\n\nClear the mask for all subsequent iterations to force vfs_poll().\n\nCc: stable@vger.kernel.org\nFixes: aa43477b04025 (\"io_uring: poll rework\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2dac97e8f691231049cb259c4ae57e79e40b537c.1668710222.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc6",
        "release_date": "2022-11-17 12:33:33 -0700 io_uring: update res mask in io_poll_check_events"
    },
    {
        "commit": "12e4e8c7ab5978eb56f9d363461a8a40a8618bf4",
        "message": "Now we can use IOCB_ALLOC_CACHE not only for iopoll'ed reads/write but\nalso for normal IRQ driven I/O.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/fb8bd092ed5a4a3b037e84e4777074d07aa5639a.1667384020.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-16 09:44:27 -0700 io_uring/rw: enable bio caches for IRQ rw"
    },
    {
        "commit": "bdcdd86ca94b5e9faa18d6f4d3dda660ac5c887e",
        "message": "When doing a nowait buffered write we can trigger the following assertion:\n\n[11138.437027] assertion failed: !path->nowait, in fs/btrfs/ctree.c:4658\n[11138.438251] ------------[ cut here ]------------\n[11138.438254] kernel BUG at fs/btrfs/messages.c:259!\n[11138.438762] invalid opcode: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC PTI\n[11138.439450] CPU: 4 PID: 1091021 Comm: fsstress Not tainted 6.1.0-rc4-btrfs-next-128 #1\n[11138.440611] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.14.0-0-g155821a1990b-prebuilt.qemu.org 04/01/2014\n[11138.442553] RIP: 0010:btrfs_assertfail+0x19/0x1b [btrfs]\n[11138.443583] Code: 5b 41 5a 41 (...)\n[11138.446437] RSP: 0018:ffffbaf0cf05b840 EFLAGS: 00010246\n[11138.447235] RAX: 0000000000000039 RBX: ffffbaf0cf05b938 RCX: 0000000000000000\n[11138.448303] RDX: 0000000000000000 RSI: ffffffffb2ef59f6 RDI: 00000000ffffffff\n[11138.449370] RBP: ffff9165f581eb68 R08: 00000000ffffffff R09: 0000000000000001\n[11138.450493] R10: ffff9167a88421f8 R11: 0000000000000000 R12: ffff9164981b1000\n[11138.451661] R13: 000000008c8f1000 R14: ffff9164991d4000 R15: ffff9164981b1000\n[11138.452225] FS:  00007f1438a66440(0000) GS:ffff9167ad600000(0000) knlGS:0000000000000000\n[11138.452949] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n[11138.453394] CR2: 00007f1438a64000 CR3: 0000000100c36002 CR4: 0000000000370ee0\n[11138.454057] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n[11138.454879] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n[11138.455779] Call Trace:\n[11138.456211]  <TASK>\n[11138.456598]  btrfs_next_old_leaf.cold+0x18/0x1d [btrfs]\n[11138.457827]  ? kmem_cache_alloc+0x18d/0x2a0\n[11138.458516]  btrfs_lookup_csums_range+0x149/0x4d0 [btrfs]\n[11138.459407]  csum_exist_in_range+0x56/0x110 [btrfs]\n[11138.460271]  can_nocow_file_extent+0x27c/0x310 [btrfs]\n[11138.461155]  can_nocow_extent+0x1ec/0x2e0 [btrfs]\n[11138.461672]  btrfs_check_nocow_lock+0x114/0x1c0 [btrfs]\n[11138.462951]  btrfs_buffered_write+0x44c/0x8e0 [btrfs]\n[11138.463482]  btrfs_do_write_iter+0x42b/0x5f0 [btrfs]\n[11138.463982]  ? lock_release+0x153/0x4a0\n[11138.464347]  io_write+0x11b/0x570\n[11138.464660]  ? lock_release+0x153/0x4a0\n[11138.465213]  ? lock_is_held_type+0xe8/0x140\n[11138.466003]  io_issue_sqe+0x63/0x4a0\n[11138.466339]  io_submit_sqes+0x238/0x770\n[11138.466741]  __do_sys_io_uring_enter+0x37b/0xb10\n[11138.467206]  ? lock_is_held_type+0xe8/0x140\n[11138.467879]  ? syscall_enter_from_user_mode+0x1d/0x50\n[11138.468688]  do_syscall_64+0x38/0x90\n[11138.469265]  entry_SYSCALL_64_after_hwframe+0x63/0xcd\n[11138.470017] RIP: 0033:0x7f1438c539e6\n\nThis is because to check if we can NOCOW, we check that if we can NOCOW\ninto an extent (it's prealloc extent or the inode has NOCOW attribute),\nand then check if there are csums for the extent's range in the csum tree.\nThe search may leave us beyond the last slot of a leaf, and then when\nwe call btrfs_next_leaf() we end up at btrfs_next_old_leaf() with a\ntime_seq of 0.\n\nThis triggers a failure of the first assertion at btrfs_next_old_leaf(),\nsince we have a nowait path. With assertions disabled, we simply don't\nrespect the NOWAIT semantics, allowing the write to block on locks or\nblocking on IO for reading an extent buffer from disk.\n\nFix this by:\n\n1) Triggering the assertion only if time_seq is not 0, which means that\n   search is being done by a tree mod log user, and in the buffered and\n   direct IO write paths we don't use the tree mod log;\n\n2) Implementing NOWAIT semantics at btrfs_next_old_leaf(). Any failure to\n   lock an extent buffer should return immediately and not retry the\n   search, as well as if we need to do IO to read an extent buffer from\n   disk.\n\nFixes: c922b016f353 (\"btrfs: assert nowait mode is not used for some btree search functions\")\nSigned-off-by: Filipe Manana <fdmanana@suse.com>\nSigned-off-by: David Sterba <dsterba@suse.com>",
        "kernel_version": "v6.1-rc7",
        "release_date": "2022-11-15 15:01:01 +0100 btrfs: fix assertion failure and blocking during nowait buffered write"
    },
    {
        "commit": "4e6b2b2e4f30c29caf89ecfa9ed4d9f97d151102",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Nothing major, just a few minor tweaks:\n\n   - Tweak for the TCP zero-copy io_uring self test (Pavel)\n\n   - Rather than use our internal cached value of number of CQ events\n     available, use what the user can see (Dylan)\n\n   - Fix a typo in a comment, added in this release (me)\n\n   - Don't allow wrapping while adding provided buffers (me)\n\n   - Fix a double poll race, and add a lockdep assertion for it too\n     (Pavel)\"\n\n* tag 'io_uring-6.1-2022-11-11' of git://git.kernel.dk/linux:\n  io_uring/poll: lockdep annote io_poll_req_insert_locked\n  io_uring/poll: fix double poll req->flags races\n  io_uring: check for rollover of buffer ID when providing buffers\n  io_uring: calculate CQEs from the user visible value\n  io_uring: fix typo in io_uring.h comment\n  selftests/net: don't tests batched TCP io_uring zc",
        "kernel_version": "v6.1-rc5",
        "release_date": "2022-11-11 14:02:44 -0800 Merge tag 'io_uring-6.1-2022-11-11' of git://git.kernel.dk/linux"
    },
    {
        "commit": "5576035f15dfcc6cb1cec236db40c2c0733b0ba4",
        "message": "Add a lockdep annotation in io_poll_req_insert_locked().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8115d8e702733754d0aea119e9b5bb63d1eb8b24.1668184658.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc5",
        "release_date": "2022-11-11 09:59:27 -0700 io_uring/poll: lockdep annote io_poll_req_insert_locked"
    },
    {
        "commit": "30a33669fa21cd3dc7d92a00ba736358059014b7",
        "message": "io_poll_double_prepare()            | io_poll_wake()\n                                    | poll->head = NULL\nsmp_load(&poll->head); /* NULL */   |\nflags = req->flags;                 |\n                                    | req->flags &= ~SINGLE_POLL;\nreq->flags = flags | DOUBLE_POLL    |\n\nThe idea behind io_poll_double_prepare() is to serialise with the\nfirst poll entry by taking the wq lock. However, it's not safe to assume\nthat io_poll_wake() is not running when we can't grab the lock and so we\nmay race modifying req->flags.\n\nSkip double poll setup if that happens. It's ok because the first poll\nentry will only be removed when it's definitely completing, e.g.\npollfree or oneshot with a valid mask.\n\nFixes: 49f1c68e048f1 (\"io_uring: optimise submission side poll_refs\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b7fab2d502f6121a7d7b199fe4d914a43ca9cdfd.1668184658.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc5",
        "release_date": "2022-11-11 09:59:27 -0700 io_uring/poll: fix double poll req->flags races"
    },
    {
        "commit": "3851d25c75ed03117268a8feb34adca5a843a126",
        "message": "We already check if the chosen starting offset for the buffer IDs fit\nwithin an unsigned short, as 65535 is the maximum value for a provided\nbuffer. But if the caller asks to add N buffers at offset M, and M + N\nwould exceed the size of the unsigned short, we simply add buffers with\nwrapping around the ID.\n\nThis is not necessarily a bug and could in fact be a valid use case, but\nit seems confusing and inconsistent with the initial check for starting\noffset. Let's check for wrap consistently, and error the addition if we\ndo need to wrap.\n\nReported-by: Olivier Langlois <olivier@trillion01.com>\nLink: https://github.com/axboe/liburing/issues/726\nCc: stable@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc5",
        "release_date": "2022-11-10 11:07:41 -0700 io_uring: check for rollover of buffer ID when providing buffers"
    },
    {
        "commit": "e487ebbd12986facc7f77129d3ca80de84841170",
        "message": "io_uring provides a simple mechanism to test long-term, R/W GUP pins\n-- via fixed buffers -- and can be used to verify that GUP pins stay\nin sync with the pages in the page table even if a page would\ntemporarily get mapped R/O or concurrent fork() could accidentially\nend up sharing pinned pages with the child.\n\nNote that this essentially re-introduces local_config support that was\nremoved recently in commit 6f83d6c74ea5 (\"Kselftests: remove support of\nlibhugetlbfs from kselftests\").\n\n[david@redhat.com: s/size_t/ssize_t/ on `cur', `total'.]\n  Link: https://lkml.kernel.org/r/445fe1ae-9e22-0d1d-4d09-272231d2f84a@redhat.com\nLink: https://lkml.kernel.org/r/20220927110120.106906-6-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Christoph von Recklinghausen <crecklin@redhat.com>\nCc: Don Dutile <ddutile@redhat.com>\nCc: Jason Gunthorpe <jgg@nvidia.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Mike Rapoport <rppt@kernel.org>\nCc: Nadav Amit <namit@vmware.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Shuah Khan <shuah@kernel.org>\nCc: Vlastimil Babka <vbabka@suse.cz>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v6.2-rc1",
        "release_date": "2022-11-08 17:37:15 -0800 selftests/vm: anon_cow: add liburing test cases"
    },
    {
        "commit": "0fc8c2acbfc789a977a50a4a9812a8e4b37958ce",
        "message": "io_cqring_wait (and it's wake function io_has_work) used cached_cq_tail in\norder to calculate the number of CQEs. cached_cq_tail is set strictly\nbefore the user visible rings->cq.tail\n\nHowever as far as userspace is concerned,  if io_uring_enter(2) is called\nwith a minimum number of events, they will verify by checking\nrings->cq.tail.\n\nIt is therefore possible for io_uring_enter(2) to return early with fewer\nevents visible to the user.\n\nInstead make the wait functions read from the user visible value, so there\nwill be no discrepency.\n\nThis is triggered eventually by the following reproducer:\n\nstruct io_uring_sqe *sqe;\nstruct io_uring_cqe *cqe;\nunsigned int cqe_ready;\nstruct io_uring ring;\nint ret, i;\n\nret = io_uring_queue_init(N, &ring, 0);\nassert(!ret);\nwhile(true) {\n\tfor (i = 0; i < N; i++) {\n\t\tsqe = io_uring_get_sqe(&ring);\n\t\tio_uring_prep_nop(sqe);\n\t\tsqe->flags |= IOSQE_ASYNC;\n\t}\n\tret = io_uring_submit(&ring);\n\tassert(ret == N);\n\n\tdo {\n\t\tret = io_uring_wait_cqes(&ring, &cqe, N, NULL, NULL);\n\t} while(ret == -EINTR);\n\tcqe_ready = io_uring_cq_ready(&ring);\n\tassert(!ret);\n\tassert(cqe_ready == N);\n\tio_uring_cq_advance(&ring, N);\n}\n\nFixes: ad3eb2c89fb2 (\"io_uring: split overflow state into SQ and CQ side\")\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20221108153016.1854297-1-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc5",
        "release_date": "2022-11-08 10:36:15 -0700 io_uring: calculate CQEs from the user visible value"
    },
    {
        "commit": "6dcabcd398946e2b0b776a8310291aeebe1ca0e6",
        "message": "Just a basic s/thig/this swap, fixing up a typo introduced by a commit\nadded in the 6.1 release.\n\nFixes: 9cda70f622cd (\"io_uring: introduce fixed buffer support for io_uring_cmd\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc5",
        "release_date": "2022-11-06 13:17:27 -0700 io_uring: fix typo in io_uring.h comment"
    },
    {
        "commit": "4869f5750afdb10a0e9cfa0252fce33e53ab681e",
        "message": "Pull block fixes from Jens Axboe:\n\n - Fixes for the ublk driver (Ming)\n\n - Fixes for error handling memory leaks (Chen Jun, Chen Zhongjin)\n\n - Explicitly clear the last request in a chain when the plug is\n   flushed, as it may have already been issued (Al)\n\n* tag 'block-6.1-2022-11-05' of git://git.kernel.dk/linux:\n  block: blk_add_rq_to_plug(): clear stale 'last' after flush\n  blk-mq: Fix kmemleak in blk_mq_init_allocated_queue\n  block: Fix possible memory leak for rq_wb on add_disk failure\n  ublk_drv: add ublk_queue_cmd() for cleanup\n  ublk_drv: avoid to touch io_uring cmd in blk_mq io path\n  ublk_drv: comment on ublk_driver entry of Kconfig\n  ublk_drv: return flag of UBLK_F_URING_CMD_COMP_IN_TASK in case of module",
        "kernel_version": "v6.1-rc4",
        "release_date": "2022-11-05 09:02:28 -0700 Merge tag 'block-6.1-2022-11-05' of git://git.kernel.dk/linux"
    },
    {
        "commit": "a348c8d4f6cf23ef04b0edaccdfe9d94c2d335db",
        "message": "If we are doing a buffered write in NOWAIT context and we can't reserve\nmetadata space due to -ENOSPC, then we should return -EAGAIN so that we\nretry the write in a context allowed to block and do metadata reservation\nwith flushing, which might succeed this time due to the allowed flushing.\n\nReturning -ENOSPC while in NOWAIT context simply makes some writes fail\nwith -ENOSPC when they would likely succeed after switching from NOWAIT\ncontext to blocking context. That is unexpected behaviour and even fio\ncomplains about it with a warning like this:\n\n  fio: io_u error on file /mnt/sdi/task_0.0.0: No space left on device: write offset=1535705088, buflen=65536\n  fio: pid=592630, err=28/file:io_u.c:1846, func=io_u error, error=No space left on device\n\nThe fio's job config is this:\n\n   [global]\n   bs=64K\n   ioengine=io_uring\n   iodepth=1\n   size=2236962133\n   nr_files=1\n   filesize=2236962133\n   direct=0\n   runtime=10\n   fallocate=posix\n   io_size=2236962133\n   group_reporting\n   time_based\n\n   [task_0]\n   rw=randwrite\n   directory=/mnt/sdi\n   numjobs=4\n\nSo fix this by returning -EAGAIN if we are in NOWAIT context and the\nmetadata reservation failed with -ENOSPC.\n\nFixes: 304e45acdb8f (\"btrfs: plumb NOWAIT through the write path\")\nReviewed-by: Josef Bacik <josef@toxicpanda.com>\nSigned-off-by: Filipe Manana <fdmanana@suse.com>\nSigned-off-by: David Sterba <dsterba@suse.com>",
        "kernel_version": "v6.1-rc4",
        "release_date": "2022-11-02 17:44:42 +0100 btrfs: fix nowait buffered write returning -ENOSPC"
    },
    {
        "commit": "9921d5013a6e51892623bf2f1c5b49eaecda55ac",
        "message": "It doesn't make sense batch submitting io_uring requests to a single TCP\nsocket without linking or some other kind of ordering. Moreover, it\ncauses spurious -EINTR fails due to interaction with task_work. Disable\nit for now and keep queue depth=1.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b547698d5938b1b1a898af1c260188d8546ded9a.1666700897.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc5",
        "release_date": "2022-11-02 08:27:24 -0600 selftests/net: don't tests batched TCP io_uring zc"
    },
    {
        "commit": "3ab6e94ca539242247d4f00414a1bde584d001ed",
        "message": "io_uring cmd is supposed to be used in ubq daemon context mainly,\nand we should try to avoid to touch it in ublk io submission context,\notherwise this data could become shared between the two contexts,\nand performance is hurt.\n\nSo link request into one per-queue list, and use same batching policy\nof io_uring command, just avoid to touch ucmd in blk-mq io context.\n\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nReviewed-by: ZiyangZhang <ZiyangZhang@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20221029010432.598367-4-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc4",
        "release_date": "2022-10-31 07:23:24 -0600 ublk_drv: avoid to touch io_uring cmd in blk_mq io path"
    },
    {
        "commit": "224e858f215a3d6304f95a92357a1753475ca9cf",
        "message": "UBLK_F_URING_CMD_COMP_IN_TASK needs to be set and returned to userspace\nif ublk driver is built as module, otherwise userspace may get wrong\nflags shown.\n\nFixes: 71f28f3136af (\"ublk_drv: add io_uring based userspace block driver\")\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nReviewed-by: ZiyangZhang <ZiyangZhang@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20221029010432.598367-2-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc4",
        "release_date": "2022-10-31 07:23:16 -0600 ublk_drv: return flag of UBLK_F_URING_CMD_COMP_IN_TASK in case of module"
    },
    {
        "commit": "4d244327dd1bab94a78fa2ab40a33d13ca18326b",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a fix for a locking regression introduced with the deferred\n  task_work running from this merge window\"\n\n* tag 'io_uring-6.1-2022-10-28' of git://git.kernel.dk/linux:\n  io_uring: unlock if __io_run_local_work locked inside\n  io_uring: use io_run_local_work_locked helper",
        "kernel_version": "v6.1-rc3",
        "release_date": "2022-10-29 18:01:16 -0700 Merge tag 'io_uring-6.1-2022-10-28' of git://git.kernel.dk/linux"
    },
    {
        "commit": "b3026767e15b488860d4bbf1649d69612bab2c25",
        "message": "It is possible for tw to lock the ring, and this was not propogated out to\nio_run_local_work. This can cause an unlock to be missed.\n\nInstead pass a pointer to locked into __io_run_local_work.\n\nFixes: 8ac5d85a89b4 (\"io_uring: add local task_work run helper that is entered locked\")\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20221027144429.3971400-3-dylany@meta.com\n[axboe: WARN_ON() -> WARN_ON_ONCE() and add a minor comment]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc3",
        "release_date": "2022-10-27 09:52:12 -0600 io_uring: unlock if __io_run_local_work locked inside"
    },
    {
        "commit": "8de11cdc96bf58b324c59a28512eb9513fd02553",
        "message": "prefer to use io_run_local_work_locked helper for consistency\n\nSigned-off-by: Dylan Yudaken <dylany@meta.com>\nLink: https://lore.kernel.org/r/20221027144429.3971400-2-dylany@meta.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc3",
        "release_date": "2022-10-27 09:51:47 -0600 io_uring: use io_run_local_work_locked helper"
    },
    {
        "commit": "942e01ab90151a16b79b5c0cb8e77530d1ee3dbb",
        "message": "Pull io_uring follow-up from Jens Axboe:\n \"Currently the zero-copy has automatic fallback to normal transmit, and\n  it was decided that it'd be cleaner to return an error instead if the\n  socket type doesn't support it.\n\n  Zero-copy does work with UDP and TCP, it's more of a future proofing\n  kind of thing (eg for samba)\"\n\n* tag 'io_uring-6.1-2022-10-22' of git://git.kernel.dk/linux:\n  io_uring/net: fail zc sendmsg when unsupported by socket\n  io_uring/net: fail zc send when unsupported by socket\n  net: flag sockets supporting msghdr originated zerocopy",
        "kernel_version": "v6.1-rc2",
        "release_date": "2022-10-23 09:55:50 -0700 Merge tag 'io_uring-6.1-2022-10-22' of git://git.kernel.dk/linux"
    },
    {
        "commit": "cc767e7c6913f770741d9fad1efa4957c2623744",
        "message": "The previous patch fails zerocopy send requests for protocols that don't\nsupport it, do the same for zerocopy sendmsg.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0854e7bb4c3d810a48ec8b5853e2f61af36a0467.1666346426.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc2",
        "release_date": "2022-10-22 08:43:03 -0600 io_uring/net: fail zc sendmsg when unsupported by socket"
    },
    {
        "commit": "edf81438799ccead7122948446d7e44b083e788d",
        "message": "If a protocol doesn't support zerocopy it will silently fall back to\ncopying. This type of behaviour has always been a source of troubles\nso it's better to fail such requests instead.\n\nCc: <stable@vger.kernel.org> # 6.0\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2db3c7f16bb6efab4b04569cd16e6242b40c5cb3.1666346426.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc2",
        "release_date": "2022-10-22 08:43:03 -0600 io_uring/net: fail zc send when unsupported by socket"
    },
    {
        "commit": "e993ffe3da4bcddea0536b03be1031bf35cd8d85",
        "message": "We need an efficient way in io_uring to check whether a socket supports\nzerocopy with msghdr provided ubuf_info. Add a new flag into the struct\nsocket flags fields.\n\nCc: <stable@vger.kernel.org> # 6.0\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nAcked-by: Jakub Kicinski <kuba@kernel.org>\nLink: https://lore.kernel.org/r/3dafafab822b1c66308bb58a0ac738b1e3f53f74.1666346426.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc2",
        "release_date": "2022-10-22 08:42:58 -0600 net: flag sockets supporting msghdr originated zerocopy"
    },
    {
        "commit": "294e73ffb0efac4c8bac2d9e6a896225098bd419",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix a potential memory leak in the error handling path of io-wq setup\n   (Rafael)\n\n - Kill an errant debug statement that got added in this release (me)\n\n - Fix an oops with an invalid direct descriptor with IORING_OP_MSG_RING\n   (Harshit)\n\n - Remove unneeded FFS_SCM flagging (Pavel)\n\n - Remove polling off the exit path (Pavel)\n\n - Move out direct descriptor debug check to the cleanup path (Pavel)\n\n - Use the proper helper rather than open-coding cached request get\n   (Pavel)\n\n* tag 'io_uring-6.1-2022-10-20' of git://git.kernel.dk/linux:\n  io-wq: Fix memory leak in worker creation\n  io_uring/msg_ring: Fix NULL pointer dereference in io_msg_send_fd()\n  io_uring/rw: remove leftover debug statement\n  io_uring: don't iopoll from io_ring_ctx_wait_and_kill()\n  io_uring: reuse io_alloc_req()\n  io_uring: kill hot path fixed file bitmap debug checks\n  io_uring: remove FFS_SCM",
        "kernel_version": "v6.1-rc2",
        "release_date": "2022-10-21 15:09:10 -0700 Merge tag 'io_uring-6.1-2022-10-20' of git://git.kernel.dk/linux"
    },
    {
        "commit": "996d3efeb091c503afd3ee6b5e20eabf446fd955",
        "message": "If the CPU mask allocation for a node fails, then the memory allocated for\nthe 'io_wqe' struct of the current node doesn't get freed on the error\nhandling path, since it has not yet been added to the 'wqes' array.\n\nThis was spotted when fuzzing v6.1-rc1 with Syzkaller:\nBUG: memory leak\nunreferenced object 0xffff8880093d5000 (size 1024):\n  comm \"syz-executor.2\", pid 7701, jiffies 4295048595 (age 13.900s)\n  hex dump (first 32 bytes):\n    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................\n    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................\n  backtrace:\n    [<00000000cb463369>] __kmem_cache_alloc_node+0x18e/0x720\n    [<00000000147a3f9c>] kmalloc_node_trace+0x2a/0x130\n    [<000000004e107011>] io_wq_create+0x7b9/0xdc0\n    [<00000000c38b2018>] io_uring_alloc_task_context+0x31e/0x59d\n    [<00000000867399da>] __io_uring_add_tctx_node.cold+0x19/0x1ba\n    [<000000007e0e7a79>] io_uring_setup.cold+0x1b80/0x1dce\n    [<00000000b545e9f6>] __x64_sys_io_uring_setup+0x5d/0x80\n    [<000000008a8a7508>] do_syscall_64+0x5d/0x90\n    [<000000004ac08bec>] entry_SYSCALL_64_after_hwframe+0x63/0xcd\n\nFixes: 0e03496d1967 (\"io-wq: use private CPU mask\")\nCc: stable@vger.kernel.org\nSigned-off-by: Rafael Mendonca <rafaelmendsr@gmail.com>\nLink: https://lore.kernel.org/r/20221020014710.902201-1-rafaelmendsr@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc2",
        "release_date": "2022-10-20 05:48:59 -0700 io-wq: Fix memory leak in worker creation"
    },
    {
        "commit": "16bbdfe5fb0e78e0acb13e45fc127e9a296913f2",
        "message": "Syzkaller produced the below call trace:\n\n BUG: KASAN: null-ptr-deref in io_msg_ring+0x3cb/0x9f0\n Write of size 8 at addr 0000000000000070 by task repro/16399\n\n CPU: 0 PID: 16399 Comm: repro Not tainted 6.1.0-rc1 #28\n Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.11.0-2.el7\n Call Trace:\n  <TASK>\n  dump_stack_lvl+0xcd/0x134\n  ? io_msg_ring+0x3cb/0x9f0\n  kasan_report+0xbc/0xf0\n  ? io_msg_ring+0x3cb/0x9f0\n  kasan_check_range+0x140/0x190\n  io_msg_ring+0x3cb/0x9f0\n  ? io_msg_ring_prep+0x300/0x300\n  io_issue_sqe+0x698/0xca0\n  io_submit_sqes+0x92f/0x1c30\n  __do_sys_io_uring_enter+0xae4/0x24b0\n....\n RIP: 0033:0x7f2eaf8f8289\n RSP: 002b:00007fff40939718 EFLAGS: 00000246 ORIG_RAX: 00000000000001aa\n RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f2eaf8f8289\n RDX: 0000000000000000 RSI: 0000000000006f71 RDI: 0000000000000004\n RBP: 00007fff409397a0 R08: 0000000000000000 R09: 0000000000000039\n R10: 0000000000000000 R11: 0000000000000246 R12: 00000000004006d0\n R13: 00007fff40939880 R14: 0000000000000000 R15: 0000000000000000\n  </TASK>\n Kernel panic - not syncing: panic_on_warn set ...\n\nWe don't have a NULL check on file_ptr in io_msg_send_fd() function,\nso when file_ptr is NUL src_file is also NULL and get_file()\ndereferences a NULL pointer and leads to above crash.\n\nAdd a NULL check to fix this issue.\n\nFixes: e6130eba8a84 (\"io_uring: add support for passing fixed file descriptors\")\nReported-by: syzkaller <syzkaller@googlegroups.com>\nSigned-off-by: Harshit Mogalapalli <harshit.m.mogalapalli@oracle.com>\nLink: https://lore.kernel.org/r/20221019171218.1337614-1-harshit.m.mogalapalli@oracle.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc2",
        "release_date": "2022-10-19 12:33:33 -0700 io_uring/msg_ring: Fix NULL pointer dereference in io_msg_send_fd()"
    },
    {
        "commit": "5c61795ea97c170347c5c4af0c159bd877b8af71",
        "message": "This debug statement was never meant to go into the upstream release,\nkill it off before it ends up in a release. It was just part of the\ntesting for the initial version of the patch.\n\nFixes: 2ec33a6c3cca (\"io_uring/rw: ensure kiocb_end_write() is always called\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc2",
        "release_date": "2022-10-16 17:24:10 -0600 io_uring/rw: remove leftover debug statement"
    },
    {
        "commit": "02bac94bd8efd75f615ac7515dd2def75b43e5b9",
        "message": "We should not be completing requests from a task context that has already\nundergone io_uring cancellations, i.e. __io_uring_cancel(), as there are\nsome assumptions, e.g. around cached task refs draining. Remove\niopolling from io_ring_ctx_wait_and_kill() as it can be called later\nafter PF_EXITING is set with the last task_work run.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7c03cc91455c4a1af49c6b9cbda4e57ea467aa11.1665891182.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc2",
        "release_date": "2022-10-16 17:08:42 -0600 io_uring: don't iopoll from io_ring_ctx_wait_and_kill()"
    },
    {
        "commit": "34f0bc427e94065e7f828e70690f8fe1e01b3a9d",
        "message": "Don't duplicate io_alloc_req() in io_req_caches_free() but reuse the\nhelper.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6005fc88274864a49fc3096c22d8bdd605cf8576.1665891182.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc2",
        "release_date": "2022-10-16 17:08:42 -0600 io_uring: reuse io_alloc_req()"
    },
    {
        "commit": "4d5059512d283dab7372d282c2fbd43c7f5a2456",
        "message": "We test file_table.bitmap in io_file_get_fixed() to check invariants,\ndon't do it, it's expensive and was showing up in profiles. No reports of\nthis triggering has come in. Move the check to the file clear instead,\nwhich will still catch any wrong usage.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/cf77f2ded68d2e5b2bc7355784d969837d48e023.1665891182.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc2",
        "release_date": "2022-10-16 17:07:53 -0600 io_uring: kill hot path fixed file bitmap debug checks"
    },
    {
        "commit": "38eddb2c75fb99b9cd78445094ca0e1bda08d102",
        "message": "THe lifetime of SCM'ed files is bound to ring_sock, which is destroyed\nstrictly after we're done with registered file tables. This means there\nis no need for the FFS_SCM hack, which was not available on 32-bit builds\nanyway.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/984226a1045adf42dc35d8bd7fb5a8bbfa472ce1.1665891182.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc2",
        "release_date": "2022-10-16 17:07:12 -0600 io_uring: remove FFS_SCM"
    },
    {
        "commit": "c98c70ed43cc35b6d5ca9713e037bfe2debc251c",
        "message": "Pull more io_uring updates from Jens Axboe:\n \"A collection of fixes that ended up either being later than the\n  initial pull, or dependent on multiple branches (6.0-late being one of\n  them) and hence deferred purposely. This contains:\n\n   - Cleanup fixes for the single submitter late 6.0 change, which we\n     pushed to 6.1 to keep the 6.0 changes small (Dylan, Pavel)\n\n   - Fix for IORING_OP_CONNECT not handling -EINPROGRESS correctly (me)\n\n   - Ensure that the zc sendmsg variant gets audited correctly (me)\n\n   - Regression fix from this merge window where kiocb_end_write()\n     doesn't always gets called, which can cause issues with fs freezing\n     (me)\n\n   - Registered files SCM handling fix (Pavel)\n\n   - Regression fix for big sqe dumping in fdinfo (Pavel)\n\n   - Registered buffers accounting fix (Pavel)\n\n   - Remove leftover notification structures, we killed them off late in\n     6.0 (Pavel)\n\n   - Minor optimizations (Pavel)\n\n   - Cosmetic variable shadowing fix (Stefan)\"\n\n* tag 'io_uring-6.1-2022-10-13' of git://git.kernel.dk/linux:\n  io_uring/rw: ensure kiocb_end_write() is always called\n  io_uring: fix fdinfo sqe offsets calculation\n  io_uring: local variable rw shadows outer variable in io_write\n  io_uring/opdef: remove 'audit_skip' from SENDMSG_ZC\n  io_uring: optimise locking for local tw with submit_wait\n  io_uring: remove redundant memory barrier in io_req_local_work_add\n  io_uring/net: handle -EINPROGRESS correct for IORING_OP_CONNECT\n  io_uring: remove notif leftovers\n  io_uring: correct pinned_vm accounting\n  io_uring/af_unix: defer registered files gc to io_uring release\n  io_uring: limit registration w/ SINGLE_ISSUER\n  io_uring: remove io_register_submitter\n  io_uring: simplify __io_uring_add_tctx_node",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-13 21:15:30 -0700 Merge tag 'io_uring-6.1-2022-10-13' of git://git.kernel.dk/linux"
    },
    {
        "commit": "2ec33a6c3cca9fe2465e82050c81f5ffdc508b36",
        "message": "A previous commit moved the notifications and end-write handling, but\nit is now missing a few spots where we also want to call both of those.\nWithout that, we can potentially be missing file notifications, and\nmore importantly, have an imbalance in the super_block writers sem\naccounting.\n\nFixes: b000145e9907 (\"io_uring/rw: defer fsnotify calls to task context\")\nReported-by: Dave Chinner <david@fromorbit.com>\nLink: https://lore.kernel.org/all/20221010050319.GC2703033@dread.disaster.area/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-12 16:30:56 -0600 io_uring/rw: ensure kiocb_end_write() is always called"
    },
    {
        "commit": "00927931cb630bbf8edb6d7f4dadb25139fc5e16",
        "message": "Only with the big sqe feature they take 128 bytes per entry, but we\nunconditionally advance by 128B. Fix it by using sq_shift.\n\nFixes: 3b8fdd1dc35e3 (\"io_uring/fdinfo: fix sqe dumping for IORING_SETUP_SQE128\")\nReported-and-tested-by: syzbot+e5198737e8a2d23d958c@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8b41287cb75d5efb8fcb5cccde845ddbbadd8372.1665449983.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-12 16:30:56 -0600 io_uring: fix fdinfo sqe offsets calculation"
    },
    {
        "commit": "c86416c6ff5ba7f7e5f3ff1dd8a9d1b3d0be827c",
        "message": "This fixes the shadowing of the outer variable rw in the function\nio_write(). No issue is caused by this, but let's silence the shadowing\nwarning anyway.\n\nReported-by: kernel test robot <lkp@intel.com>\nSigned-off-by: Stefan Roesch <shr@devkernel.io>\nLink: https://lore.kernel.org/r/20221010234330.244244-1-shr@devkernel.io\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-12 16:30:56 -0600 io_uring: local variable rw shadows outer variable in io_write"
    },
    {
        "commit": "11528491c65a493050c682786c6b7cfd9e9b4a8f",
        "message": "The msg variants of sending aren't audited separately, so we should not\nbe setting audit_skip for the zerocopy sendmsg variant either.\n\nFixes: 493108d95f14 (\"io_uring/net: zerocopy sendmsg\")\nReported-by: Paul Moore <paul@paul-moore.com>\nReviewed-by: Paul Moore <paul@paul-moore.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-12 16:30:56 -0600 io_uring/opdef: remove 'audit_skip' from SENDMSG_ZC"
    },
    {
        "commit": "44f87745d5f24a3cdf0548bf1d84fbb7316ce229",
        "message": "Running local task_work requires taking uring_lock, for submit + wait we\ncan try to run them right after submit while we still hold the lock and\nsave one lock/unlokc pair. The optimisation was implemented in the first\nlocal tw patches but got dropped for simplicity.\n\nSuggested-by: Dylan Yudaken <dylany@fb.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/281fc79d98b5d91fe4778c5137a17a2ab4693e5c.1665088876.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-12 16:30:56 -0600 io_uring: optimise locking for local tw with submit_wait"
    },
    {
        "commit": "fc86f9d3bb4904117eea70347d323fde34a47c79",
        "message": "io_cqring_wake() needs a barrier for the waitqueue_active() check.\nHowever, in the case of io_req_local_work_add(), we call llist_add()\nfirst, which implies an atomic. Hence we can replace smb_mb() with\nsmp_mb__after_atomic().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/43983bc8bc507172adda7a0f00cab1aff09fd238.1665018309.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-12 16:30:56 -0600 io_uring: remove redundant memory barrier in io_req_local_work_add"
    },
    {
        "commit": "3fb1bd68817288729179444caf1fd5c5c4d2d65d",
        "message": "We treat EINPROGRESS like EAGAIN, but if we're retrying post getting\nEINPROGRESS, then we just need to check the socket for errors and\nterminate the request.\n\nThis was exposed on a bluetooth connection request which ends up\ntaking a while and hitting EINPROGRESS, and yields a CQE result of\n-EBADFD because we're retrying a connect on a socket that is now\nconnected.\n\nCc: stable@vger.kernel.org\nFixes: 87f80d623c6c (\"io_uring: handle connect -EINPROGRESS like -EAGAIN\")\nLink: https://github.com/axboe/liburing/issues/671\nReported-by: Aidan Sun <aidansun05@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-12 16:30:56 -0600 io_uring/net: handle -EINPROGRESS correct for IORING_OP_CONNECT"
    },
    {
        "commit": "b7a817752efc850603c4c23ed78da2b990a6a34a",
        "message": "Notifications were killed but there is a couple of fields and struct\ndeclarations left, remove them.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8df8877d677be5a2b43afd936d600e60105ea960.1664849941.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-12 16:30:56 -0600 io_uring: remove notif leftovers"
    },
    {
        "commit": "42b6419d0aba47c5d8644cdc0b68502254671de5",
        "message": "->mm_account should be released only after we free all registered\nbuffers, otherwise __io_sqe_buffers_unregister() will see a NULL\n->mm_account and skip locked_vm accounting.\n\nCc: <Stable@vger.kernel.org>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6d798f65ed4ab8db3664c4d3397d4af16ca98846.1664849932.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-12 16:30:56 -0600 io_uring: correct pinned_vm accounting"
    },
    {
        "commit": "0091bfc81741b8d3aeb3b7ab8636f911b2de6e80",
        "message": "Instead of putting io_uring's registered files in unix_gc() we want it\nto be done by io_uring itself. The trick here is to consider io_uring\nregistered files for cycle detection but not actually putting them down.\nBecause io_uring can't register other ring instances, this will remove\nall refs to the ring file triggering the ->release path and clean up\nwith io_ring_ctx_free().\n\nCc: stable@vger.kernel.org\nFixes: 6b06314c47e1 (\"io_uring: add file set registration\")\nReported-and-tested-by: David Bouman <dbouman03@gmail.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>\n[axboe: add kerneldoc comment to skb, fold in skb leak fix]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-12 16:30:34 -0600 io_uring/af_unix: defer registered files gc to io_uring release"
    },
    {
        "commit": "d7cce96c449e35bbfd41e830b341b95973891eed",
        "message": "IORING_SETUP_SINGLE_ISSUER restricts what tasks can submit requests.\nExtend it to registration as well, so non-owning task can't do\nregistrations. It's not necessary at the moment but might be useful in\nthe future.\n\nCc: <stable@vger.kernel.org> # 6.0\nFixes: 97bbdc06a444 (\"io_uring: add IORING_SETUP_SINGLE_ISSUER\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f52a6a9c8a8990d4a831f73c0571e7406aac2bba.1664237592.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-07 12:25:30 -0600 io_uring: limit registration w/ SINGLE_ISSUER"
    },
    {
        "commit": "4add705e4eebbdd919741de0548d7029c8c92b68",
        "message": "this is no longer needed, as submitter_task is set at creation time.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nFixes: 97bbdc06a444 (\"io_uring: add IORING_SETUP_SINGLE_ISSUER\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-07 12:25:30 -0600 io_uring: remove io_register_submitter"
    },
    {
        "commit": "97c96e9fa36616d7890a6f3438172fc501927f01",
        "message": "Remove submitter parameter from __io_uring_add_tctx_node.\n\nIt was only called from one place, and we can do that logic in that one\nplace.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nFixes: 97bbdc06a444 (\"io_uring: add IORING_SETUP_SINGLE_ISSUER\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-07 12:25:30 -0600 io_uring: simplify __io_uring_add_tctx_node"
    },
    {
        "commit": "7c989b1da3946e40bf71be00a0b401015235605a",
        "message": "Pull passthrough updates from Jens Axboe:\n \"With these changes, passthrough NVMe support over io_uring now\n  performs at the same level as block device O_DIRECT, and in many cases\n  6-8% better.\n\n  This contains:\n\n   - Add support for fixed buffers for passthrough (Anuj, Kanchan)\n\n   - Enable batched allocations and freeing on passthrough, similarly to\n     what we support on the normal storage path (me)\n\n   - Fix from Geert fixing an issue with !CONFIG_IO_URING\"\n\n* tag 'for-6.1/passthrough-2022-10-04' of git://git.kernel.dk/linux:\n  io_uring: Add missing inline to io_uring_cmd_import_fixed() dummy\n  nvme: wire up fixed buffer support for nvme passthrough\n  nvme: pass ubuffer as an integer\n  block: extend functionality to map bvec iterator\n  block: factor out blk_rq_map_bio_alloc helper\n  block: rename bio_map_put to blk_mq_map_bio_put\n  nvme: refactor nvme_alloc_request\n  nvme: refactor nvme_add_user_metadata\n  nvme: Use blk_rq_map_user_io helper\n  scsi: Use blk_rq_map_user_io helper\n  block: add blk_rq_map_user_io\n  io_uring: introduce fixed buffer support for io_uring_cmd\n  io_uring: add io_uring_cmd_import_fixed\n  nvme: enable batched completions of passthrough IO\n  nvme: split out metadata vs non metadata end_io uring_cmd completions\n  block: allow end_io based requests in the completion batch handling\n  block: change request end_io handler to pass back a return value\n  block: enable batched allocation for blk_mq_alloc_request()\n  block: kill deprecated BUG_ON() in the flush handling",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-07 09:35:50 -0700 Merge tag 'for-6.1/passthrough-2022-10-04' of git://git.kernel.dk/linux"
    },
    {
        "commit": "0a78a376ef3c2f3d397df48909f00cd75f92137a",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - Add supported for more directly managed task_work running.\n\n   This is beneficial for real world applications that end up issuing\n   lots of system calls as part of handling work. Normal task_work will\n   always execute as we transition in and out of the kernel, even for\n   \"unrelated\" system calls. It's more efficient to defer the handling\n   of io_uring's deferred work until the application wants it to be run,\n   generally in batches.\n\n   As part of ongoing work to write an io_uring network backend for\n   Thrift, this has been shown to greatly improve performance. (Dylan)\n\n - Add IOPOLL support for passthrough (Kanchan)\n\n - Improvements and fixes to the send zero-copy support (Pavel)\n\n - Partial IO handling fixes (Pavel)\n\n - CQE ordering fixes around CQ ring overflow (Pavel)\n\n - Support sendto() for non-zc as well (Pavel)\n\n - Support sendmsg for zerocopy (Pavel)\n\n - Networking iov_iter fix (Stefan)\n\n - Misc fixes and cleanups (Pavel, me)\n\n* tag 'for-6.1/io_uring-2022-10-03' of git://git.kernel.dk/linux: (56 commits)\n  io_uring/net: fix notif cqe reordering\n  io_uring/net: don't update msg_name if not provided\n  io_uring: don't gate task_work run on TIF_NOTIFY_SIGNAL\n  io_uring/rw: defer fsnotify calls to task context\n  io_uring/net: fix fast_iov assignment in io_setup_async_msg()\n  io_uring/net: fix non-zc send with address\n  io_uring/net: don't skip notifs for failed requests\n  io_uring/rw: don't lose short results on io_setup_async_rw()\n  io_uring/rw: fix unexpected link breakage\n  io_uring/net: fix cleanup double free free_iov init\n  io_uring: fix CQE reordering\n  io_uring/net: fix UAF in io_sendrecv_fail()\n  selftest/net: adjust io_uring sendzc notif handling\n  io_uring: ensure local task_work marks task as running\n  io_uring/net: zerocopy sendmsg\n  io_uring/net: combine fail handlers\n  io_uring/net: rename io_sendzc()\n  io_uring/net: support non-zerocopy sendto\n  io_uring/net: refactor io_setup_async_addr\n  io_uring/net: don't lose partial send_zc on fail\n  ...",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-07 08:52:43 -0700 Merge tag 'for-6.1/io_uring-2022-10-03' of git://git.kernel.dk/linux"
    },
    {
        "commit": "76e45035348c247a70ed50eb29a9906657e4444f",
        "message": "Pull btrfs updates from David Sterba:\n \"There's a bunch of performance improvements, most notably the FIEMAP\n  speedup, the new block group tree to speed up mount on large\n  filesystems, more io_uring integration, some sysfs exports and the\n  usual fixes and core updates.\n\n  Summary:\n\n  Performance:\n\n   - outstanding FIEMAP speed improvement\n      - algorithmic change how extents are enumerated leads to orders of\n        magnitude speed boost (uncached and cached)\n      - extent sharing check speedup (2.2x uncached, 3x cached)\n      - add more cancellation points, allowing to interrupt seeking in\n        files with large number of extents\n      - more efficient hole and data seeking (4x uncached, 1.3x cached)\n      - sample results:\n\t    256M, 32K extents:   4s ->  29ms  (~150x)\n\t    512M, 64K extents:  30s ->  59ms  (~550x)\n\t    1G,  128K extents: 225s -> 120ms (~1800x)\n\n   - improved inode logging, especially for directories (on dbench\n     workload throughput +25%, max latency -21%)\n\n   - improved buffered IO, remove redundant extent state tracking,\n     lowering memory consumption and avoiding rb tree traversal\n\n   - add sysfs tunable to let qgroup temporarily skip exact accounting\n     when deleting snapshot, leading to a speedup but requiring a rescan\n     after that, will be used by snapper\n\n   - support io_uring and buffered writes, until now it was just for\n     direct IO, with the no-wait semantics implemented in the buffered\n     write path it now works and leads to speed improvement in IOPS\n     (2x), throughput (2.2x), latency (depends, 2x to 150x)\n\n   - small performance improvements when dropping and searching for\n     extent maps as well as when flushing delalloc in COW mode\n     (throughput +5MB/s)\n\n  User visible changes:\n\n   - new incompatible feature block-group-tree adding a dedicated tree\n     for tracking block groups, this allows a much faster load during\n     mount and avoids seeking unlike when it's scattered in the extent\n     tree items\n      - this reduces mount time for many-terabyte sized filesystems\n      - conversion tool will be provided so existing filesystem can also\n        be updated in place\n      - to reduce test matrix and feature combinations requires no-holes\n        and free-space-tree (mkfs defaults since 5.15)\n\n   - improved reporting of super block corruption detected by scrub\n\n   - scrub also tries to repair super block and does not wait until next\n     commit\n\n   - discard stats and tunables are exported in sysfs\n     (/sys/fs/btrfs/FSID/discard)\n\n   - qgroup status is exported in sysfs\n     (/sys/sys/fs/btrfs/FSID/qgroups/)\n\n   - verify that super block was not modified when thawing filesystem\n\n  Fixes:\n\n   - FIEMAP fixes\n      - fix extent sharing status, does not depend on the cached status\n        where merged\n      - flush delalloc so compressed extents are reported correctly\n\n   - fix alignment of VMA for memory mapped files on THP\n\n   - send: fix failures when processing inodes with no links (orphan\n     files and directories)\n\n   - fix race between quota enable and quota rescan ioctl\n\n   - handle more corner cases for read-only compat feature verification\n\n   - fix missed extent on fsync after dropping extent maps\n\n  Core:\n\n   - lockdep annotations to validate various transactions states and\n     state transitions\n\n   - preliminary support for fs-verity in send\n\n   - more effective memory use in scrub for subpage where sector is\n     smaller than page\n\n   - block group caching progress logic has been removed, load is now\n     synchronous\n\n   - simplify end IO callbacks and bio handling, use chained bios\n     instead of own tracking\n\n   - add no-wait semantics to several functions (tree search, nocow,\n     flushing, buffered write\n\n   - cleanups and refactoring\n\n  MM changes:\n\n   - export balance_dirty_pages_ratelimited_flags\"\n\n* tag 'for-6.1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux: (177 commits)\n  btrfs: set generation before calling btrfs_clean_tree_block in btrfs_init_new_buffer\n  btrfs: drop extent map range more efficiently\n  btrfs: avoid pointless extent map tree search when flushing delalloc\n  btrfs: remove unnecessary next extent map search\n  btrfs: remove unnecessary NULL pointer checks when searching extent maps\n  btrfs: assert tree is locked when clearing extent map from logging\n  btrfs: remove unnecessary extent map initializations\n  btrfs: remove the refcount warning/check at free_extent_map()\n  btrfs: add helper to replace extent map range with a new extent map\n  btrfs: move open coded extent map tree deletion out of inode eviction\n  btrfs: use cond_resched_rwlock_write() during inode eviction\n  btrfs: use extent_map_end() at btrfs_drop_extent_map_range()\n  btrfs: move btrfs_drop_extent_cache() to extent_map.c\n  btrfs: fix missed extent on fsync after dropping extent maps\n  btrfs: remove stale prototype of btrfs_write_inode\n  btrfs: enable nowait async buffered writes\n  btrfs: assert nowait mode is not used for some btree search functions\n  btrfs: make btrfs_buffered_write nowait compatible\n  btrfs: plumb NOWAIT through the write path\n  btrfs: make lock_and_cleanup_extent_if_need nowait compatible\n  ...",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-06 17:36:48 -0700 Merge tag 'for-6.1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux"
    },
    {
        "commit": "0e0abad2a71bcd7ba0f30e7975f5b4199ade4e60",
        "message": "If CONFIG_IO_URING is not set:\n\n    include/linux/io_uring.h:65:12: error: \u2018io_uring_cmd_import_fixed\u2019 defined but not used [-Werror=unused-function]\n       65 | static int io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,\n\t  |            ^~~~~~~~~~~~~~~~~~~~~~~~~\n\nFix this by adding the missing \"inline\" keyword.\n\nFixes: a9216fac3ed8819c (\"io_uring: add io_uring_cmd_import_fixed\")\nSigned-off-by: Geert Uytterhoeven <geert+renesas@glider.be>\nLink: https://lore.kernel.org/r/7404b4a696f64e33e5ef3c5bd3754d4f26d13e50.1664887093.git.geert+renesas@glider.be\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-10-04 08:13:20 -0600 io_uring: Add missing inline to io_uring_cmd_import_fixed() dummy"
    },
    {
        "commit": "a0debc4c7a7e1a4adf0866b105e9e4f29002c2ca",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two fixes that should go into 6.0:\n\n   - Tweak the single issuer logic to register the task at creation,\n     rather than at first submit. SINGLE_ISSUER was added for 6.0, and\n     after some discussion on this, we decided to make it a bit stricter\n     while it's still possible to do so (Dylan).\n\n   - Stefan from Samba had some doubts on the level triggered poll that\n     was added for this release. Rather than attempt to mess around with\n     it now, just do the quick one-liner to disable it for release and\n     we have time to discuss and change it for 6.1 instead (me)\"\n\n* tag 'io_uring-6.0-2022-09-29' of git://git.kernel.dk/linux:\n  io_uring/poll: disable level triggered poll\n  io_uring: register single issuer task at creation",
        "kernel_version": "v6.0",
        "release_date": "2022-09-30 09:28:39 -0700 Merge tag 'io_uring-6.0-2022-09-29' of git://git.kernel.dk/linux"
    },
    {
        "commit": "23fd22e55b767be9c31fda57205afb2023cd6aad",
        "message": "if io_uring sends passthrough command with IORING_URING_CMD_FIXED flag,\nuse the pre-registered buffer for IO (non-vectored variant). Pass the\nbuffer/length to io_uring and get the bvec iterator for the range. Next,\npass this bvec to block-layer and obtain a bio/request for subsequent\nprocessing.\n\nSigned-off-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220930062749.152261-13-anuj20.g@samsung.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-30 07:51:13 -0600 nvme: wire up fixed buffer support for nvme passthrough"
    },
    {
        "commit": "9cda70f622cdcf049521a9c2886e5fd8a90a0591",
        "message": "Add IORING_URING_CMD_FIXED flag that is to be used for sending io_uring\ncommand with previously registered buffers. User-space passes the buffer\nindex in sqe->buf_index, same as done in read/write variants that uses\nfixed buffers.\n\nSigned-off-by: Anuj Gupta <anuj20.g@samsung.com>\nSigned-off-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220930062749.152261-3-anuj20.g@samsung.com\n[axboe: shuffle valid flags check before acting on it]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-30 07:50:59 -0600 io_uring: introduce fixed buffer support for io_uring_cmd"
    },
    {
        "commit": "a9216fac3ed8819cbbda5d39dd5fcaa43dfd35d8",
        "message": "This is a new helper that callers can use to obtain a bvec iterator for\nthe previously mapped buffer. This is preparatory work to enable\nfixed-buffer support for io_uring_cmd.\n\nSigned-off-by: Anuj Gupta <anuj20.g@samsung.com>\nSigned-off-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220930062749.152261-2-anuj20.g@samsung.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-30 07:49:11 -0600 io_uring: add io_uring_cmd_import_fixed"
    },
    {
        "commit": "851eb780decb7180bcf09fad0035cba9aae669df",
        "message": "Now that the normal passthrough end_io path doesn't need the request\nanymore, we can kill the explicit blk_mq_free_request() and just pass\nback RQ_END_IO_FREE instead. This enables the batched completion from\nfreeing batches of requests at the time.\n\nThis brings passthrough IO performance at least on par with bdev based\nO_DIRECT with io_uring. With this and batche allocations, peak performance\ngoes from 110M IOPS to 122M IOPS. For IRQ based, passthrough is now also\nabout 10% faster than previously, going from ~61M to ~67M IOPS.\n\nReviewed-by: Anuj Gupta <anuj20.g@samsung.com>\nReviewed-by: Sagi Grimberg <sagi@grimberg.me>\nReviewed-by: Keith Busch <kbusch@kernel.org>\nCo-developed-by: Stefan Roesch <shr@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-30 07:49:11 -0600 nvme: enable batched completions of passthrough IO"
    },
    {
        "commit": "5853a7b5512c3017f64ca26494bd7361a12d6992",
        "message": "* for-6.1/io_uring: (56 commits)\n  io_uring/net: fix notif cqe reordering\n  io_uring/net: don't update msg_name if not provided\n  io_uring: don't gate task_work run on TIF_NOTIFY_SIGNAL\n  io_uring/rw: defer fsnotify calls to task context\n  io_uring/net: fix fast_iov assignment in io_setup_async_msg()\n  io_uring/net: fix non-zc send with address\n  io_uring/net: don't skip notifs for failed requests\n  io_uring/rw: don't lose short results on io_setup_async_rw()\n  io_uring/rw: fix unexpected link breakage\n  io_uring/net: fix cleanup double free free_iov init\n  io_uring: fix CQE reordering\n  io_uring/net: fix UAF in io_sendrecv_fail()\n  selftest/net: adjust io_uring sendzc notif handling\n  io_uring: ensure local task_work marks task as running\n  io_uring/net: zerocopy sendmsg\n  io_uring/net: combine fail handlers\n  io_uring/net: rename io_sendzc()\n  io_uring/net: support non-zerocopy sendto\n  io_uring/net: refactor io_setup_async_addr\n  io_uring/net: don't lose partial send_zc on fail\n  ...",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-30 07:47:42 -0600 Merge branch 'for-6.1/io_uring' into for-6.1/passthrough"
    },
    {
        "commit": "108893ddcc4d3aa0a4a02aeb02d478e997001227",
        "message": "send zc is not restricted to !IO_URING_F_UNLOCKED anymore and so\nwe can't use task-tw ordering trick to order notification cqes\nwith requests completions. In this case leave it alone and let\nio_send_zc_cleanup() flush it.\n\nCc: stable@vger.kernel.org\nFixes: 53bdc88aac9a2 (\"io_uring/notif: order notif vs send CQEs\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0031f3a00d492e814a4a0935a2029a46d9c9ba06.1664486545.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-29 17:46:04 -0600 io_uring/net: fix notif cqe reordering"
    },
    {
        "commit": "6f10ae8a155446248055c7ddd480ef40139af788",
        "message": "io_sendmsg_copy_hdr() may clear msg->msg_name if the userspace didn't\nprovide it, we should retain NULL in this case.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/97d49f61b5ec76d0900df658cfde3aa59ff22121.1664486545.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-29 17:46:04 -0600 io_uring/net: don't update msg_name if not provided"
    },
    {
        "commit": "46a525e199e4037516f7e498c18f065b09df32ac",
        "message": "This isn't a reliable mechanism to tell if we have task_work pending, we\nreally should be looking at whether we have any items queued. This is\nproblematic if forward progress is gated on running said task_work. One\nsuch example is reading from a pipe, where the write side has been closed\nright before the read is started. The fput() of the file queues TWA_RESUME\ntask_work, and we need that task_work to be run before ->release() is\ncalled for the pipe. If ->release() isn't called, then the read will sit\nforever waiting on data that will never arise.\n\nFix this by io_run_task_work() so it checks if we have task_work pending\nrather than rely on TIF_NOTIFY_SIGNAL for that. The latter obviously\ndoesn't work for task_work that is queued without TWA_SIGNAL.\n\nReported-by: Christiano Haesbaert <haesbaert@haesbaert.org>\nCc: stable@vger.kernel.org\nLink: https://github.com/axboe/liburing/issues/665\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-29 16:07:45 -0600 io_uring: don't gate task_work run on TIF_NOTIFY_SIGNAL"
    },
    {
        "commit": "b000145e9907809406d8164c3b2b8861d95aecd1",
        "message": "We can't call these off the kiocb completion as that might be off\nsoft/hard irq context. Defer the calls to when we process the\ntask_work for this request. That avoids valid complaints like:\n\nstack backtrace:\nCPU: 1 PID: 0 Comm: swapper/1 Not tainted 6.0.0-rc6-syzkaller-00321-g105a36f3694e #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 08/26/2022\nCall Trace:\n <IRQ>\n __dump_stack lib/dump_stack.c:88 [inline]\n dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106\n print_usage_bug kernel/locking/lockdep.c:3961 [inline]\n valid_state kernel/locking/lockdep.c:3973 [inline]\n mark_lock_irq kernel/locking/lockdep.c:4176 [inline]\n mark_lock.part.0.cold+0x18/0xd8 kernel/locking/lockdep.c:4632\n mark_lock kernel/locking/lockdep.c:4596 [inline]\n mark_usage kernel/locking/lockdep.c:4527 [inline]\n __lock_acquire+0x11d9/0x56d0 kernel/locking/lockdep.c:5007\n lock_acquire kernel/locking/lockdep.c:5666 [inline]\n lock_acquire+0x1ab/0x570 kernel/locking/lockdep.c:5631\n __fs_reclaim_acquire mm/page_alloc.c:4674 [inline]\n fs_reclaim_acquire+0x115/0x160 mm/page_alloc.c:4688\n might_alloc include/linux/sched/mm.h:271 [inline]\n slab_pre_alloc_hook mm/slab.h:700 [inline]\n slab_alloc mm/slab.c:3278 [inline]\n __kmem_cache_alloc_lru mm/slab.c:3471 [inline]\n kmem_cache_alloc+0x39/0x520 mm/slab.c:3491\n fanotify_alloc_fid_event fs/notify/fanotify/fanotify.c:580 [inline]\n fanotify_alloc_event fs/notify/fanotify/fanotify.c:813 [inline]\n fanotify_handle_event+0x1130/0x3f40 fs/notify/fanotify/fanotify.c:948\n send_to_group fs/notify/fsnotify.c:360 [inline]\n fsnotify+0xafb/0x1680 fs/notify/fsnotify.c:570\n __fsnotify_parent+0x62f/0xa60 fs/notify/fsnotify.c:230\n fsnotify_parent include/linux/fsnotify.h:77 [inline]\n fsnotify_file include/linux/fsnotify.h:99 [inline]\n fsnotify_access include/linux/fsnotify.h:309 [inline]\n __io_complete_rw_common+0x485/0x720 io_uring/rw.c:195\n io_complete_rw+0x1a/0x1f0 io_uring/rw.c:228\n iomap_dio_complete_work fs/iomap/direct-io.c:144 [inline]\n iomap_dio_bio_end_io+0x438/0x5e0 fs/iomap/direct-io.c:178\n bio_endio+0x5f9/0x780 block/bio.c:1564\n req_bio_endio block/blk-mq.c:695 [inline]\n blk_update_request+0x3fc/0x1300 block/blk-mq.c:825\n scsi_end_request+0x7a/0x9a0 drivers/scsi/scsi_lib.c:541\n scsi_io_completion+0x173/0x1f70 drivers/scsi/scsi_lib.c:971\n scsi_complete+0x122/0x3b0 drivers/scsi/scsi_lib.c:1438\n blk_complete_reqs+0xad/0xe0 block/blk-mq.c:1022\n __do_softirq+0x1d3/0x9c6 kernel/softirq.c:571\n invoke_softirq kernel/softirq.c:445 [inline]\n __irq_exit_rcu+0x123/0x180 kernel/softirq.c:650\n irq_exit_rcu+0x5/0x20 kernel/softirq.c:662\n common_interrupt+0xa9/0xc0 arch/x86/kernel/irq.c:240\n\nFixes: f63cf5192fe3 (\"io_uring: ensure that fsnotify is always called\")\nLink: https://lore.kernel.org/all/20220929135627.ykivmdks2w5vzrwg@quack3/\nReported-by: syzbot+dfcc5f4da15868df7d4d@syzkaller.appspotmail.com\nReported-by: Jan Kara <jack@suse.cz>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-29 11:00:41 -0600 io_uring/rw: defer fsnotify calls to task context"
    },
    {
        "commit": "926078b21db91b72b444277fdc2166914cf113fc",
        "message": "Enable nowait async buffered writes in btrfs_do_write_iter() and\nbtrfs_file_open().\n\nIn this version encoded buffered writes have the optimization not\nenabled. Encoded writes are enabled by using an ioctl. io_uring\ncurrently does not support ioctls. This might be enabled in the future.\n\nPerformance results:\n\n  For fio the following results have been obtained with a queue depth of\n  1 and 4k block size (runtime 600 secs):\n\n                 sequential writes:\n                 without patch           with patch      libaio     psync\n  iops:              55k                    134k          117K       148K\n  bw:               221MB/s                 538MB/s       469MB/s    592MB/s\n  clat:           15286ns                    82ns         994ns     6340ns\n\nFor an io depth of 1, the new patch improves throughput by over two\ntimes (compared to the existing behavior, where buffered writes are\nprocessed by an io-worker process) and also the latency is considerably\nreduced. To achieve the same or better performance with the existing\ncode an io depth of 4 is required.  Increasing the iodepth further does\nnot lead to improvements.\n\nThe tests have been run like this:\n\n./fio --name=seq-writers --ioengine=psync --iodepth=1 --rw=write \\\n  --bs=4k --direct=0 --size=100000m --time_based --runtime=600   \\\n  --numjobs=1 --filename=...\n./fio --name=seq-writers --ioengine=io_uring --iodepth=1 --rw=write \\\n  --bs=4k --direct=0 --size=100000m --time_based --runtime=600   \\\n  --numjobs=1 --filename=...\n./fio --name=seq-writers --ioengine=libaio --iodepth=1 --rw=write \\\n  --bs=4k --direct=0 --size=100000m --time_based --runtime=600   \\\n  --numjobs=1 --filename=...\n\nTesting:\n  This patch has been tested with xfstests, fsx, fio. xfstests shows no new\n  diffs compared to running without the patch series.\n\nReviewed-by: Filipe Manana <fdmanana@suse.com>\nSigned-off-by: Stefan Roesch <shr@fb.com>\nReviewed-by: David Sterba <dsterba@suse.com>\nSigned-off-by: David Sterba <dsterba@suse.com>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-29 17:08:29 +0200 btrfs: enable nowait async buffered writes"
    },
    {
        "commit": "3e4cb6ebbb2bad201c1186bc0b7e8cf41dd7f7e6",
        "message": "I hit a very bad problem during my tests of SENDMSG_ZC.\nBUG(); in first_iovec_segment() triggered very easily.\nThe problem was io_setup_async_msg() in the partial retry case,\nwhich seems to happen more often with _ZC.\n\niov_iter_iovec_advance() may change i->iov in order to have i->iov_offset\nbeing only relative to the first element.\n\nWhich means kmsg->msg.msg_iter.iov is no longer the\nsame as kmsg->fast_iov.\n\nBut this would rewind the copy to be the start of\nasync_msg->fast_iov, which means the internal\nstate of sync_msg->msg.msg_iter is inconsitent.\n\nI tested with 5 vectors with length like this 4, 0, 64, 20, 8388608\nand got a short writes with:\n- ret=2675244 min_ret=8388692 => remaining 5713448 sr->done_io=2675244\n- ret=-EAGAIN => io_uring_poll_arm\n- ret=4911225 min_ret=5713448 => remaining 802223  sr->done_io=7586469\n- ret=-EAGAIN => io_uring_poll_arm\n- ret=802223  min_ret=802223  => res=8388692\n\nWhile this was easily triggered with SENDMSG_ZC (queued for 6.1),\nit was a potential problem starting with 7ba89d2af17aa879dda30f5d5d3f152e587fc551\nin 5.18 for IORING_OP_RECVMSG.\nAnd also with 4c3c09439c08b03d9503df0ca4c7619c5842892e in 5.19\nfor IORING_OP_SENDMSG.\n\nHowever 257e84a5377fbbc336ff563833a8712619acce56 introduced the critical\ncode into io_setup_async_msg() in 5.11.\n\nFixes: 7ba89d2af17aa (\"io_uring: ensure recv and recvmsg handle MSG_WAITALL correctly\")\nFixes: 257e84a5377fb (\"io_uring: refactor sendmsg/recvmsg iov managing\")\nCc: stable@vger.kernel.org\nSigned-off-by: Stefan Metzmacher <metze@samba.org>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b2e7be246e2fb173520862b0c7098e55767567a2.1664436949.git.metze@samba.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-29 07:08:21 -0600 io_uring/net: fix fast_iov assignment in io_setup_async_msg()"
    },
    {
        "commit": "578b054684e6ad46f6089b726c05054fc5e3cd74",
        "message": "Pavel Begunkov says:\n\n====================\nshrink struct ubuf_info\n\nstruct ubuf_info is large but not all fields are needed for all\ncases. We have limited space in io_uring for it and large ubuf_info\nprevents some struct embedding, even though we use only a subset\nof the fields. It's also not very clean trying to use this typeless\nextra space.\n\nShrink struct ubuf_info to only necessary fields used in generic paths,\nnamely ->callback, ->refcnt and ->flags, which take only 16 bytes. And\nmake MSG_ZEROCOPY and some other users to embed it into a larger struct\nubuf_info_msgzc mimicking the former ubuf_info.\n\nNote, xen/vhost may also have some cleaning on top by creating\nnew structs containing ubuf_info but with proper types.\n====================\n\nLink: https://lore.kernel.org/r/cover.1663892211.git.asml.silence@gmail.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-28 18:51:28 -0700 Merge branch 'shrink-struct-ubuf_info'"
    },
    {
        "commit": "04360d3e05e885621a5860f987c6a8a2eac4bb27",
        "message": "We're currently ignoring the dest address with non-zerocopy send because\neven though we copy it from the userspace shortly after ->msg_name gets\nzeroed. Move msghdr init earlier.\n\nFixes: 516e82f0e043a (\"io_uring/net: support non-zerocopy sendto\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/176ced5e8568aa5d300ca899b7f05b303ebc49fd.1664409532.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-28 19:29:12 -0600 io_uring/net: fix non-zc send with address"
    },
    {
        "commit": "d59bd748db0a97a5d6a33b284b6c58b7f6f4f768",
        "message": "Stefan reports that there are issues with the level triggered\nnotification. Since we're late in the cycle, and it was introduced for\nthe 6.0 release, just disable it at prep time and we can bring this\nback when Samba is happy with it.\n\nReported-by: Stefan Metzmacher <metze@samba.org>\nReviewed-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0",
        "release_date": "2022-09-28 19:27:11 -0600 io_uring/poll: disable level triggered poll"
    },
    {
        "commit": "6ae91ac9a6aa7d6005c3c6d0f4d263fbab9f377f",
        "message": "We currently only add a notification CQE when the send succeded, i.e.\ncqe.res >= 0. However, it'd be more robust to do buffer notifications\nfor failed requests as well in case drivers decide do something fanky.\n\nAlways return a buffer notification after initial prep, don't hide it.\nThis behaviour is better aligned with documentation and the patch also\nhelps the userspace to respect it.\n\nCc: stable@vger.kernel.org # 6.0\nSuggested-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9c8bead87b2b980fcec441b8faef52188b4a6588.1664292100.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-28 07:53:15 -0600 io_uring/net: don't skip notifs for failed requests"
    },
    {
        "commit": "7bcd9683e51575c72c9289c05213150245d1c186",
        "message": "d8b6171bd58a5 (\"selftests/io_uring: test zerocopy send\") added io_uring\nzerocopy tests but forgot to enable it in make runs. Add missing\nio_uring_zerocopy_tx.sh into TEST_PROGS.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/28e743602cdd54ffc49f68bbcbcbafc59ba22dc2.1664142210.git.asml.silence@gmail.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-27 07:59:57 -0700 selftests/net: enable io_uring sendzc testing"
    },
    {
        "commit": "f76c83378851f8e70f032848c4e61203f39480e4",
        "message": "When multiple memcgs are available, it is possible to use generations as a\nframe of reference to make better choices and improve overall performance\nunder global memory pressure.  This patch adds a basic optimization to\nselect memcgs that can drop single-use unmapped clean pages first.  Doing\nso reduces the chance of going into the aging path or swapping, which can\nbe costly.\n\nA typical example that benefits from this optimization is a server running\nmixed types of workloads, e.g., heavy anon workload in one memcg and heavy\nbuffered I/O workload in the other.\n\nThough this optimization can be applied to both kswapd and direct reclaim,\nit is only added to kswapd to keep the patchset manageable.  Later\nimprovements may cover the direct reclaim path.\n\nWhile ensuring certain fairness to all eligible memcgs, proportional scans\nof individual memcgs also require proper backoff to avoid overshooting\ntheir aggregate reclaim target by too much.  Otherwise it can cause high\ndirect reclaim latency.  The conditions for backoff are:\n\n1. At low priorities, for direct reclaim, if aging fairness or direct\n   reclaim latency is at risk, i.e., aging one memcg multiple times or\n   swapping after the target is met.\n2. At high priorities, for global reclaim, if per-zone free pages are\n   above respective watermarks.\n\nServer benchmark results:\n  Mixed workloads:\n    fio (buffered I/O): +[19, 21]%\n                IOPS         BW\n      patch1-8: 1880k        7343MiB/s\n      patch1-9: 2252k        8796MiB/s\n\n    memcached (anon): +[119, 123]%\n                Ops/sec      KB/sec\n      patch1-8: 862768.65    33514.68\n      patch1-9: 1911022.12   74234.54\n\n  Mixed workloads:\n    fio (buffered I/O): +[75, 77]%\n                IOPS         BW\n      5.19-rc1: 1279k        4996MiB/s\n      patch1-9: 2252k        8796MiB/s\n\n    memcached (anon): +[13, 15]%\n                Ops/sec      KB/sec\n      5.19-rc1: 1673524.04   65008.87\n      patch1-9: 1911022.12   74234.54\n\n  Configurations:\n    (changes since patch 6)\n\n    cat mixed.sh\n    modprobe brd rd_nr=2 rd_size=56623104\n\n    swapoff -a\n    mkswap /dev/ram0\n    swapon /dev/ram0\n\n    mkfs.ext4 /dev/ram1\n    mount -t ext4 /dev/ram1 /mnt\n\n    memtier_benchmark -S /var/run/memcached/memcached.sock \\\n      -P memcache_binary -n allkeys --key-minimum=1 \\\n      --key-maximum=50000000 --key-pattern=P:P -c 1 -t 36 \\\n      --ratio 1:0 --pipeline 8 -d 2000\n\n    fio -name=mglru --numjobs=36 --directory=/mnt --size=1408m \\\n      --buffered=1 --ioengine=io_uring --iodepth=128 \\\n      --iodepth_batch_submit=32 --iodepth_batch_complete=32 \\\n      --rw=randread --random_distribution=random --norandommap \\\n      --time_based --ramp_time=10m --runtime=90m --group_reporting &\n    pid=$!\n\n    sleep 200\n\n    memtier_benchmark -S /var/run/memcached/memcached.sock \\\n      -P memcache_binary -n allkeys --key-minimum=1 \\\n      --key-maximum=50000000 --key-pattern=R:R -c 1 -t 36 \\\n      --ratio 0:1 --pipeline 8 --randomize --distinct-client-seed\n\n    kill -INT $pid\n    wait\n\nClient benchmark results:\n  no change (CONFIG_MEMCG=n)\n\nLink: https://lkml.kernel.org/r/20220918080010.2920238-10-yuzhao@google.com\nSigned-off-by: Yu Zhao <yuzhao@google.com>\nAcked-by: Brian Geffon <bgeffon@google.com>\nAcked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>\nAcked-by: Oleksandr Natalenko <oleksandr@natalenko.name>\nAcked-by: Steven Barrett <steven@liquorix.net>\nAcked-by: Suleiman Souhlal <suleiman@google.com>\nTested-by: Daniel Byrne <djbyrne@mtu.edu>\nTested-by: Donald Carr <d@chaos-reins.com>\nTested-by: Holger Hoffst\u00e4tte <holger@applied-asynchrony.com>\nTested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>\nTested-by: Shuang Zhai <szhai2@cs.rochester.edu>\nTested-by: Sofia Trinh <sofia.trinh@edi.works>\nTested-by: Vaibhav Jain <vaibhav@linux.ibm.com>\nCc: Andi Kleen <ak@linux.intel.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Barry Song <baohua@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Hillf Danton <hdanton@sina.com>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: Linus Torvalds <torvalds@linux-foundation.org>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Miaohe Lin <linmiaohe@huawei.com>\nCc: Michael Larabel <Michael@MichaelLarabel.com>\nCc: Michal Hocko <mhocko@kernel.org>\nCc: Mike Rapoport <rppt@kernel.org>\nCc: Mike Rapoport <rppt@linux.ibm.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Qi Zheng <zhengqi.arch@bytedance.com>\nCc: Tejun Heo <tj@kernel.org>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-26 19:46:09 -0700 mm: multi-gen LRU: optimize multiple memcgs"
    },
    {
        "commit": "ac35a490237446b71e3b4b782b1596967edd0aa8",
        "message": "To avoid confusion, the terms \"promotion\" and \"demotion\" will be applied\nto the multi-gen LRU, as a new convention; the terms \"activation\" and\n\"deactivation\" will be applied to the active/inactive LRU, as usual.\n\nThe aging produces young generations.  Given an lruvec, it increments\nmax_seq when max_seq-min_seq+1 approaches MIN_NR_GENS.  The aging promotes\nhot pages to the youngest generation when it finds them accessed through\npage tables; the demotion of cold pages happens consequently when it\nincrements max_seq.  Promotion in the aging path does not involve any LRU\nlist operations, only the updates of the gen counter and\nlrugen->nr_pages[]; demotion, unless as the result of the increment of\nmax_seq, requires LRU list operations, e.g., lru_deactivate_fn().  The\naging has the complexity O(nr_hot_pages), since it is only interested in\nhot pages.\n\nThe eviction consumes old generations.  Given an lruvec, it increments\nmin_seq when lrugen->lists[] indexed by min_seq%MAX_NR_GENS becomes empty.\nA feedback loop modeled after the PID controller monitors refaults over\nanon and file types and decides which type to evict when both types are\navailable from the same generation.\n\nThe protection of pages accessed multiple times through file descriptors\ntakes place in the eviction path.  Each generation is divided into\nmultiple tiers.  A page accessed N times through file descriptors is in\ntier order_base_2(N).  Tiers do not have dedicated lrugen->lists[], only\nbits in folio->flags.  The aforementioned feedback loop also monitors\nrefaults over all tiers and decides when to protect pages in which tiers\n(N>1), using the first tier (N=0,1) as a baseline.  The first tier\ncontains single-use unmapped clean pages, which are most likely the best\nchoices.  In contrast to promotion in the aging path, the protection of a\npage in the eviction path is achieved by moving this page to the next\ngeneration, i.e., min_seq+1, if the feedback loop decides so.  This\napproach has the following advantages:\n\n1. It removes the cost of activation in the buffered access path by\n   inferring whether pages accessed multiple times through file\n   descriptors are statistically hot and thus worth protecting in the\n   eviction path.\n2. It takes pages accessed through page tables into account and avoids\n   overprotecting pages accessed multiple times through file\n   descriptors. (Pages accessed through page tables are in the first\n   tier, since N=0.)\n3. More tiers provide better protection for pages accessed more than\n   twice through file descriptors, when under heavy buffered I/O\n   workloads.\n\nServer benchmark results:\n  Single workload:\n    fio (buffered I/O): +[30, 32]%\n                IOPS         BW\n      5.19-rc1: 2673k        10.2GiB/s\n      patch1-6: 3491k        13.3GiB/s\n\n  Single workload:\n    memcached (anon): -[4, 6]%\n                Ops/sec      KB/sec\n      5.19-rc1: 1161501.04   45177.25\n      patch1-6: 1106168.46   43025.04\n\n  Configurations:\n    CPU: two Xeon 6154\n    Mem: total 256G\n\n    Node 1 was only used as a ram disk to reduce the variance in the\n    results.\n\n    patch drivers/block/brd.c <<EOF\n    99,100c99,100\n    < \tgfp_flags = GFP_NOIO | __GFP_ZERO | __GFP_HIGHMEM;\n    < \tpage = alloc_page(gfp_flags);\n    ---\n    > \tgfp_flags = GFP_NOIO | __GFP_ZERO | __GFP_HIGHMEM | __GFP_THISNODE;\n    > \tpage = alloc_pages_node(1, gfp_flags, 0);\n    EOF\n\n    cat >>/etc/systemd/system.conf <<EOF\n    CPUAffinity=numa\n    NUMAPolicy=bind\n    NUMAMask=0\n    EOF\n\n    cat >>/etc/memcached.conf <<EOF\n    -m 184320\n    -s /var/run/memcached/memcached.sock\n    -a 0766\n    -t 36\n    -B binary\n    EOF\n\n    cat fio.sh\n    modprobe brd rd_nr=1 rd_size=113246208\n    swapoff -a\n    mkfs.ext4 /dev/ram0\n    mount -t ext4 /dev/ram0 /mnt\n\n    mkdir /sys/fs/cgroup/user.slice/test\n    echo 38654705664 >/sys/fs/cgroup/user.slice/test/memory.max\n    echo $$ >/sys/fs/cgroup/user.slice/test/cgroup.procs\n    fio -name=mglru --numjobs=72 --directory=/mnt --size=1408m \\\n      --buffered=1 --ioengine=io_uring --iodepth=128 \\\n      --iodepth_batch_submit=32 --iodepth_batch_complete=32 \\\n      --rw=randread --random_distribution=random --norandommap \\\n      --time_based --ramp_time=10m --runtime=5m --group_reporting\n\n    cat memcached.sh\n    modprobe brd rd_nr=1 rd_size=113246208\n    swapoff -a\n    mkswap /dev/ram0\n    swapon /dev/ram0\n\n    memtier_benchmark -S /var/run/memcached/memcached.sock \\\n      -P memcache_binary -n allkeys --key-minimum=1 \\\n      --key-maximum=65000000 --key-pattern=P:P -c 1 -t 36 \\\n      --ratio 1:0 --pipeline 8 -d 2000\n\n    memtier_benchmark -S /var/run/memcached/memcached.sock \\\n      -P memcache_binary -n allkeys --key-minimum=1 \\\n      --key-maximum=65000000 --key-pattern=R:R -c 1 -t 36 \\\n      --ratio 0:1 --pipeline 8 --randomize --distinct-client-seed\n\nClient benchmark results:\n  kswapd profiles:\n    5.19-rc1\n      40.33%  page_vma_mapped_walk (overhead)\n      21.80%  lzo1x_1_do_compress (real work)\n       7.53%  do_raw_spin_lock\n       3.95%  _raw_spin_unlock_irq\n       2.52%  vma_interval_tree_iter_next\n       2.37%  folio_referenced_one\n       2.28%  vma_interval_tree_subtree_search\n       1.97%  anon_vma_interval_tree_iter_first\n       1.60%  ptep_clear_flush\n       1.06%  __zram_bvec_write\n\n    patch1-6\n      39.03%  lzo1x_1_do_compress (real work)\n      18.47%  page_vma_mapped_walk (overhead)\n       6.74%  _raw_spin_unlock_irq\n       3.97%  do_raw_spin_lock\n       2.49%  ptep_clear_flush\n       2.48%  anon_vma_interval_tree_iter_first\n       1.92%  folio_referenced_one\n       1.88%  __zram_bvec_write\n       1.48%  memmove\n       1.31%  vma_interval_tree_iter_next\n\n  Configurations:\n    CPU: single Snapdragon 7c\n    Mem: total 4G\n\n    ChromeOS MemoryPressure [1]\n\n[1] https://chromium.googlesource.com/chromiumos/platform/tast-tests/\n\nLink: https://lkml.kernel.org/r/20220918080010.2920238-7-yuzhao@google.com\nSigned-off-by: Yu Zhao <yuzhao@google.com>\nAcked-by: Brian Geffon <bgeffon@google.com>\nAcked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>\nAcked-by: Oleksandr Natalenko <oleksandr@natalenko.name>\nAcked-by: Steven Barrett <steven@liquorix.net>\nAcked-by: Suleiman Souhlal <suleiman@google.com>\nTested-by: Daniel Byrne <djbyrne@mtu.edu>\nTested-by: Donald Carr <d@chaos-reins.com>\nTested-by: Holger Hoffst\u00e4tte <holger@applied-asynchrony.com>\nTested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>\nTested-by: Shuang Zhai <szhai2@cs.rochester.edu>\nTested-by: Sofia Trinh <sofia.trinh@edi.works>\nTested-by: Vaibhav Jain <vaibhav@linux.ibm.com>\nCc: Andi Kleen <ak@linux.intel.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Barry Song <baohua@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Hillf Danton <hdanton@sina.com>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: Linus Torvalds <torvalds@linux-foundation.org>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Miaohe Lin <linmiaohe@huawei.com>\nCc: Michael Larabel <Michael@MichaelLarabel.com>\nCc: Michal Hocko <mhocko@kernel.org>\nCc: Mike Rapoport <rppt@kernel.org>\nCc: Mike Rapoport <rppt@linux.ibm.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Qi Zheng <zhengqi.arch@bytedance.com>\nCc: Tejun Heo <tj@kernel.org>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-26 19:46:09 -0700 mm: multi-gen LRU: minimal implementation"
    },
    {
        "commit": "4d24de9425f75fe489ab651113b97f3f7b4dea62",
        "message": "The syzbot reported the below problem:\n\nBUG: Bad page map in process syz-executor198  pte:8000000071c00227 pmd:74b30067\naddr:0000000020563000 vm_flags:08100077 anon_vma:ffff8880547d2200 mapping:0000000000000000 index:20563\nfile:(null) fault:0x0 mmap:0x0 read_folio:0x0\nCPU: 1 PID: 3614 Comm: syz-executor198 Not tainted 6.0.0-rc3-next-20220901-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 08/26/2022\nCall Trace:\n <TASK>\n __dump_stack lib/dump_stack.c:88 [inline]\n dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106\n print_bad_pte.cold+0x2a7/0x2d0 mm/memory.c:565\n vm_normal_page+0x10c/0x2a0 mm/memory.c:636\n hpage_collapse_scan_pmd+0x729/0x1da0 mm/khugepaged.c:1199\n madvise_collapse+0x481/0x910 mm/khugepaged.c:2433\n madvise_vma_behavior+0xd0a/0x1cc0 mm/madvise.c:1062\n madvise_walk_vmas+0x1c7/0x2b0 mm/madvise.c:1236\n do_madvise.part.0+0x24a/0x340 mm/madvise.c:1415\n do_madvise mm/madvise.c:1428 [inline]\n __do_sys_madvise mm/madvise.c:1428 [inline]\n __se_sys_madvise mm/madvise.c:1426 [inline]\n __x64_sys_madvise+0x113/0x150 mm/madvise.c:1426\n do_syscall_x64 arch/x86/entry/common.c:50 [inline]\n do_syscall_64+0x35/0xb0 arch/x86/entry/common.c:80\n entry_SYSCALL_64_after_hwframe+0x63/0xcd\nRIP: 0033:0x7f770ba87929\nCode: 28 00 00 00 75 05 48 83 c4 28 c3 e8 11 15 00 00 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 b8 ff ff ff f7 d8 64 89 01 48\nRSP: 002b:00007f770ba18308 EFLAGS: 00000246 ORIG_RAX: 000000000000001c\nRAX: ffffffffffffffda RBX: 00007f770bb0f3f8 RCX: 00007f770ba87929\nRDX: 0000000000000019 RSI: 0000000000600003 RDI: 0000000020000000\nRBP: 00007f770bb0f3f0 R08: 00007f770ba18700 R09: 0000000000000000\nR10: 00007f770ba18700 R11: 0000000000000246 R12: 00007f770bb0f3fc\nR13: 00007ffc2d8b62ef R14: 00007f770ba18400 R15: 0000000000022000\n\nBasically the test program does the below conceptually:\n1. mmap 0x2000000 - 0x21000000 as anonymous region\n2. mmap io_uring SQ stuff at 0x20563000 with MAP_FIXED, io_uring_mmap()\n   actually remaps the pages with special PTEs\n3. call MADV_COLLAPSE for 0x20000000 - 0x21000000\n\nIt actually triggered the below race:\n\n             CPU A                                          CPU B\nmmap 0x20000000 - 0x21000000 as anon\n                                           madvise_collapse is called on this area\n                                             Retrieve start and end address from the vma (NEVER updated later!)\n                                             Collapsed the first 2M area and dropped mmap_lock\nAcquire mmap_lock\nmmap io_uring file at 0x20563000\nRelease mmap_lock\n                                             Reacquire mmap_lock\n                                             revalidate vma pass since 0x20200000 + 0x200000 > 0x20563000\n                                             scan the next 2M (0x20200000 - 0x20400000), but due to whatever reason it didn't release mmap_lock\n                                             scan the 3rd 2M area (start from 0x20400000)\n                                               get into the vma created by io_uring\n\nThe hend should be updated after MADV_COLLAPSE reacquire mmap_lock since\nthe vma may be shrunk.  We don't have to worry about shink from the other\ndirection since it could be caught by hugepage_vma_revalidate().  Either\nno valid vma is found or the vma doesn't fit anymore.\n\nLink: https://lkml.kernel.org/r/20220914162220.787703-1-shy828301@gmail.com\nFixes: 7d8faaf155454f8 (\"mm/madvise: introduce MADV_COLLAPSE sync hugepage collapse\")\nReported-by: syzbot+915f3e317adb0e85835f@syzkaller.appspotmail.com\nSigned-off-by: Yang Shi <shy828301@gmail.com>\nReviewed-by: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-26 19:46:03 -0700 mm: MADV_COLLAPSE: refetch vm_end after reacquiring mmap_lock"
    },
    {
        "commit": "c278d9f8ac0db5590909e6d9e85b5ca2b786704f",
        "message": "If a retry io_setup_async_rw() fails we lose result from the first\nio_iter_do_read(), which is a problem mostly for streams/sockets.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0e8d20cebe5fc9c96ed268463c394237daabc384.1664235732.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-26 18:44:15 -0600 io_uring/rw: don't lose short results on io_setup_async_rw()"
    },
    {
        "commit": "bf68b5b34311ee57ed40749a1257a30b46127556",
        "message": "req->cqe.res is set in io_read() to the amount of bytes left to be done,\nwhich is used to figure out whether to fail a read or not. However,\nio_read() may do another without returning, and we stash the previous\nvalue into ->bytes_done but forget to update cqe.res. Then we ask a read\nto do strictly less than cqe.res but expect the return to be exactly\ncqe.res.\n\nFix the bug by updating cqe.res for retries.\n\nCc: stable@vger.kernel.org\nReported-and-Tested-by: Beld Zhang <beldzhang@gmail.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3a1088440c7be98e5800267af922a67da0ef9f13.1664235732.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-26 18:44:15 -0600 io_uring/rw: fix unexpected link breakage"
    },
    {
        "commit": "7cae596bc31f900bb72492ff40c7f5addf72fa19",
        "message": "Instead of picking the task from the first submitter task, rather use the\ncreator task or in the case of disabled (IORING_SETUP_R_DISABLED) the\nenabling task.\n\nThis approach allows a lot of simplification of the logic here. This\nremoves init logic from the submission path, which can always be a bit\nconfusing, but also removes the need for locking to write (or read) the\nsubmitter_task.\n\nUsers that want to move a ring before submitting can create the ring\ndisabled and then enable it on the submitting task.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nFixes: 97bbdc06a444 (\"io_uring: add IORING_SETUP_SINGLE_ISSUER\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0",
        "release_date": "2022-09-26 11:26:18 -0600 io_uring: register single issuer task at creation"
    },
    {
        "commit": "4c17a496a7a0730fdfc9e249b83cc58249111532",
        "message": "Having ->async_data doesn't mean it's initialised and previously we vere\nrelying on setting F_CLEANUP at the right moment. With zc sendmsg\nthough, we set F_CLEANUP early in prep when we alloc a notif and so we\nmay allocate async_data, fail in copy_msg_hdr() leaving\nstruct io_async_msghdr not initialised correctly but with F_CLEANUP\nset, which causes a ->free_iov double free and probably other nastiness.\n\nAlways initialise ->free_iov. Also, now it might point to fast_iov when\nfails, so avoid freeing it during cleanups.\n\nReported-by: syzbot+edfd15cd4246a3fc615a@syzkaller.appspotmail.com\nFixes: 493108d95f146 (\"io_uring/net: zerocopy sendmsg\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-26 08:36:50 -0600 io_uring/net: fix cleanup double free free_iov init"
    },
    {
        "commit": "3db61221f4e8f18d1dd6e45dbe9e3702ff2d67ab",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single fix for an issue with un-reaped IOPOLL requests on ring\n  exit\"\n\n* tag 'io_uring-6.0-2022-09-23' of git://git.kernel.dk/linux:\n  io_uring: ensure that cached task references are always put on exit",
        "kernel_version": "v6.0-rc7",
        "release_date": "2022-09-24 08:27:08 -0700 Merge tag 'io_uring-6.0-2022-09-23' of git://git.kernel.dk/linux"
    },
    {
        "commit": "bbae8d1f526b56d04d51a5fc300d9de702e264dd",
        "message": "With USER_RECOVERY feature enabled, the monitor_work schedules\nquiesce_work after finding a dying ubq_daemon. The monitor_work\nshould also abort all rqs issued to userspace before the ubq_daemon is\ndying. The quiesce_work's job is to:\n(1) quiesce request queue.\n(2) check if there is any INFLIGHT rq. If so, we retry until all these\n    rqs are requeued and become IDLE. These rqs should be requeued by\n\tublk_queue_rq(), task work, io_uring fallback wq or monitor_work.\n(3) complete all ioucmds by calling io_uring_cmd_done(). We are safe to\n    do so because no ioucmd can be referenced now.\n(5) set ub's state to UBLK_S_DEV_QUIESCED, which means we are ready for\n    recovery. This state is exposed to userspace by GET_DEV_INFO.\n\nThe driver can always handle STOP_DEV and cleanup everything no matter\nub's state is LIVE or QUIESCED. After ub's state is UBLK_S_DEV_QUIESCED,\nuser can recover with new process.\n\nNote: we do not change the default behavior with reocvery feature\ndisabled. monitor_work still schedules stop_work and abort inflight\nrqs. And finally ublk_device is released.\n\nSigned-off-by: ZiyangZhang <ZiyangZhang@linux.alibaba.com>\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20220923153919.44078-5-ZiyangZhang@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-23 19:09:56 -0600 ublk_drv: consider recovery feature in aborting mechanism"
    },
    {
        "commit": "e775f93f2ab976a2cdb4a7b53063cbe890904f73",
        "message": "io_uring caches task references to avoid doing atomics for each of them\nper request. If a request is put from the same task that allocated it,\nthen we can maintain a per-ctx cache of them. This obviously relies\non io_uring always pruning caches in a reliable way, and there's\ncurrently a case off io_uring fd release where we can miss that.\n\nOne example is a ring setup with IOPOLL, which relies on the task\npolling for completions, which will free them. However, if such a task\nsubmits a request and then exits or closes the ring without reaping\nthe completion, then ring release will reap and put. If release happens\nfrom that very same task, the completed request task refs will get\nput back into the cache pool. This is problematic, as we're now beyond\nthe point of pruning caches.\n\nManually drop these caches after doing an IOPOLL reap. This releases\nreferences from the current task, which is enough. If another task\nhappens to be doing the release, then the caching will not be\ntriggered and there's no issue.\n\nCc: stable@vger.kernel.org\nFixes: e98e49b2bbf7 (\"io_uring: extend task put optimisations\")\nReported-by: Homin Rhee <hominlab@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc7",
        "release_date": "2022-09-23 18:51:08 -0600 io_uring: ensure that cached task references are always put on exit"
    },
    {
        "commit": "aa1df3a360a0c50e0f0086a785d75c2785c29967",
        "message": "Overflowing CQEs may result in reordering, which is buggy in case of\nlinks, F_MORE and so on. If we guarantee that we don't reorder for\nthe unlikely event of a CQ ring overflow, then we can further extend\nthis to not have to terminate multishot requests if it happens. For\nother operations, like zerocopy sends, we have no choice but to honor\nCQE ordering.\n\nReported-by: Dylan Yudaken <dylany@fb.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ec3bc55687b0768bbe20fb62d7d06cfced7d7e70.1663892031.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-23 15:04:20 -0600 io_uring: fix CQE reordering"
    },
    {
        "commit": "a75155faef4efcb9791f77e2652e29ce8906e05a",
        "message": "We should not assume anything about ->free_iov just from\nREQ_F_ASYNC_DATA but rather rely on REQ_F_NEED_CLEANUP, as we may\nallocate ->async_data but failed init would leave the field in not\nconsistent state. The easiest solution is to remove removing\nREQ_F_NEED_CLEANUP and so ->async_data dealloc from io_sendrecv_fail()\nand let io_send_zc_cleanup() do the job. The catch here is that we also\nneed to prevent double notif flushing, just test it for NULL and zero\nwhere it's needed.\n\nBUG: KASAN: use-after-free in io_sendrecv_fail+0x3b0/0x3e0 io_uring/net.c:1221\nWrite of size 8 at addr ffff8880771b4080 by task syz-executor.3/30199\n\nCPU: 1 PID: 30199 Comm: syz-executor.3 Not tainted 6.0.0-rc6-next-20220923-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 08/26/2022\nCall Trace:\n <TASK>\n __dump_stack lib/dump_stack.c:88 [inline]\n dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106\n print_address_description mm/kasan/report.c:284 [inline]\n print_report+0x15e/0x45d mm/kasan/report.c:395\n kasan_report+0xbb/0x1f0 mm/kasan/report.c:495\n io_sendrecv_fail+0x3b0/0x3e0 io_uring/net.c:1221\n io_req_complete_failed+0x155/0x1b0 io_uring/io_uring.c:873\n io_drain_req io_uring/io_uring.c:1648 [inline]\n io_queue_sqe_fallback.cold+0x29f/0x788 io_uring/io_uring.c:1931\n io_submit_sqe io_uring/io_uring.c:2160 [inline]\n io_submit_sqes+0x1180/0x1df0 io_uring/io_uring.c:2276\n __do_sys_io_uring_enter+0xac6/0x2410 io_uring/io_uring.c:3216\n do_syscall_x64 arch/x86/entry/common.c:50 [inline]\n do_syscall_64+0x35/0xb0 arch/x86/entry/common.c:80\n entry_SYSCALL_64_after_hwframe+0x63/0xcd\n\nFixes: c4c0009e0b56e (\"io_uring/net: combine fail handlers\")\nReported-by: syzbot+4c597a574a3f5a251bda@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/23ab8346e407ea50b1198a172c8a97e1cf22915b.1663945875.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-23 14:57:38 -0600 io_uring/net: fix UAF in io_sendrecv_fail()"
    },
    {
        "commit": "4781185da411c0b51ef9b1db557c1ea28ac11de4",
        "message": "It's not currently possible but in the future we may get\nIORING_CQE_F_MORE and so a notification even for a failed request, i.e.\nwhen cqe->res <= 0. That's precisely what the documentation says, so\nadjust the test and do IORING_CQE_F_MORE checks regardless of the main\ncompletion cqe->res.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/aac948ea753a8bfe1fa3b82fe45debcb54586369.1663953085.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-23 14:57:27 -0600 selftest/net: adjust io_uring sendzc notif handling"
    },
    {
        "commit": "ec7fd2562f57fcfd96f15fbc8ad088f954c2dcf5",
        "message": "io_uring will run task_work from contexts that have been prepared for\nwaiting, and in doing so it'll implicitly set the task running again\nto avoid issues with blocking conditions. The new deferred local\ntask_work doesn't do that, which can result in spews on this being\nan invalid condition:\n\n\u2028\u2028[  112.917576] do not call blocking ops when !TASK_RUNNING; state=1 set at [<00000000ad64af64>] prepare_to_wait_exclusive+0x3f/0xd0\n[  112.983088] WARNING: CPU: 1 PID: 190 at kernel/sched/core.c:9819 __might_sleep+0x5a/0x60\n[  112.987240] Modules linked in:\n[  112.990504] CPU: 1 PID: 190 Comm: io_uring Not tainted 6.0.0-rc6+ #1617\n[  113.053136] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.15.0-0-g2dd4b9b3f840-prebuilt.qemu.org 04/01/2014\n[  113.133650] RIP: 0010:__might_sleep+0x5a/0x60\n[  113.136507] Code: ee 48 89 df 5b 31 d2 5d e9 33 ff ff ff 48 8b 90 30 0b 00 00 48 c7 c7 90 de 45 82 c6 05 20 8b 79 01 01 48 89 d1 e8 3a 49 77 00 <0f> 0b eb d1 66 90 0f 1f 44 00 00 9c 58 f6 c4 02 74 35 65 8b 05 ed\n[  113.223940] RSP: 0018:ffffc90000537ca0 EFLAGS: 00010286\n[  113.232903] RAX: 0000000000000000 RBX: ffffffff8246782c RCX: ffffffff8270bcc8\nIOPS=133.15K, BW=520MiB/s, IOS/call=32/31\n[  113.353457] RDX: ffffc90000537b50 RSI: 00000000ffffdfff RDI: 0000000000000001\n[  113.358970] RBP: 00000000000003bc R08: 0000000000000000 R09: c0000000ffffdfff\n[  113.361746] R10: 0000000000000001 R11: ffffc90000537b48 R12: ffff888103f97280\n[  113.424038] R13: 0000000000000000 R14: 0000000000000001 R15: 0000000000000001\n[  113.428009] FS:  00007f67ae7fc700(0000) GS:ffff88842fc80000(0000) knlGS:0000000000000000\n[  113.432794] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n[  113.503186] CR2: 00007f67b8b9b3b0 CR3: 0000000102b9b005 CR4: 0000000000770ee0\n[  113.507291] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n[  113.512669] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n[  113.574374] PKRU: 55555554\n[  113.576800] Call Trace:\n[  113.578325]  <TASK>\n[  113.579799]  set_page_dirty_lock+0x1b/0x90\n[  113.582411]  __bio_release_pages+0x141/0x160\n[  113.673078]  ? set_next_entity+0xd7/0x190\n[  113.675632]  blk_rq_unmap_user+0xaa/0x210\n[  113.678398]  ? timerqueue_del+0x2a/0x40\n[  113.679578]  nvme_uring_task_cb+0x94/0xb0\n[  113.683025]  __io_run_local_work+0x8a/0x150\n[  113.743724]  ? io_cqring_wait+0x33d/0x500\n[  113.746091]  io_run_local_work.part.76+0x2e/0x60\n[  113.750091]  io_cqring_wait+0x2e7/0x500\n[  113.752395]  ? trace_event_raw_event_io_uring_req_failed+0x180/0x180\n[  113.823533]  __x64_sys_io_uring_enter+0x131/0x3c0\n[  113.827382]  ? switch_fpu_return+0x49/0xc0\n[  113.830753]  do_syscall_64+0x34/0x80\n[  113.832620]  entry_SYSCALL_64_after_hwframe+0x5e/0xc8\n\nEnsure that we mark current as TASK_RUNNING for deferred task_work\nas well.\n\nFixes: c0e0d6ba25f1 (\"io_uring: add IORING_SETUP_DEFER_TASKRUN\")\nReported-by: Stefan Roesch <shr@fb.com>\nReviewed-by: Dylan Yudaken <dylany@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 19:39:35 -0600 io_uring: ensure local task_work marks task as running"
    },
    {
        "commit": "493108d95f1464ccd101d4e5cfa7e93f1fc64d47",
        "message": "Add a zerocopy version of sendmsg.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6aabc4bdfc0ec78df6ec9328137e394af9d4e7ef.1663668091.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:15:02 -0600 io_uring/net: zerocopy sendmsg"
    },
    {
        "commit": "c4c0009e0b56ef9920020bcade1e45be52653bae",
        "message": "Merge io_send_zc_fail() into io_sendrecv_fail(), saves a few lines of\ncode and some headache for following patch.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e0eba1d577413aef5602cd45f588b9230207082d.1663668091.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:15:02 -0600 io_uring/net: combine fail handlers"
    },
    {
        "commit": "b0e9b5517eb12fa80c72e205fe28534c2e2f39b9",
        "message": "Simple renaming of io_sendzc*() functions in preparatio to adding\na zerocopy sendmsg variant.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/265af46829e6076dd220011b1858dc3151969226.1663668091.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:15:02 -0600 io_uring/net: rename io_sendzc()"
    },
    {
        "commit": "516e82f0e043a1a0e8d00800ed0ffe2137cf0e7e",
        "message": "We have normal sends, but what is missing is sendto-like requests. Add\nsendto() capabilities to IORING_OP_SEND by passing in addr just as we do\nfor IORING_OP_SEND_ZC.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/69fbd8b2cb830e57d1bf9ec351e9bf95c5b77e3f.1663668091.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:15:02 -0600 io_uring/net: support non-zerocopy sendto"
    },
    {
        "commit": "6ae61b7aa2c758ce07347ebfa9c79b6f208098d5",
        "message": "Instead of passing the right address into io_setup_async_addr() only\nspecify local on-stack storage and let the function infer where to grab\nit from. It optimises out one local variable we have to deal with.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6bfa9ab810d776853eb26ed59301e2536c3a5471.1663668091.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:15:02 -0600 io_uring/net: refactor io_setup_async_addr"
    },
    {
        "commit": "5693bcce892d7b8b15a7a92b011d3d40a023b53c",
        "message": "Partial zc send may end up in io_req_complete_failed(), which not only\nwould return invalid result but also mask out the notification leading\nto lifetime issues.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5673285b5e83e6ceca323727b4ddaa584b5cc91e.1663668091.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:15:02 -0600 io_uring/net: don't lose partial send_zc on fail"
    },
    {
        "commit": "7e6b638ed501cced4e472298d6b08dd16346f3a6",
        "message": "Just as with rw, partial send/recv may end up in\nio_req_complete_failed() and loose the result, make sure we return the\nnumber of bytes processed.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a4ff95897b5419356fca9ea55db91ac15b2975f9.1663668091.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:15:02 -0600 io_uring/net: don't lose partial send/recv on fail"
    },
    {
        "commit": "47b4c68660752facfa6247b1fc9ca9d722b8b601",
        "message": "A partially done read/write may end up in io_req_complete_failed() and\nloose the result, make sure we return the number of bytes processed.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/05e0879c226bcd53b441bf92868eadd4bf04e2fc.1663668091.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:15:02 -0600 io_uring/rw: don't lose partial IO result on fail"
    },
    {
        "commit": "a47b255e90395bdb481975ab3d9e96fcf8b3165f",
        "message": "Sometimes we have to do a little bit of a fixup on a request failuer in\nio_req_complete_failed(). Add a callback in opdef for that.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b734cff4e67cb30cca976b9face321023f37549a.1663668091.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:15:02 -0600 io_uring: add custom opcode hooks on fail"
    },
    {
        "commit": "3b8fdd1dc35e395d19efbc8391a809a5b954ecf4",
        "message": "If we have doubly sized SQEs, then we need to shift the sq index by 1\nto account for using two entries for a single request. The CQE dumping\ngets this right, but the SQE one does not.\n\nImprove the SQE dumping in general, the information dumped is pretty\nsparse and doesn't even cover the whole basic part of the SQE. Include\ninformation on the extended part of the SQE, if doubly sized SQEs are\nin use. A typical dump now looks like the following:\n\n[...]\nSQEs:\t32\n   32: opcode:URING_CMD, fd:0, flags:1, off:3225964160, addr:0x0, rw_flags:0x0, buf_index:0 user_data:2721, e0:0x0, e1:0xffffb8041000, e2:0x100000000000, e3:0x5500, e4:0x7, e5:0x0, e6:0x0, e7:0x0\n   33: opcode:URING_CMD, fd:0, flags:1, off:3225964160, addr:0x0, rw_flags:0x0, buf_index:0 user_data:2722, e0:0x0, e1:0xffffb8043000, e2:0x100000000000, e3:0x5508, e4:0x7, e5:0x0, e6:0x0, e7:0x0\n   34: opcode:URING_CMD, fd:0, flags:1, off:3225964160, addr:0x0, rw_flags:0x0, buf_index:0 user_data:2723, e0:0x0, e1:0xffffb8045000, e2:0x100000000000, e3:0x5510, e4:0x7, e5:0x0, e6:0x0, e7:0x0\n[...]\n\nFixes: ebdeb7c01d02 (\"io_uring: add support for 128-byte SQEs\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:15:02 -0600 io_uring/fdinfo: fix sqe dumping for IORING_SETUP_SQE128"
    },
    {
        "commit": "4f731705cc1f1591e15e1c3133de8ae3843c68ff",
        "message": "We already have the cq_shift, just use that to tell if we have doubly\nsized CQEs or not.\n\nWhile in there, cleanup the CQE32 vs normal CQE size printing.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:15:02 -0600 io_uring/fdinfo: get rid of unnecessary is_cqe32 variable"
    },
    {
        "commit": "c0dc995eb2295e1be6b95b60c90c59f87b009bdb",
        "message": "We removed conditional io_commit_cqring_flush() guarding against\nspurious eventfd and the io_disarm_next()'s return value is not used\nanymore, just void it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9a441c9a32a58bcc586076fa9a7d0dc33f1fb3cb.1662652536.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:15:01 -0600 io_uring: remove unused return from io_disarm_next"
    },
    {
        "commit": "7924fdfeea814b4f7ff8a16de00951ad93cccf6c",
        "message": "We'll grab uring_lock and call __io_run_local_work() with several\natomics inside even if there are no task works. Skip it if ->work_llist\nis empty.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/f6a885f372bad2d77d9cd87341b0a86a4000c0ff.1662652536.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:15:01 -0600 io_uring: add fast path for io_run_local_work()"
    },
    {
        "commit": "1f8d5bbe98a10da5348b0fab2fa679ef8d033be5",
        "message": "Let's keep checks for whether to break the iopoll loop or not same for\nnormal and defer tw, this includes ->cached_cq_tail checks guarding\nagainst polling more than asked for.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d2fa8a44f8114f55a4807528da438cde93815360.1662652536.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:15:01 -0600 io_uring/iopoll: unify tw breaking logic"
    },
    {
        "commit": "9d54bd6a3bb495f2e7e4996efdaf1bef6ad62272",
        "message": "We may propagate a positive return value of io_run_task_work() out of\nio_iopoll_check(), which breaks our tests. io_run_task_work() doesn't\nreturn anything useful for us, ignore the return value.\n\nFixes: c0e0d6ba25f1 (\"io_uring: add IORING_SETUP_DEFER_TASKRUN\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/c442bb87f79cea10b3f857cbd4b9a4f0a0493fa3.1662652536.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:14:59 -0600 io_uring/iopoll: fix unexpected returns"
    },
    {
        "commit": "6567506b68b0cae3934f1a58b35d709f38fc2e90",
        "message": "We try to restrict CQ waiters when IORING_SETUP_DEFER_TASKRUN is set,\nbut if nothing has been submitted yet it'll allow any waiter, which\nviolates the contract.\n\nFixes: c0e0d6ba25f1 (\"io_uring: add IORING_SETUP_DEFER_TASKRUN\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/b4f0d3f14236d7059d08c5abe2661ef0b78b5528.1662652536.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:14:55 -0600 io_uring: disallow defer-tw run w/ no submitters"
    },
    {
        "commit": "76de6749d1bc1817367fedda94cd7c5d325df6c4",
        "message": "In case of DEFER_TASK_WORK we try to restrict waiters to only one task,\nwhich is also the only submitter; however, we don't do it reliably,\nwhich might be very confusing and backfire in the future. E.g. we\ncurrently allow multiple tasks in io_iopoll_check().\n\nFixes: c0e0d6ba25f1 (\"io_uring: add IORING_SETUP_DEFER_TASKRUN\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/94c83c0a7fe468260ee2ec31bdb0095d6e874ba2.1662652536.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 13:14:46 -0600 io_uring: further limit non-owner defer-tw cq waiting"
    },
    {
        "commit": "ac9e5784bbe72f4f603d1af84760ec09bc0b5ccd",
        "message": "Reuse struct io_sr_msg for zerocopy sends, which is handy. There is\nonly one zerocopy specific field, namely .notif, and we have enough\nspace for it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/408c5b1b2d8869e1a12da5f5a78ed72cac112149.1662639236.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:43 -0600 io_uring/net: use io_sr_msg for sendzc"
    },
    {
        "commit": "0b048557db761d287777360a100e1d010760d209",
        "message": "In preparation for using struct io_sr_msg for zerocopy sends, clean up\ntypes. First, flags can be u16 as it's provided by the userspace in u16\nioprio, as well as addr_len. This saves us 4 bytes. Also use unsigned\nfor size and done_io, both are as well limited to u32.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/42c2639d6385b8b2181342d2af3a42d3b1c5bcd2.1662639236.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:43 -0600 io_uring/net: refactor io_sr_msg types"
    },
    {
        "commit": "cd9021e88fddf0d9fa9704564153af2bdb5dc13c",
        "message": "Add a sg_from_iter() for when we initiate non-bvec zerocopy sends, which\nhelps us to remove some extra steps from io_sg_from_iter(). The only\nthing the new function has to do before giving control away to\n__zerocopy_sg_from_iter() is to check if the skb has managed frags and\ndowngrade them if so.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/cda3dea0d36f7931f63a70f350130f085ac3f3dd.1662639236.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:43 -0600 io_uring/net: add non-bvec sg chunking callback"
    },
    {
        "commit": "6bf8ad25fcd42a719f24613deabcff2fd341c789",
        "message": "We already keep io_async_msghdr caches for normal send/recv requests,\nuse them also for zerocopy send.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/42fa615b6e0be25f47a685c35d7b5e4f1b03d348.1662639236.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:43 -0600 io_uring/net: io_async_msghdr caches for sendzc"
    },
    {
        "commit": "858c293e5d3b7fd3037883fcc0379594517c926c",
        "message": "send/recv have async_data caches but there are only used from within\nissue handlers. Extend their use also to ->prep_async, should be handy\nwith links and IOSQE_ASYNC.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b9a2264b807582a97ed606c5bfcdc2399384e8a5.1662639236.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:43 -0600 io_uring/net: use async caches for async prep"
    },
    {
        "commit": "95eafc74be5e11f9dd6a11504c27321c515ce00f",
        "message": "We should prioritise send/recv retry cases over failures, they're more\nimportant. Shuffle -ERESTARTSYS after we handled retries.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d9059691b30d0963b7269fa4a0c81ee7720555e6.1662639236.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:43 -0600 io_uring/net: reshuffle error handling"
    },
    {
        "commit": "e9a884285484a098fd607496d565c3b4e4733f63",
        "message": "There is one place when we forgot to change hand coded spin locking with\nio_cq_lock(), change it to be more consistent. Note, the unlock part is\nalready __io_cq_unlock_post().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/91699b9a00a07128f7ca66136bdbbfc67a64659e.1662639236.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:43 -0600 io_uring: use io_cq_lock consistently"
    },
    {
        "commit": "385c609f9bfcfcd1e1e649834fc61e48d2316381",
        "message": "Request referencing has changed a while ago and there is no notion left\nof submission/completion references, kill an outdated comment.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/38902e7229d68cecd62702436d627d4858b0d9d4.1662639236.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:43 -0600 io_uring: kill an outdated comment"
    },
    {
        "commit": "4ab9d465071beb95e30e2712d4c65b6ab781865b",
        "message": "In commit 934447a603b2 (\"io_uring: do not recycle buffer in READV\") a\ntemporary fix was put in io_kbuf_recycle to simply never recycle READV\nbuffers.\n\nInstead of that, rather treat READV with REQ_F_BUFFER_SELECTED the same as\na READ with REQ_F_BUFFER_SELECTED. Since READV requires iov_len of 1 they\nare essentially the same.\nIn order to do this inside io_prep_rw() add some validation to check that\nit is in fact only length 1, and also extract the length of the buffer at\nprep time.\n\nThis allows removal of the io_iov_buffer_select codepaths as they are only\nused from the READV op.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220907165152.994979-1-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:43 -0600 io_uring: allow buffer recycling in READV"
    },
    {
        "commit": "dac6a0eae793f53c62a0f83d9f5423293a7845c4",
        "message": "Combine the two checks we have for task_work running and whether or not\nwe need to shuffle the mutex into one, so we unify how task_work is run\nin the iopoll loop. This helps ensure that local task_work is run when\nneeded, and also optimizes that path to avoid a mutex shuffle if it's\nnot needed.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:43 -0600 io_uring: ensure iopoll runs local task work as well"
    },
    {
        "commit": "8ac5d85a89b48269e5aefb92b640d38367670a1b",
        "message": "We have a few spots that drop the mutex just to run local task_work,\nwhich immediately tries to grab it again. Add a helper that just passes\nin whether we're locked already.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:43 -0600 io_uring: add local task_work run helper that is entered locked"
    },
    {
        "commit": "a1119fb0711591c2aaf99be79d87ce8ebeb9d250",
        "message": "After the addition of iopoll support for passthrough, there's a bit of\na mixup here. Clean it up and get rid of the casting for the passthrough\ncommand type.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:42 -0600 io_uring: cleanly separate request types for iopoll"
    },
    {
        "commit": "5756a3a7e713bcab705a5f0c810a2b1f7f4ecfaa",
        "message": "Put this up in the same way as iopoll is done for regular read/write IO.\nMake place for storing a cookie into struct io_uring_cmd on submission.\nPerform the completion using the ->uring_cmd_iopoll handler.\n\nSigned-off-by: Kanchan Joshi <joshi.k@samsung.com>\nSigned-off-by: Pankaj Raghav <p.raghav@samsung.com>\nLink: https://lore.kernel.org/r/20220823161443.49436-3-joshi.k@samsung.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:42 -0600 io_uring: add iopoll infrastructure for io_uring_cmd"
    },
    {
        "commit": "de27e18e86173b704beaa19f0ee376f3305c4794",
        "message": "io_uring will invoke this to do completion polling on uring-cmd\noperations.\n\nSigned-off-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220823161443.49436-2-joshi.k@samsung.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:42 -0600 fs: add file_operations->uring_cmd_iopoll"
    },
    {
        "commit": "f75d5036d04cd57103fe1a50dffceb7c1040fbe7",
        "message": "Add tracing for io_run_local_task_work\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220830125013.570060-8-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:42 -0600 io_uring: trace local task work run"
    },
    {
        "commit": "21a091b970cdbcf3e8ff829234b51be6f9192766",
        "message": "Some workloads rely on a registered eventfd (via\nio_uring_register_eventfd(3)) in order to wake up and process the\nio_uring.\n\nIn the case of a ring setup with IORING_SETUP_DEFER_TASKRUN, that eventfd\nalso needs to be signalled when there are tasks to run.\n\nThis changes an old behaviour which assumed 1 eventfd signal implied at\nleast 1 CQE, however only when this new flag is set (and so old users will\nnot notice). This should be expected with the IORING_SETUP_DEFER_TASKRUN\nflag as it is not guaranteed that every task will result in a CQE.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220830125013.570060-7-dylany@fb.com\n[axboe: fold in call_rcu() serialization fix]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:42 -0600 io_uring: signal registered eventfd to process deferred task work"
    },
    {
        "commit": "d8e9214f119db5697382c63a62790a4afb5d00cd",
        "message": "Non functional change: move this function above io_eventfd_signal so it\ncan be used from there\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220830125013.570060-6-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:42 -0600 io_uring: move io_eventfd_put"
    },
    {
        "commit": "c0e0d6ba25f180ab76d3c18f8b360a119dffa634",
        "message": "Allow deferring async tasks until the user calls io_uring_enter(2) with\nthe IORING_ENTER_GETEVENTS flag. Enable this mode with a flag at\nio_uring_setup time. This functionality requires that the later\nio_uring_enter will be called from the same submission task, and therefore\nrestrict this flag to work only when IORING_SETUP_SINGLE_ISSUER is also\nset.\n\nBeing able to hand pick when tasks are run prevents the problem where\nthere is current work to be done, however task work runs anyway.\n\nFor example, a common workload would obtain a batch of CQEs, and process\neach one. Interrupting this to additional taskwork would add latency but\nnot gain anything. If instead task work is deferred to just before more\nCQEs are obtained then no additional latency is added.\n\nThe way this is implemented is by trying to keep task work local to a\nio_ring_ctx, rather than to the submission task. This is required, as the\napplication will want to wake up only a single io_ring_ctx at a time to\nprocess work, and so the lists of work have to be kept separate.\n\nThis has some other benefits like not having to check the task continually\nin handle_tw_list (and potentially unlocking/locking those), and reducing\nlocks in the submit & process completions path.\n\nThere are networking cases where using this option can reduce request\nlatency by 50%. For example a contrived example using [1] where the client\nsends 2k data and receives the same data back while doing some system\ncalls (to trigger task work) shows this reduction. The reason ends up\nbeing that if sending responses is delayed by processing task work, then\nthe client side sits idle. Whereas reordering the sends first means that\nthe client runs it's workload in parallel with the local task work.\n\n[1]:\nUsing https://github.com/DylanZA/netbench/tree/defer_run\nClient:\n./netbench  --client_only 1 --control_port 10000 --host <host> --tx \"epoll --threads 16 --per_thread 1 --size 2048 --resp 2048 --workload 1000\"\nServer:\n./netbench  --server_only 1 --control_port 10000  --rx \"io_uring --defer_taskrun 0 --workload 100\"   --rx \"io_uring  --defer_taskrun 1 --workload 100\"\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220830125013.570060-5-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:42 -0600 io_uring: add IORING_SETUP_DEFER_TASKRUN"
    },
    {
        "commit": "2327337b881d3f24949da4a4d34a6e657a71a79d",
        "message": "This is not needed, and it is normally better to wait for task work until\nafter submissions. This will allow greater batching if either work arrives\nin the meanwhile, or if the submissions cause task work to be queued up.\n\nFor SQPOLL this also no longer runs task work, but this is handled inside\nthe SQPOLL loop anyway.\n\nFor IOPOLL io_iopoll_check will run task work anyway\n\nAnd otherwise io_cqring_wait will run task work\n\nSuggested-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220830125013.570060-4-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:42 -0600 io_uring: do not run task work at the start of io_uring_enter"
    },
    {
        "commit": "b4c98d59a787eff4c8ee983bcf68266ce2199df6",
        "message": "This will be used later to know if the ring has outstanding work. Right\nnow just if there is overflow CQEs to copy to the main CQE ring, but later\nwill include deferred tasks\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220830125013.570060-3-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:42 -0600 io_uring: introduce io_has_work"
    },
    {
        "commit": "32d91f0590080597d5fc46c0c36d8885c241622e",
        "message": "'running' is set once and read once, so can easily just remove it\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220830125013.570060-2-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-21 10:30:42 -0600 io_uring: remove unnecessary variable"
    },
    {
        "commit": "38eddeedbbeac33f26845c29e7414b9313ea70db",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Nothing really major here, but figured it'd be nicer to just get these\n  flushed out for -rc6 so that the 6.1 branch will have them as well.\n  That'll make our lives easier going forward in terms of development,\n  and avoid trivial conflicts in this area.\n\n   - Simple trace rename so that the returned opcode name is consistent\n     with the enum definition (Stefan)\n\n   - Send zc rsrc request vs notification lifetime fix (Pavel)\"\n\n* tag 'io_uring-6.0-2022-09-18' of git://git.kernel.dk/linux:\n  io_uring/opdef: rename SENDZC_NOTIF to SEND_ZC\n  io_uring/net: fix zc fixed buf lifetime",
        "kernel_version": "v6.0-rc6",
        "release_date": "2022-09-18 09:25:27 -0700 Merge tag 'io_uring-6.0-2022-09-18' of git://git.kernel.dk/linux"
    },
    {
        "commit": "9bd3f728223ebcfef8e9d087bdd142f0e388215d",
        "message": "It's confusing to see the string SENDZC_NOTIF in ftrace output\nwhen using IORING_OP_SEND_ZC.\n\nFixes: b48c312be05e8 (\"io_uring/net: simplify zerocopy send user API\")\nSigned-off-by: Stefan Metzmacher <metze@samba.org>\nCc: Pavel Begunkov <asml.silence@gmail.com>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: io-uring@vger.kernel.org\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8e5cd8616919c92b6c3c7b6ea419fdffd5b97f3c.1663363798.git.metze@samba.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc6",
        "release_date": "2022-09-18 06:59:13 -0600 io_uring/opdef: rename SENDZC_NOTIF to SEND_ZC"
    },
    {
        "commit": "e3366e0234971a09f0e16f0e6fa16f4cbae45e47",
        "message": "Notifications usually outlive requests, so we need to pin buffers with\nit by assigning a rsrc to it instead of the request.\n\nFixed: b48c312be05e8 (\"io_uring/net: simplify zerocopy send user API\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/dd6406ff8a90887f2b36ed6205dac9fda17c1f35.1663366886.git.asml.silence@gmail.com\nReviewed-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc6",
        "release_date": "2022-09-18 05:07:51 -0600 io_uring/net: fix zc fixed buf lifetime"
    },
    {
        "commit": "0158137d816f60115aae2d3b4acdc67383a05c01",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two small patches:\n\n   - Fix using an unsigned type for the return value, introduced in this\n     release (Pavel)\n\n   - Stable fix for a missing check for a fixed file on put (me)\"\n\n* tag 'io_uring-6.0-2022-09-16' of git://git.kernel.dk/linux-block:\n  io_uring/msg_ring: check file type before putting\n  io_uring/rw: fix error'ed retry return values",
        "kernel_version": "v6.0-rc6",
        "release_date": "2022-09-16 06:50:25 -0700 Merge tag 'io_uring-6.0-2022-09-16' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "fc7222c3a9f56271fba02aabbfbae999042f1679",
        "message": "If we're invoked with a fixed file, follow the normal rules of not\ncalling io_fput_file(). Fixed files are permanently registered to the\nring, and do not need putting separately.\n\nCc: stable@vger.kernel.org\nFixes: aa184e8671f0 (\"io_uring: don't attempt to IOPOLL for MSG_RING requests\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc6",
        "release_date": "2022-09-15 11:44:35 -0600 io_uring/msg_ring: check file type before putting"
    },
    {
        "commit": "56f99b8d06ef1ed1c9730948f9f05ac2b930a20b",
        "message": "Today blk_queue_enter() and __bio_queue_enter() return -EBUSY for the\nnowait code path. This is not correct: they should return -EAGAIN\ninstead.\n\nThis problem was detected by fio. The following command exposed the\nabove problem:\n\nt/io_uring -p0 -d128 -b4096 -s32 -c32 -F1 -B0 -R0 -X1 -n24 -P1 -u1 -O0 /dev/ng0n1\n\nBy applying the patch, the retry case is handled correctly in the slow\npath.\n\nSigned-off-by: Stefan Roesch <shr@fb.com>\nFixes: bfd343aa1718 (\"blk-mq: don't wait in blk_mq_queue_enter() if __GFP_WAIT isn't set\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc6",
        "release_date": "2022-09-13 15:06:39 -0600 block: blk_queue_enter() / __bio_queue_enter() must return -EAGAIN for nowait"
    },
    {
        "commit": "62bb0647b14646fa6c9aa25ecdf67ad18f13523c",
        "message": "Kernel test robot reports that we test negativity of an unsigned in\nio_fixup_rw_res() after a recent change, which masks error codes and\nmesses up the return value in case I/O is re-retried and failed with\nan error.\n\nFixes: 4d9cb92ca41dd (\"io_uring/rw: fix short rw error handling\")\nReported-by: kernel test robot <lkp@intel.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9754a0970af1861e7865f9014f735c70dc60bf79.1663071587.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc6",
        "release_date": "2022-09-13 07:47:11 -0600 io_uring/rw: fix error'ed retry return values"
    },
    {
        "commit": "d2b768c3d44af4ea19c0f52e718acca01ebb22e8",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Removed function that became unused after last week's merge (Jiapeng)\n\n - Two small fixes for kbuf recycling (Pavel)\n\n - Include address copy for zc send for POLLFIRST (Pavel)\n\n - Fix for short IO handling in the normal read/write path (Pavel)\n\n* tag 'io_uring-6.0-2022-09-09' of git://git.kernel.dk/linux-block:\n  io_uring/rw: fix short rw error handling\n  io_uring/net: copy addr for zc on POLL_FIRST\n  io_uring: recycle kbuf recycle on tw requeue\n  io_uring/kbuf: fix not advancing READV kbuf ring\n  io_uring/notif: Remove the unused function io_notif_complete()",
        "kernel_version": "v6.0-rc5",
        "release_date": "2022-09-09 14:57:18 -0400 Merge tag 'io_uring-6.0-2022-09-09' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "4d9cb92ca41dd8e905a4569ceba4716c2f39c75a",
        "message": "We have a couple of problems, first reports of unexpected link breakage\nfor reads when cqe->res indicates that the IO was done in full. The\nreason here is partial IO with retries.\n\nTL;DR; we compare the result in __io_complete_rw_common() against\nreq->cqe.res, but req->cqe.res doesn't store the full length but rather\nthe length left to be done. So, when we pass the full corrected result\nvia kiocb_done() -> __io_complete_rw_common(), it fails.\n\nThe second problem is that we don't try to correct res in\nio_complete_rw(), which, for instance, might be a problem for O_DIRECT\nbut when a prefix of data was cached in the page cache. We also\ndefinitely don't want to pass a corrected result into io_rw_done().\n\nThe fix here is to leave __io_complete_rw_common() alone, always pass\nnot corrected result into it and fix it up as the last step just before\nactually finishing the I/O.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://github.com/axboe/liburing/issues/643\nReported-by: Beld Zhang <beldzhang@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc5",
        "release_date": "2022-09-09 08:57:57 -0600 io_uring/rw: fix short rw error handling"
    },
    {
        "commit": "3c8400532dd8305024ff6eea38707de20b1b9822",
        "message": "Every time we return from an issue handler and expect the request to be\nretried we should also setup it for async exec ourselves. Do that when\nwe return on IORING_RECVSEND_POLL_FIRST in io_sendzc(), otherwise it'll\nre-read the address, which might be a surprise for the userspace.\n\nFixes: 092aeedb750a9 (\"io_uring: allow to pass addr into sendzc\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ab1d0657890d6721339c56d2e161a4bba06f85d0.1662642013.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc5",
        "release_date": "2022-09-08 08:28:38 -0600 io_uring/net: copy addr for zc on POLL_FIRST"
    },
    {
        "commit": "336d28a8f38013a069f2d46e73aaa1880ef17a47",
        "message": "When we queue a request via tw for execution it's not going to be\nexecuted immediately, so when io_queue_async() hits IO_APOLL_READY\nand queues a tw but doesn't try to recycle/consume the buffer some other\nrequest may try to use the the buffer.\n\nFixes: c7fb19428d67 (\"io_uring: add support for ring mapped supplied buffers\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a19bc9e211e3184215a58e129b62f440180e9212.1662480490.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc5",
        "release_date": "2022-09-07 10:36:10 -0600 io_uring: recycle kbuf recycle on tw requeue"
    },
    {
        "commit": "df6d3422d3eed27afa23df092b3ce147c558d1a8",
        "message": "When we don't recycle a selected ring buffer we should advance the head\nof the ring, so don't just skip io_kbuf_recycle() for IORING_OP_READV\nbut adjust the ring.\n\nFixes: 934447a603b22 (\"io_uring: do not recycle buffer in READV\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/a6d85e2611471bcb5d5dcd63a8342077ddc2d73d.1662480490.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc5",
        "release_date": "2022-09-07 10:36:10 -0600 io_uring/kbuf: fix not advancing READV kbuf ring"
    },
    {
        "commit": "4fa07edbb7eacfb56b3aa64f590e9f38e7f1042c",
        "message": "The function io_notif_complete() is defined in the notif.c file, but not\ncalled elsewhere, so delete this unused function.\n\nio_uring/notif.c:24:20: warning: unused function 'io_notif_complete' [-Wunused-function].\n\nLink: https://bugzilla.openanolis.cn/show_bug.cgi?id=2047\nReported-by: Abaci Robot <abaci@linux.alibaba.com>\nSigned-off-by: Jiapeng Chong <jiapeng.chong@linux.alibaba.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20220905020436.51894-1-jiapeng.chong@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc5",
        "release_date": "2022-09-05 11:42:39 -0600 io_uring/notif: Remove the unused function io_notif_complete()"
    },
    {
        "commit": "cec53f4c8df0b3f45796127a31c10b86ec125f55",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - A single fix for over-eager retries for networking (Pavel)\n\n - Revert the notification slot support for zerocopy sends.\n\n   It turns out that even after more than a year or development and\n   testing, there's not full agreement on whether just using plain\n   ordered notifications is Good Enough to avoid the complexity of using\n   the notifications slots. Because of that, we decided that it's best\n   left to a future final decision.\n\n   We can always bring back this feature, but we can't really change it\n   or remove it once we've released 6.0 with it enabled. The reverts\n   leave the usual CQE notifications as the primary interface for\n   knowing when data was sent, and when it was acked. (Pavel)\n\n* tag 'io_uring-6.0-2022-09-02' of git://git.kernel.dk/linux-block:\n  selftests/net: return back io_uring zc send tests\n  io_uring/net: simplify zerocopy send user API\n  io_uring/notif: remove notif registration\n  Revert \"io_uring: rename IORING_OP_FILES_UPDATE\"\n  Revert \"io_uring: add zc notification flush requests\"\n  selftests/net: temporarily disable io_uring zc test\n  io_uring/net: fix overexcessive retries",
        "kernel_version": "v6.0-rc4",
        "release_date": "2022-09-02 16:37:01 -0700 Merge tag 'io_uring-6.0-2022-09-02' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "12c5b70c1897288ee6c841b5cc3ff4d27d511bd1",
        "message": "This is useful for polled IO on a file, or for polled IO with the\nio_uring passthrough mechanism. If bio allocations are done with\nREQ_POLLED for those cases, then initializing the bio set with\nBIOSET_PERCPU_CACHE enables the local per-cpu cache which eliminates\nallocations (and frees) of bio structs when possible.\n\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.1-rc1",
        "release_date": "2022-09-02 13:03:33 -0600 block: enable per-cpu bio caching for the fs bio set"
    },
    {
        "commit": "916d72c10a4ca80ea51f1421e774cb765b53f28f",
        "message": "Enable io_uring zerocopy send tests back and fix them up to follow the\nnew inteface.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c8e5018c516093bdad0b6e19f2f9847dea17e4d2.1662027856.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc4",
        "release_date": "2022-09-01 09:13:33 -0600 selftests/net: return back io_uring zc send tests"
    },
    {
        "commit": "b48c312be05e83b55a4d58bf61f80b4a3288fb7e",
        "message": "Following user feedback, this patch simplifies zerocopy send API. One of\nthe main complaints is that the current API is difficult with the\nuserspace managing notification slots, and then send retries with error\nhandling make it even worse.\n\nInstead of keeping notification slots change it to the per-request\nnotifications model, which posts both completion and notification CQEs\nfor each request when any data has been sent, and only one CQE if it\nfails. All notification CQEs will have IORING_CQE_F_NOTIF set and\nIORING_CQE_F_MORE in completion CQEs indicates whether to wait a\nnotification or not.\n\nIOSQE_CQE_SKIP_SUCCESS is disallowed with zerocopy sends for now.\n\nThis is less flexible, but greatly simplifies the user API and also the\nkernel implementation. We reuse notif helpers in this patch, but in the\nfuture there won't be need for keeping two requests.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/95287640ab98fc9417370afb16e310677c63e6ce.1662027856.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc4",
        "release_date": "2022-09-01 09:13:33 -0600 io_uring/net: simplify zerocopy send user API"
    },
    {
        "commit": "57f332246afa5929bdf2e7a5facddedb43549be4",
        "message": "We're going to remove the userspace exposed zerocopy notification API,\nremove notification registration.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6ff00b97be99869c386958a990593c9c31cf105b.1662027856.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc4",
        "release_date": "2022-09-01 09:13:33 -0600 io_uring/notif: remove notif registration"
    },
    {
        "commit": "d9808ceb3129b811becebdee3ec96d189c83e56c",
        "message": "This reverts commit 4379d5f15b3fd4224c37841029178aa8082a242e.\n\nWe removed notification flushing, also cleanup uapi preparation changes\nto not pollute it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/89edc3905350f91e1b6e26d9dbf42ee44fd451a2.1662027856.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc4",
        "release_date": "2022-09-01 09:13:33 -0600 Revert \"io_uring: rename IORING_OP_FILES_UPDATE\""
    },
    {
        "commit": "23c12d5fc02fb0712c64f3e87a27fcfa78e8af9c",
        "message": "This reverts commit 492dddb4f6e3a5839c27d41ff1fecdbe6c3ab851.\n\nSoon we won't have the very notion of notification flushing, so remove\nnotification flushing requests.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8850334ca56e65b413cb34fd158db81d7b2865a3.1662027856.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc4",
        "release_date": "2022-09-01 09:13:33 -0600 Revert \"io_uring: add zc notification flush requests\""
    },
    {
        "commit": "75847100c351c7a49dddd60d1d023bd3e6640682",
        "message": "We're going to change API, to avoid build problems with a couple of\nfollowing commits, disable io_uring testing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/12b7507223df04fbd12aa05fc0cb544b51d7ed79.1662027856.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc4",
        "release_date": "2022-09-01 09:13:33 -0600 selftests/net: temporarily disable io_uring zc test"
    },
    {
        "commit": "9c9d1896fa92e05e7af5a7a47e335f834aa4248c",
        "message": "Pull LSM support for IORING_OP_URING_CMD from Paul Moore:\n \"Add SELinux and Smack controls to the io_uring IORING_OP_URING_CMD.\n\n  These are necessary as without them the IORING_OP_URING_CMD remains\n  outside the purview of the LSMs (Luis' LSM patch, Casey's Smack patch,\n  and my SELinux patch). They have been discussed at length with the\n  io_uring folks, and Jens has given his thumbs-up on the relevant\n  patches (see the commit descriptions).\n\n  There is one patch that is not strictly necessary, but it makes\n  testing much easier and is very trivial: the /dev/null\n  IORING_OP_URING_CMD patch.\"\n\n* tag 'lsm-pr-20220829' of git://git.kernel.org/pub/scm/linux/kernel/git/pcmoore/lsm:\n  Smack: Provide read control for io_uring_cmd\n  /dev/null: add IORING_OP_URING_CMD support\n  selinux: implement the security_uring_cmd() LSM hook\n  lsm,io_uring: add LSM hooks for the new uring_cmd file op",
        "kernel_version": "v6.0-rc4",
        "release_date": "2022-08-31 09:23:16 -0700 Merge tag 'lsm-pr-20220829' of git://git.kernel.org/pub/scm/linux/kernel/git/pcmoore/lsm"
    },
    {
        "commit": "8379c0b31fbc5d20946f617f8e2fe4791e6f58c1",
        "message": "Pull btrfs fixes from David Sterba:\n \"Fixes:\n\n   - check that subvolume is writable when changing xattrs from security\n     namespace\n\n   - fix memory leak in device lookup helper\n\n   - update generation of hole file extent item when merging holes\n\n   - fix space cache corruption and potential double allocations; this\n     is a rare bug but can be serious once it happens, stable backports\n     and analysis tool will be provided\n\n   - fix error handling when deleting root references\n\n   - fix crash due to assert when attempting to cancel suspended device\n     replace, add message what to do if mount fails due to missing\n     replace item\n\n  Regressions:\n\n   - don't merge pages into bio if their page offset is not contiguous\n\n   - don't allow large NOWAIT direct reads, this could lead to short\n     reads eg. in io_uring\"\n\n* tag 'for-6.0-rc3-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux:\n  btrfs: add info when mount fails due to stale replace target\n  btrfs: replace: drop assert for suspended replace\n  btrfs: fix silent failure when deleting root reference\n  btrfs: fix space cache corruption and potential double allocations\n  btrfs: don't allow large NOWAIT direct reads\n  btrfs: don't merge pages into bio if their page offset is not contiguous\n  btrfs: update generation of hole file extent item when merging holes\n  btrfs: fix possible memory leak in btrfs_get_dev_args_from_path()\n  btrfs: check if root is readonly while setting security xattr",
        "kernel_version": "v6.0-rc3",
        "release_date": "2022-08-28 10:44:04 -0700 Merge tag 'for-6.0-rc3-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux"
    },
    {
        "commit": "dd9373402280cf4715fdc8fd5070f7d039e43511",
        "message": "Limit io_uring \"cmd\" options to files for which the caller has\nSmack read access. There may be cases where the cmd option may\nbe closer to a write access than a read, but there is no way\nto make that determination.\n\nCc: stable@vger.kernel.org\nFixes: ee692a21e9bf (\"fs,io_uring: add infrastructure for uring-cmd\")\nSigned-off-by: Casey Schaufler <casey@schaufler-ca.com>\nSigned-off-by: Paul Moore <paul@paul-moore.com>",
        "kernel_version": "v6.0-rc4",
        "release_date": "2022-08-26 14:56:35 -0400 Smack: Provide read control for io_uring_cmd"
    },
    {
        "commit": "0b0861eb91cbfdd04d6df5a031152914c1114c18",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Add missing header file to the MAINTAINERS entry for io_uring (Ammar)\n\n - liburing and the kernel ship the same io_uring.h header, but one\n   change we've had for a long time only in liburing is to ensure it's\n   C++ safe. Add extern C around it, so we can more easily sync them in\n   the future (Ammar)\n\n - Fix an off-by-one in the sync cancel added in this merge window (me)\n\n - Error handling fix for passthrough (Kanchan)\n\n - Fix for address saving for async execution for the zc tx support\n   (Pavel)\n\n - Fix ordering for TCP zc notifications, so we always have them ordered\n   correctly between \"data was sent\" and \"data was acked\". This isn't\n   strictly needed with the notification slots, but we've been pondering\n   disabling the slot support for 6.0 - and if we do, then we do require\n   the ordering to be sane. Regardless of that, it's the sane thing to\n   do in terms of API (Pavel)\n\n - Minor cleanup for indentation and lockdep annotation (Pavel)\n\n* tag 'io_uring-6.0-2022-08-26' of git://git.kernel.dk/linux-block:\n  io_uring/net: save address for sendzc async execution\n  io_uring: conditional ->async_data allocation\n  io_uring/notif: order notif vs send CQEs\n  io_uring/net: fix indentation\n  io_uring/net: fix zc send link failing\n  io_uring/net: fix must_hold annotation\n  io_uring: fix submission-failure handling for uring-cmd\n  io_uring: fix off-by-one in sync cancelation file check\n  io_uring: uapi: Add `extern \"C\"` in io_uring.h for liburing\n  MAINTAINERS: Add `include/linux/io_uring_types.h`",
        "kernel_version": "v6.0-rc3",
        "release_date": "2022-08-26 11:01:52 -0700 Merge tag 'io_uring-6.0-2022-08-26' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "dfb58b1796d19c8405a38eb457f97669440c59d4",
        "message": "Length parameter of io_sg_from_iter() can be smaller than the iterator's\nsize, as it's with TCP, so when we set from->count at the end of the\nfunction we truncate the iterator forcing TCP to return preliminary with\na short send. It affects zerocopy sends with large payload sizes and\nleads to retries and possible request failures.\n\nFixes: 3ff1a0d395c00 (\"io_uring: enable managed frags with register buffers\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0bc0d5179c665b4ef5c328377c84c7a1f298467e.1661530037.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc4",
        "release_date": "2022-08-26 10:31:42 -0600 io_uring/net: fix overexcessive retries"
    },
    {
        "commit": "707527956d90ea4f304188555a97144183af1e49",
        "message": "This patch adds support for the io_uring command pass through, aka\nIORING_OP_URING_CMD, to the /dev/null driver.  As with all of the\n/dev/null functionality, the implementation is just a simple sink\nwhere commands go to die, but it should be useful for developers who\nneed a simple IORING_OP_URING_CMD test device that doesn't require\nany special hardware.\n\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\nAcked-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Paul Moore <paul@paul-moore.com>",
        "kernel_version": "v6.0-rc4",
        "release_date": "2022-08-26 11:19:44 -0400 /dev/null: add IORING_OP_URING_CMD support"
    },
    {
        "commit": "f4d653dcaa4e4056e1630423e6a8ece4869b544f",
        "message": "Add a SELinux access control for the iouring IORING_OP_URING_CMD\ncommand.  This includes the addition of a new permission in the\nexisting \"io_uring\" object class: \"cmd\".  The subject of the new\npermission check is the domain of the process requesting access, the\nobject is the open file which points to the device/file that is the\ntarget of the IORING_OP_URING_CMD operation.  A sample policy rule\nis shown below:\n\n  allow <domain> <file>:io_uring { cmd };\n\nCc: stable@vger.kernel.org\nFixes: ee692a21e9bf (\"fs,io_uring: add infrastructure for uring-cmd\")\nSigned-off-by: Paul Moore <paul@paul-moore.com>",
        "kernel_version": "v6.0-rc4",
        "release_date": "2022-08-26 11:19:43 -0400 selinux: implement the security_uring_cmd() LSM hook"
    },
    {
        "commit": "2a5840124009f133bd09fd855963551fb2cefe22",
        "message": "io-uring cmd support was added through ee692a21e9bf (\"fs,io_uring:\nadd infrastructure for uring-cmd\"), this extended the struct\nfile_operations to allow a new command which each subsystem can use\nto enable command passthrough. Add an LSM specific for the command\npassthrough which enables LSMs to inspect the command details.\n\nThis was discussed long ago without no clear pointer for something\nconclusive, so this enables LSMs to at least reject this new file\noperation.\n\n[0] https://lkml.kernel.org/r/8adf55db-7bab-f59d-d612-ed906b948d19@schaufler-ca.com\n\nCc: stable@vger.kernel.org\nFixes: ee692a21e9bf (\"fs,io_uring: add infrastructure for uring-cmd\")\nSigned-off-by: Luis Chamberlain <mcgrof@kernel.org>\nAcked-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Paul Moore <paul@paul-moore.com>",
        "kernel_version": "v6.0-rc4",
        "release_date": "2022-08-26 11:19:43 -0400 lsm,io_uring: add LSM hooks for the new uring_cmd file op"
    },
    {
        "commit": "581711c46612c1fd7f98960f9ad53f04fdb89853",
        "message": "We usually copy all bits that a request needs from the userspace for\nasync execution, so the userspace can keep them on the stack. However,\nsend zerocopy violates this pattern for addresses and may reloads it\ne.g. from io-wq. Save the address if any in ->async_data as usual.\n\nReported-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d7512d7aa9abcd36e9afe1a4d292a24cb2d157e5.1661342812.git.asml.silence@gmail.com\n[axboe: fold in incremental fix]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc3",
        "release_date": "2022-08-25 07:52:30 -0600 io_uring/net: save address for sendzc async execution"
    },
    {
        "commit": "5916943943d19a854238d50d1fe2047467cbeb3c",
        "message": "There are opcodes that need ->async_data only in some cases and\nallocation it unconditionally may hurt performance. Add an option to\nopdef to make move the allocation part from the core io_uring to opcode\nspecific code.\nNote, we can't just set opdef->async_size to zero because there are\nother helpers that rely on it, e.g. io_alloc_async_data().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9dc62be9e88dd0ed63c48365340e8922d2498293.1661342812.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc3",
        "release_date": "2022-08-24 08:57:28 -0600 io_uring: conditional ->async_data allocation"
    },
    {
        "commit": "53bdc88aac9a21aae937452724fa4738cd843795",
        "message": "Currently, there is no ordering between notification CQEs and\ncompletions of the send flushing it, this quite complicates the\nuserspace, especially since we don't flush notification when the\nsend(+flush) request fails, i.e. there will be only one CQE. What we\ncan do is to make sure that notification completions come only after\nsends.\n\nThe easiest way to achieve this is to not try to complete a notification\ninline from io_sendzc() but defer it to task_work, considering that\nio-wq sendzc is disallowed CQEs will be naturally ordered because\ntask_works will only be executed after we're done with submission and so\ninline completion.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/cddfd1c2bf91f22b9fe08e13b7dffdd8f858a151.1661342812.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc3",
        "release_date": "2022-08-24 08:57:28 -0600 io_uring/notif: order notif vs send CQEs"
    },
    {
        "commit": "986e263def32eec89153babf469859d837507d34",
        "message": "Fix up indentation before we get complaints from tooling.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bd5754e3764215ccd7fb04cd636ea9167aaa275d.1661342812.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc3",
        "release_date": "2022-08-24 08:57:15 -0600 io_uring/net: fix indentation"
    },
    {
        "commit": "5a848b7c9e5e4d94390fbc391ccb81d40f3ccfb5",
        "message": "Failed requests should be marked with req_set_fail(), so links and cqe\nskipping work correctly, which is missing in io_sendzc(). Note,\nio_sendzc() return IOU_OK on failure, so the core code won't do the\ncleanup for us.\n\nFixes: 06a5464be84e4 (\"io_uring: wire send zc request type\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e47d46fda9db30154ce66a549bb0d3380b780520.1661342812.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc3",
        "release_date": "2022-08-24 08:57:00 -0600 io_uring/net: fix zc send link failing"
    },
    {
        "commit": "2cacedc873ab5f5945d8f1b71804b0bcea0383ff",
        "message": "Fix up the io_alloc_notif()'s __must_hold as we don't have a ctx\nargument there but should get it from the slot instead.\n\nReported-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/cbb0a920f18e0aed590bf58300af817b9befb8a3.1661342812.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc3",
        "release_date": "2022-08-24 08:57:00 -0600 io_uring/net: fix must_hold annotation"
    },
    {
        "commit": "a9c3eda7eada94e8cf29cb102aa80e1370d8fa2e",
        "message": "If ->uring_cmd returned an error value different from -EAGAIN or\n-EIOCBQUEUED, it gets overridden with IOU_OK. This invites trouble\nas caller (io_uring core code) handles IOU_OK differently than other\nerror codes.\nFix this by returning the actual error code.\n\nSigned-off-by: Kanchan Joshi <joshi.k@samsung.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc3",
        "release_date": "2022-08-23 09:46:17 -0600 io_uring: fix submission-failure handling for uring-cmd"
    },
    {
        "commit": "47abea041f897d64dbd5777f0cf7745148f85d75",
        "message": "The passed in index should be validated against the number of registered\nfiles we have, it needs to be smaller than the index value to avoid going\none beyond the end.\n\nFixes: 78a861b94959 (\"io_uring: add sync cancelation API through io_uring_register()\")\nReported-by: Luo Likang <luolikang@nsfocus.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc3",
        "release_date": "2022-08-23 07:26:08 -0600 io_uring: fix off-by-one in sync cancelation file check"
    },
    {
        "commit": "e1d0c6d05afdcff01ace698edb3b8808db1dc066",
        "message": "Make it easy for liburing to integrate uapi header with the kernel.\nPreviously, when this header changes, the liburing side can't directly\ncopy this header file due to some small differences. Sync them.\n\nLink: https://lore.kernel.org/io-uring/f1feef16-6ea2-0653-238f-4aaee35060b6@kernel.dk\nCc: Bart Van Assche <bvanassche@acm.org>\nCc: Dylan Yudaken <dylany@fb.com>\nCc: Facebook Kernel Team <kernel-team@fb.com>\nSigned-off-by: Ammar Faizi <ammarfaizi2@gnuweeb.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc3",
        "release_date": "2022-08-23 07:22:33 -0600 io_uring: uapi: Add `extern \"C\"` in io_uring.h for liburing"
    },
    {
        "commit": "c2fa700c5f7667800b6cfedcb9994eb5eb69e6cc",
        "message": "File include/linux/io_uring_types.h doesn't have a maintainer, add it\nto the io_uring section.\n\nSigned-off-by: Ammar Faizi <ammarfaizi2@gnuweeb.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc3",
        "release_date": "2022-08-23 07:22:32 -0600 MAINTAINERS: Add `include/linux/io_uring_types.h`"
    },
    {
        "commit": "79d3d1d12e6f22c904195f9356069859e2595f00",
        "message": "Dylan and Jens reported a problem where they had an io_uring test that\nwas returning short reads, and bisected it to ee5b46a353af (\"btrfs:\nincrease direct io read size limit to 256 sectors\").\n\nThe root cause is their test was doing larger reads via io_uring with\nNOWAIT and async.  This was triggering a page fault during the direct\nread, however the first page was able to work just fine and thus we\nsubmitted a 4k read for a larger iocb.\n\nBtrfs allows for partial IO's in this case specifically because we don't\nallow page faults, and thus we'll attempt to do any io that we can,\nsubmit what we could, come back and fault in the rest of the range and\ntry to do the remaining IO.\n\nHowever for !is_sync_kiocb() we'll call ->ki_complete() as soon as the\npartial dio is done, which is incorrect.  In the sync case we can exit\nthe iomap code, submit more io's, and return with the amount of IO we\nwere able to complete successfully.\n\nWe were always doing short reads in this case, but for NOWAIT we were\ngetting saved by the fact that we were limiting direct reads to\nsectorsize, and if we were larger than that we would return EAGAIN.\n\nFix the regression by simply returning EAGAIN in the NOWAIT case with\nlarger reads, that way io_uring can retry and get the larger IO and have\nthe fault logic handle everything properly.\n\nThis still leaves the AIO short read case, but that existed before this\nchange.  The way to properly fix this would be to handle partial iocb\ncompletions, but that's a lot of work, for now deal with the regression\nin the most straightforward way possible.\n\nReported-by: Dylan Yudaken <dylany@fb.com>\nFixes: ee5b46a353af (\"btrfs: increase direct io read size limit to 256 sectors\")\nReviewed-by: Filipe Manana <fdmanana@suse.com>\nSigned-off-by: Josef Bacik <josef@toxicpanda.com>\nSigned-off-by: David Sterba <dsterba@suse.com>",
        "kernel_version": "v6.0-rc3",
        "release_date": "2022-08-22 18:08:07 +0200 btrfs: don't allow large NOWAIT direct reads"
    },
    {
        "commit": "beaf139709542d4ee7d22c2312b97c35587eb9db",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A few fixes for regressions in this cycle:\n\n   - Two instances of using the wrong \"has async data\" helper (Pavel)\n\n   - Fixup zero-copy address import (Pavel)\n\n   - Bump zero-copy notification slot limit (Pavel)\"\n\n* tag 'io_uring-6.0-2022-08-19' of git://git.kernel.dk/linux-block:\n  io_uring/net: use right helpers for async_data\n  io_uring/notif: raise limit on notification slots\n  io_uring/net: improve zc addr import error handling\n  io_uring/net: use right helpers for async recycle",
        "kernel_version": "v6.0-rc2",
        "release_date": "2022-08-20 09:49:22 -0700 Merge tag 'io_uring-6.0-2022-08-19' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "3f743e9bbb8fe20f4c477e4bf6341c4187a4a264",
        "message": "There is another spot where we check ->async_data directly instead of\nusing req_has_async_data(), which is the way to do it, fix it up.\n\nFixes: 43e0bbbd0b0e3 (\"io_uring: add netmsg cache\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/42f33b9a81dd6ae65dda92f0372b0ff82d548517.1660822636.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc2",
        "release_date": "2022-08-18 07:27:20 -0600 io_uring/net: use right helpers for async_data"
    },
    {
        "commit": "e6190dd0031d335c22586d34ef898301ed20f230",
        "message": "In ublk_queue_rq(), Assume current request is a re-issued request aborted\npreviously in monitor_work because the ubq_daemon(ioucmd's task) is\nPF_EXITING. For this request, we cannot call\nio_uring_cmd_complete_in_task() anymore because at that moment io_uring\ncontext may be freed in case that no inflight ioucmd exists. Otherwise,\nwe may cause null-deref in ctx->fallback_work.\n\nAdd a check on UBLK_IO_FLAG_ABORTED to prevent the above situation. This\ncheck is safe and makes sense.\n\nNote: monitor_work sets UBLK_IO_FLAG_ABORTED and ends this request\n(releasing the tag). Then the request is restarted(allocating the tag)\nand we are here. Since releasing/allocating a tag implies smp_mb(),\nfinding UBLK_IO_FLAG_ABORTED guarantees that here is a re-issued request\naborted previously.\n\nSuggested-by: Ming Lei <ming.lei@redhat.com>\nSigned-off-by: ZiyangZhang <ZiyangZhang@linux.alibaba.com>\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20220815023633.259825-4-ZiyangZhang@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc2",
        "release_date": "2022-08-16 06:16:19 -0600 ublk_drv: do not add a re-issued request aborted previously to ioucmd's task_work"
    },
    {
        "commit": "5993000dc6b31b927403cee65fbc5f9f070fa3e4",
        "message": "1024 notification slots is rather an arbitrary value, raise it up,\neverything is accounted to memcg.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/eb78a0a5f2fa5941f8e845cdae5fb399bf7ba0be.1660566179.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc2",
        "release_date": "2022-08-15 21:34:00 -0600 io_uring/notif: raise limit on notification slots"
    },
    {
        "commit": "86dc8f23bb1b68262ca5db890ec7177b2d074640",
        "message": "We may account memory to a memcg of a request that didn't even got to\nthe network layer. It's not a bug as it'll be routinely cleaned up on\nflush, but it might be confusing for the userspace.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b8aae61f4c3ddc4da97c1da876bb73871f352d50.1660566179.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc2",
        "release_date": "2022-08-15 21:34:00 -0600 io_uring/net: improve zc addr import error handling"
    },
    {
        "commit": "063604265f967e90901996a1b173fe6df582d350",
        "message": "We have a helper that checks for whether a request contains anything in\n->async_data or not, namely req_has_async_data(). It's better to use it\nas it might have some extra considerations.\n\nFixes: 43e0bbbd0b0e3 (\"io_uring: add netmsg cache\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b7414da4e7c3c32c31fc02dfd1355af4ccf4ca5f.1660566179.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc2",
        "release_date": "2022-08-15 21:34:00 -0600 io_uring/net: use right helpers for async recycle"
    },
    {
        "commit": "1da8cf961bb13f4c3ea11373696b5ac986a47cde",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Regression fix for this merge window, fixing a wrong order of\n   arguments for io_req_set_res() for passthru (Dylan)\n\n - Fix for the audit code leaking context memory (Peilin)\n\n - Ensure that provided buffers are memcg accounted (Pavel)\n\n - Correctly handle short zero-copy sends (Pavel)\n\n - Sparse warning fixes for the recvmsg multishot command (Dylan)\n\n - Error handling fix for passthru (Anuj)\n\n - Remove randomization of struct kiocb fields, to avoid it growing in\n   size if re-arranged in such a fashion that it grows more holes or\n   padding (Keith, Linus)\n\n - Small series improving type safety of the sqe fields (Stefan)\n\n* tag 'io_uring-6.0-2022-08-13' of git://git.kernel.dk/linux-block:\n  io_uring: add missing BUILD_BUG_ON() checks for new io_uring_sqe fields\n  io_uring: make io_kiocb_to_cmd() typesafe\n  fs: don't randomize struct kiocb fields\n  io_uring: consistently make use of io_notif_to_data()\n  io_uring: fix error handling for io_uring_cmd\n  io_uring: fix io_recvmsg_prep_multishot sparse warnings\n  io_uring/net: send retry for zerocopy\n  io_uring: mem-account pbuf buckets\n  audit, io_uring, io-wq: Fix memory leak in io_sq_thread() and io_wqe_worker()\n  io_uring: pass correct parameters to io_req_set_res",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-08-13 13:28:54 -0700 Merge tag 'io_uring-6.0-2022-08-13' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "92cb6e2e5dbaea02c2fa317f3543c8918db25e89",
        "message": "If ublksrv sends UBLK_IO_NEED_GET_DATA with new allocated io buffer, we\nhave to update iod->addr in task_work before calling io_uring_cmd_done().\nThen usersapce target can handle (write)io request with the new io\nbuffer reading from updated iod.\n\nWithout this change, userspace target may touch a wrong io buffer!\n\nSigned-off-by: ZiyangZhang <ZiyangZhang@linux.alibaba.com>\nReviewed-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20220810055212.66417-1-ZiyangZhang@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc2",
        "release_date": "2022-08-13 08:35:28 -0600 ublk_drv: update iod->addr for UBLK_IO_NEED_GET_DATA"
    },
    {
        "commit": "9c71d39aa0f40d4e6bfe14958045a42c722bd327",
        "message": "Signed-off-by: Stefan Metzmacher <metze@samba.org>\nLink: https://lore.kernel.org/r/ffcaf8dc4778db4af673822df60dbda6efdd3065.1660201408.git.metze@samba.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-08-12 17:01:00 -0600 io_uring: add missing BUILD_BUG_ON() checks for new io_uring_sqe fields"
    },
    {
        "commit": "f2ccb5aed7bce1d8b3ed5b3385759a5509663028",
        "message": "We need to make sure (at build time) that struct io_cmd_data is not\ncasted to a structure that's larger.\n\nSigned-off-by: Stefan Metzmacher <metze@samba.org>\nLink: https://lore.kernel.org/r/c024cdf25ae19fc0319d4180e2298bade8ed17b8.1660201408.git.metze@samba.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-08-12 17:01:00 -0600 io_uring: make io_kiocb_to_cmd() typesafe"
    },
    {
        "commit": "addebd9ac9ca0ef8b3764907bf8018e48caffc64",
        "message": "This is a size sensitive structure and randomizing can introduce extra\npadding that breaks io_uring's fixed size expectations. There are few\nfields here as it is, half of which need a fixed order to optimally\npack, so the randomization isn't providing much.\n\nSuggested-by: Linus Torvalds <torvalds@linux-foundation.org>\nSigned-off-by: Keith Busch <kbusch@kernel.org>\nLink: https://lore.kernel.org/io-uring/b6f508ca-b1b2-5f40-7998-e4cff1cf7212@kernel.dk/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-08-12 17:00:35 -0600 fs: don't randomize struct kiocb fields"
    },
    {
        "commit": "da2634e89caa40d7546b0566ab80ff31567861c9",
        "message": "This makes the assignment typesafe. It prepares\nchanging io_kiocb_to_cmd() in the next commit.\n\nSigned-off-by: Stefan Metzmacher <metze@samba.org>\nLink: https://lore.kernel.org/r/8da6e9d12cf95ad4bc73274406d12bca7aabf72e.1660201408.git.metze@samba.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-08-11 10:56:13 -0600 io_uring: consistently make use of io_notif_to_data()"
    },
    {
        "commit": "3ed159c984079baedff740505d609badb8538e0d",
        "message": "Commit 97b388d70b53 (\"io_uring: handle completions in the core\") moved the\nerror handling from handler to core. But for io_uring_cmd handler we end\nup completing more than once (both in handler and in core) leading to\nuse_after_free.\nChange io_uring_cmd handler to avoid calling io_uring_cmd_done in case\nof error.\n\nFixes: 97b388d70b53 (\"io_uring: handle completions in the core\")\nSigned-off-by: Anuj Gupta <anuj20.g@samsung.com>\nSigned-off-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220811091459.6929-1-anuj20.g@samsung.com\n[axboe: fix ret vs req typo]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-08-11 10:56:00 -0600 io_uring: fix error handling for io_uring_cmd"
    },
    {
        "commit": "d1f6222c4978817712e0f2825ce9e830763f0695",
        "message": "Fix casts missing the __user parts. This seemed to only cause errors on\nthe alpha build, or if checked with sparse, but it was definitely an\noversight.\n\nReported-by: kernel test robot <lkp@intel.com>\nFixes: 9bb66906f23e (\"io_uring: support multishot in recvmsg\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220805115450.3921352-1-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-08-05 08:41:18 -0600 io_uring: fix io_recvmsg_prep_multishot sparse warnings"
    },
    {
        "commit": "4a933e62083ead6cd064293a7505c56165859320",
        "message": "io_uring handles short sends/recvs for stream sockets when MSG_WAITALL\nis set, however new zerocopy send is inconsistent in this regard, which\nmight be confusing. Handle short sends.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b876a4838597d9bba4f3215db60d72c33c448ad0.1659622472.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-08-04 08:35:16 -0600 io_uring/net: send retry for zerocopy"
    },
    {
        "commit": "cc18cc5e82033d406f54144ad6f8092206004684",
        "message": "Potentially, someone may create as many pbuf bucket as there are indexes\nin an xarray without any other restrictions bounding our memory usage,\nput memory needed for the buckets under memory accounting.\n\nCc: <stable@vger.kernel.org>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d34c452e45793e978d26e2606211ec9070d329ea.1659622312.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-08-04 08:35:07 -0600 io_uring: mem-account pbuf buckets"
    },
    {
        "commit": "f482aa98652795846cc55da98ebe331eb74f3d0b",
        "message": "Currently @audit_context is allocated twice for io_uring workers:\n\n  1. copy_process() calls audit_alloc();\n  2. io_sq_thread() or io_wqe_worker() calls audit_alloc_kernel() (which\n     is effectively audit_alloc()) and overwrites @audit_context,\n     causing:\n\n  BUG: memory leak\n  unreferenced object 0xffff888144547400 (size 1024):\n<...>\n    hex dump (first 32 bytes):\n      00 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00  ................\n      00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................\n    backtrace:\n      [<ffffffff8135cfc3>] audit_alloc+0x133/0x210\n      [<ffffffff81239e63>] copy_process+0xcd3/0x2340\n      [<ffffffff8123b5f3>] create_io_thread+0x63/0x90\n      [<ffffffff81686604>] create_io_worker+0xb4/0x230\n      [<ffffffff81686f68>] io_wqe_enqueue+0x248/0x3b0\n      [<ffffffff8167663a>] io_queue_iowq+0xba/0x200\n      [<ffffffff816768b3>] io_queue_async+0x113/0x180\n      [<ffffffff816840df>] io_req_task_submit+0x18f/0x1a0\n      [<ffffffff816841cd>] io_apoll_task_func+0xdd/0x120\n      [<ffffffff8167d49f>] tctx_task_work+0x11f/0x570\n      [<ffffffff81272c4e>] task_work_run+0x7e/0xc0\n      [<ffffffff8125a688>] get_signal+0xc18/0xf10\n      [<ffffffff8111645b>] arch_do_signal_or_restart+0x2b/0x730\n      [<ffffffff812ea44e>] exit_to_user_mode_prepare+0x5e/0x180\n      [<ffffffff844ae1b2>] syscall_exit_to_user_mode+0x12/0x20\n      [<ffffffff844a7e80>] do_syscall_64+0x40/0x80\n\nThen,\n\n  3. io_sq_thread() or io_wqe_worker() frees @audit_context using\n     audit_free();\n  4. do_exit() eventually calls audit_free() again, which is okay\n     because audit_free() does a NULL check.\n\nAs suggested by Paul Moore, fix it by deleting audit_alloc_kernel() and\nredundant audit_free() calls.\n\nFixes: 5bd2182d58e9 (\"audit,io_uring,io-wq: add some basic audit support to io_uring\")\nSuggested-by: Paul Moore <paul@paul-moore.com>\nCc: stable@vger.kernel.org\nSigned-off-by: Peilin Ye <peilin.ye@bytedance.com>\nAcked-by: Paul Moore <paul@paul-moore.com>\nLink: https://lore.kernel.org/r/20220803222343.31673-1-yepeilin.cs@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-08-04 08:33:54 -0600 audit, io_uring, io-wq: Fix memory leak in io_sq_thread() and io_wqe_worker()"
    },
    {
        "commit": "ff2557b7224ea9a19fb79eb4bd16d4deef57816a",
        "message": "The two parameters of 'res' and 'cflags' are swapped, so fix it.\nWithout this fix, 'ublk del' hangs forever.\n\nCc: Pavel Begunkov <asml.silence@gmail.com>\nFixes: de23077eda61f (\"io_uring: set completion results upfront\")\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20220803120757.1668278-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-08-03 08:45:25 -0600 io_uring: pass correct parameters to io_req_set_res"
    },
    {
        "commit": "a8ce5f52efce3b89ca82ce8798d0d061117465d2",
        "message": "Each ublk queue is started before adding disk, we have to cancel queues in\nublk_stop_dev() so that ubq daemon can be exited, otherwise DEL_DEV command\nmay hang forever.\n\nAlso avoid to cancel queues two times by checking if queue is ready,\notherwise use-after-free on io_uring may be triggered because ublk_stop_dev\nis called by ublk_remove() too.\n\nFixes: 71f28f3136af (\"ublk_drv: add io_uring based userspace block driver\")\nReviewed-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20220730092750.1118167-2-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-08-02 21:13:40 -0600 ublk_drv: cancel device even though disk isn't up"
    },
    {
        "commit": "c013d0af81f60cc7dbe357c4e2a925fb6738dbfe",
        "message": "Pull block updates from Jens Axboe:\n\n - Improve the type checking of request flags (Bart)\n\n - Ensure queue mapping for a single queues always picks the right queue\n   (Bart)\n\n - Sanitize the io priority handling (Jan)\n\n - rq-qos race fix (Jinke)\n\n - Reserved tags handling improvements (John)\n\n - Separate memory alignment from file/disk offset aligment for O_DIRECT\n   (Keith)\n\n - Add new ublk driver, userspace block driver using io_uring for\n   communication with the userspace backend (Ming)\n\n - Use try_cmpxchg() to cleanup the code in various spots (Uros)\n\n - Finally remove bdevname() (Christoph)\n\n - Clean up the zoned device handling (Christoph)\n\n - Clean up independent access range support (Christoph)\n\n - Clean up and improve block sysfs handling (Christoph)\n\n - Clean up and improve teardown of block devices.\n\n   This turns the usual two step process into something that is simpler\n   to implement and handle in block drivers (Christoph)\n\n - Clean up chunk size handling (Christoph)\n\n - Misc cleanups and fixes (Bart, Bo, Dan, GuoYong, Jason, Keith, Liu,\n   Ming, Sebastian, Yang, Ying)\n\n* tag 'for-5.20/block-2022-07-29' of git://git.kernel.dk/linux-block: (178 commits)\n  ublk_drv: fix double shift bug\n  ublk_drv: make sure that correct flags(features) returned to userspace\n  ublk_drv: fix error handling of ublk_add_dev\n  ublk_drv: fix lockdep warning\n  block: remove __blk_get_queue\n  block: call blk_mq_exit_queue from disk_release for never added disks\n  blk-mq: fix error handling in __blk_mq_alloc_disk\n  ublk: defer disk allocation\n  ublk: rewrite ublk_ctrl_get_queue_affinity to not rely on hctx->cpumask\n  ublk: fold __ublk_create_dev into ublk_ctrl_add_dev\n  ublk: cleanup ublk_ctrl_uring_cmd\n  ublk: simplify ublk_ch_open and ublk_ch_release\n  ublk: remove the empty open and release block device operations\n  ublk: remove UBLK_IO_F_PREFLUSH\n  ublk: add a MAINTAINERS entry\n  block: don't allow the same type rq_qos add more than once\n  mmc: fix disk/queue leak in case of adding disk failure\n  ublk_drv: fix an IS_ERR() vs NULL check\n  ublk: remove UBLK_IO_F_INTEGRITY\n  ublk_drv: remove unneeded semicolon\n  ...",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-08-02 13:46:35 -0700 Merge tag 'for-5.20/block-2022-07-29' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "42df1cbf6a4726934cc5dac12bf263aa73c49fa3",
        "message": "Pull io_uring zerocopy support from Jens Axboe:\n \"This adds support for efficient support for zerocopy sends through\n  io_uring. Both ipv4 and ipv6 is supported, as well as both TCP and\n  UDP.\n\n  The core network changes to support this is in a stable branch from\n  Jakub that both io_uring and net-next has pulled in, and the io_uring\n  changes are layered on top of that.\n\n  All of the work has been done by Pavel\"\n\n* tag 'for-5.20/io_uring-zerocopy-send-2022-07-29' of git://git.kernel.dk/linux-block: (34 commits)\n  io_uring: notification completion optimisation\n  io_uring: export req alloc from core\n  io_uring/net: use unsigned for flags\n  io_uring/net: make page accounting more consistent\n  io_uring/net: checks errors of zc mem accounting\n  io_uring/net: improve io_get_notif_slot types\n  selftests/io_uring: test zerocopy send\n  io_uring: enable managed frags with register buffers\n  io_uring: add zc notification flush requests\n  io_uring: rename IORING_OP_FILES_UPDATE\n  io_uring: flush notifiers after sendzc\n  io_uring: sendzc with fixed buffers\n  io_uring: allow to pass addr into sendzc\n  io_uring: account locked pages for non-fixed zc\n  io_uring: wire send zc request type\n  io_uring: add notification slot registration\n  io_uring: add rsrc referencing for notifiers\n  io_uring: complete notifiers in tw\n  io_uring: cache struct io_notif\n  io_uring: add zc notification infrastructure\n  ...",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-08-02 13:37:55 -0700 Merge tag 'for-5.20/io_uring-zerocopy-send-2022-07-29' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "98e247464088a11ce2328a214fdb87d4c06f8db6",
        "message": "Pull io_uring buffered writes support from Jens Axboe:\n \"This contains support for buffered writes, specifically for XFS. btrfs\n  is in progress, will be coming in the next release.\n\n  io_uring does support buffered writes on any file type, but since the\n  buffered write path just always -EAGAIN (or -EOPNOTSUPP) any attempt\n  to do so if IOCB_NOWAIT is set, any buffered write will effectively be\n  handled by io-wq offload. This isn't very efficient, and we even have\n  specific code in io-wq to serialize buffered writes to the same inode\n  to avoid further inefficiencies with thread offload.\n\n  This is particularly sad since most buffered writes don't block, they\n  simply copy data to a page and dirty it. With this pull request, we\n  can handle buffered writes a lot more effiently.\n\n  If balance_dirty_pages() needs to block, we back off on writes as\n  indicated.\n\n  This improves buffered write support by 2-3x.\n\n  Jan Kara helped with the mm bits for this, and Stefan handled the\n  fs/iomap/xfs/io_uring parts of it\"\n\n* tag 'for-5.20/io_uring-buffered-writes-2022-07-29' of git://git.kernel.dk/linux-block:\n  mm: honor FGP_NOWAIT for page cache page allocation\n  xfs: Add async buffered write support\n  xfs: Specify lockmode when calling xfs_ilock_for_iomap()\n  io_uring: Add tracepoint for short writes\n  io_uring: fix issue with io_write() not always undoing sb_start_write()\n  io_uring: Add support for async buffered writes\n  fs: Add async write file modification handling.\n  fs: Split off inode_needs_update_time and __file_update_time\n  fs: add __remove_file_privs() with flags parameter\n  fs: add a FMODE_BUF_WASYNC flags for f_mode\n  iomap: Return -EAGAIN from iomap_write_iter()\n  iomap: Add async buffered write support\n  iomap: Add flags parameter to iomap_page_create()\n  mm: Add balance_dirty_pages_ratelimited_flags() function\n  mm: Move updates of dirty_exceeded into one place\n  mm: Move starting of background writeback into the main balancing loop",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-08-02 13:27:23 -0700 Merge tag 'for-5.20/io_uring-buffered-writes-2022-07-29' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "b349b1181d24af1c151134a3c39725e94a5619dd",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - As per (valid) complaint in the last merge window, fs/io_uring.c has\n   grown quite large these days. io_uring isn't really tied to fs\n   either, as it supports a wide variety of functionality outside of\n   that.\n\n   Move the code to io_uring/ and split it into files that either\n   implement a specific request type, and split some code into helpers\n   as well. The code is organized a lot better like this, and io_uring.c\n   is now < 4K LOC (me).\n\n - Deprecate the epoll_ctl opcode. It'll still work, just trigger a\n   warning once if used. If we don't get any complaints on this, and I\n   don't expect any, then we can fully remove it in a future release\n   (me).\n\n - Improve the cancel hash locking (Hao)\n\n - kbuf cleanups (Hao)\n\n - Efficiency improvements to the task_work handling (Dylan, Pavel)\n\n - Provided buffer improvements (Dylan)\n\n - Add support for recv/recvmsg multishot support. This is similar to\n   the accept (or poll) support for have for multishot, where a single\n   SQE can trigger everytime data is received. For applications that\n   expect to do more than a few receives on an instantiated socket, this\n   greatly improves efficiency (Dylan).\n\n - Efficiency improvements for poll handling (Pavel)\n\n - Poll cancelation improvements (Pavel)\n\n - Allow specifiying a range for direct descriptor allocations (Pavel)\n\n - Cleanup the cqe32 handling (Pavel)\n\n - Move io_uring types to greatly cleanup the tracing (Pavel)\n\n - Tons of great code cleanups and improvements (Pavel)\n\n - Add a way to do sync cancelations rather than through the sqe -> cqe\n   interface, as that's a lot easier to use for some use cases (me).\n\n - Add support to IORING_OP_MSG_RING for sending direct descriptors to a\n   different ring. This avoids the usually problematic SCM case, as we\n   disallow those. (me)\n\n - Make the per-command alloc cache we use for apoll generic, place\n   limits on it, and use it for netmsg as well (me).\n\n - Various cleanups (me, Michal, Gustavo, Uros)\n\n* tag 'for-5.20/io_uring-2022-07-29' of git://git.kernel.dk/linux-block: (172 commits)\n  io_uring: ensure REQ_F_ISREG is set async offload\n  net: fix compat pointer in get_compat_msghdr()\n  io_uring: Don't require reinitable percpu_ref\n  io_uring: fix types in io_recvmsg_multishot_overflow\n  io_uring: Use atomic_long_try_cmpxchg in __io_account_mem\n  io_uring: support multishot in recvmsg\n  net: copy from user before calling __get_compat_msghdr\n  net: copy from user before calling __copy_msghdr\n  io_uring: support 0 length iov in buffer select in compat\n  io_uring: fix multishot ending when not polled\n  io_uring: add netmsg cache\n  io_uring: impose max limit on apoll cache\n  io_uring: add abstraction around apoll cache\n  io_uring: move apoll cache to poll.c\n  io_uring: consolidate hash_locked io-wq handling\n  io_uring: clear REQ_F_HASH_LOCKED on hash removal\n  io_uring: don't race double poll setting REQ_F_ASYNC_DATA\n  io_uring: don't miss setting REQ_F_DOUBLE_POLL\n  io_uring: disable multishot recvmsg\n  io_uring: only trace one of complete or overflow\n  ...",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-08-02 13:20:44 -0700 Merge tag 'for-5.20/io_uring-2022-07-29' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "14b146b688ad9593f5eee93d51a34d09a47e50b5",
        "message": "We want to use all optimisations that we have for io_uring requests like\ncompletion batching, memory caching and more but for zc notifications.\nFortunately, notification perfectly fit the request model so we can\noverlay them onto struct io_kiocb and use all the infratructure.\n\nMost of the fields of struct io_notif natively fits into io_kiocb, so we\nreplace struct io_notif with struct io_kiocb carrying struct\nio_notif_data in the cmd cache line. Then we adapt io_alloc_notif() to\nuse io_alloc_req()/io_alloc_req_refill(), and kill leftovers of hand\ncoded caching. __io_notif_complete_tw() is converted to use io_uring's\ntw infra.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9e010125175e80baf51f0ca63bdc7cc6a4a9fa56.1658913593.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-27 08:50:50 -0600 io_uring: notification completion optimisation"
    },
    {
        "commit": "bd1a3783dd749012134b142b52e5704f7c142897",
        "message": "We want to do request allocation out of the core io_uring code, make the\nallocation functions public for other io_uring parts.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0314fedd3a02a514210ba42d4720332538c65956.1658913593.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-27 08:50:50 -0600 io_uring: export req alloc from core"
    },
    {
        "commit": "293402e564a7391f38541c7694e736f5fde20aea",
        "message": "Use unsigned int type for msg flags.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5cfaed13d3191337b14b8664ca68b515d9e2d1b4.1658742118.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-25 09:48:25 -0600 io_uring/net: use unsigned for flags"
    },
    {
        "commit": "6a9ce66f4d0872861e0bbc67eee6ce5dca5dd886",
        "message": "Make network page accounting more consistent with how buffer\nregistration is working, i.e. account all memory to ctx->user.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4aacfe64bbb81b27f9ecf5d5c219c69a07e5aa56.1658742118.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-25 09:48:25 -0600 io_uring/net: make page accounting more consistent"
    },
    {
        "commit": "2e32ba5607ee2b668baa8831dd74f7cc867a1f7e",
        "message": "mm_account_pinned_pages() may fail, don't ignore the return value.\n\nFixes: e29e3bd4b968 (\"io_uring: account locked pages for non-fixed zc\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/dae0542ed8e6706071bb83ad3e7ad6a70d207fd9.1658742118.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-25 09:48:17 -0600 io_uring/net: checks errors of zc mem accounting"
    },
    {
        "commit": "cb309ae49da7a7c28f0051deea13970291134fac",
        "message": "Don't use signed int for slot indexing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e4d15aefebb5e55729dd9b5ec01ab16b70033343.1658742118.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-25 09:47:45 -0600 io_uring/net: improve io_get_notif_slot types"
    },
    {
        "commit": "d8b6171bd58a5ffb9ad1dcfa60372db1b4db6d0d",
        "message": "Add selftests for io_uring zerocopy sends and io_uring's notification\ninfrastructure. It's largely influenced by msg_zerocopy and uses it on\nthe receive side.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/03d5ec78061cf52db420f88ed0b48eb8f47ce9f7.1657643355.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:07 -0600 selftests/io_uring: test zerocopy send"
    },
    {
        "commit": "3ff1a0d395c00e42ee15f561431e0c04097595b9",
        "message": "io_uring's registered buffers infra has a good performant way of pinning\npages, so let's use SKBFL_MANAGED_FRAG_REFS when our requests are purely\nregister buffer backed.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/278731d3f20caf346cfc025fbee0b4c9ee4ed751.1657643355.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:07 -0600 io_uring: enable managed frags with register buffers"
    },
    {
        "commit": "492dddb4f6e3a5839c27d41ff1fecdbe6c3ab851",
        "message": "Overlay notification control onto IORING_OP_RSRC_UPDATE (former\nIORING_OP_FILES_UPDATE). It allows to flush a range of zc notifications\nfrom slots with indexes [sqe->off, sqe->off+sqe->len). If sqe->arg is\nnot zero, it also copies sqe->arg as a new tag for all flushed\nnotifications.\n\nNote, it doesn't flush a notification of a slot if there was no requests\nattached to it (since last flush or registration).\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/df13e2363400682a73dd9e71c3b990b8d1ff0333.1657643355.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:07 -0600 io_uring: add zc notification flush requests"
    },
    {
        "commit": "4379d5f15b3fd4224c37841029178aa8082a242e",
        "message": "IORING_OP_FILES_UPDATE will be a more generic opcode serving different\nresource types, rename it into IORING_OP_RSRC_UPDATE and add subtype\nhandling.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0a907133907d9af3415a8a7aa1802c6aa97c03c6.1657643355.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:07 -0600 io_uring: rename IORING_OP_FILES_UPDATE"
    },
    {
        "commit": "63809137ebb58f0aa2ce359117422686e3304f45",
        "message": "Allow to flush notifiers as a part of sendzc request by setting\nIORING_SENDZC_FLUSH flag. When the sendzc request succeedes it will\nflush the used [active] notifier.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e0b4d9a6797e2fd6092824fe42953db7a519bbc8.1657643355.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:07 -0600 io_uring: flush notifiers after sendzc"
    },
    {
        "commit": "10c7d33ecd51619e453cf6aeee8e326f8ba5cfea",
        "message": "Allow zerocopy sends to use fixed buffers. There is an optimisation for\nthis case, the network layer don't need to reference the pages, see\nSKBFL_MANAGED_FRAG_REFS, so io_uring have to ensure validity of fixed\nbuffers until the notifier is released.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e1d8bd1b5934e541d90c1824eb4020ae3f5f43f3.1657643355.git.asml.silence@gmail.com\n[axboe: fold in 32-bit pointer cast warning fix]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:07 -0600 io_uring: sendzc with fixed buffers"
    },
    {
        "commit": "092aeedb750a9fad0f0252d6067fc91d76ca44bd",
        "message": "Allow to specify an address to zerocopy sends making it more like\nsendto(2).\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/70417a8f7c5b51ab454690bae08adc0c187f89e8.1657643355.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:07 -0600 io_uring: allow to pass addr into sendzc"
    },
    {
        "commit": "e29e3bd4b968d50bfb3bbdcee6bfdc340f7792cf",
        "message": "Fixed buffers are RLIMIT_MEMLOCK accounted, however it doesn't cover iovec\nbased zerocopy sends. Do the accounting on the io_uring side.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/19b6e3975440f59f1f6199c7ee7acf977b4eecdc.1657643355.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:07 -0600 io_uring: account locked pages for non-fixed zc"
    },
    {
        "commit": "06a5464be84e4ae48394d34441baf34bf9706827",
        "message": "Add a new io_uring opcode IORING_OP_SENDZC. The main distinction from\nIORING_OP_SEND is that the user should specify a notification slot\nindex in sqe::notification_idx and the buffers are safe to reuse only\nwhen the used notification is flushed and completes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a80387c6a68ce9cf99b3b6ef6f71068468761fb7.1657643355.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:07 -0600 io_uring: wire send zc request type"
    },
    {
        "commit": "bc24d6bd32df0be19df3d30e74be4ba56493c0e2",
        "message": "Let the userspace to register and unregister notification slots.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a0aa8161fe3ebb2a4cc6e5dbd0cffb96e6881cf5.1657643355.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:07 -0600 io_uring: add notification slot registration"
    },
    {
        "commit": "68ef5578efc8893489400b1ec30af66dab4f75ff",
        "message": "In preparation to zerocopy sends with fixed buffers make notifiers to\nreference the rsrc node to protect the used fixed buffers. We can't just\ngrab it for a send request as notifiers can likely outlive requests that\nused it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3cd7a01d26837945b6982fa9cf15a63230f2ed4f.1657643355.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:06 -0600 io_uring: add rsrc referencing for notifiers"
    },
    {
        "commit": "e58d498e81baa9fd8acf5132d8b2d4f829361f6b",
        "message": "We need a task context to post CQEs but using wq is too expensive.\nTry to complete notifiers using task_work and fall back to wq if fails.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/089799ab665b10b78fdc614ae6d59fa7ef0d5f91.1657643355.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:06 -0600 io_uring: complete notifiers in tw"
    },
    {
        "commit": "eb4a299b2f95437af6183946c2a2e850621cefdb",
        "message": "kmalloc'ing struct io_notif is too expensive when done frequently, cache\nthem as many other resources in io_uring. Keep two list, the first one\nis from where we're getting notifiers, it's protected by ->uring_lock.\nThe second is protected by ->completion_lock, to which we queue released\nnotifiers. Then we splice one list into another when needed.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9dec18f7fcbab9f4bd40b96e5ae158b119945230.1657643355.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:06 -0600 io_uring: cache struct io_notif"
    },
    {
        "commit": "eb42cebb2cf24c48f60c32856a4bba93d42659c8",
        "message": "Add internal part of send zerocopy notifications. There are two main\nstructures, the first one is struct io_notif, which carries inside\nstruct ubuf_info and maps 1:1 to it. io_uring will be binding a number\nof zerocopy send requests to it and ask to complete (aka flush) it. When\nflushed and all attached requests and skbs complete, it'll generate one\nand only one CQE. There are intended to be passed into the network layer\nas struct msghdr::msg_ubuf.\n\nThe second concept is notification slots. The userspace will be able to\nregister an array of slots and subsequently addressing them by the index\nin the array. Slots are independent of each other. Each slot can have\nonly one notifier at a time (called active notifier) but many notifiers\nduring the lifetime. When active, a notifier not going to post any\ncompletion but the userspace can attach requests to it by specifying\nthe corresponding slot while issueing send zc requests. Eventually, the\nuserspace will want to \"flush\" the notifier losing any way to attach\nnew requests to it, however it can use the next atomatically added\nnotifier of this slot or of any other slot.\n\nWhen the network layer is done with all enqueued skbs attached to a\nnotifier and doesn't need the specified in them user data, the flushed\nnotifier will post a CQE.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3ecf54c31a85762bf679b0a432c9f43ecf7e61cc.1657643355.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:06 -0600 io_uring: add zc notification infrastructure"
    },
    {
        "commit": "e70cb60893ca64b7df06864aa16c1cf6d6c671db",
        "message": "Make io_put_task() available to non-core parts of io_uring, we'll need\nit for notification infrastructure.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3686807d4c03b72e389947b0e8692d4d44334ef0.1657643355.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:06 -0600 io_uring: export io_put_task()"
    },
    {
        "commit": "e02b66512738db161e83634255e9826c8cb51336",
        "message": "Initialise newly added ->msg_ubuf in io_recv() and io_send().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b8f9f263875a4a36e7f26cc5d55ebe315308f57d.1657643355.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:06 -0600 io_uring: initialise msghdr::msg_ubuf"
    },
    {
        "commit": "4effe18fc0da27ae5d51a702841e87fa13b8a32d",
        "message": "* for-5.20/io_uring: (716 commits)\n  io_uring: ensure REQ_F_ISREG is set async offload\n  net: fix compat pointer in get_compat_msghdr()\n  io_uring: Don't require reinitable percpu_ref\n  io_uring: fix types in io_recvmsg_multishot_overflow\n  io_uring: Use atomic_long_try_cmpxchg in __io_account_mem\n  io_uring: support multishot in recvmsg\n  net: copy from user before calling __get_compat_msghdr\n  net: copy from user before calling __copy_msghdr\n  io_uring: support 0 length iov in buffer select in compat\n  io_uring: fix multishot ending when not polled\n  io_uring: add netmsg cache\n  io_uring: impose max limit on apoll cache\n  io_uring: add abstraction around apoll cache\n  io_uring: move apoll cache to poll.c\n  io_uring: consolidate hash_locked io-wq handling\n  io_uring: clear REQ_F_HASH_LOCKED on hash removal\n  io_uring: don't race double poll setting REQ_F_ASYNC_DATA\n  io_uring: don't miss setting REQ_F_DOUBLE_POLL\n  io_uring: disable multishot recvmsg\n  io_uring: only trace one of complete or overflow\n  ...\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:41:03 -0600 Merge branch 'for-5.20/io_uring' into for-5.20/io_uring-zerocopy-send"
    },
    {
        "commit": "32e09298c8b3ff29177c825ab711a4a692d4caad",
        "message": "Merge prep net series for io_uring tx zc from the Jakub's tree.\n\n* 'io_uring-zerocopy-send' of git://git.kernel.org/pub/scm/linux/kernel/git/kuba/linux:\n  net: fix uninitialised msghdr->sg_from_iter\n  tcp: support externally provided ubufs\n  ipv6/udp: support externally provided ubufs\n  ipv4/udp: support externally provided ubufs\n  net: introduce __skb_fill_page_desc_noacc\n  net: introduce managed frags infrastructure\n  net: Allow custom iter handler in msghdr\n  skbuff: carry external ubuf_info in msghdr\n  skbuff: add SKBFL_DONT_ORPHAN flag\n  skbuff: don't mix ubuf_info from different sources\n  ipv6: avoid partial copy for zc\n  ipv4: avoid partial copy for zc",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:40:31 -0600 Merge branch 'io_uring-zerocopy-send' of git://git.kernel.org/pub/scm/linux/kernel/git/kuba/linux into for-5.20/io_uring-zerocopy-send"
    },
    {
        "commit": "0dd316ba8692c2374fbb82cce57c0b23144f2977",
        "message": "If we're creating a page cache page with FGP_CREAT but FGP_NOWAIT is\nset, we should dial back the gfp flags to avoid frivolous blocking\nwhich is trivial to hit in low memory conditions:\n\n[   10.117661]  __schedule+0x8c/0x550\n[   10.118305]  schedule+0x58/0xa0\n[   10.118897]  schedule_timeout+0x30/0xdc\n[   10.119610]  __wait_for_common+0x88/0x114\n[   10.120348]  wait_for_completion+0x1c/0x24\n[   10.121103]  __flush_work.isra.0+0x16c/0x19c\n[   10.121896]  flush_work+0xc/0x14\n[   10.122496]  __drain_all_pages+0x144/0x218\n[   10.123267]  drain_all_pages+0x10/0x18\n[   10.123941]  __alloc_pages+0x464/0x9e4\n[   10.124633]  __folio_alloc+0x18/0x3c\n[   10.125294]  __filemap_get_folio+0x17c/0x204\n[   10.126084]  iomap_write_begin+0xf8/0x428\n[   10.126829]  iomap_file_buffered_write+0x144/0x24c\n[   10.127710]  xfs_file_buffered_write+0xe8/0x248\n[   10.128553]  xfs_file_write_iter+0xa8/0x120\n[   10.129324]  io_write+0x16c/0x38c\n[   10.129940]  io_issue_sqe+0x70/0x1cc\n[   10.130617]  io_queue_sqe+0x18/0xfc\n[   10.131277]  io_submit_sqes+0x5d4/0x600\n[   10.131946]  __arm64_sys_io_uring_enter+0x224/0x600\n[   10.132752]  invoke_syscall.constprop.0+0x70/0xc0\n[   10.133616]  do_el0_svc+0xd0/0x118\n[   10.134238]  el0_svc+0x78/0xa0\n\nClear IO, FS, and reclaim flags and mark the allocation as GFP_NOWAIT and\nadd __GFP_NOWARN to avoid polluting dmesg with pointless allocations\nfailures. A caller with FGP_NOWAIT must be expected to handle the\nresulting -EAGAIN return and retry from a suitable context without NOWAIT\nset.\n\nReviewed-by: Shakeel Butt <shakeelb@google.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:32 -0600 mm: honor FGP_NOWAIT for page cache page allocation"
    },
    {
        "commit": "1c849b481b3e4f8c36f297cd3aa88ef52a19cee9",
        "message": "This adds the io_uring_short_write tracepoint to io_uring. A short write\nis issued if not all pages that are required for a write are in the page\ncache and the async buffered writes have to return EAGAIN.\n\nSigned-off-by: Stefan Roesch <shr@fb.com>\nLink: https://lore.kernel.org/r/20220616212221.2024518-13-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:32 -0600 io_uring: Add tracepoint for short writes"
    },
    {
        "commit": "e053aaf4da56cbf0afb33a0fda4a62188e2c0637",
        "message": "This is actually an older issue, but we never used to hit the -EAGAIN\npath before having done sb_start_write(). Make sure that we always call\nkiocb_end_write() if we need to retry the write, so that we keep the\ncalls to sb_start_write() etc balanced.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:32 -0600 io_uring: fix issue with io_write() not always undoing sb_start_write()"
    },
    {
        "commit": "4e17aaab54359fa2cdeb0080c822a08f2980f979",
        "message": "This enables the async buffered writes for the filesystems that support\nasync buffered writes in io-uring. Buffered writes are enabled for\nblocks that are already in the page cache or can be acquired with noio.\n\nSigned-off-by: Stefan Roesch <shr@fb.com>\nLink: https://lore.kernel.org/r/20220616212221.2024518-12-shr@fb.com\n[axboe: adapt to 5.20 branch]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:32 -0600 io_uring: Add support for async buffered writes"
    },
    {
        "commit": "f6b543fd03d347e8bf245cee4f2d54eb6ffd8fcb",
        "message": "If we're offloading requests directly to io-wq because IOSQE_ASYNC was\nset in the sqe, we can miss hashing writes appropriately because we\nhaven't set REQ_F_ISREG yet. This can cause a performance regression\nwith buffered writes, as io-wq then no longer correctly serializes writes\nto that file.\n\nEnsure that we set the flags in io_prep_async_work(), which will cause\nthe io-wq work item to be hashed appropriately.\n\nFixes: 584b0180f0f4 (\"io_uring: move read/write file prep state into actual opcode handler\")\nLink: https://lore.kernel.org/io-uring/20220608080054.GB22428@xsang-OptiPlex-9020/\nReported-by: kernel test robot <oliver.sang@intel.com>\nTested-by: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:18 -0600 io_uring: ensure REQ_F_ISREG is set async offload"
    },
    {
        "commit": "48904229928d941ce1db181b991948387ab463cd",
        "message": "The commit 8bb649ee1da3 (\"io_uring: remove ring quiesce for\nio_uring_register\") removed the worklow relying on reinit/resurrection\nof the percpu_ref, hence, initialization with that requested is a relic.\n\nThis is based on code review, this causes no real bug (and theoretically\ncan't). Technically it's a revert of commit 214828962dea (\"io_uring:\ninitialize percpu refcounters using PERCU_REF_ALLOW_REINIT\") but since\nthe flag omission is now justified, I'm not making this a revert.\n\nFixes: 8bb649ee1da3 (\"io_uring: remove ring quiesce for io_uring_register\")\nSigned-off-by: Michal Koutn\u00fd <mkoutny@suse.com>\nAcked-by: Roman Gushchin <roman.gushchin@linux.dev>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:18 -0600 io_uring: Don't require reinitable percpu_ref"
    },
    {
        "commit": "9b0fc3c054ff2eb13753104884f1045b5bb3a627",
        "message": "io_recvmsg_multishot_overflow had incorrect types on non x64 system.\nBut also it had an unnecessary INT_MAX check, which could just be done\nby changing the type of the accumulator to int (also simplifying the\ncasts).\n\nReported-by: Stephen Rothwell <sfr@canb.auug.org.au>\nFixes: a8b38c4ce724 (\"io_uring: support multishot in recvmsg\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220715130252.610639-1-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:18 -0600 io_uring: fix types in io_recvmsg_multishot_overflow"
    },
    {
        "commit": "4ccc6db0900fe337212b61650663a5dcedb69f25",
        "message": "Use atomic_long_try_cmpxchg instead of\natomic_long_cmpxchg (*ptr, old, new) == old in __io_account_mem.\nx86 CMPXCHG instruction returns success in ZF flag, so this\nchange saves a compare after cmpxchg (and related move\ninstruction in front of cmpxchg).\n\nAlso, atomic_long_try_cmpxchg implicitly assigns old *ptr value\nto \"old\" when cmpxchg fails, enabling further code simplifications.\n\nNo functional change intended.\n\nSigned-off-by: Uros Bizjak <ubizjak@gmail.com>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:18 -0600 io_uring: Use atomic_long_try_cmpxchg in __io_account_mem"
    },
    {
        "commit": "9bb66906f23e50d6db1e11f7498b72dfca1982a2",
        "message": "Similar to multishot recv, this will require provided buffers to be\nused. However recvmsg is much more complex than recv as it has multiple\noutputs. Specifically flags, name, and control messages.\n\nSupport this by introducing a new struct io_uring_recvmsg_out with 4\nfields. namelen, controllen and flags match the similar out fields in\nmsghdr from standard recvmsg(2), payloadlen is the length of the payload\nfollowing the header.\nThis struct is placed at the start of the returned buffer. Based on what\nthe user specifies in struct msghdr, the next bytes of the buffer will be\nname (the next msg_namelen bytes), and then control (the next\nmsg_controllen bytes). The payload will come at the end. The return value\nin the CQE is the total used size of the provided buffer.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220714110258.1336200-4-dylany@fb.com\n[axboe: style fixups, see link]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:18 -0600 io_uring: support multishot in recvmsg"
    },
    {
        "commit": "72c531f8ef3052c682d39dc21dcb5576afda208c",
        "message": "this is in preparation for multishot receive from io_uring, where it needs\nto have access to the original struct user_msghdr.\n\nfunctionally this should be a no-op.\n\nAcked-by: Paolo Abeni <pabeni@redhat.com>\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220714110258.1336200-3-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 net: copy from user before calling __get_compat_msghdr"
    },
    {
        "commit": "7fa875b8e53c288d616234b9daf417b0650ce1cc",
        "message": "this is in preparation for multishot receive from io_uring, where it needs\nto have access to the original struct user_msghdr.\n\nfunctionally this should be a no-op.\n\nAcked-by: Paolo Abeni <pabeni@redhat.com>\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220714110258.1336200-2-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 net: copy from user before calling __copy_msghdr"
    },
    {
        "commit": "6d2f75a0cf3048ef38c48493f66a923353bf664b",
        "message": "Match up work done in \"io_uring: allow iov_len = 0 for recvmsg and buffer\nselect\", but for compat code path.\n\nFixes: a68caad69ce5 (\"io_uring: allow iov_len = 0 for recvmsg and buffer select\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220708181838.1495428-3-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: support 0 length iov in buffer select in compat"
    },
    {
        "commit": "e2df2ccb753e3b598a9045760e79f1847f7b31cd",
        "message": "If multishot is not actually polling then return IOU_OK rather than the\nresult.\nIf the result was > 0 this will confuse things further up the callstack\nwhich expect a return <= 0.\n\nFixes: 1300ebb20286 (\"io_uring: multishot recv\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220708181838.1495428-2-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: fix multishot ending when not polled"
    },
    {
        "commit": "43e0bbbd0b0e30d232fd8e9e908125b5c49a9fbc",
        "message": "For recvmsg/sendmsg, if they don't complete inline, we currently need\nto allocate a struct io_async_msghdr for each request. This is a\nsomewhat large struct.\n\nHook up sendmsg/recvmsg to use the io_alloc_cache. This reduces the\nalloc + free overhead considerably, yielding 4-5% of extra performance\nrunning netbench.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: add netmsg cache"
    },
    {
        "commit": "9731bc9855dc169f27433fef3c4d0ff3496c512d",
        "message": "Caches like this tend to grow to the peak size, and then never get any\nsmaller. Impose a max limit on the size, to prevent it from growing too\nbig.\n\nA somewhat randomly chosen 512 is the max size we'll allow the cache\nto get. If a batch of frees come in and would bring it over that, we\nsimply start kfree'ing the surplus.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: impose max limit on apoll cache"
    },
    {
        "commit": "9b797a37c4bd83b03cedcfbd15852b836f5e562c",
        "message": "In preparation for adding limits, and one more user, abstract out the\ncore bits of the allocation+free cache.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: add abstraction around apoll cache"
    },
    {
        "commit": "9da7471ed10dab52410062be74896a6c0aa1bf3a",
        "message": "This is where it's used, move the flush handler in there.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: move apoll cache to poll.c"
    },
    {
        "commit": "e8375e43ca2d67f31b2408092eee4f91e1e140f4",
        "message": "Don't duplicate code disabling REQ_F_HASH_LOCKED for IO_URING_F_UNLOCKED\n(i.e. io-wq), move the handling into __io_arm_poll_handler().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0ff0ffdfaa65b3d536131535c3dad3c63d9b7bb0.1657203020.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: consolidate hash_locked io-wq handling"
    },
    {
        "commit": "b21a51e26e9aed66159c2c18651da22d585d2998",
        "message": "Instead of clearing REQ_F_HASH_LOCKED while arming a poll, unset the bit\nwhen we're removing the entry from the table in io_poll_tw_hash_eject().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/02e48bb88d6f1480c94ac2924c43ad1fbd48e92a.1657203020.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: clear REQ_F_HASH_LOCKED on hash removal"
    },
    {
        "commit": "ceff501790a9789c748a9b851f30f4f7e2fe4d72",
        "message": "Just as with io_poll_double_prepare() setting REQ_F_DOUBLE_POLL, we can\nrace with the first poll entry when setting REQ_F_ASYNC_DATA. Move it\nunder io_poll_double_prepare().\n\nFixes: a18427bb2d9b (\"io_uring: optimise submission side poll_refs\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/df6920f509c11115aa2bce8b34dc5fdb0eb98920.1657203020.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: don't race double poll setting REQ_F_ASYNC_DATA"
    },
    {
        "commit": "7a121ced6e6430d49fb802067b4f020f6df62362",
        "message": "When adding a second poll entry we should set REQ_F_DOUBLE_POLL\nunconditionally. We might race with the first entry removal but that\ndoesn't change the rule.\n\nFixes: a18427bb2d9b (\"io_uring: optimise submission side poll_refs\")\nReported-and-tested-by: syzbot+49950ba66096b1f0209b@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8b680d83ded07424db83e8745585e7a6d72826ef.1657203020.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: don't miss setting REQ_F_DOUBLE_POLL"
    },
    {
        "commit": "cf0dd9527eee5d6d183fa724fdee6a128cb17b8d",
        "message": "recvmsg has semantics that do not make it trivial to extend to\nmultishot. Specifically it has user pointers and returns data in the\noriginal parameter. In order to make this API useful these will need to be\nsomehow included with the provided buffers.\n\nFor now remove multishot for recvmsg as it is not useful.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220704140106.200167-1-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: disable multishot recvmsg"
    },
    {
        "commit": "e0486f3f7c1b6ab7ba7e912bf48d17f04e71d86d",
        "message": "In overflow we see a duplcate line in the trace, and in some cases 3\nlines (if initial io_post_aux_cqe fails).\nInstead just trace once for each CQE\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220630091231.1456789-13-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: only trace one of complete or overflow"
    },
    {
        "commit": "9b26e811e934eebda59362c9a03d082852552574",
        "message": "Make the trace format consistent with io_uring_complete for cflags\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220630091231.1456789-12-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: fix io_uring_cqe_overflow trace format"
    },
    {
        "commit": "b3fdea6ecb55c3ceea866ff66486927e51a982b3",
        "message": "Support multishot receive for io_uring.\nTypical server applications will run a loop where for each recv CQE it\nrequeues another recv/recvmsg.\n\nThis can be simplified by using the existing multishot functionality\ncombined with io_uring's provided buffers.\nThe API is to add the IORING_RECV_MULTISHOT flag to the SQE. CQEs will\nthen be posted (with IORING_CQE_F_MORE flag set) when data is available\nand is read. Once an error occurs or the socket ends, the multishot will\nbe removed and a completion without IORING_CQE_F_MORE will be posted.\n\nThe benefit to this is that the recv is much more performant.\n * Subsequent receives are queued up straight away without requiring the\n   application to finish a processing loop.\n * If there are more data in the socket (sat the provided buffer size is\n   smaller than the socket buffer) then the data is immediately\n   returned, improving batching.\n * Poll is only armed once and reused, saving CPU cycles\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220630091231.1456789-11-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: multishot recv"
    },
    {
        "commit": "cbd25748545c709d35734deb9220e0af0a69e5d2",
        "message": "Similar to multishot poll, drop multishot accept when CQE overflow occurs.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220630091231.1456789-10-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: fix multishot accept ordering"
    },
    {
        "commit": "a2da676376feb79224eacb9ac1f554bb3232b5de",
        "message": "On overflow, multishot poll can still complete with the IORING_CQE_F_MORE\nflag set.\nIf in the meantime the user clears a CQE and a the poll was cancelled then\nthe poll will post a CQE without the IORING_CQE_F_MORE (and likely result\n-ECANCELED).\n\nHowever when processing the application will encounter the non-overflow\nCQE which indicates that there will be no more events posted. Typical\nuserspace applications would free memory associated with the poll in this\ncase.\nIt will then subsequently receive the earlier CQE which has overflowed,\nwhich breaks the contract given by the IORING_CQE_F_MORE flag.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220630091231.1456789-9-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: fix multishot poll on overflow"
    },
    {
        "commit": "52120f0fadcbdaaa981c19327f1865a714e85268",
        "message": "Some use cases of io_post_aux_cqe would not want to overflow as is, but\nmight want to change the flags/result. For example multishot receive\nrequires in order CQE, and so if there is an overflow it would need to\nstop receiving until the overflow is taken care of.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220630091231.1456789-8-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: add allow_overflow to io_post_aux_cqe"
    },
    {
        "commit": "114eccdf0e368893b3d92e06e9788d9d94876853",
        "message": "For multishot we want a way to signal the caller that multishot has ended\nbut also this might not be an error return.\n\nFor example sockets return 0 when closed, which should end a multishot\nrecv, but still have a CQE with result 0\n\nIntroduce IOU_STOP_MULTISHOT which does this and indicates that the return\ncode is stored inside req->cqe\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220630091231.1456789-7-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:17 -0600 io_uring: add IOU_STOP_MULTISHOT return code"
    },
    {
        "commit": "2ba69707d9153eeca1ee8ac1bc55376b88978842",
        "message": "The values returned are a bit confusing, where 0 and 1 have implied\nmeaning, so add some definitions for them.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220630091231.1456789-6-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: clean up io_poll_check_events return values"
    },
    {
        "commit": "d4e097dae29c24bf33a5056a2a43cff2e45c4978",
        "message": "Rather than passing an error back to the user with a buffer attached,\nrecycle the buffer immediately.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220630091231.1456789-5-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: recycle buffers on error"
    },
    {
        "commit": "5702196e7d9d1232c41769e197eb33ba78a9b463",
        "message": "When using BUFFER_SELECT there is no technical requirement that the user\nactually provides iov, and this removes one copy_from_user call.\n\nSo allow iov_len to be 0.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220630091231.1456789-4-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: allow iov_len = 0 for recvmsg and buffer select"
    },
    {
        "commit": "32f3c434b14238a2eee0c726a1918de58c07faf6",
        "message": "Attempt to restore bgid. This is needed when recycling unused buffers as\nthe next time around it will want the correct bgid.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220630091231.1456789-3-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: restore bgid in io_put_kbuf"
    },
    {
        "commit": "b8c015598c8ef9195b8a2a5089e275c4f64ca999",
        "message": "If user gives 0 for length, we can set it from the available buffer size.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220630091231.1456789-2-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: allow 0 length for buffer select"
    },
    {
        "commit": "6e73dffbb93cb8797cd4e42e98d837edf0f1a967",
        "message": "From recently io_uring provides an option to allocate a file index for\noperation registering fixed files. However, it's utterly unusable with\nmixed approaches when for a part of files the userspace knows better\nwhere to place it, as it may race and users don't have any sane way to\npick a slot and hoping it will not be taken.\n\nLet the userspace to register a range of fixed file slots in which the\nauto-allocation happens. The use case is splittting the fixed table in\ntwo parts, where on of them is used for auto-allocation and another for\nslot-specified operations.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/66ab0394e436f38437cf7c44676e1920d09687ad.1656154403.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: let to set a range for file slot allocation"
    },
    {
        "commit": "e6130eba8a848a7a6ba6c534bd8f6d60749ae1a9",
        "message": "With IORING_OP_MSG_RING, one ring can send a message to another ring.\nExtend that support to also allow sending a fixed file descriptor to\nthat ring, enabling one ring to pass a registered descriptor to another\none.\n\nArguments are extended to pass in:\n\nsqe->addr3\tfixed file slot in source ring\nsqe->file_index\tfixed file slot in destination ring\n\nIORING_OP_MSG_RING is extended to take a command argument in sqe->addr.\nIf set to zero (or IORING_MSG_DATA), it sends just a message like before.\nIf set to IORING_MSG_SEND_FD, a fixed file descriptor is sent according\nto the above arguments.\n\nTwo common use cases for this are:\n\n1) Server needs to be shutdown or restarted, pass file descriptors to\n   another onei\n\n2) Backend is split, and one accepts connections, while others then get\n  the fd passed and handle the actual connection.\n\nBoth of those are classic SCM_RIGHTS use cases, and it's not possible to\nsupport them with direct descriptors today.\n\nBy default, this will post a CQE to the target ring, similarly to how\nIORING_MSG_DATA does it. If IORING_MSG_RING_CQE_SKIP is set, no message\nis posted to the target ring. The issuer is expected to notify the\nreceiver side separately.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: add support for passing fixed file descriptors"
    },
    {
        "commit": "f110ed8498afa6ff8e9a8c08fb26880e02117616",
        "message": "Put it with the filetable code, which is where it belongs. While doing\nso, have the helpers take a ctx rather than an io_kiocb. It doesn't make\nsense to use a request, as it's not an operation on the request itself.\nIt applies to the ring itself.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: split out fixed file installation and removal"
    },
    {
        "commit": "8fcf4c48f44bd7b1b75db139f56ff1ad6477379e",
        "message": "There is a regular need in the kernel to provide a way to declare\nhaving a dynamically sized set of trailing elements in a structure.\nKernel code should always use \u201cflexible array members\u201d[1] for these\ncases. The older style of one-element or zero-length arrays should\nno longer be used[2].\n\n[1] https://en.wikipedia.org/wiki/Flexible_array_member\n[2] https://www.kernel.org/doc/html/v5.16/process/deprecated.html#zero-length-and-one-element-arrays\n\nLink: https://github.com/KSPP/linux/issues/78\nSigned-off-by: Gustavo A. R. Silva <gustavoars@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: replace zero-length array with flexible-array member"
    },
    {
        "commit": "fbb8bb02911790147ea936f3b1c5ceb1be54bf34",
        "message": "io_uring_enter() takes ctx->refs, which was previously preventing racing\nwith register quiesce. However, as register now doesn't touch the refs,\nwe can freely kill extra ctx pinning and rely on the fact that we're\nholding a file reference preventing the ring from being destroyed.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a11c57ad33a1be53541fce90669c1b79cf4d8940.1656153286.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: remove ctx->refs pinning on enter"
    },
    {
        "commit": "3273c4407acd4348b1531e1f860fbf1da942893b",
        "message": "Registered rings are per definitions io_uring files, so we don't need to\nadditionally verify them.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/425cd64fd885b8e329a46c205ee811987691baaf.1656153286.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: don't check file ops of registered rings"
    },
    {
        "commit": "ad8b261d837400cb7ccc339e81d7c35ab018acd8",
        "message": "io_run_task_work() accounts for TIF_NOTIFY_SIGNAL, so no need to have an\nsecond check in io_run_task_work_sig().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/52ce41a592ad904511697f432141e5690fd4b968.1656153285.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: remove extra TIF_NOTIFY_SIGNAL check"
    },
    {
        "commit": "3218e5d32dbcf1b9c6dc589eca21deebb14215fa",
        "message": "Now as both normal and fallback paths use llist, just keep one node head\nin struct io_task_work and kill off ->fallback_node.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d04ebde409f7b162fe247b361b4486b193293e46.1656153285.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: fuse fallback_node and normal tw node"
    },
    {
        "commit": "37c7bd31b3e9e4b6aee3c5227f789c0b586a33a2",
        "message": "io_fail_links() is called with ->completion_lock held and for that\nreason we'd want to keep it as small as we can. Instead of doing\n__io_req_complete_post() for each linked request under the lock, fail\nthem in a task_work handler under ->uring_lock.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a2f68708b970a21f4e84ddfa7b3abd67a8fffb27.1656153285.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: improve io_fail_links()"
    },
    {
        "commit": "fe991a7688f894a74a6f6b4933bf6cd5fa086c1b",
        "message": "We really don't care about this at all in terms of performance. Outside\nof having it already be marked unlikely(), shove it into a separate\n__cold function.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: move POLLFREE handling to separate function"
    },
    {
        "commit": "795bbbc8a9a1bbbafce762c706bfb5733c9d0426",
        "message": "Make io_kbuf_recycle_ring() inline since it is the fast path of\nprovided buffer.\n\nSigned-off-by: Hao Xu <howeyxu@tencent.com>\nLink: https://lore.kernel.org/r/20220623130126.179232-1-hao.xu@linux.dev\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: kbuf: inline io_kbuf_recycle_ring()"
    },
    {
        "commit": "49f1c68e048f1706b71c8255faf8110113d1cc48",
        "message": "The final poll_refs put in __io_arm_poll_handler() takes quite some\ncycles. When we're arming from the original task context task_work won't\nbe run, so in this case we can assume that we won't race with task_works\nand so not take the initial ownership ref.\n\nOne caveat is that after arming a poll we may race with it, so we have\nto add a bunch of io_poll_get_ownership() hidden inside of\nio_poll_can_finish_inline() whenever we want to complete arming inline.\nFor the same reason we can't just set REQ_F_DOUBLE_POLL in\n__io_queue_proc() and so need to sync with the first poll entry by\ntaking its wq head lock.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8825315d7f5e182ac1578a031e546f79b1c97d01.1655990418.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: optimise submission side poll_refs"
    },
    {
        "commit": "de08356f4858628fdffb8bd7e9cceb60c7e08ead",
        "message": "__io_arm_poll_handler() errors parsing is a horror, in case it failed it\nreturns 0 and the caller is expected to look at ipt.error, which already\nled us to a number of problems before.\n\nWhen it returns a valid mask, leave it as it's not, i.e. return 1 and\nstore the mask in ipt.result_mask. In case of a failure that can be\nhandled inline return an error code (negative value), and return 0 if\n__io_arm_poll_handler() took ownership of the request and will complete\nit.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/018cacdaef5fe95d7dc56b32e85d752cab7607f6.1655990418.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: refactor poll arm error handling"
    },
    {
        "commit": "063a007996bf725ba4c7d8741701670be9858300",
        "message": "The rules for __io_arm_poll_handler()'s result parsing are complicated,\nas the first step don't pass return a mask but pass back a positive\nreturn code and fill ipt->result_mask.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/529e29e9f97f2e6e383ccd44234d8b576a83a921.1655990418.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: change arm poll return values"
    },
    {
        "commit": "5204aa8c43bd1c3428b8979229183ae8269a8c09",
        "message": "Extract a helper function for apoll allocation, makes the code easier to\nread.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2f93282b47dd678e805dd0d7097f66968ced495c.1655990418.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: add a helper for apoll alloc"
    },
    {
        "commit": "13a99017ff19f179b51e1c51559a9d7005df1830",
        "message": "Remove events argument from *io_poll_execute(), it's not needed and not\nused.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/12efd4e15c6a90cf9e5b59807cfcb57852b51dc7.1655990418.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:16 -0600 io_uring: remove events caching atavisms"
    },
    {
        "commit": "0638cd7be21221c7b583a4901e25e63d96112395",
        "message": "We store a req pointer in wqe->private but also take one bit to mark\ndouble poll entries. Replace macro helpers with inline functions for\nbetter type checking and also name the double flag.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9a61240555c64ac0b7a9b0eb59a9efeb638a35a4.1655990418.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: clean poll ->private flagging"
    },
    {
        "commit": "78a861b9495920f8609dee5b670dacbff09d359f",
        "message": "The io_uring cancelation API is async, like any other API that we expose\nthere. For the case of finding a request to cancel, or not finding one,\nit is fully sync in that when submission returns, the CQE for both the\ncancelation request and the targeted request have been posted to the\nCQ ring.\n\nHowever, if the targeted work is being executed by io-wq, the API can\nonly start the act of canceling it. This makes it difficult to use in\nsome circumstances, as the caller then has to wait for the CQEs to come\nin and match on the same cancelation data there.\n\nProvide a IORING_REGISTER_SYNC_CANCEL command for io_uring_register()\nthat does sync cancelations, always. For the io-wq case, it'll wait\nfor the cancelation to come in before returning. The only expected\nreturns from this API is:\n\n0\t\tRequest found and canceled fine.\n> 0\t\tRequests found and canceled. Only happens if asked to\n\t\tcancel multiple requests, and if the work wasn't in\n\t\tprogress.\n-ENOENT\t\tRequest not found.\n-ETIME\t\tA timeout on the operation was requested, but the timeout\n\t\texpired before we could cancel.\n\nand we won't get -EALREADY via this API.\n\nIf the timeout value passed in is -1 (tv_sec and tv_nsec), then that\nmeans that no timeout is requested. Otherwise, the timespec passed in\nis the amount of time the sync cancel will wait for a successful\ncancelation.\n\nLink: https://github.com/axboe/liburing/discussions/608\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: add sync cancelation API through io_uring_register()"
    },
    {
        "commit": "7d8ca7250197096bfa9f432c1d99b0555504bbba",
        "message": "In preparation for not having a request to pass in that carries this\nstate, add a separate cancelation flag that allows the caller to ask\nfor a fixed file for cancelation.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: add IORING_ASYNC_CANCEL_FD_FIXED cancel flag"
    },
    {
        "commit": "88f52eaad2df2cb5ab49b864d79398c9cb9a57f2",
        "message": "We just use the io_kiocb passed in to find the io_uring_task, and we\nalready pass in the ctx via cd->ctx anyway.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: have cancelation API accept io_uring_task directly"
    },
    {
        "commit": "024b8fde3320ea34d7a5a3fc9dbc47ec736cd8eb",
        "message": "__io_kbuf_recycle() is only called in io_kbuf_recycle(). Kill it and\ntweak the code so that the legacy pbuf and ring pbuf code become clear\n\nSigned-off-by: Hao Xu <howeyxu@tencent.com>\nLink: https://lore.kernel.org/r/20220622055551.642370-1-hao.xu@linux.dev\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: kbuf: kill __io_kbuf_recycle()"
    },
    {
        "commit": "c6dd763c246024ed0dd603db4284cbd0bd164e27",
        "message": "trace task_work_run to help provide stats on how often task work is run\nand what batch sizes are coming through.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220622134028.2013417-9-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: trace task_work_run"
    },
    {
        "commit": "eccd8801858f03a19a4d1f8fef27e3d6e17b21fd",
        "message": "This is useful for investigating if task_work is batching\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220622134028.2013417-8-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: add trace event for running task work"
    },
    {
        "commit": "3a0c037b0e16e38cf5d36d2ebc259e0ae644aaf4",
        "message": "Batching task work up is an important performance optimisation, as\ntask_work_add is expensive.\n\nIn order to keep the semantics replace the task_list with a fake node\nwhile processing the old list, and then do a cmpxchg at the end to see if\nthere is more work.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220622134028.2013417-6-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: batch task_work"
    },
    {
        "commit": "923d159247b732885b176b24e4bafad8eda5a477",
        "message": "Introduce helpers to atomically switch llist.\n\nWill later move this into common code\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220622134028.2013417-5-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: introduce llist helpers"
    },
    {
        "commit": "f88262e60bb9cb5740891672ce9f405e7f9393e5",
        "message": "With networking use cases we see contention on the spinlock used to\nprotect the task_list when multiple threads try and add completions at once.\nInstead we can use a lockless list, and assume that the first caller to\nadd to the list is responsible for kicking off task work.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220622134028.2013417-4-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: lockless task list"
    },
    {
        "commit": "c34398a8c018e0d3d2d30b718d03c7290c696f51",
        "message": "this is no longer needed as there is only one caller\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220622134028.2013417-3-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: remove __io_req_task_work_add"
    },
    {
        "commit": "ed5ccb3beeba0cadb0fcf353ae192021dfecf252",
        "message": "This optimisation has some built in assumptions that make it easy to\nintroduce bugs. It also does not have clear wins that make it worth keeping.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220622134028.2013417-2-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: remove priority tw list optimisation"
    },
    {
        "commit": "024f15e033a52660a045947ee56c7e842180fa81",
        "message": "We have an identical copy of io_run_task_work() for io-wq called\nio_flush_signals(), deduplicate them.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a157a4df5fa217b8bd03c73494f2fd0e24e44fbc.1655802465.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: dedup io_run_task_work"
    },
    {
        "commit": "a6b21fbb4ce3c4976ba478a9f0f10d4163038478",
        "message": "It's annoying to have io-wq.h as a dependency every time we want some of\nstruct io_wq_work_list helpers, move them into a separate file.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c1d891ce12b30767d1d2a3b7db2ca3abc1ecc4a2.1655802465.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: move list helpers to a separate file"
    },
    {
        "commit": "625d38b3fd34c58afb969810c4b3105eabb3b143",
        "message": "Since SQPOLL now uses TWA_SIGNAL_NO_IPI, there won't be task work items\nwithout TIF_NOTIFY_SIGNAL. Simplify io_run_task_work() by removing\ntask->task_works check. Even though looks it doesn't cause extra cache\nbouncing, it's still nice to not touch it an extra time when it might be\nnot cached.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/75d4f34b0c671075892821a409e28da6cb1d64fe.1655802465.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: improve io_run_task_work()"
    },
    {
        "commit": "4a0fef62788b69df09267c8e3f3f11d4bb9d50e7",
        "message": "task_work bits of io_uring_task are split into two cache lines causing\nextra cache bouncing, place them into a separate cache line. Also move\nthe most used submission path fields closer together, so there are hot.\n\nCc: stable@vger.kernel.org # 5.15+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: optimize io_uring_task layout"
    },
    {
        "commit": "bce5d70cd64a5d48aff613334b8a5fac450b9753",
        "message": "io_poll_remove() expects poll_find() to search only for poll requests and\npasses a flag for this. Just be a little bit extra cautious considering\nlots of recent poll/cancellation changes and add a WARN_ON_ONCE checking\nthat we don't get an apoll'ed request.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ec9a66f1e22f99dcd02288d4e42f3cc6bb357804.1655684496.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: add a warn_once for poll_find"
    },
    {
        "commit": "9da070b142820031a0e273097f2b39982f45bbfd",
        "message": "Improve naming of the inline/deferred completion helper so it's\nconsistent with it's *_post counterpart. Add some comments and extra\nlockdeps to ensure the locking is done right.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/797c619943dac06529e9d3fcb16e4c3cde6ad1a3.1655684496.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: consistent naming for inline completion"
    },
    {
        "commit": "c059f785840807ed5e1f2810420c1555969b6246",
        "message": "Move io_import_fixed() into rsrc.c where it belongs.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4d5becb21f332b4fef6a7cedd6a50e65e2371630.1655684496.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: move io_import_fixed()"
    },
    {
        "commit": "f337a84d39527a2c46b1230470748d7bd0672ce0",
        "message": "Fixed buffers are generic infrastructure, make io_import_fixed() opcode\nagnostic.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b1e765c8a1c2c913a05a28d2399fc53e1d3cf37a.1655684496.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: opcode independent fixed buf import"
    },
    {
        "commit": "46929b086886ade8302cff6e85ccea66bce0cf98",
        "message": "Since __io_commit_cqring_flush users moved to different files, introduce\nio_commit_cqring_flush() helper and encapsulate all flags testing details\ninside.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0da03887435dd9869ffe46dcd3962bf104afcca3.1655684496.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:15 -0600 io_uring: add io_commit_cqring_flush()"
    },
    {
        "commit": "253993210bd8aa3b39a392807c03c8ef1cd7dc3d",
        "message": "spin_lock(&ctx->completion_lock);\n/* post CQEs */\nio_commit_cqring(ctx);\nspin_unlock(&ctx->completion_lock);\nio_cqring_ev_posted(ctx);\n\nWe have many places repeating this sequence, and the three function\nunlock section is not perfect from the maintainance perspective and also\nmakes it harder to add new locking/sync trick.\n\nIntroduce two helpers. io_cq_lock(), which is simple and only grabs\n->completion_lock, and io_cq_unlock_post() encapsulating the three call\nsection.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/fe0c682bf7f7b55d9be55b0d034be9c1949277dc.1655684496.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: introduce locking helpers for CQE posting"
    },
    {
        "commit": "305bef98870816ae58357d647521891ec558a92e",
        "message": "Some io_uring-eventfd users assume that there won't be spurious wakeups.\nThat assumption has to be honoured by all io_cqring_ev_posted() callers,\nwhich is inconvenient and from time to time leads to problems but should\nbe maintained to not break the userspace.\n\nInstead of making the callers track whether a CQE was posted or not, hide\nit inside io_eventfd_signal(). It saves ->cached_cq_tail it saw last time\nand triggers the eventfd only when ->cached_cq_tail changed since then.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0ffc66bae37a2513080b601e4370e147faaa72c5.1655684496.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: hide eventfd assumptions in eventfd paths"
    },
    {
        "commit": "b321823a03dc81a5b83b063e6e1904d612b53266",
        "message": "clang complains on bitwise operations with bools, add a bit more\nverbosity to better show that we want to call io_poll_remove_all_table()\ntwice but with different arguments.\n\nReported-by: Nathan Chancellor <nathan@kernel.org>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f11d21dcdf9233e0eeb15fa13b858a05a78eb310.1655684496.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: fix io_poll_remove_all clang warnings"
    },
    {
        "commit": "ba3cdb6fbb6e8eb525c868c60e103c5711edc068",
        "message": "Don't spin trying to cancel timeouts that are reachable but not\ncancellable, e.g. already executing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ab8a7440a60bbdf69ae514f672ad050e43dd1b03.1655684496.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: improve task exit timeout cancellations"
    },
    {
        "commit": "affa87db90108d9f017f927bcdab536e32c3915e",
        "message": "io_uring_try_cancel_requests() loops until there is nothing left to do\nwith the ring, however there might be several rings and they might have\ndependencies between them, e.g. via poll requests.\n\nInstead of cancelling rings one by one, try to cancel them all and only\nthen loop over if we still potenially some work to do.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8d491fe02d8ac4c77ff38061cf86b9a827e8845c.1655684496.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: fix multi ctx cancellation"
    },
    {
        "commit": "d9dee4302a7cbd6c0142dbdf6d150acc7459de0d",
        "message": "It's not clear how widely used IOSQE_CQE_SKIP_SUCCESS is, and how often\n->flush_cqes flag prevents from completion being flushed. Sometimes it's\nhigh level of concurrency that enables it at least for one CQE, but\nsometimes it doesn't save much because nobody waiting on the CQ.\n\nRemove ->flush_cqes flag and the optimisation, it should benefit the\nnormal use case. Note, that there is no spurious eventfd problem with\nthat as checks for spuriousness were incorporated into\nio_eventfd_signal().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/692e81eeddccc096f449a7960365fa7b4a18f8e6.1655637157.git.asml.silence@gmail.com\n[axboe: remove now dead state->flush_cqes variable]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: remove ->flush_cqes optimisation"
    },
    {
        "commit": "a830ffd28780627b6287bcd5b84e9fe2dd795935",
        "message": "Move io_eventfd_signal() in the sources without any changes and kill its\nforward declaration.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9ebebb3f6f56f5a5448a621e0b6a537720c43334.1655637157.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: move io_eventfd_signal()"
    },
    {
        "commit": "9046c6415be60f51f60f8b771a74ac4e72e3599d",
        "message": "It's a good idea to first do forward declarations and then inline\nhelpers, otherwise there will be keep stumbling on dependencies\nbetween them.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1d7fa6672ed43f20ccc0c54ae201369ebc3ebfab.1655637157.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: reshuffle io_uring/io_uring.h"
    },
    {
        "commit": "d142c3ec8d160bea9801f0d727e92007787df8c0",
        "message": "We don't post events in __io_commit_cqring_flush() anymore but send all\nrequests to tw, so no need to do io_commit_cqring() there.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f2481e32375e749be89c42e4804268b608722cef.1655637157.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: remove extra io_commit_cqring()"
    },
    {
        "commit": "ad163a7e2562230c77102c60f668bac440e60cce",
        "message": "Commit 3a3d47fa9cfd (\"io_uring: make io_uring_types.h public\") moved\na bunch of io_uring types to a kernel wide header, so we could make\ntracing a bit saner rather than pass in a ton of arguments.\n\nHowever, there are a few types in there that are not really needed to\nbe system wide. Move the cancel data and mapped buffers back to the\nappropriate io_uring local headers.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: move a few private types to local headers"
    },
    {
        "commit": "48863ffd3e81b6ec98606d3479c50aa77b7a98a9",
        "message": "We have lots of trace events accepting an io_uring request and wanting\nto print some of its fields like user_data, opcode, flags and so on.\nHowever, as trace points were unaware of io_uring structures, we had to\npass all the fields as arguments. Teach trace/events/io_uring.h about\nstruct io_kiocb and stop the misery of passing a horde of arguments to\ntrace helpers.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/40ff72f92798114e56d400f2b003beb6cde6ef53.1655384063.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: clean up tracing events"
    },
    {
        "commit": "ab1c84d855cf2c284fa0f4b17fc04063659c54a1",
        "message": "Move io_uring types to linux/include, need them public so tracing can\nsee the definitions and we can clean trace/events/io_uring.h\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a15f12e8cb7289b2de0deaddcc7518d98a132d17.1655384063.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: make io_uring_types.h public"
    },
    {
        "commit": "27a9d66fec77cff0e32d2ecd5d0ac7ef878a7bb0",
        "message": "io_uring/io_uring.h already includes io_uring_types.h, no need to\ninclude it every time. Kill it in a bunch of places, it prepares us for\nfollowing patches.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/94d8c943fbe0ef949981c508ddcee7fc1c18850f.1655384063.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: kill extra io_uring_types.h includes"
    },
    {
        "commit": "b3659a65be70eb68d9fc9802c4ce81e0f943abfd",
        "message": "With IORING_SETUP_CQE32 ->cqe_cached doesn't store a real address but\nrather an implicit offset into cqes. Store the real cqe pointer and\nincrement it accordingly if CQE32.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1ee1838cba16bed96381a006950b36ba640d998c.1655455613.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: change ->cqe_cached invariant for CQE32"
    },
    {
        "commit": "e8c328c3913d381bf60f2aecdf350c04b5b7e67d",
        "message": "Deduplicate calls to io_get_cqe() from __io_fill_cqe_req().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4fa077986cc3abab7c59ff4e7c390c783885465f.1655455613.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: deduplicate io_get_cqe() calls"
    },
    {
        "commit": "ae5735c69bf28fea0b8260a03caeb906319228a2",
        "message": "Deduplicate two trace_io_uring_complete() calls in __io_fill_cqe_req().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/277ed85dba5189ab7d932164b314013a0f0b0fdc.1655455613.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: deduplicate __io_fill_cqe_req tracing"
    },
    {
        "commit": "68494a65d0e2de3d99b28ae050971b6161eabac0",
        "message": "__io_fill_cqe_req() is hot and inlined, we want it to be as small as\npossible. Add io_req_cqe_overflow() accepting only a request and doing\nall overflow accounting, and replace with it two calls to 6 argument\nio_cqring_event_overflow().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/048b9fbcce56814d77a1a540409c98c3d383edcb.1655455613.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: introduce io_req_cqe_overflow()"
    },
    {
        "commit": "faf88dde060f74117b3a86a62cb32a20f27fd636",
        "message": "__io_get_cqe() is not as hot as io_get_cqe(), no need to inline it, it\nsheds ~500B from the binary.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c1ac829198a881b7af8710926f99a3559b9f24c0.1655455613.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: don't inline __io_get_cqe()"
    },
    {
        "commit": "d245bca6375bccfd589a6a7d5007df28575bb626",
        "message": "Deduplicate some code and add a helper for filling an aux CQE, locking\nand notification.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b7c6557c8f9dc5c4cfb01292116c682a0ff61081.1655455613.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: don't expose io_fill_cqe_aux()"
    },
    {
        "commit": "f09c8643f0fad0e287b9f737955276000fd76a5d",
        "message": "Add comments to explain why it is always under uring lock when\nincrementing head in __io_kbuf_recycle. And rectify one comemnt about\nkbuf consuming in iowq case.\n\nSigned-off-by: Hao Xu <howeyxu@tencent.com>\nLink: https://lore.kernel.org/r/20220617050429.94293-1-hao.xu@linux.dev\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: kbuf: add comments for some tricky code"
    },
    {
        "commit": "9ca9fb24d5febccea354089c41f96a8ad0d853f8",
        "message": "Currently we do two extra spin lock/unlock pairs to add a poll/apoll\nrequest to the cancellation hash table and remove it from there.\n\nOn the submission side we often already hold ->uring_lock and tw\ncompletion is likely to hold it as well. Add a second cancellation hash\ntable protected by ->uring_lock. In concerns for latency because of a\nneed to have the mutex locked on the completion side, use the new table\nonly in following cases:\n\n1) IORING_SETUP_SINGLE_ISSUER: only one task grabs uring_lock, so there\n   is little to no contention and so the main tw hander will almost\n   always end up grabbing it before calling callbacks.\n\n2) IORING_SETUP_SQPOLL: same as with single issuer, only one task is\n   a major user of ->uring_lock.\n\n3) apoll: we normally grab the lock on the completion side anyway to\n   execute the request, so it's free.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1bbad9c78c454b7b92f100bbf46730a37df7194f.1655371007.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:14 -0600 io_uring: mutex locked poll hashing"
    },
    {
        "commit": "5d7943d99df9326c7b02f773b2d6f09709c30594",
        "message": "Poll cancellation will be soon need to grab ->uring_lock inside, pass\nthe locking state, i.e. issue_flags, inside the cancellation functions.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b86781d047727c07163443b57551a3fa57c7c5e1.1655371007.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: propagate locking state to poll cancel"
    },
    {
        "commit": "e6f89be61410ff5a0e690d87d7a308e81f0f5a71",
        "message": "Instead of passing around a pointer to hash buckets, add a bit of type\nsafety and wrap it into a structure.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d65bc3faba537ec2aca9eabf334394936d44bd28.1655371007.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: introduce a struct for hash table"
    },
    {
        "commit": "a2cdd5193218c557b4ee7828017cce83f251e99a",
        "message": "In preparation for having multiple cancellation hash tables, pass a\ntable pointer into io_poll_find() and other poll cancel functions.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a31c88502463dce09254240fa037352927d7ecc3.1655371007.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: pass hash table into poll_find"
    },
    {
        "commit": "97bbdc06a4446bc69d8ba71d722abae542a6b70c",
        "message": "Add a new IORING_SETUP_SINGLE_ISSUER flag and the userspace visible part\nof it, i.e. put limitations of submitters. Also, don't allow it together\nwith IOPOLL as we're not going to put it to good use.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4bcc41ee467fdf04c8aab8baf6ce3ba21858c3d4.1655371007.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: add IORING_SETUP_SINGLE_ISSUER"
    },
    {
        "commit": "0ec6dca223195aca2f7df6e432c4205b5f63305b",
        "message": "Use io_req_task_complete() for poll request completions, so it can\nutilise state completions and save lots of unnecessary locking.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ced94cb5a728d8e386c640d052fd3da3f5d6891a.1655371007.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: use state completion infra for poll reqs"
    },
    {
        "commit": "8b1dfd343ae6a64ca2b4741ab47a162fa59f43be",
        "message": "Add a variable for the number of hash buckets in io_ring_ctx_alloc(),\nmakes it more readable.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/993926ed0d614ba9a76b2a85bebae2babcb13983.1655371007.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: clean up io_ring_ctx_alloc"
    },
    {
        "commit": "4a07723fb4bb2568a31d43709904ab0d4c33d6c8",
        "message": "Don't allocate to many hash/cancellation buckets, there might be too\nmany, clamp it to 8 bits, or 256 * 64B = 16KB. We don't usually have too\nmany requests, and 256 buckets should be enough, especially since we\ndo hash search only in the cancellation path.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b9620c8072ba61a2d50eba894b89bd93a94a9abd.1655371007.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: limit the number of cancellation buckets"
    },
    {
        "commit": "4dfab8abb4721da278a2ccd45c1b6a69f8a9dd14",
        "message": "Get rid of an unnecessary extra goto in io_try_cancel() and simplify the\nfunction.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/48cf5417b43a8386c6c364dba1ad9b4c7382d158.1655371007.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: clean up io_try_cancel"
    },
    {
        "commit": "1ab1edb0a104dd683d83e7853aa5b624856f4127",
        "message": "Instead of using implicit knowledge of what is locked or not after\nio_poll_find() and co returns, pass back a pointer to the locked\nbucket if any. If set the user must to unlock the spinlock.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/dae1dc5749aa34367812ecf62f82fd3f053aae44.1655371007.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: pass poll_find lock back"
    },
    {
        "commit": "38513c464d3d45b4088c82f6e42d9cdbc5ee57e6",
        "message": "Add a new io_hash_bucket structure so that each bucket in cancel_hash\nhas separate spinlock. Use per entry lock for cancel_hash, this removes\nsome completion lock invocation and remove contension between different\ncancel_hash entries.\n\nSigned-off-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/05d1e135b0c8bce9d1441e6346776589e5783e26.1655371007.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: switch cancel_hash to use per entry spinlock"
    },
    {
        "commit": "3654ab0c51a91036d15dd2fd9c2809317edf2228",
        "message": "We now don't need to set req->refcount for poll requests since the\nreworked poll code ensures no request release race.\n\nSigned-off-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ec6fee45705890bdb968b0c175519242753c0215.1655371007.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: poll: remove unnecessary req->ref set"
    },
    {
        "commit": "53ccf69bda6f51e462f3c4ab7eb9c0ec34e78be4",
        "message": "io_put_kbuf() is huge, don't bloat the kernel with inlining.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2e21ccf0be471ffa654032914b9430813cae53f8.1655371007.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: don't inline io_put_kbuf"
    },
    {
        "commit": "7012c81593d5a3bc6d0b8720345cf887ef1df914",
        "message": "Clean up io_req_task_complete() and deduplicate io_put_kbuf() calls.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ae3148ac7eb5cce3e06895cde306e9e959d6f6ae.1655371007.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: refactor io_req_task_complete()"
    },
    {
        "commit": "75d7b3aec13be557f15d099dc612dd658d670d1d",
        "message": "REQ_F_COMPLETE_INLINE is only needed to delay queueing into the\ncompletion list to io_queue_sqe() as __io_req_complete() is inlined and\nwe don't want to bloat the kernel.\n\nAs now we complete in a more centralised fashion in io_issue_sqe() we\ncan get rid of the flag and queue to the list directly.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/600ba20a9338b8a39b249b23d3d177803613dde4.1655371007.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: kill REQ_F_COMPLETE_INLINE"
    },
    {
        "commit": "df9830d883b914296beb17e4f75017faac9c4312",
        "message": "io_issue_sqe() from the io_uring core knows how to complete requests\nbased on the returned error code, we can delegate io_read()/io_write()\ncompletion to it. Make kiocb_done() to return the right completion\ncode and propagate it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/32ef005b45d23bf6b5e6837740dc0331bb051bd4.1655371007.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: rw: delegate sync completions to core io_uring"
    },
    {
        "commit": "bb8f870031584ebf50fcc1f049fa421375556114",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: remove unused IO_REQ_CACHE_SIZE defined"
    },
    {
        "commit": "c65f5279ba02e47c073520ecff3c84408a87fd17",
        "message": "io_req_task_complete() enqueues requests for state completion itself, no\nneed for REQ_F_COMPLETE_INLINE, which is only serve the purpose of not\nbloating the kernel.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/aca80f71464ad02c06f1311d998a2d6ee0b31573.1655310733.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: don't set REQ_F_COMPLETE_INLINE in tw"
    },
    {
        "commit": "3a08576b96e365d424225dd034c651e963b3ae64",
        "message": "All ctx->check_cq events are slow path, don't test every single flag one\nby one in the hot path, but add a common guarding if.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/dff026585cea7ff3a172a7c83894a3b0111bbf6a.1655310733.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: remove check_cq checking from hot paths"
    },
    {
        "commit": "aeaa72c69473d7e68addbd31f43c7c12af252bfc",
        "message": "Luckily, nnobody completes multi-apoll requests outside the polling\nfunctions, but don't set IO_URING_F_COMPLETE_DEFER in any case as\nthere is nobody who is catching REQ_F_COMPLETE_INLINE, and so will leak\nrequests if used.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a65ed3f5effd9321ee06e6edea294a03be3e15a0.1655310733.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: never defer-complete multi-apoll"
    },
    {
        "commit": "6a02e4be8187588ac476136496af9e3feeeb9a75",
        "message": "There can be only 16 registered rings, no need to allocate an array for\nthem separately but store it in tctx.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/495f0b953c87994dd9e13de2134019054fa5830d.1655310733.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: inline ->registered_rings"
    },
    {
        "commit": "48c13d8980840e489a537e4af4b2503eb9d8a1ec",
        "message": "Add a comment on why we keep ->cancel_seq in struct io_wq_work instead\nof struct io_kiocb despite it needed only by io_uring but not io-wq.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/988e87eec9dc700b5dae933df3aefef303502f6c.1655310733.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: explain io_wq_work::cancel_seq placement"
    },
    {
        "commit": "aa1e90f64ee5c82f6d3feb69b2a19370686f1f02",
        "message": "There is a bunch of inline helpers that will be useful not only to the\ncore of io_uring, move them to headers.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/22df99c83723e44cba7e945e8519e64e3642c064.1655310733.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:13 -0600 io_uring: move small helpers to headers"
    },
    {
        "commit": "22eb2a3fdea03e6056dcfb4c8bb9d3fde5ce02ff",
        "message": "Shove all slow path data at the end of ctx and get rid of extra\nindention.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bcaf200298dd469af20787650550efc66d89bef2.1655310733.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: refactor ctx slow data placement"
    },
    {
        "commit": "aff5b2df9e8b35f9814c5a4907f471472cd6be77",
        "message": "Following timeout fields access patterns, move all of them into a\nseparate cache line inside ctx, so they don't intervene with normal\ncompletion caching, especially since timeout removals and completion\nare separated and the later is done via tw.\n\nIt also sheds some bytes from io_ring_ctx, 1216B -> 1152B\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4b163793072840de53b3cb66e0c2995e7226ff78.1655310733.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: better caching for ctx timeout fields"
    },
    {
        "commit": "b25436038f6cc20c3198792cbfab8a312d09282e",
        "message": "draining is slow path, move defer_list to the end where slow data lives\ninside the context.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e16379391ca72b490afdd24e8944baab849b4a7b.1655310733.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: move defer_list to slow data"
    },
    {
        "commit": "5ff4fdffad483dad815b3701be8b82c5a07b156e",
        "message": "The default (i.e. empty) state of register buffer is dummy_ubuf, so set\nit to dummy on init instead of NULL.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c5456aecf03d9627fbd6e65e100e2b5293a6151e.1655310733.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: make reg buf init consistent"
    },
    {
        "commit": "61a2732af4b0337f7e36093612c846e9f5962965",
        "message": "As far as we know, nobody ever adopted the epoll_ctl management via\nio_uring. Deprecate it now with a warning, and plan on removing it in\na later kernel version. When we do remove it, we can revert the following\ncommits as well:\n\n39220e8d4a2a (\"eventpoll: support non-blocking do_epoll_ctl() calls\")\n58e41a44c488 (\"eventpoll: abstract out epoll_ctl() handler\")\n\nSuggested-by: Linus Torvalds <torvalds@linux-foundation.org>\nLink: https://lore.kernel.org/io-uring/CAHk-=wiTyisXBgKnVHAGYCNvkmjk=50agS2Uk6nr+n3ssLZg2w@mail.gmail.com/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: deprecate epoll_ctl support"
    },
    {
        "commit": "b9ba8a4463cd78d0aee520c4bf2569820ac29929",
        "message": "By default, the POLL_ADD command does edge triggered poll - if we get\na non-zero mask on the initial poll attempt, we complete the request\nsuccessfully.\n\nSupport level triggered by always waiting for a notification, regardless\nof whether or not the initial mask matches the file state.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: add support for level triggered poll"
    },
    {
        "commit": "d9b57aa3cfc792ccac6858376c017dbea6cb2872",
        "message": "We already have the declarations in opdef.h, move the rest into its own\nfile rather than in the main io_uring.c file.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: move opcode table to opdef.c"
    },
    {
        "commit": "f3b44f92e59a804cf375479bda0bccbf4b6e6ef6",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: move read/write related opcodes to its own file"
    },
    {
        "commit": "c98817e6cd4471a6f6283813dd6efea162f5ac5f",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: move remaining file table manipulation to filetable.c"
    },
    {
        "commit": "73572984481907d92673255b494c0ff4f77c8ed4",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: move rsrc related data, core, and commands"
    },
    {
        "commit": "3b77495a97239faa27989f946d29b6be7dd091e9",
        "message": "Move both the opcodes related to it, and the internals code dealing with\nit.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: split provided buffers handling into its own file"
    },
    {
        "commit": "7aaff708a768144ec6459f0a58301be1a6b982fc",
        "message": "This also helps cleanup the io_uring.h cancel parts, as we can make\nthings static in the cancel.c file, mostly.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: move cancelation into its own file"
    },
    {
        "commit": "329061d3e2f9a0082a097e9558bd5497098586c6",
        "message": "Add a io_poll_issue() rather than export the general task_work locking\nand io_issue_sqe(), and put the io_op_defs definition and structure into\na separate header file so that poll can use it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: move poll handling into its own file"
    },
    {
        "commit": "cfd22e6b3319adc7c2a8a092e19ac16575dabc86",
        "message": "This kills the last per-op switch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: add opcode name to io_op_defs"
    },
    {
        "commit": "92ac8beaea1f4931f932da3dcc850547621107fc",
        "message": "Remove some dead headers we no longer need, and get rid of the\nio_ring_ctx and io_uring_fops forward declarations.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: include and forward-declaration sanitation"
    },
    {
        "commit": "c9f06aa7de153cdbe424e7e38be39f2272cf78bc",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: move io_uring_task (tctx) helpers into its own file"
    },
    {
        "commit": "a4ad4f748ea96202451f88b6385d7283d0f2e513",
        "message": "This also means moving a bit more of the fixed file handling to the\nfiletable side, which makes sense separately too.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: move fdinfo helpers to its own file"
    },
    {
        "commit": "e5550a1447bf8d82f32b58e2ba54792a6985d080",
        "message": "Convert the last spots that check for io_uring_fops to use the provided\nhelper instead.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: use io_is_uring_fops() consistently"
    },
    {
        "commit": "17437f311490d873a5157f65a84317d16270fd38",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: move SQPOLL related handling into its own file"
    },
    {
        "commit": "59915143e89fb8dc7b5bd9dcaf628d8181fd54ac",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: move timeout opcodes and handling into its own file"
    },
    {
        "commit": "e418bbc97bffda868934acfdf8a1173ab044be69",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: move our reference counting into a header"
    },
    {
        "commit": "36404b09aa609e00f8f0108356830c22b99b3cbf",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:12 -0600 io_uring: move msg_ring into its own file"
    },
    {
        "commit": "f9ead18c10589a351f395ac5aa107360f2f6ce53",
        "message": "While at it, convert the handlers to just use io_eopnotsupp_prep()\nif CONFIG_NET isn't set.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: split network related opcodes into its own file"
    },
    {
        "commit": "e0da14def1ee0a9cd9c88893321e9a3d900f8e23",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: move statx handling to its own file"
    },
    {
        "commit": "a9c210cebe13c36487a239ae7f4671a389fed127",
        "message": "Would be nice to sort out Kconfig for this and don't even compile\nepoll.c if we don't have epoll configured.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: move epoll handler to its own file"
    },
    {
        "commit": "4cf90495281b43f5f597ef4a9abcc83a63973571",
        "message": "Add it and use it for the epoll handling, if epoll isn't configured.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: add a dummy -EOPNOTSUPP prep handler"
    },
    {
        "commit": "99f15d8d61364299ae780cc739c74068a6d2538d",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: move uring_cmd handling to its own file"
    },
    {
        "commit": "cd40cae29ef815de6f7e72207b677c78f43f4688",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: split out open/close operations"
    },
    {
        "commit": "453b329be5eacfc48dd43035af82bc7f28ecfedf",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: separate out file table handling code"
    },
    {
        "commit": "f4c163dd7d4b1031772317cd3cd58dd6711ee51e",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: split out fadvise/madvise operations"
    },
    {
        "commit": "0d58472740370108f8ece9076d79f736f53b5b77",
        "message": "This splits out sync_file_range, fsync, and fallocate.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: split out fs related sync/fallocate functions"
    },
    {
        "commit": "531113bbd5bfc93e8de45440752af11c751e4aaf",
        "message": "This splits out splice and tee support.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: split out splice related operations"
    },
    {
        "commit": "11aeb71406ddd0ef526ad1df48b54aae628aad3b",
        "message": "This splits out renameat, unlinkat, mkdirat, symlinkat, and linkat.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: split out filesystem related operations"
    },
    {
        "commit": "e28683bdfc2f2cb0dab042f9079cda89dbf589d9",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: move nop into its own file"
    },
    {
        "commit": "5e2a18d93fec514fc74f58a6061b74a79764af69",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: move xattr related opcodes to its own file"
    },
    {
        "commit": "97b388d70b53fd7d286ac1b81e5a88bd6af98209",
        "message": "Normally request handlers complete requests themselves, if they don't\nreturn an error. For the latter case, the core will complete it for\nthem.\n\nThis is unhandy for pushing opcode handlers further out, as we don't\nwant a bunch of inline completion code and we don't want to make the\ncompletion path slower than it is now.\n\nLet the core handle any completion, unless the handler explicitly\nasks us not to.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: handle completions in the core"
    },
    {
        "commit": "de23077eda61f549dbdadc4b6aa95f6e7b251552",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: set completion results upfront"
    },
    {
        "commit": "e27f928ee1cb068ac6c18ea244351a4c90a61139",
        "message": "This adds definitions of structs that both the core and the various\nopcode handlers need to know about.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: add io_uring_types.h"
    },
    {
        "commit": "4d4c9cff4f702d10f65473a6b4994ce1a13e64ff",
        "message": "This can move request type specific cleanup into a private handler,\nremoving the need for the core io_uring parts to know what types\nthey are dealing with.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: define a request type cleanup handler"
    },
    {
        "commit": "890968dc03361b92957e183fe8f692371e4ef18b",
        "message": "They are really just a subset of each other, just use the one type.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: unify struct io_symlink and io_hardlink"
    },
    {
        "commit": "9a3a11f977f9972b812cc8666d1ffdb93699bd92",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: convert iouring_cmd to io_cmd_type"
    },
    {
        "commit": "ceb452e1b4ba4ab207dfe119c47bf61d4519dc2e",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: convert xattr to use io_cmd_type"
    },
    {
        "commit": "ea5af87d29cfe7323f9a401539c526bdd43416a5",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:11 -0600 io_uring: convert rsrc_update to io_cmd_type"
    },
    {
        "commit": "c1ee5595015502dedd2d602ecc65bff9907fb14b",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: convert msg and nop to io_cmd_type"
    },
    {
        "commit": "2511d3030c5eaea37baee030cb36e70426ad4e6a",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: convert splice to use io_cmd_type"
    },
    {
        "commit": "3e93a3571a17022d8bde08249e468349da6b7305",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: convert epoll to io_cmd_type"
    },
    {
        "commit": "bb040a21fd0579ec7575a0ede7185d2504c021b7",
        "message": "This converts statx, rename, unlink, mkdir, symlink, and hardlink to\nuse io_cmd_type.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: convert file system request types to use io_cmd_type"
    },
    {
        "commit": "37d4842f11c5a8d4eeb8e85bf2120a167d174456",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: convert madvise/fadvise to use io_cmd_type"
    },
    {
        "commit": "dd752582e398d22fe6798f065953f4266bf4f5db",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: convert open/close path to use io_cmd_type"
    },
    {
        "commit": "a43714ace50d85b4cb19af46152f624affeef2d3",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: convert timeout path to use io_cmd_type"
    },
    {
        "commit": "f38987f09a062c17e9c5119e23b41abc6326cd4b",
        "message": "Signed-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: convert cancel path to use io_cmd_type"
    },
    {
        "commit": "e4a71006eace14a6699d66a514b6310cd8b57f7d",
        "message": "They all share the same struct io_sync, convert them to use the\nio_cmd_type approach instead.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: convert the sync and fallocate paths to use io_cmd_type"
    },
    {
        "commit": "8ff86d85b74d547a4e5f09acc884dd4cc173d087",
        "message": "This converts accept, connect, send/recv, sendmsg/recvmsg, shutdown, and\nsocket to use io_cmd_type.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: convert net related opcodes to use io_cmd_type"
    },
    {
        "commit": "bd8587e4997a88635a37fac546223346e512a30b",
        "message": "There's a special case for recvmsg with MSG_ERRQUEUE set. This is\nproblematic as it means the core needs to know about this special\nrequest type.\n\nFor now, just add a generic flag for it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: remove recvmsg knowledge from io_arm_poll_handler()"
    },
    {
        "commit": "c24b154967b6d335189eac25be59913538e2cdb4",
        "message": "Remove struct io_poll_update from io_kiocb, and convert the poll path to\nuse the io_cmd_type approach instead.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: convert poll_update path to use io_cmd_type"
    },
    {
        "commit": "8d4388d1166faa5e16fd100157c9cb3c9e982397",
        "message": "Remove struct io_poll_iocb from io_kiocb, and convert the poll path to\nuse the io_cmd_type approach instead.\n\nWhile at it, rename io_poll_iocb to io_poll which is consistent with the\nother request type private structures.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: convert poll path to use io_cmd_type"
    },
    {
        "commit": "3c306fb2f9463fd0e0e896dde9e3e53097d0e41e",
        "message": "Remove struct io_rw from io_kiocb, and convert the read/write path to\nuse the io_cmd_type approach instead.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: convert read/write path to use io_cmd_type"
    },
    {
        "commit": "f49eca21563b6d919d49828aaed9eab5d8090361",
        "message": "Each opcode generally has a command structure in io_kiocb which it can\nuse to store data associated with that request.\n\nIn preparation for having the core layer not know about what's inside\nthese fields, add a generic io_cmd_data type and put in the union as\nwell.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: add generic command payload type to struct io_kiocb"
    },
    {
        "commit": "dc919caff6b68dab1ae56cd341d96f50c2438aea",
        "message": "Define an io_op_def->prep_async() handler and push the async preparation\nto there. Since we now have that, we can drop ->needs_async_setup, as\nthey mean the same thing.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: move req async preparation into opcode handler"
    },
    {
        "commit": "ed29b0b4fd835b058ddd151c49d021e28d631ee6",
        "message": "In preparation for splitting io_uring up a bit, move it into its own\ntop level directory. It didn't really belong in fs/ anyway, as it's\nnot a file system only API.\n\nThis adds io_uring/ and moves the core files in there, and updates the\nMAINTAINERS file for the new location.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: move to separate directory"
    },
    {
        "commit": "0702e5364f643bc86683d9f585edfe76dbabae39",
        "message": "Rather than have two giant switches for doing request preparation and\nthen for doing request issue, add a prep and issue handler for each\nof them in the io_op_defs[] request definition.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-24 18:39:10 -0600 io_uring: define a 'prep' and 'issue' handler for each opcode"
    },
    {
        "commit": "3c47fb2f4c4df33881fa540e35e21415a6ecfbb5",
        "message": "Pull in Pavel's patch from a shared branch.\n\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-22 14:53:33 -0700 Merge branch 'io_uring-zerocopy-send' of git://git.kernel.org/pub/scm/linux/kernel/git/kuba/linux"
    },
    {
        "commit": "a5235996e1b04405fbd6deea37b051715214fd2a",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Fix for a bad kfree() introduced in this cycle, and a quick fix for\n  disabling buffer recycling for IORING_OP_READV.\n\n  The latter will get reworked for 5.20, but it gets the job done for\n  5.19\"\n\n* tag 'io_uring-5.19-2022-07-21' of git://git.kernel.dk/linux-block:\n  io_uring: do not recycle buffer in READV\n  io_uring: fix free of unallocated buffer list",
        "kernel_version": "v5.19-rc8",
        "release_date": "2022-07-22 12:47:09 -0700 Merge tag 'io_uring-5.19-2022-07-21' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "e94eb459d3e4604927ab4e08f81649fcea418318",
        "message": "ub->mutex is used to protecting reading and writing ub->mm, then the\nfollowing lockdep warning is triggered.\n\nFix it by using one dedicated spin lock for protecting ub->mm.\n\n[1] lockdep warning\n[   25.046186] ======================================================\n[   25.048886] WARNING: possible circular locking dependency detected\n[   25.051610] 5.19.0-rc4_for-v5.20+ #149 Not tainted\n[   25.053665] ------------------------------------------------------\n[   25.056334] ublk/989 is trying to acquire lock:\n[   25.058296] ffff975d0329a918 (&disk->open_mutex){+.+.}-{3:3}, at: bd_register_pending_holders+0x2a/0x110\n[   25.063678]\n[   25.063678] but task is already holding lock:\n[   25.066246] ffff975d1df59708 (&ub->mutex){+.+.}-{3:3}, at: ublk_ctrl_uring_cmd+0x2df/0x730\n[   25.069423]\n[   25.069423] which lock already depends on the new lock.\n[   25.069423]\n[   25.072603]\n[   25.072603] the existing dependency chain (in reverse order) is:\n[   25.074908]\n[   25.074908] -> #3 (&ub->mutex){+.+.}-{3:3}:\n[   25.076386]        __mutex_lock+0x93/0x870\n[   25.077470]        ublk_ch_mmap+0x3a/0x140\n[   25.078494]        mmap_region+0x375/0x5a0\n[   25.079386]        do_mmap+0x33a/0x530\n[   25.080168]        vm_mmap_pgoff+0xb9/0x150\n[   25.080979]        ksys_mmap_pgoff+0x184/0x1f0\n[   25.081838]        do_syscall_64+0x37/0x80\n[   25.082653]        entry_SYSCALL_64_after_hwframe+0x46/0xb0\n[   25.083730]\n[   25.083730] -> #2 (&mm->mmap_lock#2){++++}-{3:3}:\n[   25.084707]        __might_fault+0x55/0x80\n[   25.085344]        _copy_from_user+0x1e/0xa0\n[   25.086020]        get_sg_io_hdr+0x26/0xb0\n[   25.086651]        scsi_ioctl+0x42f/0x960\n[   25.087267]        sr_block_ioctl+0xe8/0x100\n[   25.087734]        blkdev_ioctl+0x134/0x2b0\n[   25.088196]        __x64_sys_ioctl+0x8a/0xc0\n[   25.088677]        do_syscall_64+0x37/0x80\n[   25.089044]        entry_SYSCALL_64_after_hwframe+0x46/0xb0\n[   25.089548]\n[   25.089548] -> #1 (&cd->lock){+.+.}-{3:3}:\n[   25.090072]        __mutex_lock+0x93/0x870\n[   25.090452]        sr_block_open+0x64/0xe0\n[   25.090837]        blkdev_get_whole+0x26/0x90\n[   25.091445]        blkdev_get_by_dev.part.0+0x1ce/0x2f0\n[   25.092203]        blkdev_open+0x52/0x90\n[   25.092617]        do_dentry_open+0x1ca/0x360\n[   25.093499]        path_openat+0x78d/0xcb0\n[   25.094136]        do_filp_open+0xa1/0x130\n[   25.094759]        do_sys_openat2+0x76/0x130\n[   25.095454]        __x64_sys_openat+0x5c/0x70\n[   25.096078]        do_syscall_64+0x37/0x80\n[   25.096637]        entry_SYSCALL_64_after_hwframe+0x46/0xb0\n[   25.097304]\n[   25.097304] -> #0 (&disk->open_mutex){+.+.}-{3:3}:\n[   25.098229]        __lock_acquire+0x12e2/0x1f90\n[   25.098789]        lock_acquire+0xbf/0x2c0\n[   25.099256]        __mutex_lock+0x93/0x870\n[   25.099706]        bd_register_pending_holders+0x2a/0x110\n[   25.100246]        device_add_disk+0x209/0x370\n[   25.100712]        ublk_ctrl_uring_cmd+0x405/0x730\n[   25.101205]        io_issue_sqe+0xfe/0x2ac0\n[   25.101665]        io_submit_sqes+0x352/0x1820\n[   25.102131]        __do_sys_io_uring_enter+0x848/0xdc0\n[   25.102646]        do_syscall_64+0x37/0x80\n[   25.103087]        entry_SYSCALL_64_after_hwframe+0x46/0xb0\n[   25.103640]\n[   25.103640] other info that might help us debug this:\n[   25.103640]\n[   25.104549] Chain exists of:\n[   25.104549]   &disk->open_mutex --> &mm->mmap_lock#2 --> &ub->mutex\n[   25.104549]\n[   25.105611]  Possible unsafe locking scenario:\n[   25.105611]\n[   25.106258]        CPU0                    CPU1\n[   25.106677]        ----                    ----\n[   25.107100]   lock(&ub->mutex);\n[   25.107446]                                lock(&mm->mmap_lock#2);\n[   25.108045]                                lock(&ub->mutex);\n[   25.108802]   lock(&disk->open_mutex);\n[   25.109265]\n[   25.109265]  *** DEADLOCK ***\n[   25.109265]\n[   25.110117] 2 locks held by ublk/989:\n[   25.110490]  #0: ffff975d07bbf8a8 (&ctx->uring_lock){+.+.}-{3:3}, at: __do_sys_io_uring_enter+0x83e/0xdc0\n[   25.111249]  #1: ffff975d1df59708 (&ub->mutex){+.+.}-{3:3}, at: ublk_ctrl_uring_cmd+0x2df/0x730\n[   25.111943]\n[   25.111943] stack backtrace:\n[   25.112557] CPU: 2 PID: 989 Comm: ublk Not tainted 5.19.0-rc4_for-v5.20+ #149\n[   25.113137] Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 1.14.0-1.fc33 04/01/2014\n[   25.113792] Call Trace:\n[   25.114130]  <TASK>\n[   25.114417]  dump_stack_lvl+0x71/0xa0\n[   25.114771]  check_noncircular+0xdf/0x100\n[   25.115137]  ? register_lock_class+0x38/0x470\n[   25.115524]  __lock_acquire+0x12e2/0x1f90\n[   25.115887]  ? find_held_lock+0x2b/0x80\n[   25.116244]  lock_acquire+0xbf/0x2c0\n[   25.116590]  ? bd_register_pending_holders+0x2a/0x110\n[   25.117009]  __mutex_lock+0x93/0x870\n[   25.117362]  ? bd_register_pending_holders+0x2a/0x110\n[   25.117780]  ? bd_register_pending_holders+0x2a/0x110\n[   25.118201]  ? kobject_add+0x71/0x90\n[   25.118546]  ? bd_register_pending_holders+0x2a/0x110\n[   25.118958]  bd_register_pending_holders+0x2a/0x110\n[   25.119373]  device_add_disk+0x209/0x370\n[   25.119732]  ublk_ctrl_uring_cmd+0x405/0x730\n[   25.120109]  ? rcu_read_lock_sched_held+0x3c/0x70\n[   25.120514]  io_issue_sqe+0xfe/0x2ac0\n[   25.120863]  io_submit_sqes+0x352/0x1820\n[   25.121228]  ? rcu_read_lock_sched_held+0x3c/0x70\n[   25.121626]  ? __do_sys_io_uring_enter+0x83e/0xdc0\n[   25.122028]  ? find_held_lock+0x2b/0x80\n[   25.122390]  ? __do_sys_io_uring_enter+0x848/0xdc0\n[   25.122791]  __do_sys_io_uring_enter+0x848/0xdc0\n[   25.123190]  ? syscall_enter_from_user_mode+0x20/0x70\n[   25.123606]  ? syscall_enter_from_user_mode+0x20/0x70\n[   25.124024]  do_syscall_64+0x37/0x80\n[   25.124383]  entry_SYSCALL_64_after_hwframe+0x46/0xb0\n[   25.124829] RIP: 0033:0x7f120a762af6\n[   25.125223] Code: 45 c1 41 89 c2 41 b9 08 00 00 00 41 83 ca 10 f6 87 d0 00 00 00 01 8b bf cc 00 00 00 44 0f 44 d0 45 31 c0c\n[   25.126576] RSP: 002b:00007ffdcb3c5518 EFLAGS: 00000246 ORIG_RAX: 00000000000001aa\n[   25.127153] RAX: ffffffffffffffda RBX: 00000000013aef50 RCX: 00007f120a762af6\n[   25.127748] RDX: 0000000000000000 RSI: 0000000000000001 RDI: 0000000000000004\n[   25.128351] RBP: 000000000000000b R08: 0000000000000000 R09: 0000000000000008\n[   25.128956] R10: 0000000000000000 R11: 0000000000000246 R12: 00007ffdcb3c74a6\n[   25.129524] R13: 00000000013aef50 R14: 0000000000000000 R15: 00000000000003df\n[   25.130121]  </TASK>\n\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20220721153117.591394-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-21 13:14:44 -0600 ublk_drv: fix lockdep warning"
    },
    {
        "commit": "934447a603b22d98f45a679115d8402e1efdd0f7",
        "message": "READV cannot recycle buffers as it would lose some of the data required to\nreimport that buffer.\n\nReported-by: Ammar Faizi <ammarfaizi2@gnuweeb.org>\nFixes: b66e65f41426 (\"io_uring: never call io_buffer_select() for a buffer re-select\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220721131325.624788-1-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc8",
        "release_date": "2022-07-21 08:31:31 -0600 io_uring: do not recycle buffer in READV"
    },
    {
        "commit": "ec8516f3b7c40ba7050e6b3a32467e9de451ecdf",
        "message": "in the error path of io_register_pbuf_ring, only free bl if it was\nallocated.\n\nReported-by: Dipanjan Das <mail.dipanjan.das@gmail.com>\nFixes: c7fb19428d67 (\"io_uring: add support for ring mapped supplied buffers\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/all/CANX2M5bXKw1NaHdHNVqssUUaBCs8aBpmzRNVEYEvV0n44P7ioA@mail.gmail.com/\nLink: https://lore.kernel.org/all/CANX2M5YiZBXU3L6iwnaLs-HHJXRvrxM8mhPDiMDF9Y9sAvOHUA@mail.gmail.com/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc8",
        "release_date": "2022-07-21 08:29:01 -0600 io_uring: fix free of unallocated buffer list"
    },
    {
        "commit": "7f9eee196ec83fe57ad9a53f413d4246d2748e9a",
        "message": "Pavel Begunkov says:\n\n====================\nio_uring zerocopy send\n\nThe patchset implements io_uring zerocopy send. It works with both registered\nand normal buffers, mixing is allowed but not recommended. Apart from usual\nrequest completions, just as with MSG_ZEROCOPY, io_uring separately notifies\nthe userspace when buffers are freed and can be reused (see API design below),\nwhich is delivered into io_uring's Completion Queue. Those \"buffer-free\"\nnotifications are not necessarily per request, but the userspace has control\nover it and should explicitly attaching a number of requests to a single\nnotification. The series also adds some internal optimisations when used with\nregistered buffers like removing page referencing.\n\nFrom the kernel networking perspective there are two main changes. The first\none is passing ubuf_info into the network layer from io_uring (inside of an\nin kernel struct msghdr). This allows extra optimisations, e.g. ubuf_info\ncaching on the io_uring side, but also helps to avoid cross-referencing\nand synchronisation problems. The second part is an optional optimisation\nremoving page referencing for requests with registered buffers.\n\nBenchmarking UDP with an optimised version of the selftest (see [1]), which\nsends a bunch of requests, waits for completions and repeats. \"+ flush\" column\nposts one additional \"buffer-free\" notification per request, and just \"zc\"\ndoesn't post buffer notifications at all.\n\nNIC (requests / second):\nIO size | non-zc    | zc             | zc + flush\n4000    | 495134    | 606420 (+22%)  | 558971 (+12%)\n1500    | 551808    | 577116 (+4.5%) | 565803 (+2.5%)\n1000    | 584677    | 592088 (+1.2%) | 560885 (-4%)\n600     | 596292    | 598550 (+0.4%) | 555366 (-6.7%)\n\ndummy (requests / second):\nIO size | non-zc    | zc             | zc + flush\n8000    | 1299916   | 2396600 (+84%) | 2224219 (+71%)\n4000    | 1869230   | 2344146 (+25%) | 2170069 (+16%)\n1200    | 2071617   | 2361960 (+14%) | 2203052 (+6%)\n600     | 2106794   | 2381527 (+13%) | 2195295 (+4%)\n\nPreviously it also brought a massive performance speedup compared to the\nmsg_zerocopy tool (see [3]), which is probably not super interesting. There\nis also an additional bunch of refcounting optimisations that was omitted from\nthe series for simplicity and as they don't change the picture drastically,\nthey will be sent as follow up, as well as flushing optimisations closing the\nperformance gap b/w two last columns.\n\nFor TCP on localhost (with hacks enabling localhost zerocopy) and including\nadditional overhead for receive:\n\nIO size | non-zc    | zc\n1200    | 4174      | 4148\n4096    | 7597      | 11228\n\nUsing a real NIC 1200 bytes, zc is worse than non-zc ~5-10%, maybe the\nomitted optimisations will somewhat help, should look better for 4000,\nbut couldn't test properly because of setup problems.\n\nLinks:\n\n  liburing (benchmark + tests):\n  [1] https://github.com/isilence/liburing/tree/zc_v4\n\n  kernel repo:\n  [2] https://github.com/isilence/linux/tree/zc_v4\n\n  RFC v1:\n  [3] https://lore.kernel.org/io-uring/cover.1638282789.git.asml.silence@gmail.com/\n\n  RFC v2:\n  https://lore.kernel.org/io-uring/cover.1640029579.git.asml.silence@gmail.com/\n\n  Net patches based:\n  git@github.com:isilence/linux.git zc_v4-net-base\n  or\n  https://github.com/isilence/linux/tree/zc_v4-net-base\n\nAPI design overview:\n\n  The series introduces an io_uring concept of notifactors. From the userspace\n  perspective it's an entity to which it can bind one or more requests and then\n  requesting to flush it. Flushing a notifier makes it impossible to attach new\n  requests to it, and instructs the notifier to post a completion once all\n  requests attached to it are completed and the kernel doesn't need the buffers\n  anymore.\n\n  Notifications are stored in notification slots, which should be registered as\n  an array in io_uring. Each slot stores only one notifier at any particular\n  moment. Flushing removes it from the slot and the slot automatically replaces\n  it with a new notifier. All operations with notifiers are done by specifying\n  an index of a slot it's currently in.\n\n  When registering a notification the userspace specifies a u64 tag for each\n  slot, which will be copied in notification completion entries as\n  cqe::user_data. cqe::res is 0 and cqe::flags is equal to wrap around u32\n  sequence number counting notifiers of a slot.\n\n====================\n\nLink: https://lore.kernel.org/r/cover.1657643355.git.asml.silence@gmail.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-19 14:22:41 -0700 Merge branch 'io_uring-zerocopy-send' of git://git.kernel.org/pub/scm/linux/kernel/git/kuba/linux"
    },
    {
        "commit": "753f1ca4e1e50248a1b760c9774d6d6b354562cc",
        "message": "Some users like io_uring can do page pinning more efficiently, so we\nwant a way to delegate referencing to other subsystems. For that add\na new flag called SKBFL_MANAGED_FRAG_REFS. When set, skb doesn't hold\npage references and upper layers are responsivle to managing page\nlifetime.\n\nIt's allowed to convert skbs from managed to normal by calling\nskb_zcopy_downgrade_managed(). The function will take all needed\npage references and clear the flag. It's needed, for instance,\nto avoid mixing managed modes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-19 14:20:54 -0700 net: introduce managed frags infrastructure"
    },
    {
        "commit": "7c701d92b2b5e5175dbfec875816474b802b0c45",
        "message": "Make possible for network in-kernel callers like io_uring to pass in a\ncustom ubuf_info by setting it in a new field of struct msghdr.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-19 14:20:42 -0700 skbuff: carry external ubuf_info in msghdr"
    },
    {
        "commit": "cebbe577cb17ed9b04b50d9e6802a8bacffbadca",
        "message": "Call blk_cleanup_queue() in release code path for fixing request\nqueue leak.\n\nAlso for-5.20/block has cleaned up blk_cleanup_queue(), which is\nbasically merged to del_gendisk() if blk_mq_alloc_disk() is used\nfor allocating disk and queue.\n\nHowever, ublk may not add disk in case of starting device failure, then\ndel_gendisk() won't be called when removing ublk device, so blk_mq_exit_queue\nwill not be callsed, and it can be bit hard to deal with this kind of\nmerge conflict.\n\nTurns out ublk's queue/disk use model is very similar with scsi, so switch\nto scsi's model by allocating disk and queue independently, then it can be\nquite easy to handle v5.20 merge conflict by replacing blk_cleanup_queue\nwith blk_mq_destroy_queue.\n\nReported-by: Jens Axboe <axboe@kernel.dk>\nFixes: 71f28f3136af (\"ublk_drv: add io_uring based userspace block driver\")\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20220714103201.131648-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-14 07:16:04 -0600 ublk_drv: fix request queue leak"
    },
    {
        "commit": "0edb3696c1713c42f52acbd8355b545e58f782b1",
        "message": "Use task_work_add if it is available, since task_work_add can bring\nup better performance, especially batching signaling ->ubq_daemon can\nbe done.\n\nIt is observed that task_work_add() can boost iops by +4% on random\n4k io test. Also except for completing io command, all other code\npaths are same with completing io command via\nio_uring_cmd_complete_in_task.\n\nMeantime add one flag of UBLK_F_URING_CMD_COMP_IN_TASK for comparing\nthe mode easily.\n\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20220713140711.97356-3-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-14 07:15:48 -0600 ublk_drv: support to complete io command via task_work_add"
    },
    {
        "commit": "71f28f3136aff5890cd56de78abc673f8393cad9",
        "message": "This is the driver part of userspace block driver(ublk driver), the other\npart is userspace daemon part(ublksrv)[1].\n\nThe two parts communicate by io_uring's IORING_OP_URING_CMD with one\nshared cmd buffer for storing io command, and the buffer is read only for\nublksrv, each io command is indexed by io request tag directly, and is\nwritten by ublk driver.\n\nFor example, when one READ io request is submitted to ublk block driver,\nublk driver stores the io command into cmd buffer first, then completes\none IORING_OP_URING_CMD for notifying ublksrv, and the URING_CMD is issued\nto ublk driver beforehand by ublksrv for getting notification of any new\nio request, and each URING_CMD is associated with one io request by tag.\n\nAfter ublksrv gets the io command, it translates and handles the ublk io\nrequest, such as, for the ublk-loop target, ublksrv translates the request\ninto same request on another file or disk, like the kernel loop block\ndriver. In ublksrv's implementation, the io is still handled by io_uring,\nand share same ring with IORING_OP_URING_CMD command. When the target io\nrequest is done, the same IORING_OP_URING_CMD is issued to ublk driver for\nboth committing io request result and getting future notification of new\nio request.\n\nAnother thing done by ublk driver is to copy data between kernel io\nrequest and ublksrv's io buffer:\n\n1) before ubsrv handles WRITE request, copy the request's data into\n   ublksrv's userspace io buffer, so that ublksrv can handle the write\n   request\n\n2) after ubsrv handles READ request, copy ublksrv's userspace io buffer\n   into this READ request, then ublk driver can complete the READ request\n\nZero copy may be switched if mm is ready to support it.\n\nublk driver doesn't handle any logic of the specific user space driver,\nso it is small/simple enough.\n\n[1] ublksrv\n\nhttps://github.com/ming1/ubdsrv\n\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20220713140711.97356-2-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-07-14 07:15:28 -0600 ublk_drv: add io_uring based userspace block driver"
    },
    {
        "commit": "d9919d43cbf6790d2bc0c0a2743c51fc25f26919",
        "message": "Pull io_uring fix from Jens Axboe:\n \"A single fix for an issue that came up yesterday that we should plug\n  for -rc6.\n\n  This is a regression introduced in this cycle\"\n\n* tag 'io_uring-5.19-2022-07-09' of git://git.kernel.dk/linux-block:\n  io_uring: check that we have a file table when allocating update slots",
        "kernel_version": "v5.19-rc6",
        "release_date": "2022-07-10 09:14:54 -0700 Merge tag 'io_uring-5.19-2022-07-09' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "d785a773bed966a75ca1f11d108ae1897189975b",
        "message": "If IORING_FILE_INDEX_ALLOC is set asking for an allocated slot, the\nhelper doesn't check if we actually have a file table or not. The non\nalloc path does do that correctly, and returns -ENXIO if we haven't set\none up.\n\nDo the same for the allocated path, avoiding a NULL pointer dereference\nwhen trying to find a free bit.\n\nFixes: a7c41b4687f5 (\"io_uring: let IORING_OP_FILES_UPDATE support choosing fixed file slots\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc6",
        "release_date": "2022-07-09 07:02:10 -0600 io_uring: check that we have a file table when allocating update slots"
    },
    {
        "commit": "29837019d5ebb80a5f180af3107a0645c731a770",
        "message": "Pull io_uring tweak from Jens Axboe:\n \"Just a minor tweak to an addition made in this release cycle: padding\n  a 32-bit value that's in a 64-bit union to avoid any potential\n  funkiness from that\"\n\n* tag 'io_uring-5.19-2022-07-08' of git://git.kernel.dk/linux-block:\n  io_uring: explicit sqe padding for ioctl commands",
        "kernel_version": "v5.19-rc6",
        "release_date": "2022-07-08 11:25:01 -0700 Merge tag 'io_uring-5.19-2022-07-08' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "a4527e1853f8ff6e0b7c2dadad6268bd38427a31",
        "message": "When doing a direct IO read or write, we always return -ENOTBLK when we\nfind a compressed extent (or an inline extent) so that we fallback to\nbuffered IO. This however is not ideal in case we are in a NOWAIT context\n(io_uring for example), because buffered IO can block and we currently\nhave no support for NOWAIT semantics for buffered IO, so if we need to\nfallback to buffered IO we should first signal the caller that we may\nneed to block by returning -EAGAIN instead.\n\nThis behaviour can also result in short reads being returned to user\nspace, which although it's not incorrect and user space should be able\nto deal with partial reads, it's somewhat surprising and even some popular\napplications like QEMU (Link tag #1) and MariaDB (Link tag #2) don't\ndeal with short reads properly (or at all).\n\nThe short read case happens when we try to read from a range that has a\nnon-compressed and non-inline extent followed by a compressed extent.\nAfter having read the first extent, when we find the compressed extent we\nreturn -ENOTBLK from btrfs_dio_iomap_begin(), which results in iomap to\ntreat the request as a short read, returning 0 (success) and waiting for\npreviously submitted bios to complete (this happens at\nfs/iomap/direct-io.c:__iomap_dio_rw()). After that, and while at\nbtrfs_file_read_iter(), we call filemap_read() to use buffered IO to\nread the remaining data, and pass it the number of bytes we were able to\nread with direct IO. Than at filemap_read() if we get a page fault error\nwhen accessing the read buffer, we return a partial read instead of an\n-EFAULT error, because the number of bytes previously read is greater\nthan zero.\n\nSo fix this by returning -EAGAIN for NOWAIT direct IO when we find a\ncompressed or an inline extent.\n\nReported-by: Dominique MARTINET <dominique.martinet@atmark-techno.com>\nLink: https://lore.kernel.org/linux-btrfs/YrrFGO4A1jS0GI0G@atmark-techno.com/\nLink: https://jira.mariadb.org/browse/MDEV-27900?focusedCommentId=216582&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-216582\nTested-by: Dominique MARTINET <dominique.martinet@atmark-techno.com>\nCC: stable@vger.kernel.org # 5.10+\nReviewed-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Filipe Manana <fdmanana@suse.com>\nSigned-off-by: David Sterba <dsterba@suse.com>",
        "kernel_version": "v5.19-rc7",
        "release_date": "2022-07-08 19:13:22 +0200 btrfs: return -EAGAIN for NOWAIT dio reads/writes on compressed and inline extents"
    },
    {
        "commit": "bdb2c48e4b38e6dbe82533b437468999ba3ae498",
        "message": "32 bit sqe->cmd_op is an union with 64 bit values. It's always a good\nidea to do padding explicitly. Also zero check it in prep, so it can be\nused in the future if needed without compatibility concerns.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e6b95a05e970af79000435166185e85b196b2ba2.1657202417.git.asml.silence@gmail.com\n[axboe: turn bitwise OR into logical variant]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc6",
        "release_date": "2022-07-07 17:33:01 -0600 io_uring: explicit sqe padding for ioctl commands"
    },
    {
        "commit": "0a35d1622d5cd7693d75b7124913c75a7e3fabd0",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two minor tweaks:\n\n   - While we still can, adjust the send/recv based flags to be in\n     ->ioprio rather than in ->addr2. This is consistent with eg accept,\n     and also doesn't waste a full 64-bit field for flags (Pavel)\n\n   - 5.18-stable fix for re-importing provided buffers. Not much real\n     world relevance here as it'll only impact non-pollable files gone\n     async, which is more of a practical test case rather than something\n     that is used in the wild (Dylan)\"\n\n* tag 'io_uring-5.19-2022-07-01' of git://git.kernel.dk/linux-block:\n  io_uring: fix provided buffer import\n  io_uring: keep sendrecv flags in ioprio",
        "kernel_version": "v5.19-rc5",
        "release_date": "2022-07-01 10:52:01 -0700 Merge tag 'io_uring-5.19-2022-07-01' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "09007af2b627f0f195c6c53c4829b285cc3990ec",
        "message": "io_import_iovec uses the s pointer, but this was changed immediately\nafter the iovec was re-imported and so it was imported into the wrong\nplace.\n\nChange the ordering.\n\nFixes: 2be2eb02e2f5 (\"io_uring: ensure reads re-import for selected buffers\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220630132006.2825668-1-dylany@fb.com\n[axboe: ensure we don't half-import as well]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc5",
        "release_date": "2022-06-30 11:34:41 -0600 io_uring: fix provided buffer import"
    },
    {
        "commit": "29c1ac230e6056b26846c66881802b581a78ad72",
        "message": "We waste a u64 SQE field for flags even though we don't need as many\nbits and it can be used for something more useful later. Store io_uring\nspecific send/recv flags in sqe->ioprio instead of ->addr2.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nFixes: 0455d4ccec54 (\"io_uring: add POLL_FIRST support for send/sendmsg and recv/recvmsg\")\n[axboe: change comment in io_uring.h as well]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc5",
        "release_date": "2022-06-30 07:15:50 -0600 io_uring: keep sendrecv flags in ioprio"
    },
    {
        "commit": "a78418e6a04c93b9ffd3f0f601c5cb10612acb7f",
        "message": "Currently, IO priority set in task's IO context is not reflected in the\nbio->bi_ioprio for most IO (only io_uring and direct IO set it). This\nresults in odd results where process is submitting some bios with one\npriority and other bios with a different (unset) priority and due to\ndiffering priorities bios cannot be merged. Make sure bio->bi_ioprio is\nalways set on bio submission.\n\nReviewed-by: Damien Le Moal <damien.lemoal@opensource.wdc.com>\nTested-by: Damien Le Moal <damien.lemoal@opensource.wdc.com>\nSigned-off-by: Jan Kara <jack@suse.cz>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20220623074840.5960-9-jack@suse.cz\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v6.0-rc1",
        "release_date": "2022-06-27 06:29:12 -0600 block: Always initialize bio IO priority on submit"
    },
    {
        "commit": "598f2404879336277a4320ac5000394b873e049a",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A few fixes that should go into the 5.19 release. All are fixing\n  issues that either happened in this release, or going to stable.\n\n  In detail:\n\n   - A small series of fixlets for the poll handling, all destined for\n     stable (Pavel)\n\n   - Fix a merge error from myself that caused a potential -EINVAL for\n     the recv/recvmsg flag setting (me)\n\n   - Fix a kbuf recycling issue for partial IO (me)\n\n   - Use the original request for the inflight tracking (me)\n\n   - Fix an issue introduced this merge window with trace points using a\n     custom decoder function, which won't work for perf (Dylan)\"\n\n* tag 'io_uring-5.19-2022-06-24' of git://git.kernel.dk/linux-block:\n  io_uring: use original request task for inflight tracking\n  io_uring: move io_uring_get_opcode out of TP_printk\n  io_uring: fix double poll leak on repolling\n  io_uring: fix wrong arm_poll error handling\n  io_uring: fail links when poll fails\n  io_uring: fix req->apoll_events\n  io_uring: fix merge error in checking send/recv addr2 flags\n  io_uring: mark reissue requests with REQ_F_PARTIAL_IO",
        "kernel_version": "v5.19-rc4",
        "release_date": "2022-06-24 11:02:26 -0700 Merge tag 'io_uring-5.19-2022-06-24' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "386e4fb6962b9f248a80f8870aea0870ca603e89",
        "message": "In prior kernels, we did file assignment always at prep time. This meant\nthat req->task == current. But after deferring that assignment and then\npushing the inflight tracking back in, we've got the inflight tracking\nusing current when it should in fact now be using req->task.\n\nFixup that error introduced by adding the inflight tracking back after\nfile assignments got modifed.\n\nFixes: 9cae36a094e7 (\"io_uring: reinstate the inflight tracking\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc4",
        "release_date": "2022-06-23 11:06:43 -0600 io_uring: use original request task for inflight tracking"
    },
    {
        "commit": "e70b64a3f28b9f54602ae3e706b1dc1338de3df7",
        "message": "The TP_printk macro's are not supposed to use custom code ([1]) or else\ntools such as perf cannot use these events.\n\nConvert the opcode string representation to use the __string wiring that\nthe event framework provides ([2]).\n\n[1]: https://lwn.net/Articles/379903/\n[2]: https://lwn.net/Articles/381064/\n\nFixes: 033b87d24f72 (\"io_uring: use the text representation of ops in trace\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220623083743.2648321-1-dylany@fb.com\n[axboe: fixup spurious removal of sq_thread assignment]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc4",
        "release_date": "2022-06-23 08:40:36 -0600 io_uring: move io_uring_get_opcode out of TP_printk"
    },
    {
        "commit": "c0737fa9a5a5cf5a053bcc983f72d58919b997c6",
        "message": "We have re-polling for partial IO, so a request can be polled twice. If\nit used two poll entries the first time then on the second\nio_arm_poll_handler() it will find the old apoll entry and NULL\nkmalloc()'ed second entry, i.e. apoll->double_poll, so leaking it.\n\nFixes: 10c873334feba (\"io_uring: allow re-poll if we made progress\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/fee2452494222ecc7f1f88c8fb659baef971414a.1655852245.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc4",
        "release_date": "2022-06-21 17:24:37 -0600 io_uring: fix double poll leak on repolling"
    },
    {
        "commit": "9d2ad2947a53abf5e5e6527a9eeed50a3a4cbc72",
        "message": "Leaving ip.error set when a request was punted to task_work execution is\nproblematic, don't forget to clear it.\n\nFixes: aa43477b04025 (\"io_uring: poll rework\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a6c84ef4182c6962380aebe11b35bdcb25b0ccfb.1655852245.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc4",
        "release_date": "2022-06-21 17:24:37 -0600 io_uring: fix wrong arm_poll error handling"
    },
    {
        "commit": "c487a5ad48831afa6784b368ec40d0ee50f2fe1b",
        "message": "Don't forget to cancel all linked requests of poll request when\n__io_arm_poll_handler() failed.\n\nFixes: aa43477b04025 (\"io_uring: poll rework\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a78aad962460f9fdfe4aa4c0b62425c88f9415bc.1655852245.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc4",
        "release_date": "2022-06-21 17:24:37 -0600 io_uring: fail links when poll fails"
    },
    {
        "commit": "aacf2f9f382c91df73f33317e28a4c34c8038986",
        "message": "apoll_events should be set once in the beginning of poll arming just as\npoll->events and not change after. However, currently io_uring resets it\non each __io_poll_execute() for no clear reason. There is also a place\nin __io_arm_poll_handler() where we add EPOLLONESHOT to downgrade a\nmultishot, but forget to do the same thing with ->apoll_events, which is\nbuggy.\n\nFixes: 81459350d581e (\"io_uring: cache req->apoll->events in req->cflags\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nLink: https://lore.kernel.org/r/0aef40399ba75b1a4d2c2e85e6e8fd93c02fc6e4.1655814213.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc4",
        "release_date": "2022-06-21 07:49:05 -0600 io_uring: fix req->apoll_events"
    },
    {
        "commit": "b60cac14bb3c88cff2a7088d9095b01a80938c41",
        "message": "With the dropping of the IOPOLL checking in the per-opcode handlers,\nwe inadvertently left two checks in the recv/recvmsg and send/sendmsg\nprep handlers for the same thing, and one of them includes addr2 which\nholds the flags for these opcodes.\n\nFix it up and kill the redundant checks.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc4",
        "release_date": "2022-06-21 07:47:13 -0600 io_uring: fix merge error in checking send/recv addr2 flags"
    },
    {
        "commit": "1bacd264d3c3a05de4afdd1712c9dd6ccebb9490",
        "message": "If we mark for reissue, we assume that the buffer will remain stable.\nHence if are using a provided buffer, we need to ensure that we stick\nwith it for the duration of that request.\n\nThis only affects block devices that use provided buffers, as those are\nthe only ones that get marked with REQ_F_REISSUE.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc4",
        "release_date": "2022-06-20 06:39:27 -0600 io_uring: mark reissue requests with REQ_F_PARTIAL_IO"
    },
    {
        "commit": "f8e174c3071dc7614b2a6aa41494b2756d0cd93d",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Bigger than usual at this time, both because we missed -rc2, but also\n  because of some reverts that we chose to do. In detail:\n\n   - Adjust mapped buffer API while we still can (Dylan)\n\n   - Mapped buffer fixes (Dylan, Hao, Pavel, me)\n\n   - Fix for uring_cmd wrong API usage for task_work (Dylan)\n\n   - Fix for bug introduced in fixed file closing (Hao)\n\n   - Fix race in buffer/file resource handling (Pavel)\n\n   - Revert the NOP support for CQE32 and buffer selection that was\n     brought up during the merge window (Pavel)\n\n   - Remove IORING_CLOSE_FD_AND_FILE_SLOT introduced in this merge\n     window. The API needs further refining, so just yank it for now and\n     we'll revisit for a later kernel.\n\n   - Series cleaning up the CQE32 support added in this merge window,\n     making it more integrated rather than sitting on the side (Pavel)\"\n\n* tag 'io_uring-5.19-2022-06-16' of git://git.kernel.dk/linux-block: (21 commits)\n  io_uring: recycle provided buffer if we punt to io-wq\n  io_uring: do not use prio task_work_add in uring_cmd\n  io_uring: commit non-pollable provided mapped buffers upfront\n  io_uring: make io_fill_cqe_aux honour CQE32\n  io_uring: remove __io_fill_cqe() helper\n  io_uring: fix ->extra{1,2} misuse\n  io_uring: fill extra big cqe fields from req\n  io_uring: unite fill_cqe and the 32B version\n  io_uring: get rid of __io_fill_cqe{32}_req()\n  io_uring: remove IORING_CLOSE_FD_AND_FILE_SLOT\n  Revert \"io_uring: add buffer selection support to IORING_OP_NOP\"\n  Revert \"io_uring: support CQE32 for nop operation\"\n  io_uring: limit size of provided buffer ring\n  io_uring: fix types in provided buffer ring\n  io_uring: fix index calculation\n  io_uring: fix double unlock for pbuf select\n  io_uring: kbuf: fix bug of not consuming ring buffer in partial io case\n  io_uring: openclose: fix bug of closing wrong fixed file\n  io_uring: fix not locked access to fixed buf table\n  io_uring: fix races with buffer table unregister\n  ...",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-17 11:14:07 -0700 Merge tag 'io_uring-5.19-2022-06-16' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "6436c770f120a9ffeb4e791650467f30f1d062d1",
        "message": "io_arm_poll_handler() will recycle the buffer appropriately if we end\nup arming poll (or if we're ready to retry), but not for the io-wq case\nif we have attempted poll first.\n\nExplicitly recycle the buffer to avoid both hanging on to it too long,\nbut also to avoid multiple reads grabbing the same one. This can happen\nfor ring mapped buffers, since it hasn't necessarily been committed.\n\nFixes: c7fb19428d67 (\"io_uring: add support for ring mapped supplied buffers\")\nLink: https://github.com/axboe/liburing/issues/605\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-17 06:24:26 -0600 io_uring: recycle provided buffer if we punt to io-wq"
    },
    {
        "commit": "32fc810b364f3dd30930c594e461ffa1761fef39",
        "message": "io_req_task_prio_work_add has a strict assumption that it will only be\nused with io_req_task_complete. There is a codepath that assumes this is\nthe case and will not even call the completion function if it is hit.\n\nFor uring_cmd with an arbitrary completion function change the call to the\ncorrect non-priority version.\n\nFixes: ee692a21e9bf8 (\"fs,io_uring: add infrastructure for uring-cmd\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20220616135011.441980-1-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-16 09:10:26 -0600 io_uring: do not use prio task_work_add in uring_cmd"
    },
    {
        "commit": "a76c0b31eef50fdb8b21d53a6d050f59241fb88e",
        "message": "For recv/recvmsg, IO either completes immediately or gets queued for a\nretry. This isn't the case for read/readv, if eg a normal file or a block\ndevice is used. Here, an operation can get queued with the block layer.\nIf this happens, ring mapped buffers must get committed immediately to\navoid that the next read can consume the same buffer.\n\nCheck if we're dealing with pollable file, when getting a new ring mapped\nprovided buffer. If it's not, commit it immediately rather than wait post\nissue. If we don't wait, we can race with completions coming in, or just\nplain buffer reuse by committing after a retry where others could have\ngrabbed the same buffer.\n\nFixes: c7fb19428d67 (\"io_uring: add support for ring mapped supplied buffers\")\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-16 07:14:44 -0600 io_uring: commit non-pollable provided mapped buffers upfront"
    },
    {
        "commit": "c5595975b53a487bf329eeba65b5c5f34605a4c0",
        "message": "Don't let io_fill_cqe_aux() post 16B cqes for CQE32 rings, neither the\nkernel nor the userspace expect this to happen.\n\nFixes: 76c68fbf1a1f9 (\"io_uring: enable CQE32\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/64fae669fae1b7083aa15d0cd807f692b0880b9a.1655287457.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-15 05:06:56 -0600 io_uring: make io_fill_cqe_aux honour CQE32"
    },
    {
        "commit": "cd94903d3ba50d7ae797c603f68996af8d1ba1a1",
        "message": "In preparation for the following patch, inline __io_fill_cqe(), there is\nonly one user.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/71dab9afc3cde3f8b64d26f20d3b60bdc40726ff.1655287457.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-15 05:06:42 -0600 io_uring: remove __io_fill_cqe() helper"
    },
    {
        "commit": "2caf9822f0507463168a9e83f93c75b3e3fac971",
        "message": "We don't really know the state of req->extra{1,2] fields in\n__io_fill_cqe_req(), if an opcode handler is not aware of CQE32 option,\nit never sets them up properly. Track the state of those fields with a\nrequest flag.\n\nFixes: 76c68fbf1a1f9 (\"io_uring: enable CQE32\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4b3e5be512fbf4debec7270fd485b8a3b014d464.1655287457.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-15 05:06:09 -0600 io_uring: fix ->extra{1,2} misuse"
    },
    {
        "commit": "29ede2014c87576d2fc83680aa4c1d7403db0dfe",
        "message": "The only user of io_req_complete32()-like functions is cmd\nrequests. Instead of keeping the whole complete32 family, remove them\nand provide the extras in already added for inline completions\nreq->extra{1,2}. When fill_cqe_res() finds CQE32 option enabled\nit'll use those fields to fill a 32B cqe.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/af1319eb661b1f9a0abceb51cbbf72b8002e019d.1655287457.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-15 05:06:09 -0600 io_uring: fill extra big cqe fields from req"
    },
    {
        "commit": "f43de1f88841d59f27f761219b6550bd6ce3dcc1",
        "message": "We want just one function that will handle both normal cqes and 32B\ncqes. Combine __io_fill_cqe_req() and __io_fill_cqe_req32(). It's still\nnot entirely correct yet, but saves us from cases when we fill an CQE of\na wrong size.\n\nFixes: 76c68fbf1a1f9 (\"io_uring: enable CQE32\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8085c5b2f74141520f60decd45334f87e389b718.1655287457.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-15 05:06:09 -0600 io_uring: unite fill_cqe and the 32B version"
    },
    {
        "commit": "91ef75a7db0d0855284b78d60d3fcec5c353ec5a",
        "message": "There are too many cqe filling helpers, kill __io_fill_cqe{32}_req(),\nuse __io_fill_cqe{32}_req_filled() instead, and then rename it. It'll\nsimplify fixing in following patches.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c18e0d191014fb574f24721245e4e3fddd0b6917.1655287457.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-15 05:06:09 -0600 io_uring: get rid of __io_fill_cqe{32}_req()"
    },
    {
        "commit": "d884b6498d2f022098502e106d5a45ab635f2e9a",
        "message": "This partially reverts a7c41b4687f5902af70cd559806990930c8a307b\n\nEven though IORING_CLOSE_FD_AND_FILE_SLOT might save cycles for some\nusers, but it tries to do two things at a time and it's not clear how to\nhandle errors and what to return in a single result field when one part\nfails and another completes well. Kill it for now.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/837c745019b3795941eee4fcfd7de697886d645b.1655224415.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-14 10:57:40 -0600 io_uring: remove IORING_CLOSE_FD_AND_FILE_SLOT"
    },
    {
        "commit": "aa165d6d2bb55f8b1bb5047fd634311681316fa2",
        "message": "This reverts commit 3d200242a6c968af321913b635fc4014b238cba4.\n\nBuffer selection with nops was used for debugging and benchmarking but\nis useless in real life. Let's revert it before it's released.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c5012098ca6b51dfbdcb190f8c4e3c0bf1c965dc.1655224415.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-14 10:57:40 -0600 Revert \"io_uring: add buffer selection support to IORING_OP_NOP\""
    },
    {
        "commit": "8899ce4b2f7364a90e3b9cf332dfd9993c61f46c",
        "message": "This reverts commit 2bb04df7c2af9dad5d28771c723bc39b01cf7df4.\n\nCQE32 nops were used for debugging and benchmarking but it doesn't\ntarget any real use case. Revert it, we can return it back if someone\nfinds a good way to use it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5ff623d84ccb4b3f3b92a3ea41cdcfa612f3d96f.1655224415.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-14 10:57:40 -0600 Revert \"io_uring: support CQE32 for nop operation\""
    },
    {
        "commit": "feaf625e7055cdfd2c3d82cf60fb3892e27ea7bb",
        "message": "Pull io_uring fixes from Pavel.\n\n* 'io_uring/io_uring-5.19' of https://github.com/isilence/linux:\n  io_uring: fix double unlock for pbuf select\n  io_uring: kbuf: fix bug of not consuming ring buffer in partial io case\n  io_uring: openclose: fix bug of closing wrong fixed file\n  io_uring: fix not locked access to fixed buf table\n  io_uring: fix races with buffer table unregister\n  io_uring: fix races with file table unregister",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-13 06:52:52 -0600 Merge branch 'io_uring/io_uring-5.19' of https://github.com/isilence/linux into io_uring-5.19"
    },
    {
        "commit": "f9437ac0f851cea2374d53594f52fbbefdd977bd",
        "message": "The type of head and tail do not allow more than 2^15 entries in a\nprovided buffer ring, so do not allow this.\nAt 2^16 while each entry can be indexed, there is no way to\ndisambiguate full vs empty.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220613101157.3687-4-dylany@fb.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-13 05:13:33 -0600 io_uring: limit size of provided buffer ring"
    },
    {
        "commit": "c6e9fa5c0ab811f4bec36a96337f4b1bb77d142c",
        "message": "The type of head needs to match that of tail in order for rollover and\ncomparisons to work correctly.\n\nWithout this change the comparison of tail to head might incorrectly allow\nio_uring to use a buffer that userspace had not given it.\n\nFixes: c7fb19428d67 (\"io_uring: add support for ring mapped supplied buffers\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220613101157.3687-3-dylany@fb.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-13 05:13:31 -0600 io_uring: fix types in provided buffer ring"
    },
    {
        "commit": "97da4a537924d87e2261773f3ac9365abb191fc9",
        "message": "When indexing into a provided buffer ring, do not subtract 1 from the\nindex.\n\nFixes: c7fb19428d67 (\"io_uring: add support for ring mapped supplied buffers\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220613101157.3687-2-dylany@fb.com\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-13 05:13:09 -0600 io_uring: fix index calculation"
    },
    {
        "commit": "fc9375e3f763b06c3c90c5f5b2b84d3e07c1f4c2",
        "message": "io_buffer_select(), which is the only caller of io_ring_buffer_select(),\nfully handles locking, mutex unlock in io_ring_buffer_select() will lead\nto double unlock.\n\nFixes: c7fb19428d67d (\"io_uring: add support for ring mapped supplied buffers\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-13 11:37:41 +0100 io_uring: fix double unlock for pbuf select"
    },
    {
        "commit": "42db0c00e275877eb92480beaa16b33507dc3bda",
        "message": "When we use ring-mapped provided buffer, we should consume it before\narm poll if partial io has been done. Otherwise the buffer may be used\nby other requests and thus we lost the data.\n\nFixes: c7fb19428d67 (\"io_uring: add support for ring mapped supplied buffers\")\nSigned-off-by: Hao Xu <howeyxu@tencent.com>\n[pavel: 5.19 rebase]\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-13 11:37:30 +0100 io_uring: kbuf: fix bug of not consuming ring buffer in partial io case"
    },
    {
        "commit": "e71d7c56dd69f720169c1675f87a1d22d8167767",
        "message": "Don't update ret until fixed file is closed, otherwise the file slot\nbecomes the error code.\n\nFixes: a7c41b4687f5 (\"io_uring: let IORING_OP_FILES_UPDATE support choosing fixed file slots\")\nSigned-off-by: Hao Xu <howeyxu@tencent.com>\n[pavel: 5.19 rebase]\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-13 11:37:03 +0100 io_uring: openclose: fix bug of closing wrong fixed file"
    },
    {
        "commit": "05b538c1765f8d14a71ccf5f85258dcbeaf189f7",
        "message": "We can look inside the fixed buffer table only while holding\n->uring_lock, however in some cases we don't do the right async prep for\nIORING_OP_{WRITE,READ}_FIXED ending up with NULL req->imu forcing making\nan io-wq worker to try to resolve the fixed buffer without proper\nlocking.\n\nMove req->imu setup into early req init paths, i.e. io_prep_rw(), which\nis called unconditionally for rw requests and under uring_lock.\n\nFixes: 634d00df5e1cf (\"io_uring: add full-fledged dynamic buffers support\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-13 09:53:41 +0100 io_uring: fix not locked access to fixed buf table"
    },
    {
        "commit": "d11d31fc5d8a96f707facee0babdcffaafa38de2",
        "message": "Fixed buffer table quiesce might unlock ->uring_lock, potentially\nletting new requests to be submitted, don't allow those requests to\nuse the table as they will race with unregistration.\n\nReported-and-tested-by: van fantasy <g1042620637@gmail.com>\nFixes: bd54b6fe3316ec (\"io_uring: implement fixed buffers registration similar to fixed files\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-13 09:53:27 +0100 io_uring: fix races with buffer table unregister"
    },
    {
        "commit": "b0380bf6dad4601d92025841e2b7a135d566c6e3",
        "message": "Fixed file table quiesce might unlock ->uring_lock, potentially letting\nnew requests to be submitted, don't allow those requests to use the\ntable as they will race with unregistration.\n\nReported-and-tested-by: van fantasy <g1042620637@gmail.com>\nFixes: 05f3fb3c53975 (\"io_uring: avoid ring quiesce for fixed file set unregister and update\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>",
        "kernel_version": "v5.19-rc3",
        "release_date": "2022-06-13 09:53:07 +0100 io_uring: fix races with file table unregister"
    },
    {
        "commit": "dbe0ee46614016146c1b3e1fc063b44333bb2401",
        "message": "Pull file descriptor updates from Al Viro.\n\n - Descriptor handling cleanups\n\n* tag 'pull-18-rc1-work.fd' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:\n  Unify the primitives for file descriptor closing\n  fs: remove fget_many and fput_many interface\n  io_uring_enter(): don't leave f.flags uninitialized",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-06-04 18:52:00 -0700 Merge tag 'pull-18-rc1-work.fd' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs"
    },
    {
        "commit": "5ac8bdb9ad47334a9590e29daf7e4149b0a34729",
        "message": "Pull more io_uring updates from Jens Axboe:\n\n - A small series with some prep patches for the upcoming 5.20 split of\n   the io_uring.c file. No functional changes here, just minor bits that\n   are nice to get out of the way now (me)\n\n - Fix for a memory leak in high numbered provided buffer groups,\n   introduced in the merge window (me)\n\n - Wire up the new socket opcode for allocated direct descriptors,\n   making it consistent with the other opcodes that can instantiate a\n   descriptor (me)\n\n - Fix for the inflight tracking, should go into 5.18-stable as well\n   (me)\n\n - Fix for a deadlock for io-wq offloaded file slot allocations (Pavel)\n\n - Direct descriptor failure fput leak fix (Xiaoguang)\n\n - Fix for the direct descriptor allocation hinting in case of\n   unsuccessful install (Xiaoguang)\n\n* tag 'io_uring-5.19-2022-06-02' of git://git.kernel.dk/linux-block:\n  io_uring: reinstate the inflight tracking\n  io_uring: fix deadlock on iowq file slot alloc\n  io_uring: let IORING_OP_FILES_UPDATE support choosing fixed file slots\n  io_uring: defer alloc_hint update to io_file_bitmap_set()\n  io_uring: ensure fput() called correspondingly when direct install fails\n  io_uring: wire up allocated direct descriptors for socket\n  io_uring: fix a memory leak of buffer group list on exit\n  io_uring: move shutdown under the general net section\n  io_uring: unify calling convention for async prep handling\n  io_uring: add io_op_defs 'def' pointer in req init and issue\n  io_uring: make prep and issue side of req handlers named consistently\n  io_uring: make timeout prep handlers consistent with other prep handlers",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-06-03 10:10:38 -0700 Merge tag 'io_uring-5.19-2022-06-02' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "9cae36a094e7e9d6e5fe8b6dcd4642138b3eb0c7",
        "message": "After some debugging, it was realized that we really do still need the\nold inflight tracking for any file type that has io_uring_fops assigned.\nIf we don't, then trivial circular references will mean that we never get\nthe ctx cleaned up and hence it'll leak.\n\nJust bring back the inflight tracking, which then also means we can\neliminate the conditional dropping of the file when task_work is queued.\n\nFixes: d5361233e9ab (\"io_uring: drop the old style inflight file tracking\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-06-01 23:57:02 -0600 io_uring: reinstate the inflight tracking"
    },
    {
        "commit": "61c1b44a21d70d4783db02198fbf68b132f4953c",
        "message": "io_fixed_fd_install() can grab uring_lock in the slot allocation path\nwhen called from io-wq, and then call into io_install_fixed_file(),\nwhich will lock it again. Pull all locking out of\nio_install_fixed_file() into io_fixed_fd_install().\n\nFixes: 1339f24b336db (\"io_uring: allow allocated fixed files for openat/openat2\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/64116172a9d0b85b85300346bb280f3657aafc26.1654087283.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-06-01 10:12:41 -0600 io_uring: fix deadlock on iowq file slot alloc"
    },
    {
        "commit": "0e9911fa768f32f30e5678512ea405d99a7a9fef",
        "message": "This patch supports mq_ops->queue_rqs() hook. It has an advantage of\nbatch submission to virtio-blk driver. It also helps polling I/O because\npolling uses batched completion of block layer. Batch submission in\nqueue_rqs() can boost polling performance.\n\nIn queue_rqs(), it iterates plug->mq_list, collects requests that\nbelong to same HW queue until it encounters a request from other\nHW queue or sees the end of the list.\nThen, virtio-blk adds requests into virtqueue and kicks virtqueue\nto submit requests.\n\nIf there is an error, it inserts error request to requeue_list and\npasses it to ordinary block layer path.\n\nFor verification, I did fio test.\n(io_uring, randread, direct=1, bs=4K, iodepth=64 numjobs=N)\nI set 4 vcpu and 2 virtio-blk queues for VM and run fio test 5 times.\nIt shows about 2% improvement.\n\n                                 |   numjobs=2   |   numjobs=4\n      -----------------------------------------------------------\n        fio without queue_rqs()  |   291K IOPS   |   238K IOPS\n      -----------------------------------------------------------\n        fio with queue_rqs()     |   295K IOPS   |   243K IOPS\n\nFor polling I/O performance, I also did fio test as below.\n(io_uring, hipri, randread, direct=1, bs=512, iodepth=64 numjobs=4)\nI set 4 vcpu and 2 poll queues for VM.\nIt shows about 2% improvement in polling I/O.\n\n                                      |   IOPS   |  avg latency\n      -----------------------------------------------------------\n        fio poll without queue_rqs()  |   424K   |   613.05 usec\n      -----------------------------------------------------------\n        fio poll with queue_rqs()     |   435K   |   601.01 usec\n\nReviewed-by: Stefan Hajnoczi <stefanha@redhat.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Suwan Kim <suwan.kim027@gmail.com>\nMessage-Id: <20220406153207.163134-3-suwan.kim027@gmail.com>\nSigned-off-by: Michael S. Tsirkin <mst@redhat.com>\nReviewed-by: Chaitanya Kulkarni <kch@nvidia.com>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-31 12:44:23 -0400 virtio-blk: support mq_ops->queue_rqs()"
    },
    {
        "commit": "4e0400525691d0e676dbe002641f9a61261f1e1b",
        "message": "This patch supports polling I/O via virtio-blk driver. Polling\nfeature is enabled by module parameter \"poll_queues\" and it sets\ndedicated polling queues for virtio-blk. This patch improves the\npolling I/O throughput and latency.\n\nThe virtio-blk driver doesn't not have a poll function and a poll\nqueue and it has been operating in interrupt driven method even if\nthe polling function is called in the upper layer.\n\nvirtio-blk polling is implemented upon 'batched completion' of block\nlayer. virtblk_poll() queues completed request to io_comp_batch->req_list\nand later, virtblk_complete_batch() calls unmap function and ends\nthe requests in batch.\n\nvirtio-blk reads the number of poll queues from module parameter\n\"poll_queues\". If VM sets queue parameter as below,\n(\"num-queues=N\" [QEMU property], \"poll_queues=M\" [module parameter])\nIt allocates N virtqueues to virtio_blk->vqs[N] and it uses [0..(N-M-1)]\nas default queues and [(N-M)..(N-1)] as poll queues. Unlike the default\nqueues, the poll queues have no callback function.\n\nRegarding HW-SW queue mapping, the default queue mapping uses the\nexisting method that condsiders MSI irq vector. But the poll queue\ndoesn't have an irq, so it uses the regular blk-mq cpu mapping.\n\nFor verifying the improvement, I did Fio polling I/O performance test\nwith io_uring engine with the options below.\n(io_uring, hipri, randread, direct=1, bs=512, iodepth=64 numjobs=N)\nI set 4 vcpu and 4 virtio-blk queues - 2 default queues and 2 poll\nqueues for VM.\n\nAs a result, IOPS and average latency improved about 10%.\n\nTest result:\n\n- Fio io_uring poll without virtio-blk poll support\n\t-- numjobs=1 : IOPS = 339K, avg latency = 188.33us\n\t-- numjobs=2 : IOPS = 367K, avg latency = 347.33us\n\t-- numjobs=4 : IOPS = 383K, avg latency = 682.06us\n\n- Fio io_uring poll with virtio-blk poll support\n\t-- numjobs=1 : IOPS = 385K, avg latency = 165.94us\n\t-- numjobs=2 : IOPS = 408K, avg latency = 313.28us\n\t-- numjobs=4 : IOPS = 424K, avg latency = 613.05us\n\nReviewed-by: Stefan Hajnoczi <stefanha@redhat.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Max Gurtovoy <mgurtovoy@nvidia.com>\nSigned-off-by: Suwan Kim <suwan.kim027@gmail.com>\nMessage-Id: <20220406153207.163134-2-suwan.kim027@gmail.com>\nSigned-off-by: Michael S. Tsirkin <mst@redhat.com>\nReviewed-by: Chaitanya Kulkarni <kch@nvidia.com>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-31 12:44:23 -0400 virtio-blk: support polling I/O"
    },
    {
        "commit": "a7c41b4687f5902af70cd559806990930c8a307b",
        "message": "One big issue with the file registration feature is that it needs user\nspace apps to maintain free slot info about io_uring's fixed file table,\nwhich really is a burden for development. io_uring now supports choosing\nfree file slot for user space apps by using IORING_FILE_INDEX_ALLOC flag\nin accept, open, and socket operations, but they need the app to use\ndirect accept or direct open, which not all apps are prepared to use yet.\n\nTo support apps that still need real fds, make use of the registration\nfeature easier. Let IORING_OP_FILES_UPDATE support choosing fixed file\nslots, which will store picked fixed files slots in fd array and let cqe\nreturn the number of slots allocated.\n\nSuggested-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\n[axboe: move flag to uapi io_uring header, change goto to break, init]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-31 02:50:06 -0600 io_uring: let IORING_OP_FILES_UPDATE support choosing fixed file slots"
    },
    {
        "commit": "4278a0deb1f6cac40ded3362fe2a9827d7efee3d",
        "message": "io_file_bitmap_get() returns a free bitmap slot, but if it isn't\nused later, such as io_queue_rsrc_removal() returns error, in this\ncase, we should not update alloc_hint at all, which still should\nbe considered as a valid candidate for next io_file_bitmap_get()\ncalls.\n\nTo fix this issue, only update alloc_hint in io_file_bitmap_set().\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20220528015109.48039-1-xiaoguang.wang@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-31 02:50:06 -0600 io_uring: defer alloc_hint update to io_file_bitmap_set()"
    },
    {
        "commit": "8c71fe750215e688df9c7477e7bf448380d4ce9e",
        "message": "io_fixed_fd_install() may fail for short of free fixed file bitmap,\nin this case, need to call fput() correspondingly.\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20220527025400.51048-1-xiaoguang.wang@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-31 02:50:06 -0600 io_uring: ensure fput() called correspondingly when direct install fails"
    },
    {
        "commit": "fa82dd105bed389f37d919fd783ce459bb92facb",
        "message": "The socket support was merged in an earlier branch that didn't yet\nhave support for allocating direct descriptors, hence only open\nand accept got support for that.\n\nDo the one-liner to enable it now, so we have consistent support for\nany request that can instantiate a file/direct descriptor.\n\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-31 02:50:06 -0600 io_uring: wire up allocated direct descriptors for socket"
    },
    {
        "commit": "21870e02fcd385c39fe635e6531ce78302f3cd71",
        "message": "If we use a buffer group ID that is large enough to require io_uring\nto allocate it, then we don't correctly free it if the cleanup is\ndeferred to the ring exit. The explicit removal paths are fine.\n\nFixes: 9cfc7e94e42b (\"io_uring: get rid of hashed provided buffer groups\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-31 02:50:06 -0600 io_uring: fix a memory leak of buffer group list on exit"
    },
    {
        "commit": "1151a7cccbd2cfd5a552805c92c92fb264a957d5",
        "message": "Gets rid of some ifdefs and enables use of the net defines for when\nCONFIG_NET isn't set.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-31 02:50:06 -0600 io_uring: move shutdown under the general net section"
    },
    {
        "commit": "157dc813b47ab2adb4bc8a08491887bc161284c2",
        "message": "Make them consistent in preparation for defining a req async prep\nhandler. The readv/writev requests share a prep handler, move it one\nlevel down so the initial one is consistent with the others.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-31 02:50:06 -0600 io_uring: unify calling convention for async prep handling"
    },
    {
        "commit": "fcde59feb1affb6d56aecadc3868df4631480da5",
        "message": "Define and set it when appropriate, and use it consistently in the\nfunction rather than using io_op_defs[opcode].\n\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-31 02:50:02 -0600 io_uring: add io_op_defs 'def' pointer in req init and issue"
    },
    {
        "commit": "54739cc6b4e12b8bf3802536634b8e896eb796b1",
        "message": "Almost all of them are, the odd ones out are the poll remove and the\nfiles update request. Name them like the others, which is:\n\nio_#cmdname_prep\tfor request preparation\nio_#cmdname\t\tfor request issue\n\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-25 05:37:06 -0600 io_uring: make prep and issue side of req handlers named consistently"
    },
    {
        "commit": "ecddc25d1355d0ce2b486a4991b826b6e87875a9",
        "message": "All other opcodes take a {req, sqe} set for prep handling, split out\na timeout prep handler so that timeout and linked timeouts can use\nthe same one.\n\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-25 05:36:54 -0600 io_uring: make timeout prep handlers consistent with other prep handlers"
    },
    {
        "commit": "9836e93c0a7e031ac6a71c56171c229de1eea7cf",
        "message": "Pull io_uring NVMe command passthrough from Jens Axboe:\n \"On top of everything else, this adds support for passthrough for\n  io_uring.\n\n  The initial feature for this is NVMe passthrough support, which allows\n  non-filesystem based IO commands and admin commands.\n\n  To support this, io_uring grows support for SQE and CQE members that\n  are twice as big, allowing to pass in a full NVMe command without\n  having to copy data around. And to complete with more than just a\n  single 32-bit value as the output\"\n\n* tag 'for-5.19/io_uring-passthrough-2022-05-22' of git://git.kernel.dk/linux-block: (22 commits)\n  io_uring: cleanup handling of the two task_work lists\n  nvme: enable uring-passthrough for admin commands\n  nvme: helper for uring-passthrough checks\n  blk-mq: fix passthrough plugging\n  nvme: add vectored-io support for uring-cmd\n  nvme: wire-up uring-cmd support for io-passthru on char-device.\n  nvme: refactor nvme_submit_user_cmd()\n  block: wire-up support for passthrough plugging\n  fs,io_uring: add infrastructure for uring-cmd\n  io_uring: support CQE32 for nop operation\n  io_uring: enable CQE32\n  io_uring: support CQE32 in /proc info\n  io_uring: add tracing for additional CQE32 fields\n  io_uring: overflow processing for CQE32\n  io_uring: flush completions for CQE32\n  io_uring: modify io_get_cqe for CQE32\n  io_uring: add CQE32 completion processing\n  io_uring: add CQE32 setup processing\n  io_uring: change ring size calculation for CQE32\n  io_uring: store add. return values for CQE32\n  ...",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-23 13:06:15 -0700 Merge tag 'for-5.19/io_uring-passthrough-2022-05-22' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "e1a8fde7203fa8a3e3f35d4f9df47477d23529c1",
        "message": "Pull io_uring 'more data in socket' support from Jens Axboe:\n \"To be able to fully utilize the 'poll first' support in the core\n  io_uring branch, it's advantageous knowing if the socket was empty\n  after a receive. This adds support for that\"\n\n* tag 'for-5.19/io_uring-net-2022-05-22' of git://git.kernel.dk/linux-block:\n  io_uring: return hint on whether more data is available after receive\n  tcp: pass back data left in socket after receive",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-23 12:51:04 -0700 Merge tag 'for-5.19/io_uring-net-2022-05-22' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "368da430d04dbe31aded44e5f5c255ff0899ad1d",
        "message": "Pull io_uring socket() support from Jens Axboe:\n \"This adds support for socket(2) for io_uring. This is handy when using\n  direct / registered file descriptors with io_uring.\n\n  Outside of those two patches, a small series from Dylan on top that\n  improves the tracing by providing a text representation of the opcode\n  rather than needing to decode this by reading the header file every\n  time.\n\n  That sits in this branch as it was the last opcode added (until it\n  wasn't...)\"\n\n* tag 'for-5.19/io_uring-socket-2022-05-22' of git://git.kernel.dk/linux-block:\n  io_uring: use the text representation of ops in trace\n  io_uring: rename op -> opcode\n  io_uring: add io_uring_get_opcode\n  io_uring: add type to op enum\n  io_uring: add socket(2) support\n  net: add __sys_socket_file()",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-23 12:42:33 -0700 Merge tag 'for-5.19/io_uring-socket-2022-05-22' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "09beaff75e4c4cee590b0e547c7587ff6cadb5db",
        "message": "Pull io_uring xattr support from Jens Axboe:\n \"Support for the xattr variants\"\n\n* tag 'for-5.19/io_uring-xattr-2022-05-22' of git://git.kernel.dk/linux-block:\n  io_uring: cleanup error-handling around io_req_complete\n  io_uring: fix trace for reduced sqe padding\n  io_uring: add fgetxattr and getxattr support\n  io_uring: add fsetxattr and setxattr support\n  fs: split off do_getxattr from getxattr\n  fs: split off setxattr_copy and do_setxattr function from setxattr",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-23 12:30:30 -0700 Merge tag 'for-5.19/io_uring-xattr-2022-05-22' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "3a166bdbf3abc8ca2d7ffe7cffc5c7a65f260e62",
        "message": "Pull io_uring updates from Jens Axboe:\n \"Here are the main io_uring changes for 5.19. This contains:\n\n   - Fixes for sparse type warnings (Christoph, Vasily)\n\n   - Support for multi-shot accept (Hao)\n\n   - Support for io_uring managed fixed files, rather than always\n     needing the applicationt o manage the indices (me)\n\n   - Fix for a spurious poll wakeup (Dylan)\n\n   - CQE overflow fixes (Dylan)\n\n   - Support more types of cancelations (me)\n\n   - Support for co-operative task_work signaling, rather than always\n     forcing an IPI (me)\n\n   - Support for doing poll first when appropriate, rather than always\n     attempting a transfer first (me)\n\n   - Provided buffer cleanups and support for mapped buffers (me)\n\n   - Improve how io_uring handles inflight SCM files (Pavel)\n\n   - Speedups for registered files (Pavel, me)\n\n   - Organize the completion data in a struct in io_kiocb rather than\n     keep it in separate spots (Pavel)\n\n   - task_work improvements (Pavel)\n\n   - Cleanup and optimize the submission path, in general and for\n     handling links (Pavel)\n\n   - Speedups for registered resource handling (Pavel)\n\n   - Support sparse buffers and file maps (Pavel, me)\n\n   - Various fixes and cleanups (Almog, Pavel, me)\"\n\n* tag 'for-5.19/io_uring-2022-05-22' of git://git.kernel.dk/linux-block: (111 commits)\n  io_uring: fix incorrect __kernel_rwf_t cast\n  io_uring: disallow mixed provided buffer group registrations\n  io_uring: initialize io_buffer_list head when shared ring is unregistered\n  io_uring: add fully sparse buffer registration\n  io_uring: use rcu_dereference in io_close\n  io_uring: consistently use the EPOLL* defines\n  io_uring: make apoll_events a __poll_t\n  io_uring: drop a spurious inline on a forward declaration\n  io_uring: don't use ERR_PTR for user pointers\n  io_uring: use a rwf_t for io_rw.flags\n  io_uring: add support for ring mapped supplied buffers\n  io_uring: add io_pin_pages() helper\n  io_uring: add buffer selection support to IORING_OP_NOP\n  io_uring: fix locking state for empty buffer group\n  io_uring: implement multishot mode for accept\n  io_uring: let fast poll support multishot\n  io_uring: add REQ_F_APOLL_MULTISHOT for requests\n  io_uring: add IORING_ACCEPT_MULTISHOT for accept\n  io_uring: only wake when the correct events are set\n  io_uring: avoid io-wq -EAGAIN looping for !IOPOLL\n  ...",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-23 12:22:49 -0700 Merge tag 'for-5.19/io_uring-2022-05-22' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "3fe07bcd800d6e5e4e4263ca2564d69095c157bf",
        "message": "Rather than pass in a bool for whether or not this work item needs to go\ninto the priority list or not, provide separate helpers for it. For most\nuse cases, this also then gets rid of the branch for non-priority task\nwork.\n\nWhile at it, rename the prior_task_list to prio_task_list. Prior is\na confusing name for it, as it would seem to indicate that this is the\nprevious task_work list. prio makes it clear that this is a priority\ntask_work list.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-21 09:17:05 -0600 io_uring: cleanup handling of the two task_work lists"
    },
    {
        "commit": "0e7579ca732a39cc377e17509dda9bfc4f6ba78e",
        "message": "Currently 'make C=1 fs/io_uring.o' generates sparse warning:\n\n  CHECK   fs/io_uring.c\nfs/io_uring.c: note: in included file (through\ninclude/trace/trace_events.h, include/trace/define_trace.h, i\nnclude/trace/events/io_uring.h):\n./include/trace/events/io_uring.h:488:1:\n warning: incorrect type in assignment (different base types)\n    expected unsigned int [usertype] op_flags\n    got restricted __kernel_rwf_t const [usertype] rw_flags\n\nThis happen on cast of sqe->rw_flags which is defined as __kernel_rwf_t,\nthis type is bitwise and requires __force attribute for any casts.\nHowever rw_flags is a member of the union, and its access can be safely\nreplaced by using of its neighbours, so let's use poll32_events to fix\nthe sparse warning.\n\nSigned-off-by: Vasily Averin <vvs@openvz.org>\nLink: https://lore.kernel.org/r/6f009241-a63f-ae43-a04b-62841aaef293@openvz.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-19 12:27:59 -0600 io_uring: fix incorrect __kernel_rwf_t cast"
    },
    {
        "commit": "01464a73a6387b45aa4cf6ea522abd4f9e44dce5",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two small changes fixing issues from the 5.18 merge window:\n\n   - Fix wrong ordering of a tracepoint (Dylan)\n\n   - Fix MSG_RING on IOPOLL rings (me)\"\n\n* tag 'io_uring-5.18-2022-05-18' of git://git.kernel.dk/linux-block:\n  io_uring: don't attempt to IOPOLL for MSG_RING requests\n  io_uring: fix ordering of args in io_uring_queue_async_work",
        "kernel_version": "v5.18",
        "release_date": "2022-05-18 14:21:30 -1000 Merge tag 'io_uring-5.18-2022-05-18' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "8194a0089207e8fff75a4dee2b98b0a537be2c54",
        "message": "Pull audit fix from Paul Moore:\n \"A single audit patch to fix a problem where a task's audit_context was\n  not being properly reset with io_uring\"\n\n* tag 'audit-pr-20220518' of git://git.kernel.org/pub/scm/linux/kernel/git/pcmoore/audit:\n  audit,io_uring,io-wq: call __audit_uring_exit for dummy contexts",
        "kernel_version": "v5.18",
        "release_date": "2022-05-18 14:19:26 -1000 Merge tag 'audit-pr-20220518' of git://git.kernel.org/pub/scm/linux/kernel/git/pcmoore/audit"
    },
    {
        "commit": "2fcabce2d7d34f69a888146dab15b36a917f09d4",
        "message": "It's nonsensical to register a provided buffer ring, if a classic\nprovided buffer group with the same ID exists. Depending on the order of\nwhich we decide what type to pick, the other type will never get used.\nExplicitly disallow it and return an error if this is attempted.\n\nFixes: c7fb19428d67 (\"io_uring: add support for ring mapped supplied buffers\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-18 16:21:30 -0600 io_uring: disallow mixed provided buffer group registrations"
    },
    {
        "commit": "1d0dbbfa282d9be57792e3b5827dc57b010181ee",
        "message": "We use ->buf_pages != 0 to tell if this is a shared buffer ring or a\nclassic provided buffer group. If we unregister the shared ring and\nthen attempt to use it, buf_pages is zero yet the classic list head\nisn't properly initialized. This causes io_buffer_select() to think\nthat we have classic buffers available, but then we crash when we try\nand get one from the list.\n\nJust initialize the list if we unregister a shared buffer ring, leaving\nit in a sane state for either re-registration or for attempting to use\nit. And do the same for the initial setup from the classic path.\n\nFixes: c7fb19428d67 (\"io_uring: add support for ring mapped supplied buffers\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-18 16:21:11 -0600 io_uring: initialize io_buffer_list head when shared ring is unregistered"
    },
    {
        "commit": "0184f08e65348f39aa4e8a71927e4538515f4ac0",
        "message": "Honour IORING_RSRC_REGISTER_SPARSE not only for direct files but fixed\nbuffers as well. It makes the rsrc API more consistent.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/66f429e4912fe39fb3318217ff33a2853d4544be.1652879898.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-18 12:53:04 -0600 io_uring: add fully sparse buffer registration"
    },
    {
        "commit": "0bf1dbee9baf3e78bff297245178f8c9a8ef8670",
        "message": "Accessing the file table needs a rcu_dereference_protected().\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20220518084005.3255380-7-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-18 06:19:05 -0600 io_uring: use rcu_dereference in io_close"
    },
    {
        "commit": "a294bef57c55a45aef51d31e71d6892e8eba1483",
        "message": "POLL* are unannotated values for the userspace ABI, while everything\nin-kernel should use EPOLL* and the __poll_t type.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20220518084005.3255380-6-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-18 06:19:05 -0600 io_uring: consistently use the EPOLL* defines"
    },
    {
        "commit": "58f5c8d39e0ea07fdaaea6a85c49000da83dc0cc",
        "message": "apoll_events is fed to vfs_poll and the poll tables, so it should be\na __poll_t.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20220518084005.3255380-5-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-18 06:19:05 -0600 io_uring: make apoll_events a __poll_t"
    },
    {
        "commit": "ee67ba3b20f7dcd001b7743eb8e46880cb27fdc6",
        "message": "io_file_get_normal isn't marked inline, so don't claim it as such in the\nforward declaration.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20220518084005.3255380-4-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-18 06:19:05 -0600 io_uring: drop a spurious inline on a forward declaration"
    },
    {
        "commit": "984824db844a9bd6e9e15ee469241982526a6ccd",
        "message": "ERR_PTR abuses the high bits of a pointer to transport error information.\nThis is only safe for kernel pointers and not user pointers.  Fix\nio_buffer_select and its helpers to just return NULL for failure and get\nrid of this abuse.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20220518084005.3255380-3-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-18 06:18:56 -0600 io_uring: don't use ERR_PTR for user pointers"
    },
    {
        "commit": "20cbd21d899b72765e38481a926c7b2008c64350",
        "message": "Use the proper type.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20220518084005.3255380-2-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-18 06:17:52 -0600 io_uring: use a rwf_t for io_rw.flags"
    },
    {
        "commit": "c7fb19428d67dd0a2a78a4f237af01d39c78dc5a",
        "message": "Provided buffers allow an application to supply io_uring with buffers\nthat can then be grabbed for a read/receive request, when the data\nsource is ready to deliver data. The existing scheme relies on using\nIORING_OP_PROVIDE_BUFFERS to do that, but it can be difficult to use\nin real world applications. It's pretty efficient if the application\nis able to supply back batches of provided buffers when they have been\nconsumed and the application is ready to recycle them, but if\nfragmentation occurs in the buffer space, it can become difficult to\nsupply enough buffers at the time. This hurts efficiency.\n\nAdd a register op, IORING_REGISTER_PBUF_RING, which allows an application\nto setup a shared queue for each buffer group of provided buffers. The\napplication can then supply buffers simply by adding them to this ring,\nand the kernel can consume then just as easily. The ring shares the head\nwith the application, the tail remains private in the kernel.\n\nProvided buffers setup with IORING_REGISTER_PBUF_RING cannot use\nIORING_OP_{PROVIDE,REMOVE}_BUFFERS for adding or removing entries to the\nring, they must use the mapped ring. Mapped provided buffer rings can\nco-exist with normal provided buffers, just not within the same group ID.\n\nTo gauge overhead of the existing scheme and evaluate the mapped ring\napproach, a simple NOP benchmark was written. It uses a ring of 128\nentries, and submits/completes 32 at the time. 'Replenish' is how\nmany buffers are provided back at the time after they have been\nconsumed:\n\nTest\t\t\tReplenish\t\t\tNOPs/sec\n================================================================\nNo provided buffers\tNA\t\t\t\t~30M\nProvided buffers\t32\t\t\t\t~16M\nProvided buffers\t 1\t\t\t\t~10M\nRing buffers\t\t32\t\t\t\t~27M\nRing buffers\t\t 1\t\t\t\t~27M\n\nThe ring mapped buffers perform almost as well as not using provided\nbuffers at all, and they don't care if you provided 1 or more back at\nthe same time. This means application can just replenish as they go,\nrather than need to batch and compact, further reducing overhead in the\napplication. The NOP benchmark above doesn't need to do any compaction,\nso that overhead isn't even reflected in the above test.\n\nCo-developed-by: Dylan Yudaken <dylany@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-18 06:12:42 -0600 io_uring: add support for ring mapped supplied buffers"
    },
    {
        "commit": "d8c2237d0aa9c04b867ce1e281e2a30a86a68e3b",
        "message": "Abstract this out from io_sqe_buffer_register() so we can use it\nelsewhere too without duplicating this code.\n\nNo intended functional changes in this patch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-18 06:12:42 -0600 io_uring: add io_pin_pages() helper"
    },
    {
        "commit": "3d200242a6c968af321913b635fc4014b238cba4",
        "message": "Obviously not really useful since it's not transferring data, but it\nis helpful in benchmarking overhead of provided buffers.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-18 06:12:42 -0600 io_uring: add buffer selection support to IORING_OP_NOP"
    },
    {
        "commit": "e7637a492b9f1ae6b7cfcecf0aed5e4c76efa3bd",
        "message": "io_provided_buffer_select() must drop the submit lock, if needed, even\nin the error handling case. Failure to do so will leave us with the\nctx->uring_lock held, causing spew like:\n\n====================================\nWARNING: iou-wrk-366/368 still has locks held!\n5.18.0-rc6-00294-gdf8dc7004331 #994 Not tainted\n------------------------------------\n1 lock held by iou-wrk-366/368:\n #0: ffff0000c72598a8 (&ctx->uring_lock){+.+.}-{3:3}, at: io_ring_submit_lock+0x20/0x48\n\nstack backtrace:\nCPU: 4 PID: 368 Comm: iou-wrk-366 Not tainted 5.18.0-rc6-00294-gdf8dc7004331 #994\nHardware name: linux,dummy-virt (DT)\nCall trace:\n dump_backtrace.part.0+0xa4/0xd4\n show_stack+0x14/0x5c\n dump_stack_lvl+0x88/0xb0\n dump_stack+0x14/0x2c\n debug_check_no_locks_held+0x84/0x90\n try_to_freeze.isra.0+0x18/0x44\n get_signal+0x94/0x6ec\n io_wqe_worker+0x1d8/0x2b4\n ret_from_fork+0x10/0x20\n\nand triggering later hangs off get_signal() because we attempt to\nre-grab the lock.\n\nReported-by: syzbot+987d7bb19195ae45208c@syzkaller.appspotmail.com\nFixes: 149c69b04a90 (\"io_uring: abstract out provided buffer list selection\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-18 06:12:41 -0600 io_uring: fix locking state for empty buffer group"
    },
    {
        "commit": "69e9cd66ae1392437234a63a3a1d60b6655f92ef",
        "message": "Not calling the function for dummy contexts will cause the context to\nnot be reset. During the next syscall, this will cause an error in\n__audit_syscall_entry:\n\n\tWARN_ON(context->context != AUDIT_CTX_UNUSED);\n\tWARN_ON(context->name_count);\n\tif (context->context != AUDIT_CTX_UNUSED || context->name_count) {\n\t\taudit_panic(\"unrecoverable error in audit_syscall_entry()\");\n\t\treturn;\n\t}\n\nThese problematic dummy contexts are created via the following call\nchain:\n\n       exit_to_user_mode_prepare\n    -> arch_do_signal_or_restart\n    -> get_signal\n    -> task_work_run\n    -> tctx_task_work\n    -> io_req_task_submit\n    -> io_issue_sqe\n    -> audit_uring_entry\n\nCc: stable@vger.kernel.org\nFixes: 5bd2182d58e9 (\"audit,io_uring,io-wq: add some basic audit support to io_uring\")\nSigned-off-by: Julian Orth <ju.orth@gmail.com>\n[PM: subject line tweaks]\nSigned-off-by: Paul Moore <paul@paul-moore.com>",
        "kernel_version": "v5.18",
        "release_date": "2022-05-17 15:03:36 -0400 audit,io_uring,io-wq: call __audit_uring_exit for dummy contexts"
    },
    {
        "commit": "aa184e8671f0f911fc2fb3f68cd506e4d7838faa",
        "message": "We gate whether to IOPOLL for a request on whether the opcode is allowed\non a ring setup for IOPOLL and if it's got a file assigned. MSG_RING\nis the only one that allows a file yet isn't pollable, it's merely\nsupported to allow communication on an IOPOLL ring, not because we can\npoll for completion of it.\n\nPut the assigned file early and clear it, so we don't attempt to poll\nfor it.\n\nReported-by: syzbot+1a0a53300ce782f8b3ad@syzkaller.appspotmail.com\nFixes: 3f1d52abf098 (\"io_uring: defer msg-ring file validity check until command issue\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18",
        "release_date": "2022-05-17 12:46:04 -0600 io_uring: don't attempt to IOPOLL for MSG_RING requests"
    },
    {
        "commit": "2306e83e730a86fbf6855d4611438d8632ad2e73",
        "message": "When doing a NOCOW write, either through direct IO or buffered IO, we do\ntwo lookups for the block group that contains the target extent: once\nwhen we call btrfs_inc_nocow_writers() and then later again when we call\nbtrfs_dec_nocow_writers() after creating the ordered extent.\n\nThe lookups require taking a lock and navigating the red black tree used\nto track all block groups, which can take a non-negligible amount of time\nfor a large filesystem with thousands of block groups, as well as lock\ncontention and cache line bouncing.\n\nImprove on this by having a single block group search: making\nbtrfs_inc_nocow_writers() return the block group to its caller and then\nhave the caller pass that block group to btrfs_dec_nocow_writers().\n\nThis is part of a patchset comprised of the following patches:\n\n  btrfs: remove search start argument from first_logical_byte()\n  btrfs: use rbtree with leftmost node cached for tracking lowest block group\n  btrfs: use a read/write lock for protecting the block groups tree\n  btrfs: return block group directly at btrfs_next_block_group()\n  btrfs: avoid double search for block group during NOCOW writes\n\nThe following test was used to test these changes from a performance\nperspective:\n\n   $ cat test.sh\n   #!/bin/bash\n\n   modprobe null_blk nr_devices=0\n\n   NULL_DEV_PATH=/sys/kernel/config/nullb/nullb0\n   mkdir $NULL_DEV_PATH\n   if [ $? -ne 0 ]; then\n       echo \"Failed to create nullb0 directory.\"\n       exit 1\n   fi\n   echo 2 > $NULL_DEV_PATH/submit_queues\n   echo 16384 > $NULL_DEV_PATH/size # 16G\n   echo 1 > $NULL_DEV_PATH/memory_backed\n   echo 1 > $NULL_DEV_PATH/power\n\n   DEV=/dev/nullb0\n   MNT=/mnt/nullb0\n   LOOP_MNT=\"$MNT/loop\"\n   MOUNT_OPTIONS=\"-o ssd -o nodatacow\"\n   MKFS_OPTIONS=\"-R free-space-tree -O no-holes\"\n\n   cat <<EOF > /tmp/fio-job.ini\n   [io_uring_writes]\n   rw=randwrite\n   fsync=0\n   fallocate=posix\n   group_reporting=1\n   direct=1\n   ioengine=io_uring\n   iodepth=64\n   bs=64k\n   filesize=1g\n   runtime=300\n   time_based\n   directory=$LOOP_MNT\n   numjobs=8\n   thread\n   EOF\n\n   echo performance | \\\n       tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n\n   echo\n   echo \"Using config:\"\n   echo\n   cat /tmp/fio-job.ini\n   echo\n\n   umount $MNT &> /dev/null\n   mkfs.btrfs -f $MKFS_OPTIONS $DEV &> /dev/null\n   mount $MOUNT_OPTIONS $DEV $MNT\n\n   mkdir $LOOP_MNT\n\n   truncate -s 4T $MNT/loopfile\n   mkfs.btrfs -f $MKFS_OPTIONS $MNT/loopfile &> /dev/null\n   mount $MOUNT_OPTIONS $MNT/loopfile $LOOP_MNT\n\n   # Trigger the allocation of about 3500 data block groups, without\n   # actually consuming space on underlying filesystem, just to make\n   # the tree of block group large.\n   fallocate -l 3500G $LOOP_MNT/filler\n\n   fio /tmp/fio-job.ini\n\n   umount $LOOP_MNT\n   umount $MNT\n\n   echo 0 > $NULL_DEV_PATH/power\n   rmdir $NULL_DEV_PATH\n\nThe test was run on a non-debug kernel (Debian's default kernel config),\nthe result were the following.\n\nBefore patchset:\n\n  WRITE: bw=1455MiB/s (1526MB/s), 1455MiB/s-1455MiB/s (1526MB/s-1526MB/s), io=426GiB (458GB), run=300006-300006msec\n\nAfter patchset:\n\n  WRITE: bw=1503MiB/s (1577MB/s), 1503MiB/s-1503MiB/s (1577MB/s-1577MB/s), io=440GiB (473GB), run=300006-300006msec\n\n  +3.3% write throughput and +3.3% IO done in the same time period.\n\nThe test has somewhat limited coverage scope, as with only NOCOW writes\nwe get less contention on the red black tree of block groups, since we\ndon't have the extra contention caused by COW writes, namely when\nallocating data extents, pinning and unpinning data extents, but on the\nhand there's access to tree in the NOCOW path, when incrementing a block\ngroup's number of NOCOW writers.\n\nReviewed-by: Nikolay Borisov <nborisov@suse.com>\nSigned-off-by: Filipe Manana <fdmanana@suse.com>\nSigned-off-by: David Sterba <dsterba@suse.com>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-16 17:03:13 +0200 btrfs: avoid double search for block group during NOCOW writes"
    },
    {
        "commit": "d4135134ab8feb994369d44884733e8031b0f800",
        "message": "When doing a NOWAIT direct IO write, if we can NOCOW then it means we can\nproceed with the non-blocking, NOWAIT path. However reserving the metadata\nspace and qgroup meta space can often result in blocking - flushing\ndelalloc, wait for ordered extents to complete, trigger transaction\ncommits, etc, going against the semantics of a NOWAIT write.\n\nSo make the NOWAIT write path to try to reserve all the metadata it needs\nwithout resulting in a blocking behaviour - if we get -ENOSPC or -EDQUOT\nthen return -EAGAIN to make the caller fallback to a blocking direct IO\nwrite.\n\nThis is part of a patchset comprised of the following patches:\n\n  btrfs: avoid blocking on page locks with nowait dio on compressed range\n  btrfs: avoid blocking nowait dio when locking file range\n  btrfs: avoid double nocow check when doing nowait dio writes\n  btrfs: stop allocating a path when checking if cross reference exists\n  btrfs: free path at can_nocow_extent() before checking for checksum items\n  btrfs: release path earlier at can_nocow_extent()\n  btrfs: avoid blocking when allocating context for nowait dio read/write\n  btrfs: avoid blocking on space revervation when doing nowait dio writes\n\nThe following test was run before and after applying this patchset:\n\n  $ cat io-uring-nodatacow-test.sh\n  #!/bin/bash\n\n  DEV=/dev/sdc\n  MNT=/mnt/sdc\n\n  MOUNT_OPTIONS=\"-o ssd -o nodatacow\"\n  MKFS_OPTIONS=\"-R free-space-tree -O no-holes\"\n\n  NUM_JOBS=4\n  FILE_SIZE=8G\n  RUN_TIME=300\n\n  cat <<EOF > /tmp/fio-job.ini\n  [io_uring_rw]\n  rw=randrw\n  fsync=0\n  fallocate=posix\n  group_reporting=1\n  direct=1\n  ioengine=io_uring\n  iodepth=64\n  bssplit=4k/20:8k/20:16k/20:32k/10:64k/10:128k/5:256k/5:512k/5:1m/5\n  filesize=$FILE_SIZE\n  runtime=$RUN_TIME\n  time_based\n  filename=foobar\n  directory=$MNT\n  numjobs=$NUM_JOBS\n  thread\n  EOF\n\n  echo performance | \\\n     tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n\n  umount $MNT &> /dev/null\n  mkfs.btrfs -f $MKFS_OPTIONS $DEV &> /dev/null\n  mount $MOUNT_OPTIONS $DEV $MNT\n\n  fio /tmp/fio-job.ini\n\n  umount $MNT\n\nThe test was run a 12 cores box with 64G of ram, using a non-debug kernel\nconfig (Debian's default config) and a spinning disk.\n\nResult before the patchset:\n\n READ: bw=407MiB/s (427MB/s), 407MiB/s-407MiB/s (427MB/s-427MB/s), io=119GiB (128GB), run=300175-300175msec\nWRITE: bw=407MiB/s (427MB/s), 407MiB/s-407MiB/s (427MB/s-427MB/s), io=119GiB (128GB), run=300175-300175msec\n\nResult after the patchset:\n\n READ: bw=436MiB/s (457MB/s), 436MiB/s-436MiB/s (457MB/s-457MB/s), io=128GiB (137GB), run=300044-300044msec\nWRITE: bw=435MiB/s (456MB/s), 435MiB/s-435MiB/s (456MB/s-456MB/s), io=128GiB (137GB), run=300044-300044msec\n\nThat's about +7.2% throughput for reads and +6.9% for writes.\n\nSigned-off-by: Filipe Manana <fdmanana@suse.com>\nSigned-off-by: David Sterba <dsterba@suse.com>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-16 17:03:10 +0200 btrfs: avoid blocking on space revervation when doing nowait dio writes"
    },
    {
        "commit": "81132a39c152ca09832b9e4cb748129cee5f55ec",
        "message": "These two interface were added in 091141a42 commit,\nbut now there is no place to call them.\n\nThe only user of fput/fget_many() was removed in commit\n62906e89e63b (\"io_uring: remove file batch-get optimisation\").\n\nA user of get_file_rcu_many() were removed in commit\nf073531070d2 (\"init: add an init_dup helper\").\n\nAnd replace atomic_long_sub/add to atomic_long_dec/inc\ncan improve performance.\n\nHere are the test results of unixbench:\n\nCmd: ./Run -c 64 context1\n\nWithout patch:\nSystem Benchmarks Partial Index              BASELINE       RESULT    INDEX\nPipe-based Context Switching                   4000.0    2798407.0   6996.0\n                                                                   ========\nSystem Benchmarks Index Score (Partial Only)                         6996.0\n\nWith patch:\nSystem Benchmarks Partial Index              BASELINE       RESULT    INDEX\nPipe-based Context Switching                   4000.0    3486268.8   8715.7\n                                                                   ========\nSystem Benchmarks Index Score (Partial Only)                         8715.7\n\nSigned-off-by: Gou Hao <gouhao@uniontech.com>\nSigned-off-by: Al Viro <viro@zeniv.linux.org.uk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-14 18:47:28 -0400 fs: remove fget_many and fput_many interface"
    },
    {
        "commit": "4e86a2c980137f7be1ea600af5f1f5c8342ecc09",
        "message": "Refactor io_accept() to support multishot mode.\n\ntheoretical analysis:\n  1) when connections come in fast\n    - singleshot:\n              add accept sqe(userspace) --> accept inline\n                              ^                 |\n                              |-----------------|\n    - multishot:\n             add accept sqe(userspace) --> accept inline\n                                              ^     |\n                                              |--*--|\n\n    we do accept repeatedly in * place until get EAGAIN\n\n  2) when connections come in at a low pressure\n    similar thing like 1), we reduce a lot of userspace-kernel context\n    switch and useless vfs_poll()\n\ntests:\nDid some tests, which goes in this way:\n\n  server    client(multiple)\n  accept    connect\n  read      write\n  write     read\n  close     close\n\nBasically, raise up a number of clients(on same machine with server) to\nconnect to the server, and then write some data to it, the server will\nwrite those data back to the client after it receives them, and then\nclose the connection after write return. Then the client will read the\ndata and then close the connection. Here I test 10000 clients connect\none server, data size 128 bytes. And each client has a go routine for\nit, so they come to the server in short time.\ntest 20 times before/after this patchset, time spent:(unit cycle, which\nis the return value of clock())\nbefore:\n  1930136+1940725+1907981+1947601+1923812+1928226+1911087+1905897+1941075\n  +1934374+1906614+1912504+1949110+1908790+1909951+1941672+1969525+1934984\n  +1934226+1914385)/20.0 = 1927633.75\nafter:\n  1858905+1917104+1895455+1963963+1892706+1889208+1874175+1904753+1874112\n  +1874985+1882706+1884642+1864694+1906508+1916150+1924250+1869060+1889506\n  +1871324+1940803)/20.0 = 1894750.45\n\n(1927633.75 - 1894750.45) / 1927633.75 = 1.65%\n\nSigned-off-by: Hao Xu <howeyxu@tencent.com>\nLink: https://lore.kernel.org/r/20220514142046.58072-5-haoxu.linux@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-14 08:34:22 -0600 io_uring: implement multishot mode for accept"
    },
    {
        "commit": "dbc2564cfe0faff439dc46adb8c009589054ea46",
        "message": "For operations like accept, multishot is a useful feature, since we can\nreduce a number of accept sqe. Let's integrate it to fast poll, it may\nbe good for other operations in the future.\n\nSigned-off-by: Hao Xu <howeyxu@tencent.com>\nLink: https://lore.kernel.org/r/20220514142046.58072-4-haoxu.linux@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-14 08:34:22 -0600 io_uring: let fast poll support multishot"
    },
    {
        "commit": "227685ebfaba0bc7e2ddc47cef4556050b6d7a8f",
        "message": "Add a flag to indicate multishot mode for fast poll. currently only\naccept use it, but there may be more operations leveraging it in the\nfuture. Also add a mask IO_APOLL_MULTI_POLLED which stands for\nREQ_F_APOLL_MULTI | REQ_F_POLLED, to make the code short and cleaner.\n\nSigned-off-by: Hao Xu <howeyxu@tencent.com>\nLink: https://lore.kernel.org/r/20220514142046.58072-3-haoxu.linux@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-14 08:34:22 -0600 io_uring: add REQ_F_APOLL_MULTISHOT for requests"
    },
    {
        "commit": "390ed29b5e425ba00da2b6113b74a14949f71b02",
        "message": "add an accept_flag IORING_ACCEPT_MULTISHOT for accept, which is to\nsupport multishot.\n\nSigned-off-by: Hao Xu <howeyxu@tencent.com>\nLink: https://lore.kernel.org/r/20220514142046.58072-2-haoxu.linux@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-14 08:34:22 -0600 io_uring: add IORING_ACCEPT_MULTISHOT for accept"
    },
    {
        "commit": "1b1d7b4bf1d9948c8dba5ee550459ce7c65ac019",
        "message": "The check for waking up a request compares the poll_t bits, however this\nwill always contain some common flags so this always wakes up.\n\nFor files with single wait queues such as sockets this can cause the\nrequest to be sent to the async worker unnecesarily. Further if it is\nnon-blocking will complete the request with EAGAIN which is not desired.\n\nHere exclude these common events, making sure to not exclude POLLERR which\nmight be important.\n\nFixes: d7718a9d25a6 (\"io_uring: use poll driven retry for files that support it\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220512091834.728610-3-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-13 14:37:50 -0600 io_uring: only wake when the correct events are set"
    },
    {
        "commit": "e0deb6a025ae8c850dc8685be39fb27b06c88736",
        "message": "If an opcode handler semi-reliably returns -EAGAIN, io_wq_submit_work()\nmight continue busily hammer the same handler over and over again, which\nis not ideal. The -EAGAIN handling in question was put there only for\nIOPOLL, so restrict it to IOPOLL mode only where there is no other\nrecourse than to retry as we cannot wait.\n\nFixes: def596e9557c9 (\"io_uring: support for IO polling\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f168b4f24181942f3614dd8ff648221736f572e6.1652433740.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-13 06:50:42 -0600 io_uring: avoid io-wq -EAGAIN looping for !IOPOLL"
    },
    {
        "commit": "a8da73a32b6e9271a613e5a0e90a8c35f40abeb8",
        "message": "Currently to setup a fully sparse descriptor space upfront, the app needs\nto alloate an array of the full size and memset it to -1 and then pass\nthat in. Make this a bit easier by allowing a flag that simply does\nthis internally rather than needing to copy each slot separately.\n\nThis works with IORING_REGISTER_FILES2 as the flag is set in struct\nio_uring_rsrc_register, and is only allow when the type is\nIORING_RSRC_FILE as this doesn't make sense for registered buffers.\n\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-13 06:28:50 -0600 io_uring: add flag for allocating a fully sparse direct descriptor space"
    },
    {
        "commit": "09893e15f1e9910711c152cd126ccfa85581ba83",
        "message": "We currently limit these to 32K, but since we're now backing the table\nspace with vmalloc when needed, there's no reason why we can't make it\nbigger. The total space is limited by RLIMIT_NOFILE as well.\n\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-13 06:28:46 -0600 io_uring: bump max direct descriptor count to 1M"
    },
    {
        "commit": "c30c3e00cbd96f0cce478efba41dbe78dad8c774",
        "message": "If the application passes in IORING_FILE_INDEX_ALLOC as the file_slot,\nthen that's a hint to allocate a fixed file descriptor rather than have\none be passed in directly.\n\nThis can be useful for having io_uring manage the direct descriptor space,\nand also allows multi-shot support to work with fixed files.\n\nNormal accept direct requests will complete with 0 for success, and < 0\nin case of error. If io_uring is asked to allocated the direct descriptor,\nthen the direct descriptor is returned in case of success.\n\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-13 06:28:43 -0600 io_uring: allow allocated fixed files for accept"
    },
    {
        "commit": "1339f24b336db5ded9811f3fe7b948e0de207785",
        "message": "If the application passes in IORING_FILE_INDEX_ALLOC as the file_slot,\nthen that's a hint to allocate a fixed file descriptor rather than have\none be passed in directly.\n\nThis can be useful for having io_uring manage the direct descriptor space.\n\nNormal open direct requests will complete with 0 for success, and < 0\nin case of error. If io_uring is asked to allocated the direct descriptor,\nthen the direct descriptor is returned in case of success.\n\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-13 06:28:42 -0600 io_uring: allow allocated fixed files for openat/openat2"
    },
    {
        "commit": "b70b8e3331d8134ab993368a6e0eb18c1acb1b1d",
        "message": "Applications currently always pick where they want fixed files to go.\nIn preparation for allowing these types of commands with multishot\nsupport, add a basic allocator in the fixed file table.\n\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-13 06:28:40 -0600 io_uring: add basic fixed file allocator"
    },
    {
        "commit": "d78bd8adfcbc55b9dc01e9034a55b2a61a2124dc",
        "message": "In preparation for adding a basic allocator for direct descriptors,\nadd helpers that set/clear whether a file slot is used.\n\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-13 06:28:29 -0600 io_uring: track fixed files with a bitmap"
    },
    {
        "commit": "4329490a78b66ae44a9c93e433da375284162e3d",
        "message": "simplifies logics on cleanup, as well...\n\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Al Viro <viro@zeniv.linux.org.uk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-12 17:07:05 -0400 io_uring_enter(): don't leave f.flags uninitialized"
    },
    {
        "commit": "2d2d5cb6ca8424fa849ebb4edb8e8022c13860c7",
        "message": "Fix arg ordering in TP_ARGS macro, which fixes the output.\n\nFixes: 502c87d65564c (\"io-uring: Make tracepoints consistent.\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220512091834.728610-2-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18",
        "release_date": "2022-05-12 06:18:58 -0600 io_uring: fix ordering of args in io_uring_queue_async_work"
    },
    {
        "commit": "ee692a21e9bf8354bd3ec816f1cf4bff8619ed77",
        "message": "file_operations->uring_cmd is a file private handler.\nThis is somewhat similar to ioctl but hopefully a lot more sane and\nuseful as it can be used to enable many io_uring capabilities for the\nunderlying operation.\n\nIORING_OP_URING_CMD is a file private kind of request. io_uring doesn't\nknow what is in this command type, it's for the provider of ->uring_cmd()\nto deal with.\n\nCo-developed-by: Kanchan Joshi <joshi.k@samsung.com>\nSigned-off-by: Kanchan Joshi <joshi.k@samsung.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20220511054750.20432-2-joshi.k@samsung.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-11 07:40:47 -0600 fs,io_uring: add infrastructure for uring-cmd"
    },
    {
        "commit": "322842ea3c7264936b084ac949e9070ee47a5202",
        "message": "Patch series \"mm: COW fixes part 2: reliable GUP pins of anonymous pages\", v4.\n\nThis series is the result of the discussion on the previous approach [2]. \nMore information on the general COW issues can be found there.  It is\nbased on latest linus/master (post v5.17, with relevant core-MM changes\nfor v5.18-rc1).\n\nThis series fixes memory corruptions when a GUP pin (FOLL_PIN) was taken\non an anonymous page and COW logic fails to detect exclusivity of the page\nto then replacing the anonymous page by a copy in the page table: The GUP\npin lost synchronicity with the pages mapped into the page tables.\n\nThis issue, including other related COW issues, has been summarized in [3]\nunder 3):\n\"\n  3. Intra Process Memory Corruptions due to Wrong COW (FOLL_PIN)\n\n  page_maybe_dma_pinned() is used to check if a page may be pinned for\n  DMA (using FOLL_PIN instead of FOLL_GET).  While false positives are\n  tolerable, false negatives are problematic: pages that are pinned for\n  DMA must not be added to the swapcache.  If it happens, the (now pinned)\n  page could be faulted back from the swapcache into page tables\n  read-only.  Future write-access would detect the pinning and COW the\n  page, losing synchronicity.  For the interested reader, this is nicely\n  documented in feb889fb40fa (\"mm: don't put pinned pages into the swap\n  cache\").\n\n  Peter reports [8] that page_maybe_dma_pinned() as used is racy in some\n  cases and can result in a violation of the documented semantics: giving\n  false negatives because of the race.\n\n  There are cases where we call it without properly taking a per-process\n  sequence lock, turning the usage of page_maybe_dma_pinned() racy.  While\n  one case (clear_refs SOFTDIRTY tracking, see below) seems to be easy to\n  handle, there is especially one rmap case (shrink_page_list) that's hard\n  to fix: in the rmap world, we're not limited to a single process.\n\n  The shrink_page_list() issue is really subtle.  If we race with\n  someone pinning a page, we can trigger the same issue as in the FOLL_GET\n  case.  See the detail section at the end of this mail on a discussion\n  how bad this can bite us with VFIO or other FOLL_PIN user.\n\n  It's harder to reproduce, but I managed to modify the O_DIRECT\n  reproducer to use io_uring fixed buffers [15] instead, which ends up\n  using FOLL_PIN | FOLL_WRITE | FOLL_LONGTERM to pin buffer pages and can\n  similarly trigger a loss of synchronicity and consequently a memory\n  corruption.\n\n  Again, the root issue is that a write-fault on a page that has\n  additional references results in a COW and thereby a loss of\n  synchronicity and consequently a memory corruption if two parties\n  believe they are referencing the same page.\n\"\n\nThis series makes GUP pins (R/O and R/W) on anonymous pages fully\nreliable, especially also taking care of concurrent pinning via GUP-fast,\nfor example, also fully fixing an issue reported regarding NUMA balancing\n[4] recently.  While doing that, it further reduces \"unnecessary COWs\",\nespecially when we don't fork()/KSM and don't swapout, and fixes the COW\nsecurity for hugetlb for FOLL_PIN.\n\nIn summary, we track via a pageflag (PG_anon_exclusive) whether a mapped\nanonymous page is exclusive.  Exclusive anonymous pages that are mapped\nR/O can directly be mapped R/W by the COW logic in the write fault\nhandler.  Exclusive anonymous pages that want to be shared (fork(), KSM)\nfirst have to be marked shared -- which will fail if there are GUP pins on\nthe page.  GUP is only allowed to take a pin on anonymous pages that are\nexclusive.  The PT lock is the primary mechanism to synchronize\nmodifications of PG_anon_exclusive.  We synchronize against GUP-fast\neither via the src_mm->write_protect_seq (during fork()) or via\nclear/invalidate+flush of the relevant page table entry.\n\nSpecial care has to be taken about swap, migration, and THPs (whereby a\nPMD-mapping can be converted to a PTE mapping and we have to track\ninformation for subpages).  Besides these, we let the rmap code handle\nmost magic.  For reliable R/O pins of anonymous pages, we need\nFAULT_FLAG_UNSHARE logic as part of our previous approach [2], however,\nit's now 100% mapcount free and I further simplified it a bit.\n\n  #1 is a fix\n  #3-#10 are mostly rmap preparations for PG_anon_exclusive handling\n  #11 introduces PG_anon_exclusive\n  #12 uses PG_anon_exclusive and make R/W pins of anonymous pages\n   reliable\n  #13 is a preparation for reliable R/O pins\n  #14 and #15 is reused/modified GUP-triggered unsharing for R/O GUP pins\n   make R/O pins of anonymous pages reliable\n  #16 adds sanity check when (un)pinning anonymous pages\n\n[1] https://lkml.kernel.org/r/20220131162940.210846-1-david@redhat.com\n[2] https://lkml.kernel.org/r/20211217113049.23850-1-david@redhat.com\n[3] https://lore.kernel.org/r/3ae33b08-d9ef-f846-56fb-645e3b9b4c66@redhat.com\n[4] https://bugzilla.kernel.org/show_bug.cgi?id=215616\n\n\nThis patch (of 17):\n\nIn case arch_unmap_one() fails, we already did a swap_duplicate().  let's\nundo that properly via swap_free().\n\nLink: https://lkml.kernel.org/r/20220428083441.37290-1-david@redhat.com\nLink: https://lkml.kernel.org/r/20220428083441.37290-2-david@redhat.com\nFixes: ca827d55ebaa (\"mm, swap: Add infrastructure for saving page metadata on swap\")\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Khalid Aziz <khalid.aziz@oracle.com>\nAcked-by: Vlastimil Babka <vbabka@suse.cz>\nCc: Hugh Dickins <hughd@google.com>\nCc: David Rientjes <rientjes@google.com>\nCc: Shakeel Butt <shakeelb@google.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Jason Gunthorpe <jgg@nvidia.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nCc: Mike Rapoport <rppt@linux.ibm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: \"Kirill A. Shutemov\" <kirill.shutemov@linux.intel.com>\nCc: \"Matthew Wilcox (Oracle)\" <willy@infradead.org>\nCc: Jann Horn <jannh@google.com>\nCc: Michal Hocko <mhocko@kernel.org>\nCc: Nadav Amit <namit@vmware.com>\nCc: Rik van Riel <riel@surriel.com>\nCc: Roman Gushchin <guro@fb.com>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Don Dutile <ddutile@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Oleg Nesterov <oleg@redhat.com>\nCc: Jan Kara <jack@suse.cz>\nCc: Liang Zhang <zhangliang5@huawei.com>\nCc: Pedro Demarchi Gomes <pedrodemargomes@gmail.com>\nCc: Oded Gabbay <oded.gabbay@gmail.com>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 18:20:42 -0700 mm/rmap: fix missing swap_free() in try_to_unmap() after arch_unmap_one() failed"
    },
    {
        "commit": "2bb04df7c2af9dad5d28771c723bc39b01cf7df4",
        "message": "This adds support for filling the extra1 and extra2 fields for large\nCQE's.\n\nCo-developed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Stefan Roesch <shr@fb.com>\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220426182134.136504-13-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:35:34 -0600 io_uring: support CQE32 for nop operation"
    },
    {
        "commit": "76c68fbf1a1f97afed0c8f680ee4e3f4da3e720d",
        "message": "This enables large CQE's in the uring setup.\n\nCo-developed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Stefan Roesch <shr@fb.com>\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220426182134.136504-12-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:35:34 -0600 io_uring: enable CQE32"
    },
    {
        "commit": "f9b3dfcc68a502ef82e50274e2e7e9e91f6bf4e2",
        "message": "This exposes the extra1 and extra2 fields in the /proc output.\n\nSigned-off-by: Stefan Roesch <shr@fb.com>\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220426182134.136504-11-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:35:34 -0600 io_uring: support CQE32 in /proc info"
    },
    {
        "commit": "c4bb964fa092fb68645a852365dfe9855fef178a",
        "message": "This adds tracing for the extra1 and extra2 fields.\n\nCo-developed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Stefan Roesch <shr@fb.com>\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220426182134.136504-10-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:35:34 -0600 io_uring: add tracing for additional CQE32 fields"
    },
    {
        "commit": "e45a3e05008d52c6db63a3a01a9cdc7d89cd133a",
        "message": "This adds the overflow processing for large CQE's.\n\nThis adds two parameters to the io_cqring_event_overflow function and\nuses these fields to initialize the large CQE fields.\n\nAllocate enough space for large CQE's in the overflow structue. If no\nlarge CQE's are used, the size of the allocation is unchanged.\n\nThe cqe field can have a different size depending if its a large\nCQE or not. To be able to allocate different sizes, the two fields\nin the structure are re-ordered.\n\nCo-developed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Stefan Roesch <shr@fb.com>\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220426182134.136504-9-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:35:34 -0600 io_uring: overflow processing for CQE32"
    },
    {
        "commit": "0e2e5c47fed68ce203f2c6978188cc49a2a96e26",
        "message": "This flushes the completions according to their CQE type: the same\nprocessing is done for the default CQE size, but for large CQE's the\nextra1 and extra2 fields are filled in.\n\nSigned-off-by: Stefan Roesch <shr@fb.com>\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220426182134.136504-8-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:35:34 -0600 io_uring: flush completions for CQE32"
    },
    {
        "commit": "2fee6bc6407848043798698116b8fd81d1fe470a",
        "message": "Modify accesses to the CQE array to take large CQE's into account. The\nindex needs to be shifted by one for large CQE's.\n\nSigned-off-by: Stefan Roesch <shr@fb.com>\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220426182134.136504-7-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:35:34 -0600 io_uring: modify io_get_cqe for CQE32"
    },
    {
        "commit": "effcf8bdeb03aa726e9db834325c650e1700b041",
        "message": "This adds the completion processing for the large CQE's and makes sure\nthat the extra1 and extra2 fields are passed through.\n\nCo-developed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Stefan Roesch <shr@fb.com>\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220426182134.136504-6-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:35:34 -0600 io_uring: add CQE32 completion processing"
    },
    {
        "commit": "916587984facd01a2f4a2e327d721601a94ed1ed",
        "message": "This adds two new function to setup and fill the CQE32 result structure.\n\nSigned-off-by: Stefan Roesch <shr@fb.com>\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220426182134.136504-5-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:35:34 -0600 io_uring: add CQE32 setup processing"
    },
    {
        "commit": "baf9cb643b485d57c404b0ea9c1865036dde9eb7",
        "message": "This changes the function rings_size to take large CQE's into account.\n\nCo-developed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Stefan Roesch <shr@fb.com>\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220426182134.136504-4-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:35:34 -0600 io_uring: change ring size calculation for CQE32"
    },
    {
        "commit": "4e5bc0a9a1d0be5b20a0366fbfbe5a99d61c6003",
        "message": "This reuses the hash list node for the storage we need to hold the two\n64-bit values that must be passed back.\n\nCo-developed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Stefan Roesch <shr@fb.com>\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220426182134.136504-3-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:35:33 -0600 io_uring: store add. return values for CQE32"
    },
    {
        "commit": "7a51e5b44b92686eebd3e1b46b86e1eb4db975db",
        "message": "This adds the big_cqe array to the struct io_uring_cqe to support large\nCQE's.\n\nCo-developed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Stefan Roesch <shr@fb.com>\nReviewed-by: Kanchan Joshi <joshi.k@samsung.com>\nLink: https://lore.kernel.org/r/20220426182134.136504-2-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:35:33 -0600 io_uring: support CQE32 in io_uring_cqe"
    },
    {
        "commit": "ebdeb7c01d025cb059f05dc26b9dc914e46dd43f",
        "message": "Normal SQEs are 64-bytes in length, which is fine for all the commands\nwe support. However, in preparation for supporting passthrough IO,\nprovide an option for setting up a ring with 128-byte SQEs.\n\nWe continue to use the same type for io_uring_sqe, it's marked and\ncommented with a zero sized array pad at the end. This provides up\nto 80 bytes of data for a passthrough command - 64 bytes for the\nextra added data, and 16 bytes available at the end of the existing\nSQE.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:35:33 -0600 io_uring: add support for 128-byte SQEs"
    },
    {
        "commit": "b5ba65df47cabcba6fe7a03f8f57513e9f78f72f",
        "message": "* for-5.19/io_uring-socket:\n  io_uring: use the text representation of ops in trace\n  io_uring: rename op -> opcode\n  io_uring: add io_uring_get_opcode\n  io_uring: add type to op enum\n  io_uring: add socket(2) support\n  net: add __sys_socket_file()\n  io_uring: fix trace for reduced sqe padding\n  io_uring: add fgetxattr and getxattr support\n  io_uring: add fsetxattr and setxattr support\n  fs: split off do_getxattr from getxattr\n  fs: split off setxattr_copy and do_setxattr function from setxattr",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:35:28 -0600 Merge branch 'for-5.19/io_uring-socket' into for-5.19/io_uring-passthrough"
    },
    {
        "commit": "1308689906ad35b017eec8e595a2beb6f2f972fb",
        "message": "* for-5.19/io_uring: (85 commits)\n  io_uring: don't clear req->kbuf when buffer selection is done\n  io_uring: eliminate the need to track provided buffer ID separately\n  io_uring: move provided buffer state closer to submit state\n  io_uring: move provided and fixed buffers into the same io_kiocb area\n  io_uring: abstract out provided buffer list selection\n  io_uring: never call io_buffer_select() for a buffer re-select\n  io_uring: get rid of hashed provided buffer groups\n  io_uring: always use req->buf_index for the provided buffer group\n  io_uring: ignore ->buf_index if REQ_F_BUFFER_SELECT isn't set\n  io_uring: kill io_rw_buffer_select() wrapper\n  io_uring: make io_buffer_select() return the user address directly\n  io_uring: kill io_recv_buffer_select() wrapper\n  io_uring: use 'sr' vs 'req->sr_msg' consistently\n  io_uring: add POLL_FIRST support for send/sendmsg and recv/recvmsg\n  io_uring: check IOPOLL/ioprio support upfront\n  io_uring: replace smp_mb() with smp_mb__after_atomic() in io_sq_thread()\n  io_uring: add IORING_SETUP_TASKRUN_FLAG\n  io_uring: use TWA_SIGNAL_NO_IPI if IORING_SETUP_COOP_TASKRUN is used\n  io_uring: set task_work notify method at init time\n  io-wq: use __set_notify_signal() to wake workers\n  ...",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:35:11 -0600 Merge branch 'for-5.19/io_uring' into for-5.19/io_uring-passthrough"
    },
    {
        "commit": "7ccba24d3bc084d891def1a6fea504e4cb327a8c",
        "message": "It's not needed as the REQ_F_BUFFER_SELECTED flag tracks the state of\nwhether or not kbuf is valid, so just drop it.\n\nSuggested-by: Dylan Yudaken <dylany@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:29:06 -0600 io_uring: don't clear req->kbuf when buffer selection is done"
    },
    {
        "commit": "1dbd023eb083249026d51f41cee48a7b199b3d4e",
        "message": "We have io_kiocb->buf_index which is used for either fixed buffers, or\nfor provided buffers. For the latter, it's used to hold the buffer group\nID for buffer selection. Post selection, req->kbuf->bid is used to get\nthe buffer ID.\n\nStore the buffer ID, when selected, in req->buf_index. If we do end up\nrecycling the buffer, reset it back to the buffer group ID.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:29:06 -0600 io_uring: eliminate the need to track provided buffer ID separately"
    },
    {
        "commit": "660cbfa2340af1d25db4f7c4e93b8c1722bc72aa",
        "message": "The timeout and other items that follow are less hot, so let's move the\nprovided buffer state above that.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:29:06 -0600 io_uring: move provided buffer state closer to submit state"
    },
    {
        "commit": "a4f8d94cfb7c69f996b6a52b1fcbec2f2504dd3f",
        "message": "These are mutually exclusive - if you use provided buffers, then you\ncannot use fixed buffers and vice versa. Move them into the same spot\nin the io_kiocb, which is also advantageous for provided buffers as\nthey get near the submit side hot cacheline.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:29:06 -0600 io_uring: move provided and fixed buffers into the same io_kiocb area"
    },
    {
        "commit": "149c69b04a901c8b611b643af8f4dd6b104e7379",
        "message": "In preparation for providing another way to select a buffer, move the\nexisting logic into a helper.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:29:06 -0600 io_uring: abstract out provided buffer list selection"
    },
    {
        "commit": "b66e65f41426ec82b92ad4d9a752802bf9e2e383",
        "message": "Callers already have room to store the addr and length information,\nclean it up by having the caller just assign the previously provided\ndata.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:29:06 -0600 io_uring: never call io_buffer_select() for a buffer re-select"
    },
    {
        "commit": "9cfc7e94e42be9c73072606b84d4574a0a2ec270",
        "message": "Use a plain array for any group ID that's less than 64, and punt\nanything beyond that to an xarray. 64 fits in a page even for 4KB\npage sizes and with the planned additions.\n\nThis makes the expected group usage faster by avoiding a hash and lookup\nto find our list, and it uses less memory upfront by not allocating any\nmemory for provided buffers unless it's actually being used.\n\nSuggested-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:29:06 -0600 io_uring: get rid of hashed provided buffer groups"
    },
    {
        "commit": "4e9067025259d1227c7f4f18a02c166c93e49290",
        "message": "The read/write opcodes use it already, but the recv/recvmsg do not. If\nwe switch them over and read and validate this at init time while we're\nchecking if the opcode supports it anyway, then we can do it in one spot\nand we don't have to pass in a separate group ID for io_buffer_select().\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:29:06 -0600 io_uring: always use req->buf_index for the provided buffer group"
    },
    {
        "commit": "bb68d504f7c4183178b00c1af68fca870728e7e0",
        "message": "There's no point in validity checking buf_index if the request doesn't\nhave REQ_F_BUFFER_SELECT set, as we will never use it for that case.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:29:06 -0600 io_uring: ignore ->buf_index if REQ_F_BUFFER_SELECT isn't set"
    },
    {
        "commit": "e5b003495e934321f78ba1f95e48da4a8c3a3a35",
        "message": "After the recent changes, this is direct call to io_buffer_select()\nanyway. With this change, there are no wrappers left for provided\nbuffer selection.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:29:06 -0600 io_uring: kill io_rw_buffer_select() wrapper"
    },
    {
        "commit": "c54d52c2d6131d112176b26aa97dc5e1af930d6e",
        "message": "There's no point in having callers provide a kbuf, we're just returning\nthe address anyway.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-09 06:29:02 -0600 io_uring: make io_buffer_select() return the user address directly"
    },
    {
        "commit": "b366bd7d9613971e666cdbfe7a08c653d329bf15",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single file assignment fix this week\"\n\n* tag 'io_uring-5.18-2022-05-06' of git://git.kernel.dk/linux-block:\n  io_uring: assign non-fixed early for async work",
        "kernel_version": "v5.18-rc6",
        "release_date": "2022-05-07 10:41:41 -0700 Merge tag 'io_uring-5.18-2022-05-06' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "c5febea0956fd3874e8fb59c6f84d68f128d68f8",
        "message": "With io_uring we have started supporting tasks that are for most\npurposes user space tasks that exclusively run code in kernel mode.\n\nThe kernel task that exec's init and tasks that exec user mode\nhelpers are also user mode tasks that just run kernel code\nuntil they call kernel execve.\n\nPass kernel_clone_args into copy_thread so these oddball\ntasks can be supported more cleanly and easily.\n\nv2: Fix spelling of kenrel_clone_args on h8300\nLink: https://lkml.kernel.org/r/20220506141512.516114-2-ebiederm@xmission.com\nSigned-off-by: \"Eric W. Biederman\" <ebiederm@xmission.com>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-07 09:01:48 -0500 fork: Pass struct kernel_clone_args into copy_thread"
    },
    {
        "commit": "9396ed850f2e0ea286de372968987157aeeca617",
        "message": "It's just a thin wrapper around io_buffer_select(), get rid of it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-05 17:15:15 -0600 io_uring: kill io_recv_buffer_select() wrapper"
    },
    {
        "commit": "0a352aaa9473006f6bc5b0b2d707827d1c1afe80",
        "message": "For all of send/sendmsg and recv/recvmsg we have the local 'sr' variable,\nyet some cases still use req->sr_msg which sr points to. Use 'sr'\nconsistently.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-05 17:14:33 -0600 io_uring: use 'sr' vs 'req->sr_msg' consistently"
    },
    {
        "commit": "0455d4ccec548b0fb51db39a4d3350a7a80a0222",
        "message": "If IORING_RECVSEND_POLL_FIRST is set for recv/recvmsg or send/sendmsg,\nthen we arm poll first rather than attempt a receive or send upfront.\nThis can be useful if we expect there to be no data (or space) available\nfor the request, as we can then avoid wasting time on the initial\nissue attempt.\n\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-05 17:09:31 -0600 io_uring: add POLL_FIRST support for send/sendmsg and recv/recvmsg"
    },
    {
        "commit": "73911426aaaadbae54fa72359b33a7b6a56947db",
        "message": "Don't punt this check to the op prep handlers, add the support to\nio_op_defs and we can check them while setting up the request.\n\nThis reduces the text size by 500 bytes on aarch64, and makes this less\nfragile by having the check in one spot and needing opcodes to opt in\nto IOPOLL or ioprio support.\n\nReviewed-by: Hao Xu <howeyxu@tencent.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-05 17:09:15 -0600 io_uring: check IOPOLL/ioprio support upfront"
    },
    {
        "commit": "c29722fad4aabbf6bb841b8f058f858ec911df56",
        "message": "Log the anonymous inode class name in the security hook\ninode_init_security_anon.  This name is the key for name based type\ntransitions on the anon_inode security class on creation.  Example:\n\n    type=AVC msg=audit(02/16/22 22:02:50.585:216) : avc:  granted \\\n        { create } for  pid=2136 comm=mariadbd anonclass=[io_uring] \\\n        scontext=system_u:system_r:mysqld_t:s0 \\\n        tcontext=system_u:system_r:mysqld_iouring_t:s0 tclass=anon_inode\n\nAdd a new LSM audit data type holding the inode and the class name.\n\nSigned-off-by: Christian G\u00f6ttsche <cgzones@googlemail.com>\n[PM: adjusted 'anonclass' to be a trusted string, cgzones approved]\nSigned-off-by: Paul Moore <paul@paul-moore.com>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-03 16:09:03 -0400 selinux: log anon inode class name"
    },
    {
        "commit": "9650b453a3d4b1b8ed4ea8bcb9b40109608d1faf",
        "message": "So far bio is marked as REQ_POLLED if RWF_HIPRI/IOCB_HIPRI is passed\nfrom userspace sync io interface, then block layer tries to poll until\nthe bio is completed. But the current implementation calls\nblk_io_schedule() if bio_poll() returns 0, and this way causes io hang or\ntimeout easily.\n\nBut looks no one reports this kind of issue, which should have been\ntriggered in normal io poll sanity test or blktests block/007 as\nobserved by Changhui, that means it is very likely that no one uses it\nor no one cares it.\n\nAlso after io_uring is invented, io poll for sync dio becomes legacy\ninterface.\n\nSo ignore RWF_HIPRI hint for sync dio.\n\nCC: linux-mm@kvack.org\nCc: linux-xfs@vger.kernel.org\nReported-by: Changhui Zhong <czhong@redhat.com>\nSuggested-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nTested-by: Changhui Zhong <czhong@redhat.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20220420143110.2679002-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-05-02 10:07:42 -0600 block: ignore RWF_HIPRI hint for sync dio"
    },
    {
        "commit": "a196c78b5443fc61af2c0490213b9d125482cbd1",
        "message": "We defer file assignment to ensure that fixed files work with links\nbetween a direct accept/open and the links that follow it. But this has\nthe side effect that normal file assignment is then not complete by the\ntime that request submission has been done.\n\nFor deferred execution, if the file is a regular file, assign it when\nwe do the async prep anyway.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc6",
        "release_date": "2022-05-02 08:09:39 -0600 io_uring: assign non-fixed early for async work"
    },
    {
        "commit": "f2e030dd7aaea5a937a2547dc980fab418fbc5e7",
        "message": "The IORING_SQ_NEED_WAKEUP flag is now set using atomic_or() which\nimplies a full barrier on some architectures but it is not required to\ndo so. Use the more appropriate smp_mb__after_atomic() which avoids the\nextra barrier on those architectures.\n\nSigned-off-by: Almog Khaikin <almogkh@gmail.com>\nLink: https://lore.kernel.org/r/20220426163403.112692-1-almogkh@gmail.com\nFixes: 8018823e6987 (\"io_uring: serialize ctx->rings->sq_flags with atomic_or/and\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-30 08:39:54 -0600 io_uring: replace smp_mb() with smp_mb__after_atomic() in io_sq_thread()"
    },
    {
        "commit": "ef060ea9e4fd3b763e7060a3af0a258d2d5d7c0d",
        "message": "If IORING_SETUP_COOP_TASKRUN is set to use cooperative scheduling for\nrunning task_work, then IORING_SETUP_TASKRUN_FLAG can be set so the\napplication can tell if task_work is pending in the kernel for this\nring. This allows use cases like io_uring_peek_cqe() to still function\nappropriately, or for the task to know when it would be useful to\ncall io_uring_wait_cqe() to run pending events.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20220426014904.60384-7-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-30 08:39:54 -0600 io_uring: add IORING_SETUP_TASKRUN_FLAG"
    },
    {
        "commit": "e1169f06d5bbdbc2b22ae4e3083a4bf75ae5ecee",
        "message": "If this is set, io_uring will never use an IPI to deliver a task_work\nnotification. This can be used in the common case where a single task or\nthread communicates with the ring, and doesn't rely on\nio_uring_cqe_peek().\n\nThis provides a noticeable win in performance, both from eliminating\nthe IPI itself, but also from avoiding interrupting the submitting\ntask unnecessarily.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20220426014904.60384-6-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-30 08:39:54 -0600 io_uring: use TWA_SIGNAL_NO_IPI if IORING_SETUP_COOP_TASKRUN is used"
    },
    {
        "commit": "9f010507bbc1be19dbeedc1a254209fea44adc14",
        "message": "While doing so, switch SQPOLL to TWA_SIGNAL_NO_IPI as well, as that\njust does a task wakeup and then we can remove the special wakeup we\nhave in task_work_add.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20220426014904.60384-5-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-30 08:39:54 -0600 io_uring: set task_work notify method at init time"
    },
    {
        "commit": "3a4b89a25ce59a87fcfff1f030cba8d544fd402c",
        "message": "Rather than require ctx->completion_lock for ensuring that we don't\nclobber the flags, use the atomic bitop helpers instead. This removes\nthe need to grab the completion_lock, in preparation for needing to set\nor clear sq_flags when we don't know the status of this lock.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20220426014904.60384-3-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-30 08:39:54 -0600 io_uring: serialize ctx->rings->sq_flags with atomic_or/and"
    },
    {
        "commit": "f548a12efd5ab97e6b1fb332e5634ce44b3d9328",
        "message": "For now just use a CQE flag for this, with big CQE support we could\nreturn the actual number of bytes left.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-29 21:12:12 -0600 io_uring: return hint on whether more data is available after receive"
    },
    {
        "commit": "a4c76853609107f0e5e7b51571d966785fe89cb3",
        "message": "Merge net branch with the required patch for supporting the io_uring\nfeature that passes back whether we had more data in the socket or not.\n\n* 'tcp-pass-back-data-left-in-socket-after-receive' of git://git.kernel.org/pub/scm/linux/kernel/git/kuba/linux:\n  tcp: pass back data left in socket after receive",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-29 21:11:15 -0600 Merge branch 'tcp-pass-back-data-left-in-socket-after-receive' of git://git.kernel.org/pub/scm/linux/kernel/git/kuba/linux into for-5.19/io_uring-net"
    },
    {
        "commit": "27738039fcdc6cb63400fe9b820b4027753568b7",
        "message": "* for-5.19/io_uring-socket: (73 commits)\n  io_uring: use the text representation of ops in trace\n  io_uring: rename op -> opcode\n  io_uring: add io_uring_get_opcode\n  io_uring: add type to op enum\n  io_uring: add socket(2) support\n  net: add __sys_socket_file()\n  io_uring: fix trace for reduced sqe padding\n  io_uring: add fgetxattr and getxattr support\n  io_uring: add fsetxattr and setxattr support\n  fs: split off do_getxattr from getxattr\n  fs: split off setxattr_copy and do_setxattr function from setxattr\n  io_uring: return an error when cqe is dropped\n  io_uring: use constants for cq_overflow bitfield\n  io_uring: rework io_uring_enter to simplify return value\n  io_uring: trace cqe overflows\n  io_uring: add trace support for CQE overflow\n  io_uring: allow re-poll if we made progress\n  io_uring: support MSG_WAITALL for IORING_OP_SEND(MSG)\n  io_uring: add support for IORING_ASYNC_CANCEL_ANY\n  io_uring: allow IORING_OP_ASYNC_CANCEL with 'fd' key\n  ...",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-29 21:10:52 -0600 Merge branch 'for-5.19/io_uring-socket' into for-5.19/io_uring-net"
    },
    {
        "commit": "63b7b3ea9442f1342299ddc58f7366e7ecd7e29f",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Pretty boring:\n\n   - three patches just adding reserved field checks (me, Eugene)\n\n   - Fixing a potential regression with IOPOLL caused by a block change\n     (Joseph)\"\n\nBoring is good.\n\n* tag 'io_uring-5.18-2022-04-29' of git://git.kernel.dk/linux-block:\n  io_uring: check that data field is 0 in ringfd unregister\n  io_uring: fix uninitialized field in rw io_kiocb\n  io_uring: check reserved fields for recv/recvmsg\n  io_uring: check reserved fields for send/sendmsg",
        "kernel_version": "v5.18-rc5",
        "release_date": "2022-04-29 14:51:57 -0700 Merge tag 'io_uring-5.18-2022-04-29' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "303cc749c8659d5f1ccf97973591313ec0bdacd3",
        "message": "Only allow data field to be 0 in struct io_uring_rsrc_update user\narguments to allow for future possible usage.\n\nFixes: e7a6c00dc77a (\"io_uring: add support for registering ring file descriptors\")\nSigned-off-by: Eugene Syromiatnikov <esyr@redhat.com>\nLink: https://lore.kernel.org/r/20220429142218.GA28696@asgard.redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc5",
        "release_date": "2022-04-29 08:39:43 -0600 io_uring: check that data field is 0 in ringfd unregister"
    },
    {
        "commit": "033b87d24f7257c45506bd043ad85ed24a9925e2",
        "message": "It is annoying to translate opcodes to textwhen tracing io_uring. Use the\nio_uring_get_opcode function instead to use the text representation.\n\nA downside here might have been that if the opcode is invalid it will not\nbe obvious, however the opcode is already overridden in these cases to\n0 (NOP) in io_init_req(). Therefore this is a non issue.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220426082907.3600028-5-dylany@fb.com\n[axboe: don't include register, those are not req opcodes]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-28 17:06:03 -0600 io_uring: use the text representation of ops in trace"
    },
    {
        "commit": "32452a3eb8b64e01e2be717f518c0be046975b9d",
        "message": "io_rw_init_file does not initialize kiocb->private, so when iocb_bio_iopoll\nreads kiocb->private it can contain uninitialized data.\n\nFixes: 3e08773c3841 (\"block: switch polling to be bio based\")\nSigned-off-by: Joseph Ravichandran <jravi@mit.edu>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc5",
        "release_date": "2022-04-28 13:13:43 -0600 io_uring: fix uninitialized field in rw io_kiocb"
    },
    {
        "commit": "5a1e99b61b0c81388cde0c808b3e4173907df19f",
        "message": "We should check unused fields for non-zero and -EINVAL if they are set,\nmaking it consistent with other opcodes.\n\nFixes: aa1fa28fc73e (\"io_uring: add support for recvmsg()\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc5",
        "release_date": "2022-04-26 20:48:37 -0600 io_uring: check reserved fields for recv/recvmsg"
    },
    {
        "commit": "588faa1ea5eecb351100ee5d187b9be99210f70d",
        "message": "We should check unused fields for non-zero and -EINVAL if they are set,\nmaking it consistent with other opcodes.\n\nFixes: 0fa03c624d8f (\"io_uring: add support for sendmsg()\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc5",
        "release_date": "2022-04-26 20:48:31 -0600 io_uring: check reserved fields for send/sendmsg"
    },
    {
        "commit": "1460af7de6ab33da82b8a1f03ce0f8e831a9e29e",
        "message": "do this for consistency with the other trace messages\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220426082907.3600028-4-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-26 06:50:42 -0600 io_uring: rename op -> opcode"
    },
    {
        "commit": "33337d03f04f9ea3a50ac2d3490a5d7a3ba9be82",
        "message": "In some debug scenarios it is useful to have the text representation of\nthe opcode. Add this function in preparation.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220426082907.3600028-3-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-26 06:50:42 -0600 io_uring: add io_uring_get_opcode"
    },
    {
        "commit": "cc51eaa8b530bf070e76847a717adcbf603469b7",
        "message": "It is useful to have a type enum for opcodes, to allow the compiler to\nassert that every value is used in a switch statement.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220426082907.3600028-2-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-26 06:50:42 -0600 io_uring: add type to op enum"
    },
    {
        "commit": "69cc1b6fa565993b62210f314614be166d902a54",
        "message": "If IO_URING_SCM_ALL isn't set, as it would not be on 32-bit builds,\nthen we trigger a warning:\n\nfs/io_uring.c: In function '__io_sqe_files_unregister':\nfs/io_uring.c:8992:13: warning: unused variable 'i' [-Wunused-variable]\n 8992 |         int i;\n      |             ^\n\nMove the ifdef up to include the 'i' variable declaration.\n\nReported-by: Stephen Rothwell <sfr@canb.auug.org.au>\nFixes: 5e45690a1cb8 (\"io_uring: store SCM state in io_fixed_file->file_ptr\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-25 16:43:45 -0600 io_uring: fix compile warning for 32-bit builds"
    },
    {
        "commit": "4ffaa94b9c047fe0e82b1f271554f31f0e2e2867",
        "message": "Move common error-handling to io_req_complete, so that various callers\navoid repeating that. Few callers (io_tee, io_splice) require slightly\ndifferent handling. These are changed to use __io_req_complete instead.\n\nSuggested-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Kanchan Joshi <joshi.k@samsung.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20220422101048.419942-1-joshi.k@samsung.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:29:33 -0600 io_uring: cleanup error-handling around io_req_complete"
    },
    {
        "commit": "1374e08e2d44863c931910797852589803997668",
        "message": "Supports both regular socket(2) where a normal file descriptor is\ninstantiated when called, or direct descriptors.\n\nLink: https://lore.kernel.org/r/20220412202240.234207-3-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:19:21 -0600 io_uring: add socket(2) support"
    },
    {
        "commit": "da214a475f8bd1d3e9e7a19ddfeb4d1617551bab",
        "message": "This works like __sys_socket(), except instead of allocating and\nreturning a socket fd, it just returns the file associated with the\nsocket. No fd is installed into the process file table.\n\nThis is similar to do_accept(), and allows io_uring to use this without\ninstantiating a file descriptor in the process file table.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nAcked-by: David S. Miller <davem@davemloft.net>\nLink: https://lore.kernel.org/r/20220412202240.234207-2-axboe@kernel.dk",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:19:21 -0600 net: add __sys_socket_file()"
    },
    {
        "commit": "0200ce6a57c5de802f4e438485c14cc9d63d5f4b",
        "message": "__pad2 is only 1 u64 now, the other one is addr3. Adjust the trace so\nthat it matches up.\n\nFixes: a56834e0fafe (\"io_uring: add fgetxattr and getxattr support\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:46 -0600 io_uring: fix trace for reduced sqe padding"
    },
    {
        "commit": "a56834e0fafe0adf7f22a28a5dbec3e8c3031a0e",
        "message": "This adds support to io_uring for the fgetxattr and getxattr API.\n\nSigned-off-by: Stefan Roesch <shr@fb.com>\nAcked-by: Christian Brauner <brauner@kernel.org>\nLink: https://lore.kernel.org/r/20220323154420.3301504-5-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:38 -0600 io_uring: add fgetxattr and getxattr support"
    },
    {
        "commit": "e9621e2bec80fe63f677a759066a5089b292f43a",
        "message": "This adds support to io_uring for the fsetxattr and setxattr API.\n\nSigned-off-by: Stefan Roesch <shr@fb.com>\nAcked-by: Christian Brauner <christian.brauner@ubuntu.com>\nLink: https://lore.kernel.org/r/20220323154420.3301504-4-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:37 -0600 io_uring: add fsetxattr and setxattr support"
    },
    {
        "commit": "c975cad931570004b5f51248424a2a696fb65630",
        "message": "This splits off do_getxattr function from the getxattr function. This will\nallow io_uring to call it from its io worker.\n\nSigned-off-by: Stefan Roesch <shr@fb.com>\nAcked-by: Christian Brauner <christian.brauner@ubuntu.com>\nLink: https://lore.kernel.org/r/20220323154420.3301504-3-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:37 -0600 fs: split off do_getxattr from getxattr"
    },
    {
        "commit": "1a91794ce8481a293c5ef432feb440aee1455619",
        "message": "This splits of the setup part of the function setxattr in its own\ndedicated function called setxattr_copy. In addition it also exposes a new\nfunction called do_setxattr for making the setxattr call.\n\nThis makes it possible to call these two functions from io_uring in the\nprocessing of an xattr request.\n\nSigned-off-by: Stefan Roesch <shr@fb.com>\nAcked-by: Christian Brauner <christian.brauner@ubuntu.com>\nLink: https://lore.kernel.org/r/20220323154420.3301504-2-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:37 -0600 fs: split off setxattr_copy and do_setxattr function from setxattr"
    },
    {
        "commit": "155bc9505dbd6613585abbf0be6466f1c21536c4",
        "message": "Right now io_uring will not actively inform userspace if a CQE is\ndropped. This is extremely rare, requiring a CQ ring overflow, as well as\na GFP_ATOMIC kmalloc failure. However the consequences could cause for\nexample applications to go into an undefined state, possibly waiting for a\nCQE that never arrives.\n\nReturn an error code (EBADR) in these cases. Since this is expected to be\nincredibly rare, try and avoid as much as possible affecting the hot code\npaths, and so it only is returned lazily and when there is no other\navailable CQEs.\n\nOnce the error is returned, reset the error condition assuming the user is\neither ok with it or will clean up appropriately.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220421091345.2115755-6-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:18 -0600 io_uring: return an error when cqe is dropped"
    },
    {
        "commit": "10988a0a67ba918c5b3ac7de47efa7470264a291",
        "message": "Prepare to use this bitfield for more flags by using constants instead of\nmagic value 0\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220421091345.2115755-5-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:18 -0600 io_uring: use constants for cq_overflow bitfield"
    },
    {
        "commit": "3e813c9026720c1291ef3c91caa2e26f88e28367",
        "message": "io_uring_enter returns the count submitted preferrably over an error\ncode. In some code paths this check is not required, so reorganise the\ncode so that the check is only done as needed.\nThis is also a prep for returning error codes only in waiting scenarios.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220421091345.2115755-4-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:18 -0600 io_uring: rework io_uring_enter to simplify return value"
    },
    {
        "commit": "08dcd0288f6ecb97bc94260856e6b44cca4149e2",
        "message": "Trace cqe overflows in io_uring. Print ocqe before the check, so if it is\nNULL it indicates that it has been dropped.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220421091345.2115755-3-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:18 -0600 io_uring: trace cqe overflows"
    },
    {
        "commit": "47894438e916ca3fe44db33c2d4a1670fd296d6f",
        "message": "Add trace function for overflowing CQ ring.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220421091345.2115755-2-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:18 -0600 io_uring: add trace support for CQE overflow"
    },
    {
        "commit": "10c873334febaeea9aa0c25c10b5ac0951b77a5f",
        "message": "We currently check REQ_F_POLLED before arming async poll for a\nnotification to retry. If it's set, then we don't allow poll and will\npunt to io-wq instead. This is done to prevent a situation where a buggy\ndriver will repeatedly return that there's space/data available yet we\nget -EAGAIN.\n\nHowever, if we already transferred data, then it should be safe to rely\non poll again. Gate the check on whether or not REQ_F_PARTIAL_IO is\nalso set.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:18 -0600 io_uring: allow re-poll if we made progress"
    },
    {
        "commit": "4c3c09439c08b03d9503df0ca4c7619c5842892e",
        "message": "Like commit 7ba89d2af17a for recv/recvmsg, support MSG_WAITALL for the\nsend side. If this flag is set and we do a short send, retry for a\nstream of seqpacket socket.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:18 -0600 io_uring: support MSG_WAITALL for IORING_OP_SEND(MSG)"
    },
    {
        "commit": "970f256edb8c1259c8ed48d52b38215135396126",
        "message": "Rather than match on a specific key, be it user_data or file, allow\ncanceling any request that we can lookup. Works like\nIORING_ASYNC_CANCEL_ALL in that it cancels multiple requests, but it\ndoesn't key off user_data or the file.\n\nCan't be set with IORING_ASYNC_CANCEL_FD, as that's a key selector.\nOnly one may be used at the time.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nLink: https://lore.kernel.org/r/20220418164402.75259-6-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:18 -0600 io_uring: add support for IORING_ASYNC_CANCEL_ANY"
    },
    {
        "commit": "4bf94615b8886305199ed5755cb72fea88258d15",
        "message": "Currently sqe->addr must contain the user_data of the request being\ncanceled. Introduce the IORING_ASYNC_CANCEL_FD flag, which tells the\nkernel that we're keying off the file fd instead for cancelation. This\nallows canceling any request that a) uses a file, and b) was assigned the\nfile based on the value being passed in.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nLink: https://lore.kernel.org/r/20220418164402.75259-5-axboe@kernel.dk",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:18 -0600 io_uring: allow IORING_OP_ASYNC_CANCEL with 'fd' key"
    },
    {
        "commit": "8e29da69feade64ec7fe9e1a2824b967c5183a21",
        "message": "The current cancelation will lookup and cancel the first request it\nfinds based on the key passed in. Add a flag that allows to cancel any\nrequest that matches they key. It completes with the number of requests\nfound and canceled, or res < 0 if an error occured.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nLink: https://lore.kernel.org/r/20220418164402.75259-4-axboe@kernel.dk",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:18 -0600 io_uring: add support for IORING_ASYNC_CANCEL_ALL"
    },
    {
        "commit": "b21432b4d580d064b6dee6d66c9a6b0071b499ce",
        "message": "In preparation for being able to not only key cancel off the user_data,\npass in the io_cancel_data struct for the various functions that deal\nwith request cancelation.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nLink: https://lore.kernel.org/r/20220418164402.75259-3-axboe@kernel.dk",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:18 -0600 io_uring: pass in struct io_cancel_data consistently"
    },
    {
        "commit": "98d3dcc8be97a11f7f3b674acb25f81673bb43c3",
        "message": "It's only called from one location, and it always passes in 'false'.\nKill the argument, and just pass in 'false' to io_poll_find().\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nLink: https://lore.kernel.org/r/20220418164402.75259-2-axboe@kernel.dk",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:18 -0600 io_uring: remove dead 'poll_only' argument to io_poll_cancel()"
    },
    {
        "commit": "81ec803b4ecda446aa965e1c0d8eb4241847ed9c",
        "message": "Split timeout handling into removal + failing, so we can reduce\nspinlocking time and remove another instance of triple nested locking.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0f00d115f9d4c5749028f19623708ad3695512d6.1650458197.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:17 -0600 io_uring: refactor io_disarm_next() locking"
    },
    {
        "commit": "3645c2000a7694022c39c545676c12fb9190855a",
        "message": "Move ->timeout_lock grabbing inside of io_timeout_cancel(), so\nwe can do io_req_task_queue_fail() outside of the lock. It's much nicer\nthan relying on triple nested locking.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/cde758c2897930d31e205ed8f476d4ec879a8849.1650458197.git.asml.silence@gmail.com\n[axboe: drop now wrong timeout_lock annotation]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:17 -0600 io_uring: move timeout locking in io_timeout_cancel()"
    },
    {
        "commit": "5e45690a1cb8cb591ccf4a517cdd6ad6cb369ac4",
        "message": "A previous commit removed SCM accounting for non-unix sockets, as those\nare the only ones that can cause a fixed file reference. While that is\ntrue, it also means we're now dereferencing the file as part of the\nworkqueue driven __io_sqe_files_unregister() after the process has\nexited. This isn't safe for SCM files, as unix gc may have already\nreaped them when the process exited. KASAN complains about this:\n\n[   12.307040] Freed by task 0:\n[   12.307592]  kasan_save_stack+0x28/0x4c\n[   12.308318]  kasan_set_track+0x28/0x38\n[   12.309049]  kasan_set_free_info+0x24/0x44\n[   12.309890]  ____kasan_slab_free+0x108/0x11c\n[   12.310739]  __kasan_slab_free+0x14/0x1c\n[   12.311482]  slab_free_freelist_hook+0xd4/0x164\n[   12.312382]  kmem_cache_free+0x100/0x1dc\n[   12.313178]  file_free_rcu+0x58/0x74\n[   12.313864]  rcu_core+0x59c/0x7c0\n[   12.314675]  rcu_core_si+0xc/0x14\n[   12.315496]  _stext+0x30c/0x414\n[   12.316287]\n[   12.316687] Last potentially related work creation:\n[   12.317885]  kasan_save_stack+0x28/0x4c\n[   12.318845]  __kasan_record_aux_stack+0x9c/0xb0\n[   12.319976]  kasan_record_aux_stack_noalloc+0x10/0x18\n[   12.321268]  call_rcu+0x50/0x35c\n[   12.322082]  __fput+0x2fc/0x324\n[   12.322873]  ____fput+0xc/0x14\n[   12.323644]  task_work_run+0xac/0x10c\n[   12.324561]  do_notify_resume+0x37c/0xe74\n[   12.325420]  el0_svc+0x5c/0x68\n[   12.326050]  el0t_64_sync_handler+0xb0/0x12c\n[   12.326918]  el0t_64_sync+0x164/0x168\n[   12.327657]\n[   12.327976] Second to last potentially related work creation:\n[   12.329134]  kasan_save_stack+0x28/0x4c\n[   12.329864]  __kasan_record_aux_stack+0x9c/0xb0\n[   12.330735]  kasan_record_aux_stack+0x10/0x18\n[   12.331576]  task_work_add+0x34/0xf0\n[   12.332284]  fput_many+0x11c/0x134\n[   12.332960]  fput+0x10/0x94\n[   12.333524]  __scm_destroy+0x80/0x84\n[   12.334213]  unix_destruct_scm+0xc4/0x144\n[   12.334948]  skb_release_head_state+0x5c/0x6c\n[   12.335696]  skb_release_all+0x14/0x38\n[   12.336339]  __kfree_skb+0x14/0x28\n[   12.336928]  kfree_skb_reason+0xf4/0x108\n[   12.337604]  unix_gc+0x1e8/0x42c\n[   12.338154]  unix_release_sock+0x25c/0x2dc\n[   12.338895]  unix_release+0x58/0x78\n[   12.339531]  __sock_release+0x68/0xec\n[   12.340170]  sock_close+0x14/0x20\n[   12.340729]  __fput+0x18c/0x324\n[   12.341254]  ____fput+0xc/0x14\n[   12.341763]  task_work_run+0xac/0x10c\n[   12.342367]  do_notify_resume+0x37c/0xe74\n[   12.343086]  el0_svc+0x5c/0x68\n[   12.343510]  el0t_64_sync_handler+0xb0/0x12c\n[   12.344086]  el0t_64_sync+0x164/0x168\n\nWe have an extra bit we can use in file_ptr on 64-bit, use that to store\nwhether this file is SCM'ed or not, avoiding the need to look at the\nfile contents itself. This does mean that 32-bit will be stuck with SCM\nfor all registered files, just like 64-bit did before the referenced\ncommit.\n\nFixes: 1f59bc0f18cf (\"io_uring: don't scm-account for non af_unix sockets\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:18:06 -0600 io_uring: store SCM state in io_fixed_file->file_ptr"
    },
    {
        "commit": "7ac1edc4a9bbbfc43c4a1fa7c2884029df3363bf",
        "message": "The ctx argument of io_req_put_rsrc() is not used, kill it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bb51bf3ff02775b03e6ea21bc79c25d7870d1644.1650311386.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:52 -0600 io_uring: kill ctx arg from io_req_put_rsrc"
    },
    {
        "commit": "25a15d3c668bf9cd23780fb0191b7f7f5054d879",
        "message": "Add a simple helper to encapsulating dropping rsrc nodes references,\nit's cleaner and will help if we'd change rsrc refcounting or play with\npercpu_ref_put() [no]inlining.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/63fdd953ac75898734cd50e8f69e95e6664f46fe.1650311386.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:52 -0600 io_uring: add a helper for putting rsrc nodes"
    },
    {
        "commit": "c1bdf8ed1e84ccbcf5e35fac1b311094572e5047",
        "message": "req->fixed_rsrc_refs keeps a pointer to rsrc node pcpu references, but\nit's more natural just to store rsrc node directly. There were some\nreasons for that in the past but not anymore.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/cee1c86ec9023f3e4f6ce8940d58c017ef8782f4.1650311386.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:52 -0600 io_uring: store rsrc node in req instead of refs"
    },
    {
        "commit": "772f5e002b9ef65c22956990be9ecb2df58cd9c9",
        "message": "All io_assign_file() callers do error handling themselves,\nreq_set_fail() in the io_assign_file()'s fail path needlessly bloats the\nkernel and is not the best abstraction to have. Simplify the error path.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/eff77fb1eac2b6a90cca5223813e6a396ffedec0.1650311386.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:52 -0600 io_uring: refactor io_assign_file error path"
    },
    {
        "commit": "93f052cb39e1f8c7a002e2ecbc7f45f354dcf0c7",
        "message": "We have io_ring_submit_[un]lock() functions helping us with conditional\n->uring_lock locking, use them in io_file_get_fixed() instead of hand\ncoding.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c9c9ff1e046f6eb68da0a251962a697f8a2275fa.1650311386.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:52 -0600 io_uring: use right helpers for file assign locking"
    },
    {
        "commit": "a6d97a8a77cb66cbbf6a5afc6708b1c2f5b00ca2",
        "message": "We have several racy reads, mark them with data_race() to demonstrate\nthis fact.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7e56e750d294c70b2a56938bd733386f19f0eb53.1650056133.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:52 -0600 io_uring: add data_race annotations"
    },
    {
        "commit": "17b147f6c1f21caef554004e19207810cbc08a8e",
        "message": "Inline io_req_complete_fail_submit(), there is only one caller and the\nname doesn't tell us much.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/fe5851af01dcd39fc84b71b8539c7cbe4658fb6d.1650056133.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:52 -0600 io_uring: inline io_req_complete_fail_submit()"
    },
    {
        "commit": "924a07e482baae0ced5fb9922b827c46f0c0972d",
        "message": "Remove one extra if for non-linked path of io_submit_sqe().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/03183199d1bf494b4a72eca16d792c8a5945acb4.1650056133.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:52 -0600 io_uring: refactor io_submit_sqe()"
    },
    {
        "commit": "df3becde8d9dcaaf17a615009301479f8f9c6323",
        "message": "Remove the lazy link fail logic from io_submit_sqe() and hide it into a\nhelper. It simplifies the code and will be needed in next patches.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6a68aca9cf4492132da1d7c8a09068b74aba3c65.1650056133.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:49 -0600 io_uring: refactor lazy link fail"
    },
    {
        "commit": "da1a08c5b28176398773de30a056ba7b994e6988",
        "message": "Add a macro for all link request flags to avoid duplication.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/df38b883e31e7e0ca4e364d25a0743862961b180.1650056133.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:49 -0600 io_uring: introduce IO_REQ_LINK_FLAGS"
    },
    {
        "commit": "7bfa9badc793ba5e8b530dc3a3cb092a1c22685b",
        "message": "io_queue_sqe() is a part of the submission path and we try hard to keep\nit inlined, so shed some extra bytes from it by moving the error\nchecking part into io_queue_sqe_arm_apoll() and renaming it accordingly.\n\nnote: io_queue_sqe_arm_apoll() is not inlined, thus the patch doesn't\nchange the number of function calls for the apoll path.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9b79edd246336decfaca79b949a15ac69123490d.1650056133.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:49 -0600 io_uring: refactor io_queue_sqe()"
    },
    {
        "commit": "77955efbc46220adce9ecb803fb9ff9042bd7236",
        "message": "Rename io_queue_async_work(). The name is pretty old but now doesn't\nreflect well what the function is doing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5d4b25c54cccf084f9f2fd63bd4e4fa4515e998e.1650056133.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:49 -0600 io_uring: rename io_queue_async_work()"
    },
    {
        "commit": "cbc2e2038845ddc6d1eb5969eff4f02bd5187b47",
        "message": "Inline io_queue_sqe() as there is only one caller left, and rename\n__io_queue_sqe().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d5742683b7a7caceb1c054e91e5b9135b0f3b858.1650056133.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:49 -0600 io_uring: inline io_queue_sqe()"
    },
    {
        "commit": "cb2d344c7551c250c07c009639eb81d54235ef8b",
        "message": "We try to aggresively inline the submission path, so it's a good idea to\nnot pollute it with colder code. One of them is linked timeout\npreparation + queue, which can be extracted into a function.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ecf74df7ac77389b6d9211211ec4954e91de98ba.1650056133.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:49 -0600 io_uring: helper for prep+queuing linked timeouts"
    },
    {
        "commit": "f5c6cf2a310d8de3bd02aa8a217f8ca63df6f236",
        "message": "Inline io_free_req() into its only user and remove an underscore prefix\nfrom __io_free_req().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ed114edef5c256a644f4839bb372df70d8df8e3f.1650056133.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:49 -0600 io_uring: inline io_free_req()"
    },
    {
        "commit": "4e118cd9e9e609c805822ee423aa23ae406e7da0",
        "message": "We have several spots where a call to io_fill_cqe_req() is immediately\nfollowed by io_put_req_deferred(). Replace them with\n__io_req_complete_post() and get rid of io_put_req_deferred() and\nio_fill_cqe_req().\n\n> size ./fs/io_uring.o\n   text    data     bss     dec     hex filename\n  86942   13734       8  100684   1894c ./fs/io_uring.o\n> size ./fs/io_uring.o\n   text    data     bss     dec     hex filename\n  86438   13654       8  100100   18704 ./fs/io_uring.o\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/10672a538774ac8986bee6468d960527af59169d.1650056133.git.asml.silence@gmail.com\n[axboe: fold in followup fix]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:49 -0600 io_uring: kill io_put_req_deferred()"
    },
    {
        "commit": "971cf9c19e97bad172bdb7142a23e6489d1b38f0",
        "message": "Get rid of some useless local variables\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7798327b684b7015f7e4300420142ddfcd317297.1650056133.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:49 -0600 io_uring: minor refactoring for some tw handlers"
    },
    {
        "commit": "f22190570b213dcc84216ac07cfd0eeada010013",
        "message": "When we meet PF_EXITING in io_poll_check_events(), don't overcomplicate\nthe code with io_poll_mark_cancelled() but just return -ECANCELED and\nthe callers will deal with the rest.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f0cc981af82a5b193658f8f44397eeb3bf838b7b.1650056133.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:49 -0600 io_uring: clean poll tw PF_EXITING handling"
    },
    {
        "commit": "d8da428b7a9a71bc2ee6bf628a8c0f9beb96a195",
        "message": "io_get_cqe() is expensive because of a bunch of loads, masking, etc.\nHowever, most of the time we should have enough of entries in the CQ,\nso we can cache two pointers representing a range of contiguous CQE\nmemory we can use. When the range is exhausted we'll go through a slower\npath to set up a new range. When there are no CQEs avaliable, pointers\nwill naturally point to the same address.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/487eeef00f3146537b3d9c1a9cef2fc0b9a86f81.1649771823.git.asml.silence@gmail.com\n[axboe: santinel -> sentinel]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:46 -0600 io_uring: optimise io_get_cqe()"
    },
    {
        "commit": "1cd15904b6e836f46e925229da8950785252b315",
        "message": "Considering all inlining io_submit_sqe() is huge and usually ends up\ncalling some other functions.\n\nWe decrement @left in io_submit_sqes() just before calling\nio_submit_sqe() and use it later after the call. Considering how huge\nio_submit_sqe() is, there is not much hope @left will be treated\ngracefully by compilers.\n\nDecrement it after the call, not only it's easier on register spilling\nand probably saves stack write/read, but also at least for x64 uses\nCPU flags set by the dec instead of doing (read/write and tests).\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/807f9a276b54ee8ff4e42e2b78721484f1c71743.1649771823.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:46 -0600 io_uring: optimise submission left counting"
    },
    {
        "commit": "8e6971a819dfd289e40642405a9200076f5dd17f",
        "message": "Instead of keeping @submitted in io_submit_sqes(), which for each\niteration requires comparison with the initial number of SQEs, store the\nnumber of SQEs left to submit. We'll need nr only for when we're done\nwith SQE handling.\n\nnote: if we can't allocate a req for the first SQE we always has been\nreturning -EAGAIN to the userspace, save this behaviour by looking into\nthe cache in a slow path.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c3b3df9aeae4c2f7a53fd8386385742e4e261e77.1649771823.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:46 -0600 io_uring: optimise submission loop invariant"
    },
    {
        "commit": "fa05457a603e7c8e97cd4cfb62de64bc9547ecb5",
        "message": "Don't hand code wq_stack_add_head() to ->free_list, which serves for\nrecycling io_kiocb, add a helper doing it for us.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f206f575486a8dd3d52f074ab37ed146b2d215b7.1649771823.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:46 -0600 io_uring: add helper to return req to cache list"
    },
    {
        "commit": "88ab95be7e408ff9cedc81ad5402ad4bd700baf9",
        "message": "Add io_req_cache_empty(), which checks if there are requests in the\ninline req cache or not. It'll be needed in the future, but also nicely\ncleans up a few spots poking into ->free_list directly.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b18662389f3fb483d0bd07906647f65f6037475a.1649771823.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:46 -0600 io_uring: helper for empty req cache checks"
    },
    {
        "commit": "23a5c43b2fc0cc210dcb36264fe3050468c79b17",
        "message": "io_flush_cached_reqs() isn't descriptive and has only one caller, inline\nit into __io_alloc_req_refill().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ec38abe65a883d9fe6b169793119ce86806655a4.1649771823.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:46 -0600 io_uring: inline io_flush_cached_reqs"
    },
    {
        "commit": "e126391c092014af7ea6786648c9f1d3fce512a3",
        "message": "All good users should not set IOSQE_IO_*LINK flags for the last request\nof a link. io_uring flushes collected links at the end of submission,\nbut it's not the optimal way and so we don't care too much about it.\nReplace io_queue_sqe() call with io_queue_sqe_fallback() as the former\none is inlined and will generate a bunch of extra code. This will also\nhelp compilers with the submission path inlining.\n\n> size ./fs/io_uring.o\n   text    data     bss     dec     hex filename\n  87265   13734       8  101007   18a8f ./fs/io_uring.o\n> size ./fs/io_uring.o\n   text    data     bss     dec     hex filename\n  87073   13734       8  100815   189cf ./fs/io_uring.o\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/01fb5e417ef49925d544a0b0bae30409845ed2b4.1649771823.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:46 -0600 io_uring: shrink final link flush"
    },
    {
        "commit": "90e7c35fb89154439a4a604699faf36adbfa871b",
        "message": "We can do CQE filling a bit more efficiently when req->cqe is fully\nfilled by memcpy()'ing it to the userspace instead of doing it field by\nfield. It's easier on register spilling, removes a couple of extra\nloads/stores and write combines two u32 memory writes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ee3f514ff28b1fe3347a8eca93a9d91647f2eaad.1649771823.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 18:02:46 -0600 io_uring: memcpy CQE from req"
    },
    {
        "commit": "cef216fc32d7628206c523994e7e267e7a8dda59",
        "message": "We already have req->{result,user_data,cflags}, which mimic struct\nio_uring_cqe and are intended to store CQE data. Combine them into a\nstruct io_uring_cqe field.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e1efe65d5005cd6a9ec3440767eb15a9fa9351cf.1649771823.git.asml.silence@gmail.com\n[axboe: add mirror cqe to cater to fd union]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:37:06 -0600 io_uring: explicitly keep a CQE in io_kiocb"
    },
    {
        "commit": "8b3171bdf53c51c8edd609c3087932a24c42087f",
        "message": "Rename io_sqe_file_register(), so the name better reflects what the\nfunction is doing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d5091518883786969e244d2f0854a47bbdaa5061.1649334991.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:32 -0600 io_uring: rename io_sqe_file_register"
    },
    {
        "commit": "73b25d3badbf3642d15fe267ffc5b1a5c0bc07b3",
        "message": "Merge io_sqe_file_register() and io_sqe_file_register(). The only\nreal difference left between them is from where we get an skb.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/dddda3039c71fcbec24b3465cbe8c7e7ae7bb0e8.1649334991.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:19 -0600 io_uring: deduplicate SCM accounting"
    },
    {
        "commit": "e390510af0461e743a8f0031fc235552c40778f9",
        "message": "There is an old API nuisance where io_uring's SCM accounting functions\ntraverse fixed file tables and so requires them to be set in advance,\nwhich leads to some implicit rules of how io_sqe_file_register() should\nbe used.\n\n__io_sqe_files_scm() now works with only one file at a time, pass a file\ndirectly and get rid of all fixed table dereferencing inside. Clean\nio_sqe_file_register() callers.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/fb32031d892e61a7748c70da7999725d5e798671.1649334991.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:19 -0600 io_uring: don't pass around fixed index for scm"
    },
    {
        "commit": "dca58c6a08a92c5c674140047de7f72f9cc843d0",
        "message": "__io_sqe_files_scm() is now called only from one place passing a single\nfile, so nr argument can be killed and __io_sqe_files_scm() simplified.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/66b492bc66dc8356d45d64076bb31d677d11a7c9.1649334991.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:19 -0600 io_uring: refactor __io_sqe_files_scm"
    },
    {
        "commit": "a03a2a209e826f646dff49202b8b1eb8427c2d08",
        "message": "Channel all SCM accounting through io_sqe_file_register(), so we do it\nuniformely for updates and initial registration and can kill duplicated\ncode. Registration might be slightly slower in some case, but first we\nskip most of SCM accounting now so it's not a problem. Moreover, it's\nnicer for an empty set registration as we don't even try to allocate\nskb for them anymore.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6c9afbeb22812777d0c43e52353b63db5b87ed1e.1649334991.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:19 -0600 io_uring: uniform SCM accounting"
    },
    {
        "commit": "1f59bc0f18cf46abe27ea18cfa4cb7f1b4166896",
        "message": "io_uring deals with file reference loops by registering all fixed files\nin the SCM/GC infrastrucure. However, only a small subset of all file\ntypes can keep long-term references to other files and those that don't\nare not interesting for the garbage collector as they can't be in a\nreference loop. They neither can be directly recycled by GC nor affect\nloop searching.\n\nLet's skip io_uring SCM accounting for loop-less files, i.e. all but\naf_unix sockets, quite imroving fixed file updates performance and\ngreatly helpnig with memory footprint.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9c44ecf6e89d69130a8c4360cce2183ffc5ddd6f.1649277098.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:19 -0600 io_uring: don't scm-account for non af_unix sockets"
    },
    {
        "commit": "b4f20bb4e6d55a971d5f5555a971978a6263d8de",
        "message": "We don't need to call this for every loop. This is particularly\ntroublesome if we are task_work intensive, and get woken more often than\nwe desire due to that.\n\nJust do it at the end, that's always safe as we initialize the waitqueue\nlist head anyway. This can save a considerable amount of hammering on\nthe waitqueue lock, which is also hot from the request completion side.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:19 -0600 io_uring: move finish_wait() outside of loop in cqring_wait()"
    },
    {
        "commit": "775a1f2f99483bda4e43376ebbc6b459c8fbf20a",
        "message": "A small refactoring for io_req_add_compl_list() deduplicating some code.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f0a5272b45efe4ffc41cb79b99784e39c699aade.1648209006.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:19 -0600 io_uring: refactor io_req_add_compl_list()"
    },
    {
        "commit": "963c6abbb4e4ee3d00ca7992f751c49b38b07e68",
        "message": "Some tooling keep complaining about self assignment in\nio_for_each_link(), the code is correct but still let's workaround it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f0de77b0b0f8309554ba6fba34327b7813bcc3ff.1648209006.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:19 -0600 io_uring: silence io_for_each_link() warning"
    },
    {
        "commit": "9d170164dbac3f674b5ec2189e2a279eb0b7625f",
        "message": "In most cases io_put_task() is called from the submitter task and go\nthrough a higly optimised fast path, which has to be inlined. The other\nbranch though is bulkier and we don't care about it as much because it\nimplies atomics and other heavy calls. Extract it into a helper, which\nis expected not to be inlined.\n\n[before] size ./fs/io_uring.o\n   text    data     bss     dec     hex filename\n  89328   13646       8  102982   19246 ./fs/io_uring.o\n[after] size ./fs/io_uring.o\n   text    data     bss     dec     hex filename\n  89096   13646       8  102750   1915e ./fs/io_uring.o\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/dec213db0e0b8605132da81e0a0be687a4d140cb.1648209006.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:19 -0600 io_uring: partially uninline io_put_task()"
    },
    {
        "commit": "f8929630514505ef8c6cf70d8b7a2bbf7e43b225",
        "message": "Refactor io_ring_submit_[un]lock(), make it accept issue_flags and\nremove manual IO_URING_F_UNLOCKED checks. It also allows us to place\nlockdep annotations inside instead of sprinkling them in a bunch of\nplaces. There is only one user that doesn't fit now, so hand code\nlocking in __io_rsrc_put_work().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e55c2c06767676a801252e8094c9ab09912487a4.1648209006.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:19 -0600 io_uring: cleanup conditional submit locking"
    },
    {
        "commit": "d487b43cd3276e40723641510cbd2d33da4f6800",
        "message": "Both submittion and iopolling requires holding uring_lock. IOPOLL can\nusers do them together in a single syscall, however it would still do 2\npairs of lock/unlock. Optimise this case combining locking into one\nlock/unlock pair, which especially nice for low QD.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/034b6c41658648ad3ad3c9485ac8eb546f010bc4.1647957378.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:16 -0600 io_uring: optimise mutex locking for submit+iopoll"
    },
    {
        "commit": "773697b610bff9451cf67d4f57b08d68c4b1a832",
        "message": "Syscall should only iopoll for events when it's a IOPOLL ring and is not\nSQPOLL. Instead of check both flags every time we can save it in ring\nflags so it's easier to use. We don't care much about an extra if there,\nhowever it will be inconvenient to copy-paste this chunk with checks in\nfuture patches.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7fd2f8fc2606305aa06dd8c0ff8f76a66b39c383.1647957378.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:16 -0600 io_uring: pre-calculate syscall iopolling decision"
    },
    {
        "commit": "f81440d33cc61bd80f37e1045f0f5a21d043eed3",
        "message": "IOPOLL doesn't use additional arguments like sigsets, but it still\nneeds some basic verification, which is currently done by\nio_get_ext_arg(). This patch adds a separate function for the IOPOLL\npath, which is a bit simpler and doesn't do extra. This prepares us for\nfurther patches, which would have hurt inlining in the hot path otherwise.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/71b23fca412e3374b74be7711cfd42a3d9d5dfe0.1647957378.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:16 -0600 io_uring: split off IOPOLL argument verifiction"
    },
    {
        "commit": "57859f4d93db3eecd5dd680d3b3520eea80c996b",
        "message": "Move fast check out of io_queue_next(), it makes req->flags checks in\n__io_submit_flush_completions() a bit clearer and grants us better\ncomtrol, e.g. can remove now not justified unlikely() in\n__io_submit_flush_completions(). Also, we don't care about having this\ncheck in io_free_req() as the function is a slow path and\nio_req_find_next() handles it correctly.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1f9e1cc80adbb11b37017d511df4a2c6141a3f08.1647897811.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:16 -0600 io_uring: clean up io_queue_next()"
    },
    {
        "commit": "b605a7fabb607adbe4ea7cc97b69ae6e0555e7b2",
        "message": "There is a new (req->flags & REQ_F_POLLED) check in\n__io_submit_flush_completions() for poll recycling, however\nio_free_batch_list() is a much better place for it. First, we prefer it\nafter putting the last req ref just to avoid potential problems in the\nfuture. Also, it'll enable the recycling for IOPOLL and also will place\nit closer to all other req->flags bits clean up requests.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/31dfe1dafda66ba3ce36b301884ec7e162c777d1.1647897811.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:16 -0600 io_uring: move poll recycling later in compl flushing"
    },
    {
        "commit": "a538be5be328229d4da3343d4d6514bb4d5c3d5d",
        "message": "We do several req->flags checks in the fast path of\nio_free_batch_list(). One explicit check of REQ_F_REFCOUNT, and two\nother hidden in io_queue_next() and io_dismantle_req(). Moreover, there\nis a io_req_put_rsrc_locked() call in between, so there is no hope\nreq->flags will be preserved in registers.\n\nAll those flags if not a slow path than definitely a slower path, so\nput them all under a single flags mask check and save several mem\nreloads and ifs.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0fb493f73f2009aea395c570c2932fecaa4e1244.1647897811.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:16 -0600 io_uring: optimise io_free_batch_list"
    },
    {
        "commit": "7819a1f6ac0393f1f7861dfece6ffd5ef010f0f9",
        "message": "Move the fast path from io_req_find_next() into callers. It prepares us\nfor further changes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/10bd0e564472dde0c7f8d90ae317d05356cd565a.1647897811.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:16 -0600 io_uring: refactor io_req_find_next"
    },
    {
        "commit": "60053be859b33f7a381a3f1755db5caffaa3cab8",
        "message": "Now io_commit_cqring() is simple and it tolerates well being called\nwithout a new CQE filled, so kill a bunch of not needed anymore\nguards.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/36aed692dff402bba00a444a63a9cd2e97a340ea.1647897811.git.asml.silence@gmail.com\n[axboe: fold in followup fix]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:16 -0600 io_uring: remove extra ifs around io_commit_cqring"
    },
    {
        "commit": "68ca8fc00277ad04c975c382bd6e2d500e5c7185",
        "message": "There should be no completions stashed when we first get into\ntctx_task_work(), so move completion flushing checks a bit later\nafter we had a chance to execute some task works.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c6765c804f3c438591b9825ab9c43d22039073c4.1647897811.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.19-rc1",
        "release_date": "2022-04-24 17:34:16 -0600 io_uring: small optimisation of tctx_task_work"
    },
    {
        "commit": "1f5e98e723a0be814181524a7e6aaf87a805cdc9",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Just two small fixes - one fixing a potential leak for the iovec for\n  larger requests added in this cycle, and one fixing a theoretical leak\n  with CQE_SKIP and IOPOLL\"\n\n* tag 'io_uring-5.18-2022-04-22' of git://git.kernel.dk/linux-block:\n  io_uring: fix leaks on IOPOLL and CQE_SKIP\n  io_uring: free iovec if file assignment fails",
        "kernel_version": "v5.18-rc4",
        "release_date": "2022-04-23 09:42:13 -0700 Merge tag 'io_uring-5.18-2022-04-22' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "c0713540f6d55c53dca65baaead55a5a8b20552d",
        "message": "If all completed requests in io_do_iopoll() were marked with\nREQ_F_CQE_SKIP, we'll not only skip CQE posting but also\nio_free_batch_list() leaking memory and resources.\n\nMove @nr_events increment before REQ_F_CQE_SKIP check. We'll potentially\nreturn the value greater than the real one, but iopolling will deal with\nit and the userspace will re-iopoll if needed. In anyway, I don't think\nthere are many use cases for REQ_F_CQE_SKIP + IOPOLL.\n\nFixes: 83a13a4181b0e (\"io_uring: tweak iopoll CQE_SKIP event counting\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5072fc8693fbfd595f89e5d4305bfcfd5d2f0a64.1650186611.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc4",
        "release_date": "2022-04-17 06:54:11 -0600 io_uring: fix leaks on IOPOLL and CQE_SKIP"
    },
    {
        "commit": "323b190ba2debbcc03c01d2edaf1ec6b43e6ae43",
        "message": "We just return failure in this case, but we need to release the iovec\nfirst. If we're doing IO with more than FAST_IOV segments, then the\niovec is allocated and must be freed.\n\nReported-by: syzbot+96b43810dfe9c3bb95ed@syzkaller.appspotmail.com\nFixes: 584b0180f0f4 (\"io_uring: move read/write file prep state into actual opcode handler\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc4",
        "release_date": "2022-04-16 21:14:00 -0600 io_uring: free iovec if file assignment fails"
    },
    {
        "commit": "0647b9cc7feac30eb6c397ccb746aaa91e21e0de",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Ensure we check and -EINVAL any use of reserved or struct padding.\n\n   Although we generally always do that, it's missed in two spots for\n   resource updates, one for the ring fd registration from this merge\n   window, and one for the extended arg. Make sure we have all of them\n   handled. (Dylan)\n\n - A few fixes for the deferred file assignment (me, Pavel)\n\n - Add a feature flag for the deferred file assignment so apps can tell\n   we handle it correctly (me)\n\n - Fix a small perf regression with the current file position fix in\n   this merge window (me)\n\n* tag 'io_uring-5.18-2022-04-14' of git://git.kernel.dk/linux-block:\n  io_uring: abort file assignment prior to assigning creds\n  io_uring: fix poll error reporting\n  io_uring: fix poll file assign deadlock\n  io_uring: use right issue_flags for splice/tee\n  io_uring: verify pad field is 0 in io_get_ext_arg\n  io_uring: verify resv is 0 in ringfd register/unregister\n  io_uring: verify that resv2 is 0 in io_uring_rsrc_update2\n  io_uring: move io_uring_rsrc_update2 validation\n  io_uring: fix assign file locking issue\n  io_uring: stop using io_wq_work as an fd placeholder\n  io_uring: move apoll->events cache\n  io_uring: io_kiocb_update_pos() should not touch file for non -1 offset\n  io_uring: flag the fact that linked file assignment is sane",
        "kernel_version": "v5.18-rc3",
        "release_date": "2022-04-15 11:33:20 -0700 Merge tag 'io_uring-5.18-2022-04-14' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "701521403cfb228536b3947035c8a6eca40d8e58",
        "message": "We need to either restore creds properly if we fail on the file\nassignment, or just do the file assignment first instead. Let's do\nthe latter as it's simpler, should make no difference here for\nfile assignment.\n\nLink: https://lore.kernel.org/lkml/000000000000a7edb305dca75a50@google.com/\nReported-by: syzbot+60c52ca98513a8760a91@syzkaller.appspotmail.com\nFixes: 6bf9c47a3989 (\"io_uring: defer file assignment\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc3",
        "release_date": "2022-04-14 20:23:40 -0600 io_uring: abort file assignment prior to assigning creds"
    },
    {
        "commit": "7179c3ce3dbff646c55f7cd664a895f462f049e5",
        "message": "We should not return an error code in req->result in\nio_poll_check_events(), because it may get mangled and returned as\nsuccess. Just return the error code directly, the callers will fail the\nrequest or proceed accordingly.\n\nFixes: 6bf9c47a3989 (\"io_uring: defer file assignment\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5f03514ee33324dc811fb93df84aee0f695fb044.1649862516.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc3",
        "release_date": "2022-04-13 10:25:37 -0600 io_uring: fix poll error reporting"
    },
    {
        "commit": "cce64ef01308b677a687d90927fc2b2e0e1cba67",
        "message": "We pass \"unlocked\" into io_assign_file() in io_poll_check_events(),\nwhich can lead to double locking.\n\nFixes: 6bf9c47a3989 (\"io_uring: defer file assignment\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2476d4ae46554324b599ee4055447b105f20a75a.1649862516.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc3",
        "release_date": "2022-04-13 10:25:37 -0600 io_uring: fix poll file assign deadlock"
    },
    {
        "commit": "e941976659f1f6834077a1596bf53e6bdb10e90b",
        "message": "Pass right issue_flags into into io_file_get_fixed() instead of\nIO_URING_F_UNLOCKED. It's probably not a problem at the moment but let's\ndo it safer.\n\nFixes: 6bf9c47a3989 (\"io_uring: defer file assignment\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7d242daa9df5d776907686977cd29fbceb4a2d8d.1649862516.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc3",
        "release_date": "2022-04-13 10:25:36 -0600 io_uring: use right issue_flags for splice/tee"
    },
    {
        "commit": "d2347b9695dafe5c388a5f9aeb70e27a7a4d29cf",
        "message": "Ensure that only 0 is passed for pad here.\n\nFixes: c73ebb685fb6 (\"io_uring: add timeout support for io_uring_enter()\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220412163042.2788062-5-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc3",
        "release_date": "2022-04-12 10:46:54 -0600 io_uring: verify pad field is 0 in io_get_ext_arg"
    },
    {
        "commit": "6fb53cf8ff2c4713247df523404d24f466b98f52",
        "message": "Only allow resv field to be 0 in struct io_uring_rsrc_update user\narguments.\n\nFixes: e7a6c00dc77a (\"io_uring: add support for registering ring file descriptors\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220412163042.2788062-4-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc3",
        "release_date": "2022-04-12 10:46:54 -0600 io_uring: verify resv is 0 in ringfd register/unregister"
    },
    {
        "commit": "d8a3ba9c143bf89c032deced8a686ffa53b46098",
        "message": "Verify that the user does not pass in anything but 0 for this field.\n\nFixes: 992da01aa932 (\"io_uring: change registration/upd/rsrc tagging ABI\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220412163042.2788062-3-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc3",
        "release_date": "2022-04-12 10:46:54 -0600 io_uring: verify that resv2 is 0 in io_uring_rsrc_update2"
    },
    {
        "commit": "565c5e616e8061b40a2e1d786c418a7ac3503a8d",
        "message": "Move validation to be more consistently straight after\ncopy_from_user. This is already done in io_register_rsrc_update and so\nthis removes that redundant check.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220412163042.2788062-2-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc3",
        "release_date": "2022-04-12 10:46:54 -0600 io_uring: move io_uring_rsrc_update2 validation"
    },
    {
        "commit": "0f8da75b51ac863b9435368bd50691718cc454b0",
        "message": "io-wq work cancellation path can't take uring_lock as how it's done on\nfile assignment, we have to handle IO_WQ_WORK_CANCEL first, this fixes\nencountered hangs.\n\nFixes: 6bf9c47a3989 (\"io_uring: defer file assignment\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0d9b9f37841645518503f6a207e509d14a286aba.1649773463.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc3",
        "release_date": "2022-04-12 08:39:49 -0600 io_uring: fix assign file locking issue"
    },
    {
        "commit": "82733d168cbd3fe9dab603f05894316b99008924",
        "message": "There are two reasons why this isn't the best idea:\n\n- It's an odd area to grab a bit of storage space, hence it's an odd area\n  to grab storage from.\n- It puts the 3rd io_kiocb cacheline into the hot path, where normal hot\n  path just needs the first two.\n\nUse 'cflags' for joint fd/cflags storage. We only need fd until we\nsuccessfully issue, and we only need cflags once a request is done and is\ncompleted.\n\nFixes: 6bf9c47a3989 (\"io_uring: defer file assignment\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc3",
        "release_date": "2022-04-11 17:06:20 -0600 io_uring: stop using io_wq_work as an fd placeholder"
    },
    {
        "commit": "2804ecd8d3e3730b4f999cc1ff4b2441e1f4d513",
        "message": "In preparation for fixing a regression with pulling in an extra cacheline\nfor IO that doesn't usually touch the last cacheline of the io_kiocb,\nmove the cached location of apoll->events to space shared with some other\ncompletion data. Like cflags, this isn't used until after the request\nhas been completed, so we can piggy back on top of comp_list.\n\nFixes: 81459350d581 (\"io_uring: cache req->apoll->events in req->cflags\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc3",
        "release_date": "2022-04-11 17:06:13 -0600 io_uring: move apoll->events cache"
    },
    {
        "commit": "6f83ab22adcb77a5824d2c274dace0d99e21319f",
        "message": "-1 tells use to use the current position, but we check if the file is\na stream regardless of that. Fix up io_kiocb_update_pos() to only\ndip into file if we need to. This is both more efficient and also drops\n12 bytes of text on aarch64 and 64 bytes on x86-64.\n\nFixes: b4aec4001595 (\"io_uring: do not recalculate ppos unnecessarily\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc3",
        "release_date": "2022-04-11 16:38:21 -0600 io_uring: io_kiocb_update_pos() should not touch file for non -1 offset"
    },
    {
        "commit": "c4212f3eb89fd5654f0a6ed2ee1d13fcb86cb664",
        "message": "Give applications a way to tell if the kernel supports sane linked files,\nas in files being assigned at the right time to be able to reliably\ndo <open file direct into slot X><read file from slot X> while using\nIOSQE_IO_LINK to order them.\n\nNot really a bug fix, but flag it as such so that it gets pulled in with\nbackports of the deferred file assignment.\n\nFixes: 6bf9c47a3989 (\"io_uring: defer file assignment\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc3",
        "release_date": "2022-04-10 19:08:18 -0600 io_uring: flag the fact that linked file assignment is sane"
    },
    {
        "commit": "4d6f9f2475f6f288b8b144bc263636c0b09cb4ef",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A bit bigger than usual post merge window, largely due to a revert and\n  a fix of at what point files are assigned for requests.\n\n  The latter fixing a linked request use case where a dependent link can\n  rely on what file is assigned consistently.\n\n  Summary:\n\n   - 32-bit compat fix for IORING_REGISTER_IOWQ_AFF (Eugene)\n\n   - File assignment fixes (me)\n\n   - Revert of the NAPI poll addition from this merge window. The author\n     isn't available right now to engage on this, so let's revert it and\n     we can retry for the 5.19 release (me, Jakub)\n\n   - Fix a timeout removal race (me)\n\n   - File update and SCM fixes (Pavel)\"\n\n* tag 'io_uring-5.18-2022-04-08' of git://git.kernel.dk/linux-block:\n  io_uring: fix race between timeout flush and removal\n  io_uring: use nospec annotation for more indexes\n  io_uring: zero tag on rsrc removal\n  io_uring: don't touch scm_fp_list after queueing skb\n  io_uring: nospec index for tags on files update\n  io_uring: implement compat handling for IORING_REGISTER_IOWQ_AFF\n  Revert \"io_uring: Add support for napi_busy_poll\"\n  io_uring: drop the old style inflight file tracking\n  io_uring: defer file assignment\n  io_uring: propagate issue_flags state down to file assignment\n  io_uring: move read/write file prep state into actual opcode handler\n  io_uring: defer splice/tee file validity check until command issue\n  io_uring: don't check req->file in io_fsync_prep()",
        "kernel_version": "v5.18-rc2",
        "release_date": "2022-04-08 18:50:14 -1000 Merge tag 'io_uring-5.18-2022-04-08' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "e677edbcabee849bfdd43f1602bccbecf736a646",
        "message": "io_flush_timeouts() assumes the timeout isn't in progress of triggering\nor being removed/canceled, so it unconditionally removes it from the\ntimeout list and attempts to cancel it.\n\nLeave it on the list and let the normal timeout cancelation take care\nof it.\n\nCc: stable@vger.kernel.org # 5.5+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc2",
        "release_date": "2022-04-08 14:50:05 -0600 io_uring: fix race between timeout flush and removal"
    },
    {
        "commit": "4cdd158be9d09223737df83136a1fb65269d809a",
        "message": "There are still several places that using pre array_index_nospec()\nindexes, fix them up.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b01ef5ee83f72ed35ad525912370b729f5d145f4.1649336342.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc2",
        "release_date": "2022-04-07 11:17:47 -0600 io_uring: use nospec annotation for more indexes"
    },
    {
        "commit": "8f0a24801bb44aa58496945aabb904c729176772",
        "message": "Automatically default rsrc tag in io_queue_rsrc_removal(), it's safer\nthan leaving it there and relying on the rest of the code to behave and\nnot use it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1cf262a50df17478ea25b22494dcc19f3a80301f.1649336342.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc2",
        "release_date": "2022-04-07 11:17:47 -0600 io_uring: zero tag on rsrc removal"
    },
    {
        "commit": "a07211e3001435fe8591b992464cd8d5e3c98c5a",
        "message": "It's safer to not touch scm_fp_list after we queued an skb to which it\nwas assigned, there might be races lurking if we screw subtle sync\nguarantees on the io_uring side.\n\nFixes: 6b06314c47e14 (\"io_uring: add file set registration\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc2",
        "release_date": "2022-04-07 11:17:47 -0600 io_uring: don't touch scm_fp_list after queueing skb"
    },
    {
        "commit": "34bb77184123ae401100a4d156584f12fa630e5c",
        "message": "Don't forget to array_index_nospec() for indexes before updating rsrc\ntags in __io_sqe_files_update(), just use already safe and precalculated\nindex @i.\n\nFixes: c3bdad0271834 (\"io_uring: add generic rsrc update with tags\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc2",
        "release_date": "2022-04-07 11:17:47 -0600 io_uring: nospec index for tags on files update"
    },
    {
        "commit": "0f5e4b83b37a96e3643951588ed7176b9b187c0a",
        "message": "Similarly to the way it is done im mbind syscall.\n\nCc: stable@vger.kernel.org # 5.14\nFixes: fe76421d1da1dcdb (\"io_uring: allow user configurable IO thread CPU affinity\")\nSigned-off-by: Eugene Syromiatnikov <esyr@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc2",
        "release_date": "2022-04-07 11:17:47 -0600 io_uring: implement compat handling for IORING_REGISTER_IOWQ_AFF"
    },
    {
        "commit": "cb318216732579da80202fe3e622a504e55b3a0f",
        "message": "This reverts commit adc8682ec69012b68d5ab7123e246d2ad9a6f94b.\n\nThere's some discussion on the API not being as good as it can be.\nRather than ship something and be stuck with it forever, let's revert\nthe NAPI support for now and work on getting something sorted out\nfor the next kernel release instead.\n\nLink: https://lore.kernel.org/io-uring/b7bbc124-8502-0ee9-d4c8-7c41b4487264@kernel.dk/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc2",
        "release_date": "2022-04-07 11:17:47 -0600 Revert \"io_uring: Add support for napi_busy_poll\""
    },
    {
        "commit": "d5361233e9ab920e135819f73dd8466355f1fddd",
        "message": "io_uring tracks requests that are referencing an io_uring descriptor to\nbe able to cancel without worrying about loops in the references. Since\nwe now assign the file at execution time, the easier approach is to drop\na potentially problematic reference before we punt the request. This\neliminates the need to special case these types of files beyond just\nmarking them as such, and simplifies cancelation quite a bit.\n\nThis also fixes a recent issue where an async punted tee operation would\nwith the io_uring descriptor as the output file would crash when\nattempting to get a reference to the file from the io-wq worker. We\ncould have worked around that, but this is the much cleaner fix.\n\nFixes: 6bf9c47a3989 (\"io_uring: defer file assignment\")\nReported-by: syzbot+c4b9303500a21750b250@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc2",
        "release_date": "2022-04-07 11:17:37 -0600 io_uring: drop the old style inflight file tracking"
    },
    {
        "commit": "6bf9c47a398911e0ab920e362115153596c80432",
        "message": "If an application uses direct open or accept, it knows in advance what\ndirect descriptor value it will get as it picks it itself. This allows\ncombined requests such as:\n\nsqe = io_uring_get_sqe(ring);\nio_uring_prep_openat_direct(sqe, ..., file_slot);\nsqe->flags |= IOSQE_IO_LINK | IOSQE_CQE_SKIP_SUCCESS;\n\nsqe = io_uring_get_sqe(ring);\nio_uring_prep_read(sqe,file_slot, buf, buf_size, 0);\nsqe->flags |= IOSQE_FIXED_FILE;\n\nio_uring_submit(ring);\n\nwhere we prepare both a file open and read, and only get a completion\nevent for the read when both have completed successfully.\n\nCurrently links are fully prepared before the head is issued, but that\nfails if the dependent link needs a file assigned that isn't valid until\nthe head has completed.\n\nConversely, if the same chain is performed but the fixed file slot is\nalready valid, then we would be unexpectedly returning data from the\nold file slot rather than the newly opened one. Make sure we're\nconsistent here.\n\nAllow deferral of file setup, which makes this documented case work.\n\nCc: stable@vger.kernel.org # v5.15+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc2",
        "release_date": "2022-04-07 11:17:28 -0600 io_uring: defer file assignment"
    },
    {
        "commit": "5106dd6e74ab6c94daac1c357094f11e6934b36f",
        "message": "We'll need this in a future patch, when we could be assigning the file\nafter the prep stage. While at it, get rid of the io_file_get() helper,\nit just makes the code harder to read.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc2",
        "release_date": "2022-04-07 11:17:24 -0600 io_uring: propagate issue_flags state down to file assignment"
    },
    {
        "commit": "584b0180f0f4d67d7145950fe68c625f06c88b10",
        "message": "In preparation for not necessarily having a file assigned at prep time,\ndefer any initialization associated with the file to when the opcode\nhandler is run.\n\nCc: stable@vger.kernel.org # v5.15+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc2",
        "release_date": "2022-04-04 16:50:20 -0600 io_uring: move read/write file prep state into actual opcode handler"
    },
    {
        "commit": "a3e4bc23d5470b2beb7cc42a86b6a3e75b704c15",
        "message": "In preparation for not using the file at prep time, defer checking if this\nfile refers to a valid io_uring instance until issue time.\n\nThis also means we can get rid of the cleanup flag for splice and tee.\n\nCc: stable@vger.kernel.org # v5.15+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc2",
        "release_date": "2022-04-04 16:50:20 -0600 io_uring: defer splice/tee file validity check until command issue"
    },
    {
        "commit": "ec858afda857e361182ceafc3d2ba2b164b8e889",
        "message": "This is a leftover from the really old days where we weren't able to\ntrack and error early if we need a file and it wasn't assigned. Kill\nthe check.\n\nCc: stable@vger.kernel.org # v5.15+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc2",
        "release_date": "2022-04-03 17:07:54 -0600 io_uring: don't check req->file in io_fsync_prep()"
    },
    {
        "commit": "3b1509f275ce13865c28ce254c36dc7c915808eb",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A little bit all over the map, some regression fixes for this merge\n  window, and some general fixes that are stable bound. In detail:\n\n   - Fix an SQPOLL memory ordering issue (Almog)\n\n   - Accept fixes (Dylan)\n\n   - Poll fixes (me)\n\n   - Fixes for provided buffers and recycling (me)\n\n   - Tweak to IORING_OP_MSG_RING command added in this merge window (me)\n\n   - Memory leak fix (Pavel)\n\n   - Misc fixes and tweaks (Pavel, me)\"\n\n* tag 'for-5.18/io_uring-2022-04-01' of git://git.kernel.dk/linux-block:\n  io_uring: defer msg-ring file validity check until command issue\n  io_uring: fail links if msg-ring doesn't succeeed\n  io_uring: fix memory leak of uid in files registration\n  io_uring: fix put_kbuf without proper locking\n  io_uring: fix invalid flags for io_put_kbuf()\n  io_uring: improve req fields comments\n  io_uring: enable EPOLLEXCLUSIVE for accept poll\n  io_uring: improve task work cache utilization\n  io_uring: fix async accept on O_NONBLOCK sockets\n  io_uring: remove IORING_CQE_F_MSG\n  io_uring: add flag for disabling provided buffer recycling\n  io_uring: ensure recv and recvmsg handle MSG_WAITALL correctly\n  io_uring: don't recycle provided buffer if punted to async worker\n  io_uring: fix assuming triggered poll waitqueue is the single poll\n  io_uring: bump poll refs to full 31-bits\n  io_uring: remove poll entry from list when canceling all\n  io_uring: fix memory ordering when SQPOLL thread goes to sleep\n  io_uring: ensure that fsnotify is always called\n  io_uring: recycle provided before arming poll",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-04-01 16:10:51 -0700 Merge tag 'for-5.18/io_uring-2022-04-01' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "5291984004edfcc7510024e52eaed044573b79c7",
        "message": "Expanded testing of DM's bio polling support (using more fio threads\nto dm-linear ontop of null_blk) exposed the possibility for polled\nbios to hang (repeatedly polling in io_uring) when null_blk responds\nwith BLK_STS_AGAIN (due to lack of resources):\n\n1) io_complete_rw_iopoll() is called from blkdev_bio_end_io_async() to\n   notify kiocb is done, that is the completion interface between block\n   layer and io_uring\n\n2) io_complete_rw_iopoll() is called from io_do_iopoll()\n\n3) dm returns BLK_STS_AGAIN for one bio (on behalf of underlying\n   driver), then io_complete_rw_iopoll is called, but io_do_iopoll()\n   doesn't handle -EAGAIN at all (due to logic in io_rw_should_reissue)\n\n4) reason for dm's BLK_STS_AGAIN is underlying null_blk driver ran out\n   of requests (easier to reproduce by setting low hw_queue_depth).\n\n5) dm should handle BLK_STS_AGAIN for POLLED underlying IO, and may\n   retry in dm layer.\n\nThis fix adds REQ_POLLED specific BLK_STS_AGAIN handling to\ndm_io_complete() that clears REQ_POLLED and requeues the bio to DM\nusing queue_io().\n\nFixes: b99fdcdc3636 (\"dm: support bio polling\")\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\n[snitzer: revised header, reused dm_io_complete's REQ_POLLED case]\nSigned-off-by: Mike Snitzer <snitzer@kernel.org>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-04-01 13:23:12 -0400 dm: fix bio polling to handle possibile BLK_STS_AGAIN"
    },
    {
        "commit": "3f1d52abf098c85b177b8c6f5b310e8347d1bc42",
        "message": "In preparation for not using the file at prep time, defer checking if this\nfile refers to a valid io_uring instance until issue time.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-29 14:04:28 -0600 io_uring: defer msg-ring file validity check until command issue"
    },
    {
        "commit": "9666d4206e9a14ff612e374b6b572b3efc797d46",
        "message": "We must always call req_set_fail() if the request is failed, otherwise\nwe won't sever links for dependent chains correctly.\n\nFixes: 4f57f06ce218 (\"io_uring: add support for IORING_OP_MSG_RING command\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-29 10:51:08 -0600 io_uring: fail links if msg-ring doesn't succeeed"
    },
    {
        "commit": "c86d18f4aa93e0e66cda0e55827cd03eea6bc5f8",
        "message": "When there are no files for __io_sqe_files_scm() to process in the\nrange, it'll free everything and return. However, it forgets to put uid.\n\nFixes: 08a451739a9b5 (\"io_uring: allow sparse fixed file sets\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/accee442376f33ce8aaebb099d04967533efde92.1648226048.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-25 16:07:06 -0600 io_uring: fix memory leak of uid in files registration"
    },
    {
        "commit": "8197b053a83335dd1b7eb7581a933924e25c1025",
        "message": "io_put_kbuf_comp() should only be called while holding\n->completion_lock, however there is no such assumption in io_clean_op()\nand thus it can corrupt ->io_buffer_comp. Take the lock there, and\nworkaround the only user of io_clean_op() calling it with locks. Not\nthe prettiest solution, but it's easier to refactor it for-next.\n\nFixes: cc3cec8367cba (\"io_uring: speedup provided buffer handling\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/743e2130b73ec6d48c4c5dd15db896c433431e6d.1648212967.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-25 07:43:53 -0600 io_uring: fix put_kbuf without proper locking"
    },
    {
        "commit": "ab0ac0959b028779ea43002db81daa12203cb57d",
        "message": "io_req_complete_failed() doesn't require callers to hold ->uring_lock,\nuse IO_URING_F_UNLOCKED version of io_put_kbuf(). The only affected\nplace is the fail path of io_apoll_task_func(). Also add a lockdep\nannotation to catch such bugs in the future.\n\nFixes: 3b2b78a8eb7cc (\"io_uring: extend provided buf return to fails\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ccf602dbf8df3b6a8552a262d8ee0a13a086fbc7.1648212967.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-25 07:43:53 -0600 io_uring: fix invalid flags for io_put_kbuf()"
    },
    {
        "commit": "41cdcc2202d4c466534b8f38975d2e6b16317c0c",
        "message": "Move a misplaced comment about req->creds and add a line with\nassumptions about req->link.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1e51d1e6b1f3708c2d4127b4e371f9daa4c5f859.1648209006.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-25 06:37:52 -0600 io_uring: improve req fields comments"
    },
    {
        "commit": "52dd86406dfa322c8d42b3a4328858abdc6f1d85",
        "message": "When polling sockets for accept, use EPOLLEXCLUSIVE. This is helpful\nwhen multiple accept SQEs are submitted.\n\nFor O_NONBLOCK sockets multiple queued SQEs would previously have all\ncompleted at once, but most with -EAGAIN as the result. Now only one\nwakes up and completes.\n\nFor sockets without O_NONBLOCK there is no user facing change, but\ninternally the extra requests would previously be queued onto a worker\nthread as they would wake up with no connection waiting, and be\npunted. Now they do not wake up unnecessarily.\n\nCo-developed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220325093755.4123343-1-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-25 06:30:23 -0600 io_uring: enable EPOLLEXCLUSIVE for accept poll"
    },
    {
        "commit": "34d2bfe7d4b65b375d0edf704133a6b6970f9d81",
        "message": "While profiling task_work intensive workloads, I noticed that most of\nthe time in tctx_task_work() is spending stalled on loading 'req'. This\nis one of the unfortunate side effects of using linked lists,\nparticularly when they end up being passe around.\n\nPrefetch the next request, if there is one. There's a sufficient amount\nof work in between that this makes it available for the next loop.\n\nWhile fiddling with the cache layout, move the link outside of the\nhot completion cacheline. It's rarely used in hot workloads, so better\nto bring in kbuf which is used for networked loads with provided buffers.\n\nThis reduces tctx_task_work() overhead from ~3% to 1-1.5% in my testing.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-24 17:09:26 -0600 io_uring: improve task work cache utilization"
    },
    {
        "commit": "a73825ba70c93e1eb39a845bb3d9885a787f8ffe",
        "message": "Do not set REQ_F_NOWAIT if the socket is non blocking. When enabled this\ncauses the accept to immediately post a CQE with EAGAIN, which means you\ncannot perform an accept SQE on a NONBLOCK socket asynchronously.\n\nBy removing the flag if there is no pending accept then poll is armed as\nusual and when a connection comes in the CQE is posted.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220324143435.2875844-1-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-24 16:10:29 -0600 io_uring: fix async accept on O_NONBLOCK sockets"
    },
    {
        "commit": "7ef66d186eb95f987a97fb3329b65c840e2dc9bf",
        "message": "This was introduced with the message ring opcode, but isn't strictly\nrequired for the request itself. The sender can encode what is needed\nin user_data, which is passed to the receiver. It's unclear if having\na separate flag that essentially says \"This CQE did not originate from\nan SQE on this ring\" provides any real utility to applications. While\nwe can always re-introduce a flag to provide this information, we cannot\ntake it away at a later point in time.\n\nRemove the flag while we still can, before it's in a released kernel.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-24 06:53:18 -0600 io_uring: remove IORING_CQE_F_MSG"
    },
    {
        "commit": "8a3e8ee56417f5e0e66580d93941ed9d6f4c8274",
        "message": "If we need to continue doing this IO, then we don't want a potentially\nselected buffer recycled. Add a flag for that.\n\nSet this for recv/recvmsg if they do partial IO.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-23 16:40:24 -0600 io_uring: add flag for disabling provided buffer recycling"
    },
    {
        "commit": "7ba89d2af17aa879dda30f5d5d3f152e587fc551",
        "message": "We currently don't attempt to get the full asked for length even if\nMSG_WAITALL is set, if we get a partial receive. If we do see a partial\nreceive, then just note how many bytes we did and return -EAGAIN to\nget it retried.\n\nThe iov is advanced appropriately for the vector based case, and we\nmanually bump the buffer and remainder for the non-vector case.\n\nCc: stable@vger.kernel.org\nReported-by: Constantine Gavrilov <constantine.gavrilov@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-23 16:40:20 -0600 io_uring: ensure recv and recvmsg handle MSG_WAITALL correctly"
    },
    {
        "commit": "4d55f238f8b89124f73e50abbd05e413def514fe",
        "message": "We only really need to recycle the buffer when going async for a file\ntype that has an indefinite reponse time (eg non-file/bdev). And for\nfiles that to arm poll, the async worker will arm poll anyway and the\nbuffer will get recycled there.\n\nIn that latter case, we're not holding ctx->uring_lock. Ensure we take\nthe issue_flags into account and acquire it if we need to.\n\nFixes: b1c62645758e (\"io_uring: recycle provided buffers if request goes async\")\nReported-by: Stefan Roesch <shr@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-23 06:26:06 -0600 io_uring: don't recycle provided buffer if punted to async worker"
    },
    {
        "commit": "d89a4fac0fbc6fe5fc24d1c9a889440dcf410368",
        "message": "syzbot reports a recent regression:\n\nBUG: KASAN: use-after-free in __wake_up_common+0x637/0x650 kernel/sched/wait.c:101\nRead of size 8 at addr ffff888011e8a130 by task syz-executor413/3618\n\nCPU: 0 PID: 3618 Comm: syz-executor413 Tainted: G        W         5.17.0-syzkaller-01402-g8565d64430f8 #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nCall Trace:\n <TASK>\n __dump_stack lib/dump_stack.c:88 [inline]\n dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106\n print_address_description.constprop.0.cold+0x8d/0x303 mm/kasan/report.c:255\n __kasan_report mm/kasan/report.c:442 [inline]\n kasan_report.cold+0x83/0xdf mm/kasan/report.c:459\n __wake_up_common+0x637/0x650 kernel/sched/wait.c:101\n __wake_up_common_lock+0xd0/0x130 kernel/sched/wait.c:138\n tty_release+0x657/0x1200 drivers/tty/tty_io.c:1781\n __fput+0x286/0x9f0 fs/file_table.c:317\n task_work_run+0xdd/0x1a0 kernel/task_work.c:164\n exit_task_work include/linux/task_work.h:32 [inline]\n do_exit+0xaff/0x29d0 kernel/exit.c:806\n do_group_exit+0xd2/0x2f0 kernel/exit.c:936\n __do_sys_exit_group kernel/exit.c:947 [inline]\n __se_sys_exit_group kernel/exit.c:945 [inline]\n __x64_sys_exit_group+0x3a/0x50 kernel/exit.c:945\n do_syscall_x64 arch/x86/entry/common.c:50 [inline]\n do_syscall_64+0x35/0xb0 arch/x86/entry/common.c:80\n entry_SYSCALL_64_after_hwframe+0x44/0xae\nRIP: 0033:0x7f439a1fac69\n\nwhich is due to leaving the request on the waitqueue mistakenly. The\nreproducer is using a tty device, which means we end up arming the same\npoll queue twice (it uses the same poll waitqueue for both), but in\nio_poll_wake() we always just clear REQ_F_SINGLE_POLL regardless of which\nentry triggered. This leaves one waitqueue potentially armed after we're\ndone, which then blows up in tty when the waitqueue is attempted removed.\n\nWe have no room to store this information, so simply encode it in the\nwait_queue_entry->private where we store the io_kiocb request pointer.\n\nFixes: 91eac1c69c20 (\"io_uring: cache poll/double-poll state with a request flag\")\nReported-by: syzbot+09ad4050dd3a120bfccd@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-23 06:26:05 -0600 io_uring: fix assuming triggered poll waitqueue is the single poll"
    },
    {
        "commit": "e2c0cb7c0cc72939b61a7efee376206725796625",
        "message": "The previous commit:\n\n1bc84c40088 (\"io_uring: remove poll entry from list when canceling all\")\n\nremoved a potential overflow condition for the poll references. They\nare currently limited to 20-bits, even if we have 31-bits available. The\nupper bit is used to mark for cancelation.\n\nBump the poll ref space to 31-bits, making that kind of situation much\nharder to trigger in general. We'll separately add overflow checking\nand handling.\n\nFixes: aa43477b0402 (\"io_uring: poll rework\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-23 06:26:05 -0600 io_uring: bump poll refs to full 31-bits"
    },
    {
        "commit": "a8c49af3be5f0b4e105ef678bcf14ef102c270be",
        "message": "Currently memcg stats show several types of kernel memory: kernel stack,\npage tables, sock, vmalloc, and slab.  However, there are other\nallocations with __GFP_ACCOUNT (or supersets such as GFP_KERNEL_ACCOUNT)\nthat are not accounted in any of those stats, a few examples are:\n\n - various kvm allocations (e.g. allocated pages to create vcpus)\n - io_uring\n - tmp_page in pipes during pipe_write()\n - bpf ringbuffers\n - unix sockets\n\nKeeping track of the total kernel memory is essential for the ease of\nmigration from cgroup v1 to v2 as there are large discrepancies between\nv1's kmem.usage_in_bytes and the sum of the available kernel memory\nstats in v2.  Adding separate memcg stats for all __GFP_ACCOUNT kernel\nallocations is an impractical maintenance burden as there a lot of those\nall over the kernel code, with more use cases likely to show up in the\nfuture.\n\nTherefore, add a \"kernel\" memcg stat that is analogous to kmem page\ncounter, with added benefits such as using rstat infrastructure which\naggregates stats more efficiently.  Additionally, this provides a\nlighter alternative in case the legacy kmem is deprecated in the future\n\n[yosryahmed@google.com: v2]\n  Link: https://lkml.kernel.org/r/20220203193856.972500-1-yosryahmed@google.com\n\nLink: https://lkml.kernel.org/r/20220201200823.3283171-1-yosryahmed@google.com\nSigned-off-by: Yosry Ahmed <yosryahmed@google.com>\nAcked-by: Shakeel Butt <shakeelb@google.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nCc: Michal Hocko <mhocko@kernel.org>\nCc: Muchun Song <songmuchun@bytedance.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-22 15:57:02 -0700 memcg: add per-memcg total kernel memory stat"
    },
    {
        "commit": "61bc84c4008812d784c398cfb54118c1ba396dfc",
        "message": "When the ring is exiting, as part of the shutdown, poll requests are\nremoved. But io_poll_remove_all() does not remove entries when finding\nthem, and since completions are done out-of-band, we can find and remove\nthe same entry multiple times.\n\nWe do guard the poll execution by poll ownership, but that does not\nexclude us from reissuing a new one once the previous removal ownership\ngoes away.\n\nThis can race with poll execution as well, where we then end up seeing\nreq->apoll be NULL because a previous task_work requeue finished the\nrequest.\n\nRemove the poll entry when we find it and get ownership of it. This\nprevents multiple invocations from finding it.\n\nFixes: aa43477b0402 (\"io_uring: poll rework\")\nReported-by: Dylan Yudaken <dylany@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-21 19:03:24 -0600 io_uring: remove poll entry from list when canceling all"
    },
    {
        "commit": "b080cee72ef355669cbc52ff55dc513d37433600",
        "message": "Pull io_uring statx fixes from Jens Axboe:\n \"On top of the main io_uring branch, this is to ensure that the\n  filename component of statx is stable after submit.\n\n  That requires a few VFS related changes\"\n\n* tag 'for-5.18/io_uring-statx-2022-03-18' of git://git.kernel.dk/linux-block:\n  io-uring: Make statx API stable",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-21 16:29:24 -0700 Merge tag 'for-5.18/io_uring-statx-2022-03-18' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "af472a9efdf65cbb3398cb6478ec0e89fbc84109",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - Fixes for current file position. Still doesn't have the f_pos_lock\n   sorted, but it's a step in the right direction (Dylan)\n\n - Tracing updates (Dylan, Stefan)\n\n - Improvements to io-wq locking (Hao)\n\n - Improvements for provided buffers (me, Pavel)\n\n - Support for registered file descriptors (me, Xiaoguang)\n\n - Support for ring messages (me)\n\n - Poll improvements (me)\n\n - Fix for fixed buffers and non-iterator reads/writes (me)\n\n - Support for NAPI on sockets (Olivier)\n\n - Ring quiesce improvements (Usama)\n\n - Misc fixes (Olivier, Pavel)\n\n* tag 'for-5.18/io_uring-2022-03-18' of git://git.kernel.dk/linux-block: (42 commits)\n  io_uring: terminate manual loop iterator loop correctly for non-vecs\n  io_uring: don't check unrelated req->open.how in accept request\n  io_uring: manage provided buffers strictly ordered\n  io_uring: fold evfd signalling under a slower path\n  io_uring: thin down io_commit_cqring()\n  io_uring: shuffle io_eventfd_signal() bits around\n  io_uring: remove extra barrier for non-sqpoll iopoll\n  io_uring: fix provided buffer return on failure for kiocb_done()\n  io_uring: extend provided buf return to fails\n  io_uring: refactor timeout cancellation cqe posting\n  io_uring: normilise naming for fill_cqe*\n  io_uring: cache poll/double-poll state with a request flag\n  io_uring: cache req->apoll->events in req->cflags\n  io_uring: move req->poll_refs into previous struct hole\n  io_uring: make tracing format consistent\n  io_uring: recycle apoll_poll entries\n  io_uring: remove duplicated member check for io_msg_ring_prep()\n  io_uring: allow submissions to continue on error\n  io_uring: recycle provided buffers if request goes async\n  io_uring: ensure reads re-import for selected buffers\n  ...",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-21 16:24:45 -0700 Merge tag 'for-5.18/io_uring-2022-03-18' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "649bb75d19c93f5459f450191953dff4825fda3e",
        "message": "Without a full memory barrier between the store to the flags and the\nload of the SQ tail the two operations can be reordered and this can\nlead to a situation where the SQPOLL thread goes to sleep while the\napplication writes to the SQ tail and doesn't see the wakeup flag.\nThis memory barrier pairs with a full memory barrier in the application\nbetween its store to the SQ tail and its load of the flags.\n\nSigned-off-by: Almog Khaikin <almogkh@gmail.com>\nLink: https://lore.kernel.org/r/20220321090059.46313-1-almogkh@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-21 06:33:29 -0600 io_uring: fix memory ordering when SQPOLL thread goes to sleep"
    },
    {
        "commit": "f63cf5192fe3418ad5ae1a4412eba5694b145f79",
        "message": "Ensure that we call fsnotify_modify() if we write a file, and that we\ndo fsnotify_access() if we read it. This enables anyone using inotify\non the file to get notified.\n\nDitto for fallocate, ensure that fsnotify_modify() is called.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-20 17:53:38 -0600 io_uring: ensure that fsnotify is always called"
    },
    {
        "commit": "abdad709ed8fe4fd3b865ed1010de37a49601ff4",
        "message": "We currently have a race where we recycle the selected buffer if poll\nreturns IO_APOLL_OK. But that's too late, as the poll could already be\ntriggering or have triggered. If that race happens, then we're putting a\nbuffer that's already being used.\n\nFix this by recycling before we arm poll. This does mean that we'll\nsometimes almost instantly re-select the buffer, but it's rare enough in\ntesting that it should not pose a performance issue.\n\nFixes: b1c62645758e (\"io_uring: recycle provided buffers if request goes async\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-20 07:23:57 -0600 io_uring: recycle provided before arming poll"
    },
    {
        "commit": "5e929367468c8f97cd1ffb0417316cecfebef94b",
        "message": "The fix for not advancing the iterator if we're using fixed buffers is\nbroken in that it can hit a condition where we don't terminate the loop.\nThis results in io-wq looping forever, asking to read (or write) 0 bytes\nfor every subsequent loop.\n\nReported-by: Joel Jaeschke <joel.jaeschke@gmail.com>\nLink: https://github.com/axboe/liburing/issues/549\nFixes: 16c8d2df7ec0 (\"io_uring: ensure symmetry in handling iter types in loop_rw_iter()\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-18 11:42:48 -0600 io_uring: terminate manual loop iterator loop correctly for non-vecs"
    },
    {
        "commit": "adf3a9e9f556613197583a1884f0de40a8bb6fb9",
        "message": "Looks like a victim of too much copy/paste, we should not be looking\nat req->open.how in accept. The point is to check CLOEXEC and error\nout, which we don't invalid direct descriptors on exec. Hence any\nattempt to get a direct descriptor with CLOEXEC is invalid.\n\nNo harm is done here, as req->open.how.flags overlaps with\nreq->accept.flags, but it's very confusing and might change if either of\nthose command structs are modified.\n\nFixes: aaa4db12ef7b (\"io_uring: accept directly into fixed file table\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-18 10:57:19 -0600 io_uring: don't check unrelated req->open.how in accept request"
    },
    {
        "commit": "dbc7d452e7cf7d3ebc0064e68d30e28d86d3939a",
        "message": "Workloads using provided buffers benefit from using and returning buffers\nin the right order, and so does TLBs for that matter. Manage the internal\nbuffer list in a straight list, rather than use the head buffer as the\ninsertion node. Use a hashed list for the buffer group IDs instead of\nxarray, the overhead is much lower this way. xarray provides internal\nlocking and other trickery that is handy for some uses cases, but\nio_uring already locks internally for the buffer manipulation and needs\nnone of that.\n\nThis is good for about a 2% reduction in overhead, combination of the\nimproved management and the fact that the workload has an easier time\nbundling back provided buffers.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-17 17:20:10 -0600 io_uring: manage provided buffers strictly ordered"
    },
    {
        "commit": "9aa8dfde4869ccdec0a7290b686dbc10e079e163",
        "message": "Add ->has_evfd flag, which is true IFF there is an eventfd attached, and\nuse it to hide io_eventfd_signal() into __io_commit_cqring_flush() and\ncombine fast checks in a single if. Also, gcc 11.2 wasn't inlining\nio_cqring_ev_posted() without this change, so helps with that as well.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f6168471997decded475a063f92915787975a30b.1647481208.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-16 20:26:32 -0600 io_uring: fold evfd signalling under a slower path"
    },
    {
        "commit": "9333f6b4628c8037a89ed23e1188d4b7dc5d74e4",
        "message": "io_commit_cqring() is currently always under spinlock section, so it's\nalways better to keep it as slim as possible. Move\n__io_commit_cqring_flush() out of it into ev_posted*(). If fast checks\ndo fail and this post-processing is required, we'll reacquire\n->completion_lock, which is fine as we don't care about performance of\ndraining and offset timeouts.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ec4e81fd720d3bc7bca8cb9152e080dad1a052f1.1647481208.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-16 20:26:32 -0600 io_uring: thin down io_commit_cqring()"
    },
    {
        "commit": "66fc25ca6b7ec4124606e0d59c71c6bcf14e05bb",
        "message": "A preparation patch, which moves a fast ->io_ev_fd check out of\nio_eventfd_signal() into ev_posted*(). Compilers are smart enough for it\nto not change anything, but will need it later.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ec4091ac76d43912b73917e8db651c2dac4b7b01.1647481208.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-16 20:26:32 -0600 io_uring: shuffle io_eventfd_signal() bits around"
    },
    {
        "commit": "0f84747177b962c32243a57cb454193bdba4fe8d",
        "message": "smp_mb() in io_cqring_ev_posted_iopoll() is only there because of\nwaitqueue_active(). However, non-SQPOLL IOPOLL ring doesn't wake the CQ\nand so the barrier there is useless. Kill it, it's usually pretty\nexpensive.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d72e8ef6f7a3f6a72e18fad8409f7d47afc8da7d.1647481208.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-16 20:26:32 -0600 io_uring: remove extra barrier for non-sqpoll iopoll"
    },
    {
        "commit": "b91ef1872869d99cd42e908eb9754b81115c2c05",
        "message": "Use io_req_complete_failed() in kiocb_done(). This cleans up the code,\nbut also ensures that a provided buffers is correctly freed on failure.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a4880106fcf199d5810707fe2d17126fcdf18bc4.1647481208.git.asml.silence@gmail.com\n[axboe: split from previous patch]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-16 20:24:47 -0600 io_uring: fix provided buffer return on failure for kiocb_done()"
    },
    {
        "commit": "3b2b78a8eb7cc38d207c5ee516769bc3f44d19ea",
        "message": "It's never a good idea to put provided buffers without notifying the\nuserspace, it'll lead to userspace leaks, so add io_put_kbuf() in\nio_req_complete_failed(). The fail helper is called by all sorts of\nrequests, but it's still safe to do as io_put_kbuf() will return 0 in\nfor all requests that don't support and so don't expect provided buffers.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a4880106fcf199d5810707fe2d17126fcdf18bc4.1647481208.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-16 20:24:28 -0600 io_uring: extend provided buf return to fails"
    },
    {
        "commit": "6695490dc85781fe98b782f36f27c13710dbc921",
        "message": "io_fill_cqe*() is not always the best way to post CQEs just because\nthere is enough of infrastructure on top. Replace a raw call to a\nvariant of it inside of io_timeout_cancel(), which also saves us some\nbloating and might help with batching later.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/46113ec4345764b4aef3b384ce38cceabaeedcbb.1647481208.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-16 20:11:15 -0600 io_uring: refactor timeout cancellation cqe posting"
    },
    {
        "commit": "ae4da18941c1c13a9bd6f1d39888ca9a4ff3db91",
        "message": "Restore consistency in __io_fill_cqe* like helpers, always honouring\n\"io_\" prefix and adding \"req\" when we're passing in a request.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bd016ff5c1a4f74687828069d2619d8a65e0c6d7.1647481208.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-16 20:11:00 -0600 io_uring: normilise naming for fill_cqe*"
    },
    {
        "commit": "91eac1c69c202d9dad8bf717ae5b92db70bfe5cf",
        "message": "With commit \"io_uring: cache req->apoll->events in req->cflags\" applied,\nwe now have just io_poll_remove_entries() dipping into req->apoll when\nit isn't strictly necessary.\n\nMark poll and double-poll with a flag, so we know if we need to look\nat apoll->double_poll. This avoids pulling in those cachelines if we\ndon't need them. The common case is that the poll wake handler already\nremoved these entries while hot off the completion path.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-16 16:59:10 -0600 io_uring: cache poll/double-poll state with a request flag"
    },
    {
        "commit": "81459350d581e958ee9c6e76031f77333881c23c",
        "message": "When we arm poll on behalf of a different type of request, like a network\nreceive, then we allocate req->apoll as our poll entry. Running network\nworkloads shows io_poll_check_events() as the most expensive part of\nio_uring, and it's all due to having to pull in req->apoll instead of\njust the request which we have hot already.\n\nCache poll->events in req->cflags, which isn't used until the request\ncompletes anyway. This isn't strictly needed for regular poll, where\nreq->poll.events is used and thus already hot, but for the sake of\nunification we do it all around.\n\nThis saves 3-4% of overhead in certain request workloads.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-16 16:55:05 -0600 io_uring: cache req->apoll->events in req->cflags"
    },
    {
        "commit": "521d61fc760aebdbf8938347a40c9538c0a70034",
        "message": "This serves two purposes:\n\n- We now have the last cacheline mostly unused for generic workloads,\n  instead of having to pull in the poll refs explicitly for workloads\n  that rely on poll arming.\n\n- It shrinks the io_kiocb from 232 to 224 bytes.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-16 12:53:23 -0600 io_uring: move req->poll_refs into previous struct hole"
    },
    {
        "commit": "052ebf1fbb1cab86b145a68d80219c8c57321cbd",
        "message": "Make the tracing formatting for user_data and flags consistent.\n\nHaving consistent formatting allows one for example to grep for a specific\nuser_data/flags and be able to trace a single sqe through easily.\n\nChange user_data to 0x%llx and flags to 0x%x everywhere. The '0x' is\nuseful to disambiguate for example \"user_data 100\".\n\nAdditionally remove the '=' for flags in io_uring_req_failed, again for consistency.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220316095204.2191498-1-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-16 05:45:28 -0600 io_uring: make tracing format consistent"
    },
    {
        "commit": "4d9237e32c5db4f07f749a7ff1dd9b366bf3600e",
        "message": "Particularly for networked workloads, io_uring intensively uses its\npoll based backend to get a notification when data/space is available.\nProfiling workloads, we see 3-4% of alloc+free that is directly attributed\nto just the apoll allocation and free (and the rest being skb alloc+free).\n\nFor the fast path, we have ctx->uring_lock held already for both issue\nand the inline completions, and we can utilize that to avoid any extra\nlocking needed to have a basic recycling cache for the apoll entries on\nboth the alloc and free side.\n\nDouble poll still requires an allocation. But those are rare and not\na fast path item.\n\nWith the simple cache in place, we see a 3-4% reduction in overhead for\nthe workload.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-15 10:54:08 -0600 io_uring: recycle apoll_poll entries"
    },
    {
        "commit": "f3b6a41eb2bbdf545a42e54d637c34f4b1fdf5b9",
        "message": "Julia and the kernel test robot report that the prep handling for this\ncommand inadvertently checks one field twice:\n\nfs/io_uring.c:4338:42-56: duplicated argument to && or ||\n\nGet rid of it.\n\nReported-by: kernel test robot <lkp@intel.com>\nReported-by: Julia Lawall <julia.lawall@lip6.fr>\nFixes: 4f57f06ce218 (\"io_uring: add support for IORING_OP_MSG_RING command\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-12 06:50:13 -0700 io_uring: remove duplicated member check for io_msg_ring_prep()"
    },
    {
        "commit": "bcbb7bf6ccde7cb969a5642879832bc84ebf06a3",
        "message": "By default, io_uring will stop submitting a batch of requests if we run\ninto an error submitting a request. This isn't strictly necessary, as\nthe error result is passed out-of-band via a CQE anyway. And it can be\na bit confusing for some applications.\n\nProvide a way to setup a ring that will continue submitting on error,\nwhen the error CQE has been posted.\n\nThere's still one case that will break out of submission. If we fail\nallocating a request, then we'll still return -ENOMEM. We could in theory\npost a CQE for that condition too even if we never got a request. Leave\nthat for a potential followup.\n\nReported-by: Dylan Yudaken <dylany@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 13:05:25 -0700 io_uring: allow submissions to continue on error"
    },
    {
        "commit": "b1c62645758eb438179e3a0769168cb7b0a94d6b",
        "message": "If we are using provided buffers, it's less than useful to have a buffer\nselected and pinned if a request needs to go async or arms poll for\nnotification trigger on when we can process it.\n\nRecycle the buffer in those events, so we don't pin it for the duration\nof the request.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 09:55:01 -0700 io_uring: recycle provided buffers if request goes async"
    },
    {
        "commit": "2be2eb02e2f5a096c351e5b70c46cfef259dabcd",
        "message": "If we drop buffers between scheduling a retry, then we need to re-import\nwhen we start the request again.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 09:55:01 -0700 io_uring: ensure reads re-import for selected buffers"
    },
    {
        "commit": "9af177ee3ef14c17ef10893c257fa4e2008a3d74",
        "message": "Most of the logic in io_read() deals with regular files, and in some ways\nit would make sense to split the handling into S_IFREG and others. But\nat least for retry, we don't need to bother setting up a bunch of state\njust to abort in the loop later. In particular, don't bother forcing\nsetup of async data for a normal non-vectored read when we don't need it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 09:45:57 -0700 io_uring: retry early for reads if we can poll"
    },
    {
        "commit": "adc8682ec69012b68d5ab7123e246d2ad9a6f94b",
        "message": "The sqpoll thread can be used for performing the napi busy poll in a\nsimilar way that it does io polling for file systems supporting direct\naccess bypassing the page cache.\n\nThe other way that io_uring can be used for napi busy poll is by\ncalling io_uring_enter() to get events.\n\nIf the user specify a timeout value, it is distributed between polling\nand sleeping by using the systemwide setting\n/proc/sys/net/core/busy_poll.\n\nThe changes have been tested with this program:\nhttps://github.com/lano1106/io_uring_udp_ping\n\nand the result is:\nWithout sqpoll:\nNAPI busy loop disabled:\nrtt min/avg/max/mdev = 40.631/42.050/58.667/1.547 us\nNAPI busy loop enabled:\nrtt min/avg/max/mdev = 30.619/31.753/61.433/1.456 us\n\nWith sqpoll:\nNAPI busy loop disabled:\nrtt min/avg/max/mdev = 42.087/44.438/59.508/1.533 us\nNAPI busy loop enabled:\nrtt min/avg/max/mdev = 35.779/37.347/52.201/0.924 us\n\nCo-developed-by: Hao Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nLink: https://lore.kernel.org/r/810bd9408ffc510ff08269e78dca9df4af0b9e4e.1646777484.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 09:18:30 -0700 io_uring: Add support for napi_busy_poll"
    },
    {
        "commit": "950e79dd73131a263b2dd7fec521ceafd28d2724",
        "message": "Move up the block manipulating the sig variable to execute code\nthat may encounter an error and exit first before continuing\nexecuting the rest of the function and avoid useless computations\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nLink: https://lore.kernel.org/r/84513f7cc1b1fb31d8f4cb910aee033391d036b4.1646777484.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 09:18:30 -0700 io_uring: minor io_cqring_wait() optimization"
    },
    {
        "commit": "4f57f06ce2186c31c3da52386125dc57b1cd6f96",
        "message": "This adds support for IORING_OP_MSG_RING, which allows an SQE to signal\nanother ring. That allows either waking up someone waiting on the ring,\nor even passing a 64-bit value via the user_data field in the CQE.\n\nsqe->fd must contain the fd of a ring that should receive the CQE.\nsqe->off will be propagated to the cqe->user_data on the target ring,\nand sqe->len will be propagated to cqe->res. The results CQE will have\nIORING_CQE_F_MSG set in its flags, to indicate that this CQE was generated\nfrom a messaging request rather than a SQE issued locally on that ring.\nThis effectively allows passing a 64-bit and a 32-bit quantify between\nthe two rings.\n\nThis request type has the following request specific error cases:\n\n- -EBADFD. Set if the sqe->fd doesn't point to a file descriptor that is\n  of the io_uring type.\n- -EOVERFLOW. Set if we were not able to deliver a request to the target\n  ring.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 09:16:04 -0700 io_uring: add support for IORING_OP_MSG_RING command"
    },
    {
        "commit": "cc3cec8367cba76a8ae4c271eba8450f3efc1ba3",
        "message": "In testing high frequency workloads with provided buffers, we spend a\nlot of time in allocating and freeing the buffer units themselves.\nRather than repeatedly free and alloc them, add a recycling cache\ninstead. There are two caches:\n\n- ctx->io_buffers_cache. This is the one we grab from in the submission\n  path, and it's protected by ctx->uring_lock. For inline completions,\n  we can recycle straight back to this cache and not need any extra\n  locking.\n\n- ctx->io_buffers_comp. If we're not under uring_lock, then we use this\n  list to recycle buffers. It's protected by the completion_lock.\n\nOn adding a new buffer, check io_buffers_cache. If it's empty, check if\nwe can splice entries from the io_buffers_comp_cache.\n\nThis reduces about 5-10% of overhead from provided buffers, bringing it\npretty close to the non-provided path.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 06:33:14 -0700 io_uring: speedup provided buffer handling"
    },
    {
        "commit": "e7a6c00dc77aedf27a601738ea509f1caea6d673",
        "message": "Lots of workloads use multiple threads, in which case the file table is\nshared between them. This makes getting and putting the ring file\ndescriptor for each io_uring_enter(2) system call more expensive, as it\ninvolves an atomic get and put for each call.\n\nSimilarly to how we allow registering normal file descriptors to avoid\nthis overhead, add support for an io_uring_register(2) API that allows\nto register the ring fds themselves:\n\n1) IORING_REGISTER_RING_FDS - takes an array of io_uring_rsrc_update\n   structs, and registers them with the task.\n2) IORING_UNREGISTER_RING_FDS - takes an array of io_uring_src_update\n   structs, and unregisters them.\n\nWhen a ring fd is registered, it is internally represented by an offset.\nThis offset is returned to the application, and the application then\nuses this offset and sets IORING_ENTER_REGISTERED_RING for the\nio_uring_enter(2) system call. This works just like using a registered\nfile descriptor, rather than a real one, in an SQE, where\nIOSQE_FIXED_FILE gets set to tell io_uring that we're using an internal\noffset/descriptor rather than a real file descriptor.\n\nIn initial testing, this provides a nice bump in performance for\nthreaded applications in real world cases where the batch count (eg\nnumber of requests submitted per io_uring_enter(2) invocation) is low.\nIn a microbenchmark, submitting NOP requests, we see the following\nincreases in performance:\n\nRequests per syscall\tBaseline\tRegistered\tIncrease\n----------------------------------------------------------------\n1\t\t\t ~7030K\t\t ~8080K\t\t+15%\n2\t\t\t~13120K\t\t~14800K\t\t+13%\n4\t\t\t~22740K\t\t~25300K\t\t+11%\n\nCo-developed-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 06:32:49 -0700 io_uring: add support for registering ring file descriptors"
    },
    {
        "commit": "63c36549737e8132e89ec6563d26523895ae3121",
        "message": "Fix incorrect name reference in comment. ki_filp does not exist in the\nstruct, but file does.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220224105157.1332353-1-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 06:32:49 -0700 io_uring: documentation fixup"
    },
    {
        "commit": "b4aec40015953b65f2f114641e7fd7714c8df8e6",
        "message": "There is a slight optimisation to be had by calculating the correct pos\npointer inside io_kiocb_update_pos and then using that later.\n\nIt seems code size drops by a bit:\n000000000000a1b0 0000000000000400 t io_read\n000000000000a5b0 0000000000000319 t io_write\n\nvs\n000000000000a1b0 00000000000003f6 t io_read\n000000000000a5b0 0000000000000310 t io_write\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 06:32:49 -0700 io_uring: do not recalculate ppos unnecessarily"
    },
    {
        "commit": "d34e1e5b396a0dbaa4a29b7138df662cfb9d8e8e",
        "message": "Update kiocb->ki_pos at execution time rather than in io_prep_rw().\nio_prep_rw() happens before the job is enqueued to a worker and so the\noffset might be read multiple times before being executed once.\n\nEnsures that the file position in a set of _linked_ SQEs will be only\nobtained after earlier SQEs have completed, and so will include their\nincremented file position.\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 06:32:49 -0700 io_uring: update kiocb->ki_pos at execution time"
    },
    {
        "commit": "af9c45ecebaf1b428306f41421f4bcffe439f735",
        "message": "io_kiocb_ppos is called in both branches, and it seems that the compiler\ndoes not fuse this. Fusing removes a few bytes from loop_rw_iter.\n\nBefore:\n$ nm -S fs/io_uring.o | grep loop_rw_iter\n0000000000002430 0000000000000124 t loop_rw_iter\n\nAfter:\n$ nm -S fs/io_uring.o | grep loop_rw_iter\n0000000000002430 000000000000010d t loop_rw_iter\n\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 06:32:49 -0700 io_uring: remove duplicated calls to io_kiocb_ppos"
    },
    {
        "commit": "c5020bc8d9295ad4a3e09d858a28b26a25d4afab",
        "message": "Avoid testing TIF_NOTIFY_SIGNAL twice by calling task_sigpending()\ndirectly from io_run_task_work_sig()\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nLink: https://lore.kernel.org/r/bd7c0495f7656e803e5736708591bb665e6eaacd.1645041650.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 06:32:49 -0700 io_uring: Remove unneeded test in io_run_task_work_sig()"
    },
    {
        "commit": "d5ec1dfaf59bf1632d7f2114d209bf80bfbd907a",
        "message": "This introduces the __fill_cqe function. This is necessary\nto correctly issue the io_uring_complete tracepoint.\n\nSigned-off-by: Stefan Roesch <shr@fb.com>\nLink: https://lore.kernel.org/r/20220214180430.70572-2-shr@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 06:32:49 -0700 io-uring: add __fill_cqe function"
    },
    {
        "commit": "42abc95f05bff5180ac40c7ba5726b73c1d5e2f4",
        "message": "wqe->lock is abused, it now protects acct->work_list, hash stuff,\nnr_workers, wqe->free_list and so on. Lets first get the work_list out\nof the wqe-lock mess by introduce a specific lock for work list. This\nis the first step to solve the huge contension between work insertion\nand work consumption.\ngood thing:\n  - split locking for bound and unbound work list\n  - reduce contension between work_list visit and (worker's)free_list.\n\nFor the hash stuff, since there won't be a work with same file in both\nbound and unbound work list, thus they won't visit same hash entry. it\nworks well to use the new lock to protect hash stuff.\n\nResults:\nset max_unbound_worker = 4, test with echo-server:\nnice -n -15 ./io_uring_echo_server -p 8081 -f -n 1000 -l 16\n(-n connection, -l workload)\nbefore this patch:\nSamples: 2M of event 'cycles:ppp', Event count (approx.): 1239982111074\nOverhead  Command          Shared Object         Symbol\n  28.59%  iou-wrk-10021    [kernel.vmlinux]      [k] native_queued_spin_lock_slowpath\n   8.89%  io_uring_echo_s  [kernel.vmlinux]      [k] native_queued_spin_lock_slowpath\n   6.20%  iou-wrk-10021    [kernel.vmlinux]      [k] _raw_spin_lock\n   2.45%  io_uring_echo_s  [kernel.vmlinux]      [k] io_prep_async_work\n   2.36%  iou-wrk-10021    [kernel.vmlinux]      [k] _raw_spin_lock_irqsave\n   2.29%  iou-wrk-10021    [kernel.vmlinux]      [k] io_worker_handle_work\n   1.29%  io_uring_echo_s  [kernel.vmlinux]      [k] io_wqe_enqueue\n   1.06%  iou-wrk-10021    [kernel.vmlinux]      [k] io_wqe_worker\n   1.06%  io_uring_echo_s  [kernel.vmlinux]      [k] _raw_spin_lock\n   1.03%  iou-wrk-10021    [kernel.vmlinux]      [k] __schedule\n   0.99%  iou-wrk-10021    [kernel.vmlinux]      [k] tcp_sendmsg_locked\n\nwith this patch:\nSamples: 1M of event 'cycles:ppp', Event count (approx.): 708446691943\nOverhead  Command          Shared Object         Symbol\n  16.86%  iou-wrk-10893    [kernel.vmlinux]      [k] native_queued_spin_lock_slowpat\n   9.10%  iou-wrk-10893    [kernel.vmlinux]      [k] _raw_spin_lock\n   4.53%  io_uring_echo_s  [kernel.vmlinux]      [k] native_queued_spin_lock_slowpat\n   2.87%  iou-wrk-10893    [kernel.vmlinux]      [k] io_worker_handle_work\n   2.57%  iou-wrk-10893    [kernel.vmlinux]      [k] _raw_spin_lock_irqsave\n   2.56%  io_uring_echo_s  [kernel.vmlinux]      [k] io_prep_async_work\n   1.82%  io_uring_echo_s  [kernel.vmlinux]      [k] _raw_spin_lock\n   1.33%  iou-wrk-10893    [kernel.vmlinux]      [k] io_wqe_worker\n   1.26%  io_uring_echo_s  [kernel.vmlinux]      [k] try_to_wake_up\n\nspin_lock failure from 25.59% + 8.89% =  34.48% to 16.86% + 4.53% = 21.39%\nTPS is similar, while cpu usage is from almost 400% to 350%\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20220206095241.121485-2-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 06:32:49 -0700 io-wq: decouple work_list protection from the big wqe->lock"
    },
    {
        "commit": "f0a4e62bb5343b3b7163bc851cfb4bebfefcc4e7",
        "message": "Clang warns:\n\n  fs/io_uring.c:9396:9: warning: variable 'ret' is uninitialized when used here [-Wuninitialized]\n          return ret;\n                 ^~~\n  fs/io_uring.c:9373:13: note: initialize the variable 'ret' to silence this warning\n          int fd, ret;\n                     ^\n                      = 0\n  1 warning generated.\n\nJust return 0 directly and reduce the scope of ret to the if statement,\nas that is the only place that it is used, which is how the function was\nbefore the fixes commit.\n\nFixes: 1a75fac9a0f9 (\"io_uring: avoid ring quiesce while registering/unregistering eventfd\")\nLink: https://github.com/ClangBuiltLinux/linux/issues/1579\nSigned-off-by: Nathan Chancellor <nathan@kernel.org>\nReviewed-by: Nick Desaulniers <ndesaulniers@google.com>\nLink: https://lore.kernel.org/r/20220207162410.1013466-1-nathan@kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 06:32:49 -0700 io_uring: Fix use of uninitialized ret in io_eventfd_register()"
    },
    {
        "commit": "8bb649ee1da321ec3a1bbec048a425c5ea18ea21",
        "message": "None of the opcodes in io_uring_register use ring quiesce anymore. Hence\nio_register_op_must_quiesce always returns false and io_ctx_quiesce is\nnever called.\n\nSigned-off-by: Usama Arif <usama.arif@bytedance.com>\nLink: https://lore.kernel.org/r/20220204145117.1186568-6-usama.arif@bytedance.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 06:32:49 -0700 io_uring: remove ring quiesce for io_uring_register"
    },
    {
        "commit": "ff16cfcfdaafa3c4287be92c6944fb8ea6dc75d1",
        "message": "IORING_SETUP_R_DISABLED prevents submitting requests and so there will be\nno requests until IORING_REGISTER_ENABLE_RINGS is called. And\nIORING_REGISTER_RESTRICTIONS works only before\nIORING_REGISTER_ENABLE_RINGS is called. Hence ring quiesce is not needed\nfor these opcodes.\n\nSigned-off-by: Usama Arif <usama.arif@bytedance.com>\nLink: https://lore.kernel.org/r/20220204145117.1186568-5-usama.arif@bytedance.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 06:32:49 -0700 io_uring: avoid ring quiesce while registering restrictions and enabling rings"
    },
    {
        "commit": "c75312dd592b31427caa88170b61fdc3ae5b2891",
        "message": "This is done using the RCU data structure (io_ev_fd). eventfd_async is\nmoved from io_ring_ctx to io_ev_fd which is RCU protected hence avoiding\nring quiesce which is much more expensive than an RCU lock. The place\nwhere eventfd_async is read is already under rcu_read_lock so there is no\nextra RCU read-side critical section needed.\n\nSigned-off-by: Usama Arif <usama.arif@bytedance.com>\nLink: https://lore.kernel.org/r/20220204145117.1186568-4-usama.arif@bytedance.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 06:32:49 -0700 io_uring: avoid ring quiesce while registering async eventfd"
    },
    {
        "commit": "77bc59b498174ea4f120d477d2cd0cf90fc58235",
        "message": "This is done by creating a new RCU data structure (io_ev_fd) as part of\nio_ring_ctx that holds the eventfd_ctx.\n\nThe function io_eventfd_signal is executed under rcu_read_lock with a\nsingle rcu_dereference to io_ev_fd so that if another thread unregisters\nthe eventfd while io_eventfd_signal is still being executed, the\neventfd_signal for which io_eventfd_signal was called completes\nsuccessfully.\n\nThe process of registering/unregistering eventfd is already done under\nuring_lock so multiple threads won't enter a race condition while\nregistering/unregistering eventfd.\n\nWith the above approach ring quiesce can be avoided which is much more\nexpensive then using RCU lock. On the system tested, io_uring_register\nwith IORING_REGISTER_EVENTFD takes less than 1ms with RCU lock, compared\nto 15ms before with ring quiesce.\n\nSigned-off-by: Usama Arif <usama.arif@bytedance.com>\nLink: https://lore.kernel.org/r/20220204145117.1186568-3-usama.arif@bytedance.com\n[axboe: long line fixups]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 06:32:49 -0700 io_uring: avoid ring quiesce while registering/unregistering eventfd"
    },
    {
        "commit": "2757be22c0f4c06d359f802fa1fc57286fa26975",
        "message": "The information on whether eventfd is registered is not very useful and\nwould result in the tracepoint being enclosed in an rcu_readlock in a\nlater patch that tries to avoid ring quiesce for registering eventfd.\n\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Usama Arif <usama.arif@bytedance.com>\nLink: https://lore.kernel.org/r/20220204145117.1186568-2-usama.arif@bytedance.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-10 06:32:49 -0700 io_uring: remove trace for eventfd"
    },
    {
        "commit": "3ee65c0f0778b8fa95381cd7676cde2c03e0f889",
        "message": "Pull btrfs fixes from David Sterba:\n \"A few more fixes for various problems that have user visible effects\n  or seem to be urgent:\n\n   - fix corruption when combining DIO and non-blocking io_uring over\n     multiple extents (seen on MariaDB)\n\n   - fix relocation crash due to premature return from commit\n\n   - fix quota deadlock between rescan and qgroup removal\n\n   - fix item data bounds checks in tree-checker (found on a fuzzed\n     image)\n\n   - fix fsync of prealloc extents after EOF\n\n   - add missing run of delayed items after unlink during log replay\n\n   - don't start relocation until snapshot drop is finished\n\n   - fix reversed condition for subpage writers locking\n\n   - fix warning on page error\"\n\n* tag 'for-5.17-rc6-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux:\n  btrfs: fallback to blocking mode when doing async dio over multiple extents\n  btrfs: add missing run of delayed items after unlink during log replay\n  btrfs: qgroup: fix deadlock between rescan worker and remove qgroup\n  btrfs: fix relocation crash due to premature return from btrfs_commit_transaction()\n  btrfs: do not start relocation until in progress drops are done\n  btrfs: tree-checker: use u64 for item data end to avoid overflow\n  btrfs: do not WARN_ON() if we have PageError set\n  btrfs: fix lost prealloc extents beyond eof after full fsync\n  btrfs: subpage: fix a wrong check on subpage->writers",
        "kernel_version": "v5.17-rc7",
        "release_date": "2022-03-06 12:19:36 -0800 Merge tag 'for-5.17-rc6-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux"
    },
    {
        "commit": "ca93e44bfb5fd7996b76f0f544999171f647f93b",
        "message": "Some users recently reported that MariaDB was getting a read corruption\nwhen using io_uring on top of btrfs. This started to happen in 5.16,\nafter commit 51bd9563b6783d (\"btrfs: fix deadlock due to page faults\nduring direct IO reads and writes\"). That changed btrfs to use the new\niomap flag IOMAP_DIO_PARTIAL and to disable page faults before calling\niomap_dio_rw(). This was necessary to fix deadlocks when the iovector\ncorresponds to a memory mapped file region. That type of scenario is\nexercised by test case generic/647 from fstests.\n\nFor this MariaDB scenario, we attempt to read 16K from file offset X\nusing IOCB_NOWAIT and io_uring. In that range we have 4 extents, each\nwith a size of 4K, and what happens is the following:\n\n1) btrfs_direct_read() disables page faults and calls iomap_dio_rw();\n\n2) iomap creates a struct iomap_dio object, its reference count is\n   initialized to 1 and its ->size field is initialized to 0;\n\n3) iomap calls btrfs_dio_iomap_begin() with file offset X, which finds\n   the first 4K extent, and setups an iomap for this extent consisting\n   of a single page;\n\n4) At iomap_dio_bio_iter(), we are able to access the first page of the\n   buffer (struct iov_iter) with bio_iov_iter_get_pages() without\n   triggering a page fault;\n\n5) iomap submits a bio for this 4K extent\n   (iomap_dio_submit_bio() -> btrfs_submit_direct()) and increments\n   the refcount on the struct iomap_dio object to 2; The ->size field\n   of the struct iomap_dio object is incremented to 4K;\n\n6) iomap calls btrfs_iomap_begin() again, this time with a file\n   offset of X + 4K. There we setup an iomap for the next extent\n   that also has a size of 4K;\n\n7) Then at iomap_dio_bio_iter() we call bio_iov_iter_get_pages(),\n   which tries to access the next page (2nd page) of the buffer.\n   This triggers a page fault and returns -EFAULT;\n\n8) At __iomap_dio_rw() we see the -EFAULT, but we reset the error\n   to 0 because we passed the flag IOMAP_DIO_PARTIAL to iomap and\n   the struct iomap_dio object has a ->size value of 4K (we submitted\n   a bio for an extent already). The 'wait_for_completion' variable\n   is not set to true, because our iocb has IOCB_NOWAIT set;\n\n9) At the bottom of __iomap_dio_rw(), we decrement the reference count\n   of the struct iomap_dio object from 2 to 1. Because we were not\n   the only ones holding a reference on it and 'wait_for_completion' is\n   set to false, -EIOCBQUEUED is returned to btrfs_direct_read(), which\n   just returns it up the callchain, up to io_uring;\n\n10) The bio submitted for the first extent (step 5) completes and its\n    bio endio function, iomap_dio_bio_end_io(), decrements the last\n    reference on the struct iomap_dio object, resulting in calling\n    iomap_dio_complete_work() -> iomap_dio_complete().\n\n11) At iomap_dio_complete() we adjust the iocb->ki_pos from X to X + 4K\n    and return 4K (the amount of io done) to iomap_dio_complete_work();\n\n12) iomap_dio_complete_work() calls the iocb completion callback,\n    iocb->ki_complete() with a second argument value of 4K (total io\n    done) and the iocb with the adjust ki_pos of X + 4K. This results\n    in completing the read request for io_uring, leaving it with a\n    result of 4K bytes read, and only the first page of the buffer\n    filled in, while the remaining 3 pages, corresponding to the other\n    3 extents, were not filled;\n\n13) For the application, the result is unexpected because if we ask\n    to read N bytes, it expects to get N bytes read as long as those\n    N bytes don't cross the EOF (i_size).\n\nMariaDB reports this as an error, as it's not expecting a short read,\nsince it knows it's asking for read operations fully within the i_size\nboundary. This is typical in many applications, but it may also be\nquestionable if they should react to such short reads by issuing more\nread calls to get the remaining data. Nevertheless, the short read\nhappened due to a change in btrfs regarding how it deals with page\nfaults while in the middle of a read operation, and there's no reason\nwhy btrfs can't have the previous behaviour of returning the whole data\nthat was requested by the application.\n\nThe problem can also be triggered with the following simple program:\n\n  /* Get O_DIRECT */\n  #ifndef _GNU_SOURCE\n  #define _GNU_SOURCE\n  #endif\n\n  #include <stdio.h>\n  #include <stdlib.h>\n  #include <unistd.h>\n  #include <fcntl.h>\n  #include <errno.h>\n  #include <string.h>\n  #include <liburing.h>\n\n  int main(int argc, char *argv[])\n  {\n      char *foo_path;\n      struct io_uring ring;\n      struct io_uring_sqe *sqe;\n      struct io_uring_cqe *cqe;\n      struct iovec iovec;\n      int fd;\n      long pagesize;\n      void *write_buf;\n      void *read_buf;\n      ssize_t ret;\n      int i;\n\n      if (argc != 2) {\n          fprintf(stderr, \"Use: %s <directory>\\n\", argv[0]);\n          return 1;\n      }\n\n      foo_path = malloc(strlen(argv[1]) + 5);\n      if (!foo_path) {\n          fprintf(stderr, \"Failed to allocate memory for file path\\n\");\n          return 1;\n      }\n      strcpy(foo_path, argv[1]);\n      strcat(foo_path, \"/foo\");\n\n      /*\n       * Create file foo with 2 extents, each with a size matching\n       * the page size. Then allocate a buffer to read both extents\n       * with io_uring, using O_DIRECT and IOCB_NOWAIT. Before doing\n       * the read with io_uring, access the first page of the buffer\n       * to fault it in, so that during the read we only trigger a\n       * page fault when accessing the second page of the buffer.\n       */\n       fd = open(foo_path, O_CREAT | O_TRUNC | O_WRONLY |\n                O_DIRECT, 0666);\n       if (fd == -1) {\n           fprintf(stderr,\n                   \"Failed to create file 'foo': %s (errno %d)\",\n                   strerror(errno), errno);\n           return 1;\n       }\n\n       pagesize = sysconf(_SC_PAGE_SIZE);\n       ret = posix_memalign(&write_buf, pagesize, 2 * pagesize);\n       if (ret) {\n           fprintf(stderr, \"Failed to allocate write buffer\\n\");\n           return 1;\n       }\n\n       memset(write_buf, 0xab, pagesize);\n       memset(write_buf + pagesize, 0xcd, pagesize);\n\n       /* Create 2 extents, each with a size matching page size. */\n       for (i = 0; i < 2; i++) {\n           ret = pwrite(fd, write_buf + i * pagesize, pagesize,\n                        i * pagesize);\n           if (ret != pagesize) {\n               fprintf(stderr,\n                     \"Failed to write to file, ret = %ld errno %d (%s)\\n\",\n                      ret, errno, strerror(errno));\n               return 1;\n           }\n           ret = fsync(fd);\n           if (ret != 0) {\n               fprintf(stderr, \"Failed to fsync file\\n\");\n               return 1;\n           }\n       }\n\n       close(fd);\n       fd = open(foo_path, O_RDONLY | O_DIRECT);\n       if (fd == -1) {\n           fprintf(stderr,\n                   \"Failed to open file 'foo': %s (errno %d)\",\n                   strerror(errno), errno);\n           return 1;\n       }\n\n       ret = posix_memalign(&read_buf, pagesize, 2 * pagesize);\n       if (ret) {\n           fprintf(stderr, \"Failed to allocate read buffer\\n\");\n           return 1;\n       }\n\n       /*\n        * Fault in only the first page of the read buffer.\n        * We want to trigger a page fault for the 2nd page of the\n        * read buffer during the read operation with io_uring\n        * (O_DIRECT and IOCB_NOWAIT).\n        */\n       memset(read_buf, 0, 1);\n\n       ret = io_uring_queue_init(1, &ring, 0);\n       if (ret != 0) {\n           fprintf(stderr, \"Failed to create io_uring queue\\n\");\n           return 1;\n       }\n\n       sqe = io_uring_get_sqe(&ring);\n       if (!sqe) {\n           fprintf(stderr, \"Failed to get io_uring sqe\\n\");\n           return 1;\n       }\n\n       iovec.iov_base = read_buf;\n       iovec.iov_len = 2 * pagesize;\n       io_uring_prep_readv(sqe, fd, &iovec, 1, 0);\n\n       ret = io_uring_submit_and_wait(&ring, 1);\n       if (ret != 1) {\n           fprintf(stderr,\n                   \"Failed at io_uring_submit_and_wait()\\n\");\n           return 1;\n       }\n\n       ret = io_uring_wait_cqe(&ring, &cqe);\n       if (ret < 0) {\n           fprintf(stderr, \"Failed at io_uring_wait_cqe()\\n\");\n           return 1;\n       }\n\n       printf(\"io_uring read result for file foo:\\n\\n\");\n       printf(\"  cqe->res == %d (expected %d)\\n\", cqe->res, 2 * pagesize);\n       printf(\"  memcmp(read_buf, write_buf) == %d (expected 0)\\n\",\n              memcmp(read_buf, write_buf, 2 * pagesize));\n\n       io_uring_cqe_seen(&ring, cqe);\n       io_uring_queue_exit(&ring);\n\n       return 0;\n  }\n\nWhen running it on an unpatched kernel:\n\n  $ gcc io_uring_test.c -luring\n  $ mkfs.btrfs -f /dev/sda\n  $ mount /dev/sda /mnt/sda\n  $ ./a.out /mnt/sda\n  io_uring read result for file foo:\n\n    cqe->res == 4096 (expected 8192)\n    memcmp(read_buf, write_buf) == -205 (expected 0)\n\nAfter this patch, the read always returns 8192 bytes, with the buffer\nfilled with the correct data. Although that reproducer always triggers\nthe bug in my test vms, it's possible that it will not be so reliable\non other environments, as that can happen if the bio for the first\nextent completes and decrements the reference on the struct iomap_dio\nobject before we do the atomic_dec_and_test() on the reference at\n__iomap_dio_rw().\n\nFix this in btrfs by having btrfs_dio_iomap_begin() return -EAGAIN\nwhenever we try to satisfy a non blocking IO request (IOMAP_NOWAIT flag\nset) over a range that spans multiple extents (or a mix of extents and\nholes). This avoids returning success to the caller when we only did\npartial IO, which is not optimal for writes and for reads it's actually\nincorrect, as the caller doesn't expect to get less bytes read than it has\nrequested (unless EOF is crossed), as previously mentioned. This is also\nthe type of behaviour that xfs follows (xfs_direct_write_iomap_begin()),\neven though it doesn't use IOMAP_DIO_PARTIAL.\n\nA test case for fstests will follow soon.\n\nLink: https://lore.kernel.org/linux-btrfs/CABVffEM0eEWho+206m470rtM0d9J8ue85TtR-A_oVTuGLWFicA@mail.gmail.com/\nLink: https://lore.kernel.org/linux-btrfs/CAHF2GV6U32gmqSjLe=XKgfcZAmLCiH26cJ2OnHGp5x=VAH4OHQ@mail.gmail.com/\nCC: stable@vger.kernel.org # 5.16+\nReviewed-by: Josef Bacik <josef@toxicpanda.com>\nSigned-off-by: Filipe Manana <fdmanana@suse.com>\nSigned-off-by: David Sterba <dsterba@suse.com>",
        "kernel_version": "v5.17-rc7",
        "release_date": "2022-03-04 15:09:21 +0100 btrfs: fallback to blocking mode when doing async dio over multiple extents"
    },
    {
        "commit": "115dcec65f61d53e25e1bed5e380468b30f98b14",
        "message": "Replace the existing region based migration protocol with an ioctl based\nprotocol. The two protocols have the same general semantic behaviors, but\nthe way the data is transported is changed.\n\nThis is the STOP_COPY portion of the new protocol, it defines the 5 states\nfor basic stop and copy migration and the protocol to move the migration\ndata in/out of the kernel.\n\nCompared to the clarification of the v1 protocol Alex proposed:\n\nhttps://lore.kernel.org/r/163909282574.728533.7460416142511440919.stgit@omen\n\nThis has a few deliberate functional differences:\n\n - ERROR arcs allow the device function to remain unchanged.\n\n - The protocol is not required to return to the original state on\n   transition failure. Instead userspace can execute an unwind back to\n   the original state, reset, or do something else without needing kernel\n   support. This simplifies the kernel design and should userspace choose\n   a policy like always reset, avoids doing useless work in the kernel\n   on error handling paths.\n\n - PRE_COPY is made optional, userspace must discover it before using it.\n   This reflects the fact that the majority of drivers we are aware of\n   right now will not implement PRE_COPY.\n\n - segmentation is not part of the data stream protocol, the receiver\n   does not have to reproduce the framing boundaries.\n\nThe hybrid FSM for the device_state is described as a Mealy machine by\ndocumenting each of the arcs the driver is required to implement. Defining\nthe remaining set of old/new device_state transitions as 'combination\ntransitions' which are naturally defined as taking multiple FSM arcs along\nthe shortest path within the FSM's digraph allows a complete matrix of\ntransitions.\n\nA new VFIO_DEVICE_FEATURE of VFIO_DEVICE_FEATURE_MIG_DEVICE_STATE is\ndefined to replace writing to the device_state field in the region. This\nallows returning a brand new FD whenever the requested transition opens\na data transfer session.\n\nThe VFIO core code implements the new feature and provides a helper\nfunction to the driver. Using the helper the driver only has to\nimplement 6 of the FSM arcs and the other combination transitions are\nelaborated consistently from those arcs.\n\nA new VFIO_DEVICE_FEATURE of VFIO_DEVICE_FEATURE_MIGRATION is defined to\nreport the capability for migration and indicate which set of states and\narcs are supported by the device. The FSM provides a lot of flexibility to\nmake backwards compatible extensions but the VFIO_DEVICE_FEATURE also\nallows for future breaking extensions for scenarios that cannot support\neven the basic STOP_COPY requirements.\n\nThe VFIO_DEVICE_FEATURE_MIG_DEVICE_STATE with the GET option (i.e.\nVFIO_DEVICE_FEATURE_GET) can be used to read the current migration state\nof the VFIO device.\n\nData transfer sessions are now carried over a file descriptor, instead of\nthe region. The FD functions for the lifetime of the data transfer\nsession. read() and write() transfer the data with normal Linux stream FD\nsemantics. This design allows future expansion to support poll(),\nio_uring, and other performance optimizations.\n\nThe complicated mmap mode for data transfer is discarded as current qemu\ndoesn't take meaningful advantage of it, and the new qemu implementation\navoids substantially all the performance penalty of using a read() on the\nregion.\n\nLink: https://lore.kernel.org/all/20220224142024.147653-10-yishaih@nvidia.com\nSigned-off-by: Jason Gunthorpe <jgg@nvidia.com>\nTested-by: Shameer Kolothum <shameerali.kolothum.thodi@huawei.com>\nReviewed-by: Kevin Tian <kevin.tian@intel.com>\nReviewed-by: Alex Williamson <alex.williamson@redhat.com>\nReviewed-by: Cornelia Huck <cohuck@redhat.com>\nSigned-off-by: Yishai Hadas <yishaih@nvidia.com>\nSigned-off-by: Leon Romanovsky <leonro@nvidia.com>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-03-03 12:57:39 +0200 vfio: Define device migration protocol v2"
    },
    {
        "commit": "3a5f59b17f9dec448976626663a73841460d7ab4",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Add a conditional schedule point in io_add_buffers() (Eric)\n\n - Fix for a quiesce speedup merged in this release (Dylan)\n\n - Don't convert to jiffies for event timeout waiting, it's way too\n   coarse when we accept a timespec as input (me)\n\n* tag 'io_uring-5.17-2022-02-23' of git://git.kernel.dk/linux-block:\n  io_uring: disallow modification of rsrc_data during quiesce\n  io_uring: don't convert to jiffies for waiting on timeouts\n  io_uring: add a schedule point in io_add_buffers()",
        "kernel_version": "v5.17-rc6",
        "release_date": "2022-02-24 11:08:15 -0800 Merge tag 'io_uring-5.17-2022-02-23' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "80912cef18f16f8fe59d1fb9548d4364342be360",
        "message": "io_rsrc_ref_quiesce will unlock the uring while it waits for references to\nthe io_rsrc_data to be killed.\nThere are other places to the data that might add references to data via\ncalls to io_rsrc_node_switch.\nThere is a race condition where this reference can be added after the\ncompletion has been signalled. At this point the io_rsrc_ref_quiesce call\nwill wake up and relock the uring, assuming the data is unused and can be\nfreed - although it is actually being used.\n\nTo fix this check in io_rsrc_ref_quiesce if a resource has been revived.\n\nReported-by: syzbot+ca8bf833622a1662745b@syzkaller.appspotmail.com\nCc: stable@vger.kernel.org\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220222161751.995746-1-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc6",
        "release_date": "2022-02-22 09:57:32 -0700 io_uring: disallow modification of rsrc_data during quiesce"
    },
    {
        "commit": "228339662b398a59b3560cd571deb8b25b253c7e",
        "message": "If an application calls io_uring_enter(2) with a timespec passed in,\nconvert that timespec to ktime_t rather than jiffies. The latter does\nnot provide the granularity the application may expect, and may in\nfact provided different granularity on different systems, depending\non what the HZ value is configured at.\n\nTurn the timespec into an absolute ktime_t, and use that with\nschedule_hrtimeout() instead.\n\nLink: https://github.com/axboe/liburing/issues/531\nCc: stable@vger.kernel.org\nReported-by: Bob Chen <chenbo.chen@alibaba-inc.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc6",
        "release_date": "2022-02-21 05:55:42 -0700 io_uring: don't convert to jiffies for waiting on timeouts"
    },
    {
        "commit": "f240762f88b4b1b58561939ffd44837759756477",
        "message": "Looping ~65535 times doing kmalloc() calls can trigger soft lockups,\nespecially with DEBUG features (like KASAN).\n\n[  253.536212] watchdog: BUG: soft lockup - CPU#64 stuck for 26s! [b219417889:12575]\n[  253.544433] Modules linked in: vfat fat i2c_mux_pca954x i2c_mux spidev cdc_acm xhci_pci xhci_hcd sha3_generic gq(O)\n[  253.544451] CPU: 64 PID: 12575 Comm: b219417889 Tainted: G S         O      5.17.0-smp-DEV #801\n[  253.544457] RIP: 0010:kernel_text_address (./include/asm-generic/sections.h:192 ./include/linux/kallsyms.h:29 kernel/extable.c:67 kernel/extable.c:98)\n[  253.544464] Code: 0f 93 c0 48 c7 c1 e0 63 d7 a4 48 39 cb 0f 92 c1 20 c1 0f b6 c1 5b 5d c3 90 0f 1f 44 00 00 55 48 89 e5 41 57 41 56 53 48 89 fb <48> c7 c0 00 00 80 a0 41 be 01 00 00 00 48 39 c7 72 0c 48 c7 c0 40\n[  253.544468] RSP: 0018:ffff8882d8baf4c0 EFLAGS: 00000246\n[  253.544471] RAX: 1ffff1105b175e00 RBX: ffffffffa13ef09a RCX: 00000000a13ef001\n[  253.544474] RDX: ffffffffa13ef09a RSI: ffff8882d8baf558 RDI: ffffffffa13ef09a\n[  253.544476] RBP: ffff8882d8baf4d8 R08: ffff8882d8baf5e0 R09: 0000000000000004\n[  253.544479] R10: ffff8882d8baf5e8 R11: ffffffffa0d59a50 R12: ffff8882eab20380\n[  253.544481] R13: ffffffffa0d59a50 R14: dffffc0000000000 R15: 1ffff1105b175eb0\n[  253.544483] FS:  00000000016d3380(0000) GS:ffff88af48c00000(0000) knlGS:0000000000000000\n[  253.544486] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n[  253.544488] CR2: 00000000004af0f0 CR3: 00000002eabfa004 CR4: 00000000003706e0\n[  253.544491] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n[  253.544492] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n[  253.544494] Call Trace:\n[  253.544496]  <TASK>\n[  253.544498] ? io_queue_sqe (fs/io_uring.c:7143)\n[  253.544505] __kernel_text_address (kernel/extable.c:78)\n[  253.544508] unwind_get_return_address (arch/x86/kernel/unwind_frame.c:19)\n[  253.544514] arch_stack_walk (arch/x86/kernel/stacktrace.c:27)\n[  253.544517] ? io_queue_sqe (fs/io_uring.c:7143)\n[  253.544521] stack_trace_save (kernel/stacktrace.c:123)\n[  253.544527] ____kasan_kmalloc (mm/kasan/common.c:39 mm/kasan/common.c:45 mm/kasan/common.c:436 mm/kasan/common.c:515)\n[  253.544531] ? ____kasan_kmalloc (mm/kasan/common.c:39 mm/kasan/common.c:45 mm/kasan/common.c:436 mm/kasan/common.c:515)\n[  253.544533] ? __kasan_kmalloc (mm/kasan/common.c:524)\n[  253.544535] ? kmem_cache_alloc_trace (./include/linux/kasan.h:270 mm/slab.c:3567)\n[  253.544541] ? io_issue_sqe (fs/io_uring.c:4556 fs/io_uring.c:4589 fs/io_uring.c:6828)\n[  253.544544] ? __io_queue_sqe (fs/io_uring.c:?)\n[  253.544551] __kasan_kmalloc (mm/kasan/common.c:524)\n[  253.544553] kmem_cache_alloc_trace (./include/linux/kasan.h:270 mm/slab.c:3567)\n[  253.544556] ? io_issue_sqe (fs/io_uring.c:4556 fs/io_uring.c:4589 fs/io_uring.c:6828)\n[  253.544560] io_issue_sqe (fs/io_uring.c:4556 fs/io_uring.c:4589 fs/io_uring.c:6828)\n[  253.544564] ? __kasan_slab_alloc (mm/kasan/common.c:45 mm/kasan/common.c:436 mm/kasan/common.c:469)\n[  253.544567] ? __kasan_slab_alloc (mm/kasan/common.c:39 mm/kasan/common.c:45 mm/kasan/common.c:436 mm/kasan/common.c:469)\n[  253.544569] ? kmem_cache_alloc_bulk (mm/slab.h:732 mm/slab.c:3546)\n[  253.544573] ? __io_alloc_req_refill (fs/io_uring.c:2078)\n[  253.544578] ? io_submit_sqes (fs/io_uring.c:7441)\n[  253.544581] ? __se_sys_io_uring_enter (fs/io_uring.c:10154 fs/io_uring.c:10096)\n[  253.544584] ? __x64_sys_io_uring_enter (fs/io_uring.c:10096)\n[  253.544587] ? do_syscall_64 (arch/x86/entry/common.c:50 arch/x86/entry/common.c:80)\n[  253.544590] ? entry_SYSCALL_64_after_hwframe (??:?)\n[  253.544596] __io_queue_sqe (fs/io_uring.c:?)\n[  253.544600] io_queue_sqe (fs/io_uring.c:7143)\n[  253.544603] io_submit_sqe (fs/io_uring.c:?)\n[  253.544608] io_submit_sqes (fs/io_uring.c:?)\n[  253.544612] __se_sys_io_uring_enter (fs/io_uring.c:10154 fs/io_uring.c:10096)\n[  253.544616] __x64_sys_io_uring_enter (fs/io_uring.c:10096)\n[  253.544619] do_syscall_64 (arch/x86/entry/common.c:50 arch/x86/entry/common.c:80)\n[  253.544623] entry_SYSCALL_64_after_hwframe (??:?)\n\nFixes: ddf0322db79c (\"io_uring: add IORING_OP_PROVIDE_BUFFERS\")\nSigned-off-by: Eric Dumazet <edumazet@google.com>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Pavel Begunkov <asml.silence@gmail.com>\nCc: io-uring <io-uring@vger.kernel.org>\nReported-by: syzbot <syzkaller@googlegroups.com>\nLink: https://lore.kernel.org/r/20220215041003.2394784-1-eric.dumazet@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc6",
        "release_date": "2022-02-15 07:47:16 -0700 io_uring: add a schedule point in io_add_buffers()"
    },
    {
        "commit": "199b7f84c428d90e1858dafa583f7b1d587cbeb8",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix a false-positive warning from an older gcc (Alviro)\n\n - Allow oom killer invocations from io_uring_setup (Shakeel)\n\n* tag 'io_uring-5.17-2022-02-11' of git://git.kernel.dk/linux-block:\n  mm: io_uring: allow oom-killer from io_uring_setup\n  io_uring: Clean up a false-positive warning from GCC 9.3.0",
        "kernel_version": "v5.17-rc4",
        "release_date": "2022-02-11 11:18:42 -0800 Merge tag 'io_uring-5.17-2022-02-11' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "0a3f1e0beacf6cc8ae5f846b0641c1df476e83d6",
        "message": "On an overcommitted system which is running multiple workloads of\nvarying priorities, it is preferred to trigger an oom-killer to kill a\nlow priority workload than to let the high priority workload receiving\nENOMEMs. On our memory overcommitted systems, we are seeing a lot of\nENOMEMs instead of oom-kills because io_uring_setup callchain is using\n__GFP_NORETRY gfp flag which avoids the oom-killer. Let's remove it and\nallow the oom-killer to kill a lower priority job.\n\nSigned-off-by: Shakeel Butt <shakeelb@google.com>\nLink: https://lore.kernel.org/r/20220125051736.2981459-1-shakeelb@google.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc4",
        "release_date": "2022-02-07 08:44:01 -0700 mm: io_uring: allow oom-killer from io_uring_setup"
    },
    {
        "commit": "0d7c1153d9291197c1dc473cfaade77acb874b4b",
        "message": "In io_recv(), if import_single_range() fails, the @flags variable is\nuninitialized, then it will goto out_free.\n\nAfter the goto, the compiler doesn't know that (ret < min_ret) is\nalways true, so it thinks the \"if ((flags & MSG_WAITALL) ...\"  path\ncould be taken.\n\nThe complaint comes from gcc-9 (Debian 9.3.0-22) 9.3.0:\n```\n  fs/io_uring.c:5238 io_recvfrom() error: uninitialized symbol 'flags'\n```\nFix this by bypassing the @ret and @flags check when\nimport_single_range() fails.\n\nReasons:\n 1. import_single_range() only returns -EFAULT when it fails.\n 2. At that point, @flags is uninitialized and shouldn't be read.\n\nReported-by: kernel test robot <lkp@intel.com>\nReported-by: Dan Carpenter <dan.carpenter@oracle.com>\nReported-by: \"Chen, Rong A\" <rong.a.chen@intel.com>\nLink: https://lore.gnuweeb.org/timl/d33bb5a9-8173-f65b-f653-51fc0681c6d6@intel.com/\nCc: Pavel Begunkov <asml.silence@gmail.com>\nSuggested-by: Ammar Faizi <ammarfaizi2@gnuweeb.org>\nFixes: 7297ce3d59449de49d3c9e1f64ae25488750a1fc (\"io_uring: improve send/recv error handling\")\nSigned-off-by: Alviro Iskandar Setiawan <alviro.iskandar@gmail.com>\nSigned-off-by: Ammar Faizi <ammarfaizi2@gnuweeb.org>\nLink: https://lore.kernel.org/r/20220207140533.565411-1-ammarfaizi2@gnuweeb.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc4",
        "release_date": "2022-02-07 08:38:07 -0700 io_uring: Clean up a false-positive warning from GCC 9.3.0"
    },
    {
        "commit": "aea0b9f2486da8497f35c7114b764bf55e17c7ea",
        "message": "Make the name of the anon inode fd \"[landlock-ruleset]\" instead of\n\"landlock-ruleset\". This is minor but most anon inode fds already\ncarry square brackets around their name:\n\n    [eventfd]\n    [eventpoll]\n    [fanotify]\n    [fscontext]\n    [io_uring]\n    [pidfd]\n    [signalfd]\n    [timerfd]\n    [userfaultfd]\n\nFor the sake of consistency lets do the same for the landlock-ruleset anon\ninode fd that comes with landlock. We did the same in\n1cdc415f1083 (\"uapi, fsopen: use square brackets around \"fscontext\" [ver #2]\")\nfor the new mount api.\n\nCc: linux-security-module@vger.kernel.org\nSigned-off-by: Christian Brauner <christian.brauner@ubuntu.com>\nLink: https://lore.kernel.org/r/20211011133704.1704369-1-brauner@kernel.org\nCc: stable@vger.kernel.org\nSigned-off-by: Micka\u00ebl Sala\u00fcn <mic@linux.microsoft.com>",
        "kernel_version": "v5.18-rc1",
        "release_date": "2022-02-04 14:07:44 +0100 landlock: Use square brackets around \"landlock-ruleset\""
    },
    {
        "commit": "3b58e9f3a301e175d2de6f7fa1e834c4605e1c73",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Just two small fixes this time:\n\n   - Fix a bug that can lead to node registration taking 1 second, when\n     it should finish much quicker (Dylan)\n\n   - Remove an unused argument from a function (Usama)\"\n\n* tag 'io_uring-5.17-2022-01-28' of git://git.kernel.dk/linux-block:\n  io_uring: remove unused argument from io_rsrc_node_alloc\n  io_uring: fix bug in slow unregistering of nodes",
        "kernel_version": "v5.17-rc2",
        "release_date": "2022-01-29 14:53:07 +0200 Merge tag 'io_uring-5.17-2022-01-28' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "f6133fbd373811066c8441737e65f384c8f31974",
        "message": "io_ring_ctx is not used in the function.\n\nSigned-off-by: Usama Arif <usama.arif@bytedance.com>\nLink: https://lore.kernel.org/r/20220127140444.4016585-1-usama.arif@bytedance.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc2",
        "release_date": "2022-01-27 10:18:53 -0700 io_uring: remove unused argument from io_rsrc_node_alloc"
    },
    {
        "commit": "b36a2050040b2d839bdc044007cdd57101d7f881",
        "message": "In some cases io_rsrc_ref_quiesce will call io_rsrc_node_switch_start,\nand then immediately flush the delayed work queue &ctx->rsrc_put_work.\n\nHowever the percpu_ref_put does not immediately destroy the node, it\nwill be called asynchronously via RCU. That ends up with\nio_rsrc_node_ref_zero only being called after rsrc_put_work has been\nflushed, and so the process ends up sleeping for 1 second unnecessarily.\n\nThis patch executes the put code immediately if we are busy\nquiescing.\n\nFixes: 4a38aed2a0a7 (\"io_uring: batch reap of dead file registrations\")\nSigned-off-by: Dylan Yudaken <dylany@fb.com>\nLink: https://lore.kernel.org/r/20220121123856.3557884-1-dylany@fb.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc2",
        "release_date": "2022-01-23 09:12:53 -0700 io_uring: fix bug in slow unregistering of nodes"
    },
    {
        "commit": "f3a78227eef20c0ba13bbf9401f0a340bca3ad16",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix the io_uring POLLFREE handling, similarly to how it was done for\n   aio (Pavel)\n\n - Remove (now) unused function (Jiapeng)\n\n - Small series fixing an issue with work cancelations. A window exists\n   where work isn't locatable in the pending list, and isn't active in a\n   worker yet either. (me)\n\n* tag 'io_uring-5.17-2022-01-21' of git://git.kernel.dk/linux-block:\n  io-wq: delete dead lock shuffling code\n  io_uring: perform poll removal even if async work removal is successful\n  io-wq: add intermediate work step between pending list and active work\n  io-wq: perform both unstarted and started work cancelations in one go\n  io-wq: invoke work cancelation with wqe->lock held\n  io-wq: make io_worker lock a raw spinlock\n  io-wq: remove useless 'work' argument to __io_worker_busy()\n  io_uring: fix UAF due to missing POLLFREE handling\n  io_uring: Remove unused function req_ref_put",
        "kernel_version": "v5.17-rc1",
        "release_date": "2022-01-21 16:07:21 +0200 Merge tag 'io_uring-5.17-2022-01-21' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "ccbf726171b7328f800bc98005132fd77eb1a175",
        "message": "An active work can have poll armed, hence it's not enough to just do\nthe async work removal and return the value if it's different from \"not\nfound\". Rather than make poll removal special, just fall through to do\nthe remaining type lookups and removals.\n\nReported-by: Florian Fischer <florian.fl.fischer@fau.de>\nLink: https://lore.kernel.org/io-uring/20220118151337.fac6cthvbnu7icoc@pasture/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2022-01-18 19:28:43 -0700 io_uring: perform poll removal even if async work removal is successful"
    },
    {
        "commit": "791f3465c4afde02d7f16cf7424ca87070b69396",
        "message": "Fixes a problem described in 50252e4b5e989\n(\"aio: fix use-after-free due to missing POLLFREE handling\")\nand copies the approach used there.\n\nIn short, we have to forcibly eject a poll entry when we meet POLLFREE.\nWe can't rely on io_poll_get_ownership() as can't wait for potentially\nrunning tw handlers, so we use the fact that wqs are RCU freed. See\nEric's patch and comments for more details.\n\nReported-by: Eric Biggers <ebiggers@google.com>\nLink: https://lore.kernel.org/r/20211209010455.42744-6-ebiggers@kernel.org\nReported-and-tested-by: syzbot+5426c7ed6868c705ca14@syzkaller.appspotmail.com\nFixes: 221c5eb233823 (\"io_uring: add support for IORING_OP_POLL\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4ed56b6f548f7ea337603a82315750449412748a.1642161259.git.asml.silence@gmail.com\n[axboe: drop non-functional change from patch]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2022-01-14 06:48:35 -0700 io_uring: fix UAF due to missing POLLFREE handling"
    },
    {
        "commit": "e1a7aa25ff45636a6c1930bf2430c8b802e93d9c",
        "message": "Pull SCSI updates from James Bottomley:\n \"This series consists of the usual driver updates (ufs, pm80xx, lpfc,\n  mpi3mr, mpt3sas, hisi_sas, libsas) and minor updates and bug fixes.\n\n  The most impactful change is likely the switch from GFP_DMA to\n  GFP_KERNEL in a bunch of drivers, but even that shouldn't affect too\n  many people\"\n\n* tag 'scsi-misc' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi: (121 commits)\n  scsi: mpi3mr: Bump driver version to 8.0.0.61.0\n  scsi: mpi3mr: Fixes around reply request queues\n  scsi: mpi3mr: Enhanced Task Management Support Reply handling\n  scsi: mpi3mr: Use TM response codes from MPI3 headers\n  scsi: mpi3mr: Add io_uring interface support in I/O-polled mode\n  scsi: mpi3mr: Print cable mngnt and temp threshold events\n  scsi: mpi3mr: Support Prepare for Reset event\n  scsi: mpi3mr: Add Event acknowledgment logic\n  scsi: mpi3mr: Gracefully handle online FW update operation\n  scsi: mpi3mr: Detect async reset that occurred in firmware\n  scsi: mpi3mr: Add IOC reinit function\n  scsi: mpi3mr: Handle offline FW activation in graceful manner\n  scsi: mpi3mr: Code refactor of IOC init - part2\n  scsi: mpi3mr: Code refactor of IOC init - part1\n  scsi: mpi3mr: Fault IOC when internal command gets timeout\n  scsi: mpi3mr: Display IOC firmware package version\n  scsi: mpi3mr: Handle unaligned PLL in unmap cmnds\n  scsi: mpi3mr: Increase internal cmnds timeout to 60s\n  scsi: mpi3mr: Do access status validation before adding devices\n  scsi: mpi3mr: Add support for PCIe Managed Switch SES device\n  ...",
        "kernel_version": "v5.17-rc1",
        "release_date": "2022-01-14 14:37:34 +0100 Merge tag 'scsi-misc' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi"
    },
    {
        "commit": "c84b8a3fef663933007e885535591b9d30bdc860",
        "message": "Fix the following clang warnings:\n\nfs/io_uring.c:1195:20: warning: unused function 'req_ref_put'\n[-Wunused-function].\n\nFixes: aa43477b0402 (\"io_uring: poll rework\")\nReported-by: Abaci Robot <abaci@linux.alibaba.com>\nSigned-off-by: Jiapeng Chong <jiapeng.chong@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20220113162005.3011-1-jiapeng.chong@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2022-01-13 12:43:05 -0700 io_uring: Remove unused function req_ref_put"
    },
    {
        "commit": "42a7b4ed45e7667836fae4fb0e1ac6340588b1b0",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - Support for prioritized work completions (Hao)\n\n - Simplification of reissue (Pavel)\n\n - Add support for CQE skip (Pavel)\n\n - Memory leak fix going to 5.15-stable (Pavel)\n\n - Re-write of internal poll. This both cleans up that code, and gets us\n   ready to fix the POLLFREE issue (Pavel)\n\n - Various cleanups (GuoYong, Pavel, Hao)\n\n* tag 'for-5.17/io_uring-2022-01-11' of git://git.kernel.dk/linux-block: (31 commits)\n  io_uring: fix not released cached task refs\n  io_uring: remove redundant tab space\n  io_uring: remove unused function parameter\n  io_uring: use completion batching for poll rem/upd\n  io_uring: single shot poll removal optimisation\n  io_uring: poll rework\n  io_uring: kill poll linking optimisation\n  io_uring: move common poll bits\n  io_uring: refactor poll update\n  io_uring: remove double poll on poll update\n  io_uring: code clean for some ctx usage\n  io_uring: batch completion in prior_task_list\n  io_uring: split io_req_complete_post() and add a helper\n  io_uring: add helper for task work execution code\n  io_uring: add a priority tw list for irq completion work\n  io-wq: add helper to merge two wq_lists\n  io_uring: reuse io_req_task_complete for timeouts\n  io_uring: tweak iopoll CQE_SKIP event counting\n  io_uring: simplify selected buf handling\n  io_uring: move up io_put_kbuf() and io_put_rw_kbuf()\n  ...",
        "kernel_version": "v5.17-rc1",
        "release_date": "2022-01-12 10:20:35 -0800 Merge tag 'for-5.17/io_uring-2022-01-11' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "3cc7fdb9f90a25ae92250bf9e6cf3b9556b230e9",
        "message": "tctx_task_work() may get run after io_uring cancellation and so there\nwill be no one to put cached in tctx task refs that may have been added\nback by tw handlers using inline completion infra, Call\nio_uring_drop_tctx_refs() at the end of the main tw handler to release\nthem.\n\nCc: stable@vger.kernel.org # 5.15+\nReported-by: Lukas Bulwahn <lukas.bulwahn@gmail.com>\nFixes: e98e49b2bbf7 (\"io_uring: extend task put optimisations\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/69f226b35fbdb996ab799a8bbc1c06bf634ccec1.1641688805.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2022-01-09 09:22:49 -0700 io_uring: fix not released cached task refs"
    },
    {
        "commit": "f51d46d0e7cb5b8494aa534d276a9d8915a2443d",
        "message": "commit 021a24460dc2 (\"block: add QUEUE_FLAG_NOWAIT\") added support\nfor checking whether a given bdev supports handling of REQ_NOWAIT or not.\nSince then commit 6abc49468eea (\"dm: add support for REQ_NOWAIT and enable\nit for linear target\") added support for REQ_NOWAIT for dm. This uses\na similar approach to incorporate REQ_NOWAIT for md based bios.\n\nThis patch was tested using t/io_uring tool within FIO. A nvme drive\nwas partitioned into 2 partitions and a simple raid 0 configuration\n/dev/md0 was created.\n\nmd0 : active raid0 nvme4n1p1[1] nvme4n1p2[0]\n      937423872 blocks super 1.2 512k chunks\n\nBefore patch:\n\n$ ./t/io_uring /dev/md0 -p 0 -a 0 -d 1 -r 100\n\nRunning top while the above runs:\n\n$ ps -eL | grep $(pidof io_uring)\n\n  38396   38396 pts/2    00:00:00 io_uring\n  38396   38397 pts/2    00:00:15 io_uring\n  38396   38398 pts/2    00:00:13 iou-wrk-38397\n\nWe can see iou-wrk-38397 io worker thread created which gets created\nwhen io_uring sees that the underlying device (/dev/md0 in this case)\ndoesn't support nowait.\n\nAfter patch:\n\n$ ./t/io_uring /dev/md0 -p 0 -a 0 -d 1 -r 100\n\nRunning top while the above runs:\n\n$ ps -eL | grep $(pidof io_uring)\n\n  38341   38341 pts/2    00:10:22 io_uring\n  38341   38342 pts/2    00:10:37 io_uring\n\nAfter running this patch, we don't see any io worker thread\nbeing created which indicated that io_uring saw that the\nunderlying device does support nowait. This is the exact behaviour\nnoticed on a dm device which also supports nowait.\n\nFor all the other raid personalities except raid0, we would need\nto train pieces which involves make_request fn in order for them\nto correctly handle REQ_NOWAIT.\n\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Vishal Verma <vverma@digitalocean.com>\nSigned-off-by: Song Liu <song@kernel.org>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2022-01-06 08:37:02 -0800 md: add support for REQ_NOWAIT"
    },
    {
        "commit": "c0235652ee5194fc75926daa580817e63ceb37ab",
        "message": "When show fdinfo, SqMask follow two tab space, which is inconsistent with\nother parameters. Remove one, so it lines up nicely.\n\nSigned-off-by: GuoYong Zheng <zhenggy@chinatelecom.cn>\nLink: https://lore.kernel.org/r/1641377585-1891-1-git-send-email-zhenggy@chinatelecom.cn\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2022-01-05 12:31:37 -0700 io_uring: remove redundant tab space"
    },
    {
        "commit": "00f6e68b8d59bf006db54e3e257684f44d26195c",
        "message": "Parameter res2 is not used in __io_complete_rw, remove it.\n\nFixes: 6b19b766e8f0 (\"fs: get rid of the res2 iocb->ki_complete argument\")\nSigned-off-by: GuoYong Zheng <zhenggy@chinatelecom.cn>\nLink: https://lore.kernel.org/r/1641377522-1851-1-git-send-email-zhenggy@chinatelecom.cn\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2022-01-05 12:29:26 -0700 io_uring: remove unused function parameter"
    },
    {
        "commit": "cc8e9ba71a8626bd322d1945a8fc0c8a52131a63",
        "message": "Use __io_req_complete() in io_poll_update(), so we can utilise\ncompletion batching for both update/remove request and the poll\nwe're killing (if any).\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e2bdc6c5abd9e9b80f09b86d8823eb1c780362cd.1639605189.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-28 09:51:15 -0800 io_uring: use completion batching for poll rem/upd"
    },
    {
        "commit": "eb0089d629ba413ebf820733ad11b4b2bed45514",
        "message": "We don't need to poll oneshot request if we've got a desired mask in\nio_poll_wake(), task_work will clean it up correctly, but as we already\nhold a wq spinlock, we can remove ourselves and save on additional\nspinlocking in io_poll_remove_entries().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ee170a344a18c9ef36b554d806c64caadfd61c31.1639605189.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-28 09:51:15 -0800 io_uring: single shot poll removal optimisation"
    },
    {
        "commit": "aa43477b040251f451db0d844073ac00a8ab66ee",
        "message": "It's not possible to go forward with the current state of io_uring\npolling, we need a more straightforward and easier synchronisation.\nThere are a lot of problems with how it is at the moment, including\nmissing events on rewait.\n\nThe main idea here is to introduce a notion of request ownership while\npolling, no one but the owner can modify any part but ->poll_refs of\nstruct io_kiocb, that grants us protection against all sorts of races.\n\nMain users of such exclusivity are poll task_work handler, so before\nqueueing a tw one should have/acquire ownership, which will be handed\noff to the tw handler.\nThe other user is __io_arm_poll_handler() do initial poll arming. It\nstarts taking the ownership, so tw handlers won't be run until it's\nreleased later in the function after vfs_poll. note: also prevents\nraces in __io_queue_proc().\nPoll wake/etc. may not be able to get ownership, then they need to\nincrease the poll refcount and the task_work should notice it and retry\nif necessary, see io_poll_check_events().\nThere is also IO_POLL_CANCEL_FLAG flag to notify that we want to kill\nrequest.\n\nIt makes cancellations more reliable, enables double multishot polling,\nfixes double poll rewait, fixes missing poll events and fixes another\nbunch of races.\n\nEven though it adds some overhead for new refcounting, and there are a\ncouple of nice performance wins:\n- no req->refs refcounting for poll requests anymore\n- if the data is already there (once measured for some test to be 1-2%\n  of all apoll requests), it removes it doesn't add atomics and removes\n  spin_lock/unlock pair.\n- works well with multishots, we don't do remove from queue / add to\n  queue for each new poll event.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6b652927c77ed9580ea4330ac5612f0e0848c946.1639605189.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-28 09:51:15 -0800 io_uring: poll rework"
    },
    {
        "commit": "ab1dab960b8352cee082db0f8a54dc92a948bfd7",
        "message": "With IORING_FEAT_FAST_POLL in place, io_put_req_find_next() for poll\nrequests doesn't make much sense, and in any case re-adding it\nshouldn't be a problem considering batching in tctx_task_work(). We can\nremove it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/15699682bf81610ec901d4e79d6da64baa9f70be.1639605189.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-28 09:51:15 -0800 io_uring: kill poll linking optimisation"
    },
    {
        "commit": "5641897a5e8fb8abeb07e89c71a788d3db3ec75e",
        "message": "Move some poll helpers/etc up, we'll need them there shortly\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6c5c3dba24c86aad5cd389a54a8c7412e6a0621d.1639605189.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-28 09:51:14 -0800 io_uring: move common poll bits"
    },
    {
        "commit": "2bbb146d96f4b45e17d6aeede300796bc1a96d68",
        "message": "Clean up io_poll_update() and unify cancellation paths for remove and\nupdate.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5937138b6265a1285220e2fab1b28132c1d73ce3.1639605189.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-28 09:51:14 -0800 io_uring: refactor poll update"
    },
    {
        "commit": "e840b4baf3cfb37e2ead4f649a45bb78178677ff",
        "message": "Before updating a poll request we should remove it from poll queues,\nincluding the double poll entry.\n\nFixes: b69de288e913 (\"io_uring: allow events and user_data update of running poll requests\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ac39e7f80152613603b8a6cc29a2b6063ac2434f.1639605189.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-28 09:51:14 -0800 io_uring: remove double poll on poll update"
    },
    {
        "commit": "a026fa5404316787c2104bec3f8ff506acf85b98",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Single fix for not clearing kiocb->ki_pos back to 0 for a stream,\n  destined for stable as well\"\n\n* tag 'io_uring-5.16-2021-12-23' of git://git.kernel.dk/linux-block:\n  io_uring: zero iocb->ki_pos for stream file types",
        "kernel_version": "v5.16-rc7",
        "release_date": "2021-12-23 15:32:07 -0800 Merge tag 'io_uring-5.16-2021-12-23' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "afd3a5793fe2a217513bc5eb2228a5ca8e8b556a",
        "message": "Add support for the io_uring interface in I/O-polled mode.\n\nThis feature is disabled in the driver by default. To enable the feature, a\nmodule parameter \"poll_queues\" has to be set with the desired number of\npolling queues.\n\nWhen the feature is enabled, the driver reserves a certain number of\noperational queue pairs for the poll_queues either from the available queue\npairs or creates additional queue pairs based on the operational queue\navailability.\n\nThe Polling queues will have corresponding IRQ and ISR functions as similar\nto default queues. However, the IRQ line is disabled by the driver for\npoll_queues.\n\nLink: https://lore.kernel.org/r/20211220141159.16117-22-sreekanth.reddy@broadcom.com\nSigned-off-by: Sreekanth Reddy <sreekanth.reddy@broadcom.com>\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-23 00:04:24 -0500 scsi: mpi3mr: Add io_uring interface support in I/O-polled mode"
    },
    {
        "commit": "7b9762a5e8837b92a027d58d396a9d27f6440c36",
        "message": "io_uring supports using offset == -1 for using the current file position,\nand we read that in as part of read/write command setup. For the non-iter\nread/write types we pass in NULL for the position pointer, but for the\niter types we should not be passing any anything but 0 for the position\nfor a stream.\n\nClear kiocb->ki_pos if the file is a stream, don't leave it as -1. If we\ndo, then the request will error with -ESPIPE.\n\nFixes: ba04291eb66e (\"io_uring: allow use of offset == -1 to mean file position\")\nLink: https://github.com/axboe/liburing/discussions/501\nReported-by: Samuel Williams <samuel.williams@oriontransfer.co.nz>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc7",
        "release_date": "2021-12-22 20:34:32 -0700 io_uring: zero iocb->ki_pos for stream file types"
    },
    {
        "commit": "cb29eee3b28c79f26aff9e396a55bf2cb831e1d9",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single fix, fixing an issue with the worker creation change\n  that was merged last week\"\n\n* tag 'io_uring-5.16-2021-12-17' of git://git.kernel.dk/linux-block:\n  io-wq: drop wqe lock before creating new worker",
        "kernel_version": "v5.16-rc6",
        "release_date": "2021-12-17 11:31:46 -0800 Merge tag 'io_uring-5.16-2021-12-17' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "33ce2aff7d340bf48875ccd80628c884cf8017ae",
        "message": "There are some functions doing ctx = req->ctx while still using\nreq->ctx, update those places.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20211214055904.61772-1-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-14 06:49:06 -0700 io_uring: code clean for some ctx usage"
    },
    {
        "commit": "d800c65c2d4eccebb27ffb7808e842d5b533823c",
        "message": "We have two io-wq creation paths:\n\n- On queue enqueue\n- When a worker goes to sleep\n\nThe latter invokes worker creation with the wqe->lock held, but that can\nrun into problems if we end up exiting and need to cancel the queued work.\nsyzbot caught this:\n\n============================================\nWARNING: possible recursive locking detected\n5.16.0-rc4-syzkaller #0 Not tainted\n--------------------------------------------\niou-wrk-6468/6471 is trying to acquire lock:\nffff88801aa98018 (&wqe->lock){+.+.}-{2:2}, at: io_worker_cancel_cb+0xb7/0x210 fs/io-wq.c:187\n\nbut task is already holding lock:\nffff88801aa98018 (&wqe->lock){+.+.}-{2:2}, at: io_wq_worker_sleeping+0xb6/0x140 fs/io-wq.c:700\n\nother info that might help us debug this:\n Possible unsafe locking scenario:\n\n       CPU0\n       ----\n  lock(&wqe->lock);\n  lock(&wqe->lock);\n\n *** DEADLOCK ***\n\n May be due to missing lock nesting notation\n\n1 lock held by iou-wrk-6468/6471:\n #0: ffff88801aa98018 (&wqe->lock){+.+.}-{2:2}, at: io_wq_worker_sleeping+0xb6/0x140 fs/io-wq.c:700\n\nstack backtrace:\nCPU: 1 PID: 6471 Comm: iou-wrk-6468 Not tainted 5.16.0-rc4-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nCall Trace:\n <TASK>\n __dump_stack lib/dump_stack.c:88 [inline]\n dump_stack_lvl+0x1dc/0x2d8 lib/dump_stack.c:106\n print_deadlock_bug kernel/locking/lockdep.c:2956 [inline]\n check_deadlock kernel/locking/lockdep.c:2999 [inline]\n validate_chain+0x5984/0x8240 kernel/locking/lockdep.c:3788\n __lock_acquire+0x1382/0x2b00 kernel/locking/lockdep.c:5027\n lock_acquire+0x19f/0x4d0 kernel/locking/lockdep.c:5637\n __raw_spin_lock include/linux/spinlock_api_smp.h:133 [inline]\n _raw_spin_lock+0x2a/0x40 kernel/locking/spinlock.c:154\n io_worker_cancel_cb+0xb7/0x210 fs/io-wq.c:187\n io_wq_cancel_tw_create fs/io-wq.c:1220 [inline]\n io_queue_worker_create+0x3cf/0x4c0 fs/io-wq.c:372\n io_wq_worker_sleeping+0xbe/0x140 fs/io-wq.c:701\n sched_submit_work kernel/sched/core.c:6295 [inline]\n schedule+0x67/0x1f0 kernel/sched/core.c:6323\n schedule_timeout+0xac/0x300 kernel/time/timer.c:1857\n wait_woken+0xca/0x1b0 kernel/sched/wait.c:460\n unix_msg_wait_data net/unix/unix_bpf.c:32 [inline]\n unix_bpf_recvmsg+0x7f9/0xe20 net/unix/unix_bpf.c:77\n unix_stream_recvmsg+0x214/0x2c0 net/unix/af_unix.c:2832\n sock_recvmsg_nosec net/socket.c:944 [inline]\n sock_recvmsg net/socket.c:962 [inline]\n sock_read_iter+0x3a7/0x4d0 net/socket.c:1035\n call_read_iter include/linux/fs.h:2156 [inline]\n io_iter_do_read fs/io_uring.c:3501 [inline]\n io_read fs/io_uring.c:3558 [inline]\n io_issue_sqe+0x144c/0x9590 fs/io_uring.c:6671\n io_wq_submit_work+0x2d8/0x790 fs/io_uring.c:6836\n io_worker_handle_work+0x808/0xdd0 fs/io-wq.c:574\n io_wqe_worker+0x395/0x870 fs/io-wq.c:630\n ret_from_fork+0x1f/0x30\n\nWe can safely drop the lock before doing work creation, making the two\ncontexts the same in that regard.\n\nReported-by: syzbot+b18b8be69df33a3918e9@syzkaller.appspotmail.com\nFixes: 71a85387546e (\"io-wq: check for wq exit after adding new worker task_work\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc6",
        "release_date": "2021-12-13 09:04:01 -0700 io-wq: drop wqe lock before creating new worker"
    },
    {
        "commit": "f152165ada75e1efc7bffbea8a188652bcd04f32",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A few fixes that are all bound for stable:\n\n   - Two syzbot reports for io-wq that turned out to be separate fixes,\n     but ultimately very closely related\n\n   - io_uring task_work running on cancelations\"\n\n* tag 'io_uring-5.16-2021-12-10' of git://git.kernel.dk/linux-block:\n  io-wq: check for wq exit after adding new worker task_work\n  io_uring: ensure task_work gets run as part of cancelations\n  io-wq: remove spurious bit clear on task_work addition",
        "kernel_version": "v5.16-rc5",
        "release_date": "2021-12-11 09:19:44 -0800 Merge tag 'io_uring-5.16-2021-12-10' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "78a780602075d8b00c98070fa26e389b3b3efa72",
        "message": "If we successfully cancel a work item but that work item needs to be\nprocessed through task_work, then we can be sleeping uninterruptibly\nin io_uring_cancel_generic() and never process it. Hence we don't\nmake forward progress and we end up with an uninterruptible sleep\nwarning.\n\nWhile in there, correct a comment that should be IFF, not IIF.\n\nReported-and-tested-by: syzbot+21e6887c0be14181206d@syzkaller.appspotmail.com\nCc: stable@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc5",
        "release_date": "2021-12-10 13:56:28 -0700 io_uring: ensure task_work gets run as part of cancelations"
    },
    {
        "commit": "42288cb44c4b5fff7653bc392b583a2b8bd6a8c0",
        "message": "Several ->poll() implementations are special in that they use a\nwaitqueue whose lifetime is the current task, rather than the struct\nfile as is normally the case.  This is okay for blocking polls, since a\nblocking poll occurs within one task; however, non-blocking polls\nrequire another solution.  This solution is for the queue to be cleared\nbefore it is freed, using 'wake_up_poll(wq, EPOLLHUP | POLLFREE);'.\n\nHowever, that has a bug: wake_up_poll() calls __wake_up() with\nnr_exclusive=1.  Therefore, if there are multiple \"exclusive\" waiters,\nand the wakeup function for the first one returns a positive value, only\nthat one will be called.  That's *not* what's needed for POLLFREE;\nPOLLFREE is special in that it really needs to wake up everyone.\n\nConsidering the three non-blocking poll systems:\n\n- io_uring poll doesn't handle POLLFREE at all, so it is broken anyway.\n\n- aio poll is unaffected, since it doesn't support exclusive waits.\n  However, that's fragile, as someone could add this feature later.\n\n- epoll doesn't appear to be broken by this, since its wakeup function\n  returns 0 when it sees POLLFREE.  But this is fragile.\n\nAlthough there is a workaround (see epoll), it's better to define a\nfunction which always sends POLLFREE to all waiters.  Add such a\nfunction.  Also make it verify that the queue really becomes empty after\nall waiters have been woken up.\n\nReported-by: Linus Torvalds <torvalds@linux-foundation.org>\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/r/20211209010455.42744-2-ebiggers@kernel.org\nSigned-off-by: Eric Biggers <ebiggers@google.com>",
        "kernel_version": "v5.16-rc5",
        "release_date": "2021-12-09 10:49:56 -0800 wait: add wake_up_pollfree()"
    },
    {
        "commit": "f28c240e7152462f0750a8939db28d985ecf7c67",
        "message": "In previous patches, we have already gathered some tw with\nio_req_task_complete() as callback in prior_task_list, let's complete\nthem in batch while we cannot grab uring lock. In this way, we batch\nthe req_complete_post path.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20211208052125.351587-1-haoxu@linux.alibaba.com\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-08 11:34:48 -0700 io_uring: batch completion in prior_task_list"
    },
    {
        "commit": "a37fae8aaa62b05c11f059fee8fedf4313975abd",
        "message": "Split io_req_complete_post(), this is a prep for the next patch.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20211207093951.247840-5-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-07 15:02:42 -0700 io_uring: split io_req_complete_post() and add a helper"
    },
    {
        "commit": "9f8d032a364b2b579c6ce5a62b967056f8711e69",
        "message": "Add a helper for task work execution code. We will use it later.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20211207093951.247840-4-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-07 15:01:57 -0700 io_uring: add helper for task work execution code"
    },
    {
        "commit": "4813c3779261fab4067edea28155a98c65a41b5f",
        "message": "Now we have a lot of task_work users, some are just to complete a req\nand generate a cqe. Let's put the work to a new tw list which has a\nhigher priority, so that it can be handled quickly and thus to reduce\navg req latency and users can issue next round of sqes earlier.\nAn explanatory case:\n\norigin timeline:\n    submit_sqe-->irq-->add completion task_work\n    -->run heavy work0~n-->run completion task_work\nnow timeline:\n    submit_sqe-->irq-->add completion task_work\n    -->run completion task_work-->run heavy work0~n\n\nLimitation: this optimization is only for those that submission and\nreaping process are in different threads. Otherwise anyhow we have to\nsubmit new sqes after returning to userspace, then the order of TWs\ndoesn't matter.\n\nTested this patch(and the following ones) by manually replace\n__io_queue_sqe() in io_queue_sqe() by io_req_task_queue() to construct\n'heavy' task works. Then test with fio:\n\nioengine=io_uring\nsqpoll=1\nthread=1\nbs=4k\ndirect=1\nrw=randread\ntime_based=1\nruntime=600\nrandrepeat=0\ngroup_reporting=1\nfilename=/dev/nvme0n1\n\nTried various iodepth.\nThe peak IOPS for this patch is 710K, while the old one is 665K.\nFor avg latency, difference shows when iodepth grow:\ndepth and avg latency(usec):\n\tdepth      new          old\n\t 1        7.05         7.10\n\t 2        8.47         8.60\n\t 4        10.42        10.42\n\t 8        13.78        13.22\n\t 16       27.41        24.33\n\t 32       49.40        53.08\n\t 64       102.53       103.36\n\t 128      196.98       205.61\n\t 256      372.99       414.88\n         512      747.23       791.30\n         1024     1472.59      1538.72\n         2048     3153.49      3329.01\n         4096     6387.86      6682.54\n         8192     12150.25     12774.14\n         16384    23085.58     26044.71\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20211207093951.247840-3-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-07 15:01:57 -0700 io_uring: add a priority tw list for irq completion work"
    },
    {
        "commit": "eaab9b57305496067e225155ca86bf77c9a982f7",
        "message": "The time spent in io_schedule() and also the interrupt latency are\nsignificant when submitting direct I/O to a UFS device. Hence this patch\nthat implements polling support. User space software can enable polling by\npassing the RWF_HIPRI flag to the preadv2() system call or the\nIORING_SETUP_IOPOLL flag to the io_uring interface.\n\nAlthough the block layer supports to partition the tag space for\ninterrupt-based completions (HCTX_TYPE_DEFAULT) purposes and polling\n(HCTX_TYPE_POLL), the choice has been made to use the same hardware queue\nfor both hctx types because partitioning the tag space would negatively\naffect performance.\n\nOn my test setup this patch increases IOPS from 2736 to 22000 (8x) for the\nfollowing test:\n\nfor hipri in 0 1; do\n    fio --ioengine=io_uring --iodepth=1 --rw=randread \\\n    --runtime=60 --time_based=1 --direct=1 --name=qd1 \\\n    --filename=/dev/block/sda --ioscheduler=none --gtod_reduce=1 \\\n    --norandommap --hipri=$hipri\ndone\n\nLink: https://lore.kernel.org/r/20211203231950.193369-18-bvanassche@acm.org\nTested-by: Bean Huo <beanhuo@micron.com>\nReviewed-by: Bean Huo <beanhuo@micron.com>\nSigned-off-by: Bart Van Assche <bvanassche@acm.org>\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-06 22:30:34 -0500 scsi: ufs: Implement polling support"
    },
    {
        "commit": "a90c8bf6590676035336ae98cc51bce1aeb96c33",
        "message": "With kbuf unification io_req_task_complete() is now a generic function,\nuse it for timeout's tw completions.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7142fa3cbaf3a4140d59bcba45cbe168cf40fac2.1638714983.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-05 08:56:24 -0700 io_uring: reuse io_req_task_complete for timeouts"
    },
    {
        "commit": "83a13a4181b0e874d1f196e11b953c3c9f009f68",
        "message": "When iopolling the userspace specifies the minimum number of \"events\" it\nexpects. Previously, we had one CQE per request, so the definition of\nan \"event\" was unequivocal, but that's not more the case anymore with\nREQ_F_CQE_SKIP.\n\nCurrently it counts the number of completed requests, replace it with\nthe number of posted CQEs. This allows users of the \"one CQE per link\"\nscheme to wait for all N links in a single syscall, which is not\npossible without the patch and requires extra context switches.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d5a965c4d2249827392037bbd0186f87fea49c55.1638714983.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-05 08:56:24 -0700 io_uring: tweak iopoll CQE_SKIP event counting"
    },
    {
        "commit": "d1fd1c201d750711e17377acb4914d3ea29a608c",
        "message": "As selected buffers are now stored in a separate field in a request, get\nrid of rw/recv specific helpers and simplify the code.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bd4a866d8d91b044f748c40efff9e4eacd07536e.1638714983.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-05 08:56:24 -0700 io_uring: simplify selected buf handling"
    },
    {
        "commit": "3648e5265cfa51492a65ee5a01f151807ec46dee",
        "message": "Move them up to avoid explicit declaration. We will use them in later\npatches.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3631243d6fc4a79bbba0cd62597fc8cd5be95924.1638714983.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-05 08:56:24 -0700 io_uring: move up io_put_kbuf() and io_put_rw_kbuf()"
    },
    {
        "commit": "8b9a02280ebe209ae14fc478b0285745544a257c",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single fix preventing repeated retries of task_work based io-wq\n  thread creation, fixing a regression from when io-wq was made more (a\n  bit too much) resilient against signals\"\n\n* tag 'io_uring-5.16-2021-12-03' of git://git.kernel.dk/linux-block:\n  io-wq: don't retry task_work creation failure on fatal conditions",
        "kernel_version": "v5.16-rc4",
        "release_date": "2021-12-04 08:34:59 -0800 Merge tag 'io_uring-5.16-2021-12-03' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "0a467d0fdd9594fbb449ebc93852533332c528fd",
        "message": "refcount_t is not as expensive as it used to be, but it's still more\nexpensive than the io_uring method of using atomic_t and just checking\nfor potential over/underflow.\n\nThis borrows that same implementation, which in turn is based on the\nmm implementation from Linus.\n\nReviewed-by: Keith Busch <kbusch@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-03 14:51:29 -0700 block: switch to atomic_t for request references"
    },
    {
        "commit": "2385ebf38f94d4f7761b1e9a4973d04753da02c2",
        "message": "Complete poll requests via blk_mq_add_to_batch() and\nblk_mq_end_request_batch(), so that we can cover batched complete\ncode path by running null_blk test.\n\nMeantime this way shows ~14% IOPS boost on 't/io_uring /dev/nullb0'\nin my test.\n\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nLink: https://lore.kernel.org/r/20211203081703.3506020-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-12-03 06:36:28 -0700 block: null_blk: batched complete poll requests"
    },
    {
        "commit": "2087009c74d41ab8579f08157bca55b7d0857ee5",
        "message": "Like commit f6223ff79966, timeout removal should also validate the\ntimespec that is being passed in.\n\nSigned-off-by: Ye Bin <yebin10@huawei.com>\nLink: https://lore.kernel.org/r/20211129041537.1936270-1-yebin10@huawei.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-11-29 06:46:39 -0700 io_uring: validate timespec for timeout removals"
    },
    {
        "commit": "86799cdfbcd2308cbad6c1dc983b81595b77b639",
        "message": "Pull more io_uring fixes from Jens Axboe:\n \"The locking fixup that was applied earlier this rc has both a deadlock\n  and IRQ safety issue, let's get that ironed out before -rc3. This\n  contains:\n\n   - Link traversal locking fix (Pavel)\n\n   - Cancelation fix (Pavel)\n\n   - Relocate cond_resched() for huge buffer chain freeing, avoiding a\n     softlockup warning (Ye)\n\n   - Fix timespec validation (Ye)\"\n\n* tag 'io_uring-5.16-2021-11-27' of git://git.kernel.dk/linux-block:\n  io_uring: Fix undefined-behaviour in io_issue_sqe\n  io_uring: fix soft lockup when call __io_remove_buffers\n  io_uring: fix link traversal locking\n  io_uring: fail cancellation for EXITING tasks",
        "kernel_version": "v5.16-rc3",
        "release_date": "2021-11-27 11:28:37 -0800 Merge tag 'io_uring-5.16-2021-11-27' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "f6223ff799666235a80d05f8137b73e5580077b9",
        "message": "We got issue as follows:\n================================================================================\nUBSAN: Undefined behaviour in ./include/linux/ktime.h:42:14\nsigned integer overflow:\n-4966321760114568020 * 1000000000 cannot be represented in type 'long long int'\nCPU: 1 PID: 2186 Comm: syz-executor.2 Not tainted 4.19.90+ #12\nHardware name: linux,dummy-virt (DT)\nCall trace:\n dump_backtrace+0x0/0x3f0 arch/arm64/kernel/time.c:78\n show_stack+0x28/0x38 arch/arm64/kernel/traps.c:158\n __dump_stack lib/dump_stack.c:77 [inline]\n dump_stack+0x170/0x1dc lib/dump_stack.c:118\n ubsan_epilogue+0x18/0xb4 lib/ubsan.c:161\n handle_overflow+0x188/0x1dc lib/ubsan.c:192\n __ubsan_handle_mul_overflow+0x34/0x44 lib/ubsan.c:213\n ktime_set include/linux/ktime.h:42 [inline]\n timespec64_to_ktime include/linux/ktime.h:78 [inline]\n io_timeout fs/io_uring.c:5153 [inline]\n io_issue_sqe+0x42c8/0x4550 fs/io_uring.c:5599\n __io_queue_sqe+0x1b0/0xbc0 fs/io_uring.c:5988\n io_queue_sqe+0x1ac/0x248 fs/io_uring.c:6067\n io_submit_sqe fs/io_uring.c:6137 [inline]\n io_submit_sqes+0xed8/0x1c88 fs/io_uring.c:6331\n __do_sys_io_uring_enter fs/io_uring.c:8170 [inline]\n __se_sys_io_uring_enter fs/io_uring.c:8129 [inline]\n __arm64_sys_io_uring_enter+0x490/0x980 fs/io_uring.c:8129\n invoke_syscall arch/arm64/kernel/syscall.c:53 [inline]\n el0_svc_common+0x374/0x570 arch/arm64/kernel/syscall.c:121\n el0_svc_handler+0x190/0x260 arch/arm64/kernel/syscall.c:190\n el0_svc+0x10/0x218 arch/arm64/kernel/entry.S:1017\n================================================================================\n\nAs ktime_set only judge 'secs' if big than KTIME_SEC_MAX, but if we pass\nnegative value maybe lead to overflow.\nTo address this issue, we must check if 'sec' is negative.\n\nSigned-off-by: Ye Bin <yebin10@huawei.com>\nLink: https://lore.kernel.org/r/20211118015907.844807-1-yebin10@huawei.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc3",
        "release_date": "2021-11-27 06:41:38 -0700 io_uring: Fix undefined-behaviour in io_issue_sqe"
    },
    {
        "commit": "1d0254e6b47e73222fd3d6ae95cccbaafe5b3ecf",
        "message": "I got issue as follows:\n[ 567.094140] __io_remove_buffers: [1]start ctx=0xffff8881067bf000 bgid=65533 buf=0xffff8881fefe1680\n[  594.360799] watchdog: BUG: soft lockup - CPU#2 stuck for 26s! [kworker/u32:5:108]\n[  594.364987] Modules linked in:\n[  594.365405] irq event stamp: 604180238\n[  594.365906] hardirqs last  enabled at (604180237): [<ffffffff93fec9bd>] _raw_spin_unlock_irqrestore+0x2d/0x50\n[  594.367181] hardirqs last disabled at (604180238): [<ffffffff93fbbadb>] sysvec_apic_timer_interrupt+0xb/0xc0\n[  594.368420] softirqs last  enabled at (569080666): [<ffffffff94200654>] __do_softirq+0x654/0xa9e\n[  594.369551] softirqs last disabled at (569080575): [<ffffffff913e1d6a>] irq_exit_rcu+0x1ca/0x250\n[  594.370692] CPU: 2 PID: 108 Comm: kworker/u32:5 Tainted: G            L    5.15.0-next-20211112+ #88\n[  594.371891] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS ?-20190727_073836-buildvm-ppc64le-16.ppc.fedoraproject.org-3.fc31 04/01/2014\n[  594.373604] Workqueue: events_unbound io_ring_exit_work\n[  594.374303] RIP: 0010:_raw_spin_unlock_irqrestore+0x33/0x50\n[  594.375037] Code: 48 83 c7 18 53 48 89 f3 48 8b 74 24 10 e8 55 f5 55 fd 48 89 ef e8 ed a7 56 fd 80 e7 02 74 06 e8 43 13 7b fd fb bf 01 00 00 00 <e8> f8 78 474\n[  594.377433] RSP: 0018:ffff888101587a70 EFLAGS: 00000202\n[  594.378120] RAX: 0000000024030f0d RBX: 0000000000000246 RCX: 1ffffffff2f09106\n[  594.379053] RDX: 0000000000000000 RSI: ffffffff9449f0e0 RDI: 0000000000000001\n[  594.379991] RBP: ffffffff9586cdc0 R08: 0000000000000001 R09: fffffbfff2effcab\n[  594.380923] R10: ffffffff977fe557 R11: fffffbfff2effcaa R12: ffff8881b8f3def0\n[  594.381858] R13: 0000000000000246 R14: ffff888153a8b070 R15: 0000000000000000\n[  594.382787] FS:  0000000000000000(0000) GS:ffff888399c00000(0000) knlGS:0000000000000000\n[  594.383851] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n[  594.384602] CR2: 00007fcbe71d2000 CR3: 00000000b4216000 CR4: 00000000000006e0\n[  594.385540] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n[  594.386474] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n[  594.387403] Call Trace:\n[  594.387738]  <TASK>\n[  594.388042]  find_and_remove_object+0x118/0x160\n[  594.389321]  delete_object_full+0xc/0x20\n[  594.389852]  kfree+0x193/0x470\n[  594.390275]  __io_remove_buffers.part.0+0xed/0x147\n[  594.390931]  io_ring_ctx_free+0x342/0x6a2\n[  594.392159]  io_ring_exit_work+0x41e/0x486\n[  594.396419]  process_one_work+0x906/0x15a0\n[  594.399185]  worker_thread+0x8b/0xd80\n[  594.400259]  kthread+0x3bf/0x4a0\n[  594.401847]  ret_from_fork+0x22/0x30\n[  594.402343]  </TASK>\n\nMessage from syslogd@localhost at Nov 13 09:09:54 ...\nkernel:watchdog: BUG: soft lockup - CPU#2 stuck for 26s! [kworker/u32:5:108]\n[  596.793660] __io_remove_buffers: [2099199]start ctx=0xffff8881067bf000 bgid=65533 buf=0xffff8881fefe1680\n\nWe can reproduce this issue by follow syzkaller log:\nr0 = syz_io_uring_setup(0x401, &(0x7f0000000300), &(0x7f0000003000/0x2000)=nil, &(0x7f0000ff8000/0x4000)=nil, &(0x7f0000000280)=<r1=>0x0, &(0x7f0000000380)=<r2=>0x0)\nsendmsg$ETHTOOL_MSG_FEATURES_SET(0xffffffffffffffff, &(0x7f0000003080)={0x0, 0x0, &(0x7f0000003040)={&(0x7f0000000040)=ANY=[], 0x18}}, 0x0)\nsyz_io_uring_submit(r1, r2, &(0x7f0000000240)=@IORING_OP_PROVIDE_BUFFERS={0x1f, 0x5, 0x0, 0x401, 0x1, 0x0, 0x100, 0x0, 0x1, {0xfffd}}, 0x0)\nio_uring_enter(r0, 0x3a2d, 0x0, 0x0, 0x0, 0x0)\n\nThe reason above issue  is 'buf->list' has 2,100,000 nodes, occupied cpu lead\nto soft lockup.\nTo solve this issue, we need add schedule point when do while loop in\n'__io_remove_buffers'.\nAfter add  schedule point we do regression, get follow data.\n[  240.141864] __io_remove_buffers: [1]start ctx=0xffff888170603000 bgid=65533 buf=0xffff8881116fcb00\n[  268.408260] __io_remove_buffers: [1]start ctx=0xffff8881b92d2000 bgid=65533 buf=0xffff888130c83180\n[  275.899234] __io_remove_buffers: [2099199]start ctx=0xffff888170603000 bgid=65533 buf=0xffff8881116fcb00\n[  296.741404] __io_remove_buffers: [1]start ctx=0xffff8881b659c000 bgid=65533 buf=0xffff8881010fe380\n[  305.090059] __io_remove_buffers: [2099199]start ctx=0xffff8881b92d2000 bgid=65533 buf=0xffff888130c83180\n[  325.415746] __io_remove_buffers: [1]start ctx=0xffff8881b92d1000 bgid=65533 buf=0xffff8881a17d8f00\n[  333.160318] __io_remove_buffers: [2099199]start ctx=0xffff8881b659c000 bgid=65533 buf=0xffff8881010fe380\n...\n\nFixes:8bab4c09f24e(\"io_uring: allow conditional reschedule for intensive iterators\")\nSigned-off-by: Ye Bin <yebin10@huawei.com>\nLink: https://lore.kernel.org/r/20211122024737.2198530-1-yebin10@huawei.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc3",
        "release_date": "2021-11-27 06:41:32 -0700 io_uring: fix soft lockup when call __io_remove_buffers"
    },
    {
        "commit": "6af3f48bf6156a7f02e91aca64e2927c4bebda03",
        "message": "WARNING: inconsistent lock state\n5.16.0-rc2-syzkaller #0 Not tainted\ninconsistent {HARDIRQ-ON-W} -> {IN-HARDIRQ-W} usage.\nffff888078e11418 (&ctx->timeout_lock\n){?.+.}-{2:2}\n, at: io_timeout_fn+0x6f/0x360 fs/io_uring.c:5943\n{HARDIRQ-ON-W} state was registered at:\n  [...]\n  spin_unlock_irq include/linux/spinlock.h:399 [inline]\n  __io_poll_remove_one fs/io_uring.c:5669 [inline]\n  __io_poll_remove_one fs/io_uring.c:5654 [inline]\n  io_poll_remove_one+0x236/0x870 fs/io_uring.c:5680\n  io_poll_remove_all+0x1af/0x235 fs/io_uring.c:5709\n  io_ring_ctx_wait_and_kill+0x1cc/0x322 fs/io_uring.c:9534\n  io_uring_release+0x42/0x46 fs/io_uring.c:9554\n  __fput+0x286/0x9f0 fs/file_table.c:280\n  task_work_run+0xdd/0x1a0 kernel/task_work.c:164\n  exit_task_work include/linux/task_work.h:32 [inline]\n  do_exit+0xc14/0x2b40 kernel/exit.c:832\n\n674ee8e1b4a41 (\"io_uring: correct link-list traversal locking\") fixed a\ndata race but introduced a possible deadlock and inconsistentcy in irq\nstates. E.g.\n\nio_poll_remove_all()\n    spin_lock_irq(timeout_lock)\n    io_poll_remove_one()\n        spin_lock/unlock_irq(poll_lock);\n    spin_unlock_irq(timeout_lock)\n\nAnother type of problem is freeing a request while holding\n->timeout_lock, which may leads to a deadlock in\nio_commit_cqring() -> io_flush_timeouts() and other places.\n\nHaving 3 nested locks is also too ugly. Add io_match_task_safe(), which\nwould briefly take and release timeout_lock for race prevention inside,\nso the actuall request cancellation / free / etc. code doesn't have it\ntaken.\n\nReported-by: syzbot+ff49a3059d49b0ca0eec@syzkaller.appspotmail.com\nReported-by: syzbot+847f02ec20a6609a328b@syzkaller.appspotmail.com\nReported-by: syzbot+3368aadcd30425ceb53b@syzkaller.appspotmail.com\nReported-by: syzbot+51ce8887cdef77c9ac83@syzkaller.appspotmail.com\nReported-by: syzbot+3cb756a49d2f394a9ee3@syzkaller.appspotmail.com\nFixes: 674ee8e1b4a41 (\"io_uring: correct link-list traversal locking\")\nCc: stable@kernel.org # 5.15+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/397f7ebf3f4171f1abe41f708ac1ecb5766f0b68.1637937097.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc3",
        "release_date": "2021-11-26 08:35:57 -0700 io_uring: fix link traversal locking"
    },
    {
        "commit": "617a89484debcd4e7999796d693cf0b77d2519de",
        "message": "WARNING: CPU: 1 PID: 20 at fs/io_uring.c:6269 io_try_cancel_userdata+0x3c5/0x640 fs/io_uring.c:6269\nCPU: 1 PID: 20 Comm: kworker/1:0 Not tainted 5.16.0-rc1-syzkaller #0\nWorkqueue: events io_fallback_req_func\nRIP: 0010:io_try_cancel_userdata+0x3c5/0x640 fs/io_uring.c:6269\nCall Trace:\n <TASK>\n io_req_task_link_timeout+0x6b/0x1e0 fs/io_uring.c:6886\n io_fallback_req_func+0xf9/0x1ae fs/io_uring.c:1334\n process_one_work+0x9b2/0x1690 kernel/workqueue.c:2298\n worker_thread+0x658/0x11f0 kernel/workqueue.c:2445\n kthread+0x405/0x4f0 kernel/kthread.c:327\n ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295\n </TASK>\n\nWe need original task's context to do cancellations, so if it's dying\nand the callback is executed in a fallback mode, fail the cancellation\nattempt.\n\nFixes: 89b263f6d56e6 (\"io_uring: run linked timeouts from task_work\")\nCc: stable@kernel.org # 5.15+\nReported-by: syzbot+ab0cfe96c2b3cd1c1153@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4c41c5f379c6941ad5a07cd48cb66ed62199cf7e.1637937097.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc3",
        "release_date": "2021-11-26 08:35:43 -0700 io_uring: fail cancellation for EXITING tasks"
    },
    {
        "commit": "de4444f5964933225b365a503db9a595b6588616",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A locking fix for link traversal, and fixing up an outdated function\n  name in a comment\"\n\n* tag 'io_uring-5.16-2021-11-25' of git://git.kernel.dk/linux-block:\n  io_uring: correct link-list traversal locking\n  io_uring: fix missed comment from *task_file rename",
        "kernel_version": "v5.16-rc3",
        "release_date": "2021-11-25 10:57:45 -0800 Merge tag 'io_uring-5.16-2021-11-25' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "b6c7db32183251204f124b10d6177d46558ca7b8",
        "message": "It's better to use REQ_F_IO_DRAIN for req->flags rather than\nIOSQE_IO_DRAIN though they have same value.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20211125092103.224502-3-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-11-25 09:00:42 -0700 io_uring: better to use REQ_F_IO_DRAIN for req->flags"
    },
    {
        "commit": "e302f1046f4c209291b07ff7bc4d15ca26891f16",
        "message": "ctx->cq_extra should be protected by completion lock so that the\nreq_need_defer() does the right check.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20211125092103.224502-2-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-11-25 09:00:42 -0700 io_uring: fix no lock protection for ctx->cq_extra"
    },
    {
        "commit": "5562a8d71aa32ea27133d8b10406b3dcd57c01a5",
        "message": "Current IOSQE_IO_DRAIN implementation doesn't work well with CQE\nskipping and it's not allowed, otherwise some requests might be not\nexecuted until the ring is destroyed and the userspace would hang.\n\nLet's fail all drain requests after seeing IOSQE_CQE_SKIP_SUCCESS at\nleast once. All drained requests prior to that will get run normally,\nso there should be no stalls. However, even though such mixing wouldn't\nlead to issues at the moment, it's still not allowed as the behaviour\nmay change.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bcf7164f8bf3eb54b7bb7b4fd119907fa4d4d43b.1636559119.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-11-24 11:17:53 -0700 io_uring: disable drain with cqe skip"
    },
    {
        "commit": "3d4aeb9f98058c3bdfef5286e240cf18c50fee89",
        "message": "When no of queued for the batch completion requests need to post an CQE,\nsee IOSQE_CQE_SKIP_SUCCESS, avoid grabbing ->completion_lock and other\ncommit/post.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8d4b4a08bca022cbe19af00266407116775b3e4d.1636559119.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-11-24 11:17:53 -0700 io_uring: don't spinlock when not posting CQEs"
    },
    {
        "commit": "04c76b41ca974b508522831441dd7e5b1b59cbb0",
        "message": "Emitting a CQE is expensive from the kernel perspective. Often, it's\nalso not convenient for the userspace, spends some cycles on processing\nand just complicates the logic. A similar problems goes for linked\nrequests, where we post an CQE for each request in the link.\n\nIntroduce a new flags, IOSQE_CQE_SKIP_SUCCESS, trying to help with it.\nWhen set and a request completed successfully, it won't generate a CQE.\nWhen fails, it produces an CQE, but all following linked requests will\nbe CQE-less, regardless whether they have IOSQE_CQE_SKIP_SUCCESS or not.\nThe notion of \"fail\" is the same as for link failing-cancellation, where\nit's opcode dependent, and _usually_ result >= 0 is a success, but not\nalways.\n\nLinked timeouts are a bit special. When the requests it's linked to was\nnot attempted to be executed, e.g. failing linked requests, it follows\nthe description above. Otherwise, whether a linked timeout will post a\ncompletion or not solely depends on IOSQE_CQE_SKIP_SUCCESS of that\nlinked timeout request. Linked timeout never \"fail\" during execution, so\nfor them it's unconditional. It's expected for users to not really care\nabout the result of it but rely solely on the result of the master\nrequest. Another reason for such a treatment is that it's racy, and the\ntimeout callback may be running awhile the master request posts its\ncompletion.\n\nuse case 1:\nIf one doesn't care about results of some requests, e.g. normal\ntimeouts, just set IOSQE_CQE_SKIP_SUCCESS. Error result will still be\nposted and need to be handled.\n\nuse case 2:\nSet IOSQE_CQE_SKIP_SUCCESS for all requests of a link but the last,\nand it'll post a completion only for the last one if everything goes\nright, otherwise there will be one only one CQE for the first failed\nrequest.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0220fbe06f7cf99e6fc71b4297bb1cb6c0e89c2c.1636559119.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-11-24 11:17:53 -0700 io_uring: add option to skip CQE posting"
    },
    {
        "commit": "913a571affedd17239c4d4ea90c8874b32fc2191",
        "message": "Split io_cqring_fill_event() into a couple of more targeted functions.\nThe first on is io_fill_cqe_aux() for completions that are not\nassociated with request completions and doing the ->cq_extra accounting.\nExamples are additional CQEs from multishot poll and rsrc notifications.\n\nThe second is io_fill_cqe_req(), should be called when it's a normal\nrequest completion. Nothing more to it at the moment, will be used in\nlater patches.\n\nThe last one is inlined __io_fill_cqe() for a finer grained control,\nshould be used with caution and in hottest places.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/59a9117a4a44fc9efcf04b3afa51e0d080f5943c.1636559119.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-11-24 11:17:53 -0700 io_uring: clean cqe filling functions"
    },
    {
        "commit": "2ea537ca02b12e6e03dfcac82013ff289a75eed8",
        "message": "kiocb_done() accepts a pointer to struct kiocb, pass struct io_kiocb\n(i.e. io_uring's request) instead so we can get rid of useless\ncontainer_of().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/252016eed77806f58b48251a85cd8c645f900433.1637524285.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-11-23 12:24:20 -0700 io_uring: improve argument types of kiocb_done()"
    },
    {
        "commit": "f3251183b298912e09297cb22614361c63122e82",
        "message": "Apparently, implicit 0 to NULL conversion with ERR_PTR is not\nrecommended and makes some tooling like Smatch to complain. Handle it\nexplicitly, compilers are perfectly capable to optimise it out.\n\nLink: https://lore.kernel.org/all/20211108134937.GA2863@kili/\nReported-by: Dan Carpenter <dan.carpenter@oracle.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5c6ed369ad95075dab345df679f8677b8fe66656.1637524285.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-11-23 12:24:20 -0700 io_uring: clean __io_import_iovec()"
    },
    {
        "commit": "7297ce3d59449de49d3c9e1f64ae25488750a1fc",
        "message": "Hide all error handling under common if block, removes two extra ifs on\nthe success path and keeps the handling more condensed.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5761545158a12968f3caf30f747eea65ed75dfc1.1637524285.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-11-23 12:24:20 -0700 io_uring: improve send/recv error handling"
    },
    {
        "commit": "06bdea20c1076471f7ab7d3ad7f35cbcbd59a8e3",
        "message": "Simplify failed resubmission prep in kiocb_done(), it's a bit ugly with\nconditional logic and hand handling cflags / select buffers. Instead,\npunt to tw and use io_req_task_complete() already handling all the\ncases.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/667c33484b05b612e9420e1b1d5f4dc46d0ee9ce.1637524285.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.17-rc1",
        "release_date": "2021-11-23 12:24:20 -0700 io_uring: simplify reissue in kiocb_done"
    },
    {
        "commit": "674ee8e1b4a41d2fdffc885c55350c3fbb38c22a",
        "message": "As io_remove_next_linked() is now under ->timeout_lock (see\nio_link_timeout_fn), we should update locking around io_for_each_link()\nand io_match_task() to use the new lock.\n\nCc: stable@kernel.org # 5.15+\nFixes: 89850fce16a1a (\"io_uring: run timeouts from task_work\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b54541cedf7de59cb5ae36109e58529ca16e66aa.1637631883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc3",
        "release_date": "2021-11-22 19:31:54 -0700 io_uring: correct link-list traversal locking"
    },
    {
        "commit": "f6f9b278f2059478e9a57ac221995105641c7498",
        "message": "Fix comment referring to function \"io_uring_del_task_file()\", now called\n\"io_uring_del_tctx_node()\".\n\nFixes: eef51daa72f7 (\"io_uring: rename function *task_file\")\nSigned-off-by: Kamal Mostafa <kamal@canonical.com>\nLink: https://lore.kernel.org/r/20211116175530.31608-1-kamal@canonical.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc3",
        "release_date": "2021-11-16 17:24:34 -0700 io_uring: fix missed comment from *task_file rename"
    },
    {
        "commit": "2b7196a219bf7c424b4308e712fc981887927c38",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single fix here for a buffered write hash stall, which is also\n  affecting stable\"\n\n* tag 'io_uring-5.16-2021-11-13' of git://git.kernel.dk/linux-block:\n  io-wq: serialize hash clear with wakeup",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-11-13 12:42:50 -0800 Merge tag 'io_uring-5.16-2021-11-13' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "007301c472efd5c86e69e883dd889c555f131ab5",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Minor fixes that should go into the 5.16 release:\n\n   - Fix max worker setting not working correctly on NUMA (Beld)\n\n   - Correctly return current setting for max workers if zeroes are\n     passed in (Pavel)\n\n   - io_queue_sqe_arm_apoll() cleanup, as identified during the initial\n     merge (Pavel)\n\n   - Misc fixes (Nghia, me)\"\n\n* tag 'io_uring-5.16-2021-11-09' of git://git.kernel.dk/linux-block:\n  io_uring: honour zeroes as io-wq worker limits\n  io_uring: remove dead 'sqe' store\n  io_uring: remove redundant assignment to ret in io_register_iowq_max_workers()\n  io-wq: fix max-workers not correctly set on multi-node system\n  io_uring: clean up io_queue_sqe_arm_apoll",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-11-09 11:11:37 -0800 Merge tag 'io_uring-5.16-2021-11-09' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "bad119b9a00019054f0c9e2045f312ed63ace4f4",
        "message": "When we pass in zero as an io-wq worker number limit it shouldn't\nactually change the limits but return the old value, follow that\nbehaviour with deferred limits setup as well.\n\nCc: stable@kernel.org # 5.15\nReported-by: Beld Zhang <beldzhang@gmail.com>\nFixes: e139a1ec92f8d (\"io_uring: apply max_workers limit to all future users\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1b222a92f7a78a24b042763805e891a4cdd4b544.1636384034.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-11-08 08:39:48 -0700 io_uring: honour zeroes as io-wq worker limits"
    },
    {
        "commit": "a19577808fd33d9e64e015808fbca2769a96721b",
        "message": "The kernel test robot correctly identifies that we store sqe twice,\nremove the earlier one that is done before validating the index.\n\nFixes: f75d118349be (\"io_uring: harder fdinfo sq/cq ring iterating\")\nReported-by: kernel test robot <lkp@intel.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-11-05 09:31:05 -0600 io_uring: remove dead 'sqe' store"
    },
    {
        "commit": "83956c86fffe0465408c7d62e925d88748075e00",
        "message": "After the assignment, only exit path with label 'err' uses ret as\nreturn value. However,before exiting through this path with label 'err',\nret is assigned with the return value of io_wq_max_workers(). Hence, the\ninitial assignment is redundant and can be removed.\n\nSigned-off-by: Nghia Le <nghialm78@gmail.com>\nLink: https://lore.kernel.org/r/20211102190521.28291-1-nghialm78@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-11-02 14:29:12 -0600 io_uring: remove redundant assignment to ret in io_register_iowq_max_workers()"
    },
    {
        "commit": "9881024aab8094a53756c7aee42564306c8e3580",
        "message": "The fix for linked timeout unprep got a bit distored with two rebases,\nhandle linked timeouts for IO_APOLL_READY as with all other cases, i.e.\nqueue it at the end of the function.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/130b1ea5605bbd81d7b874a95332295799d33b81.1635863773.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-11-02 09:26:14 -0600 io_uring: clean up io_queue_sqe_arm_apoll"
    },
    {
        "commit": "cdab10bf3285ee354e8f50254aa799631b7a95e0",
        "message": "Pull selinux updates from Paul Moore:\n\n - Add LSM/SELinux/Smack controls and auditing for io-uring.\n\n   As usual, the individual commit descriptions have more detail, but we\n   were basically missing two things which we're adding here:\n\n      + establishment of a proper audit context so that auditing of\n        io-uring ops works similarly to how it does for syscalls (with\n        some io-uring additions because io-uring ops are *not* syscalls)\n\n      + additional LSM hooks to enable access control points for some of\n        the more unusual io-uring features, e.g. credential overrides.\n\n   The additional audit callouts and LSM hooks were done in conjunction\n   with the io-uring folks, based on conversations and RFC patches\n   earlier in the year.\n\n - Fixup the binder credential handling so that the proper credentials\n   are used in the LSM hooks; the commit description and the code\n   comment which is removed in these patches are helpful to understand\n   the background and why this is the proper fix.\n\n - Enable SELinux genfscon policy support for securityfs, allowing\n   improved SELinux filesystem labeling for other subsystems which make\n   use of securityfs, e.g. IMA.\n\n* tag 'selinux-pr-20211101' of git://git.kernel.org/pub/scm/linux/kernel/git/pcmoore/selinux:\n  security: Return xattr name from security_dentry_init_security()\n  selinux: fix a sock regression in selinux_ip_postroute_compat()\n  binder: use cred instead of task for getsecid\n  binder: use cred instead of task for selinux checks\n  binder: use euid from cred instead of using task\n  LSM: Avoid warnings about potentially unused hook variables\n  selinux: fix all of the W=1 build warnings\n  selinux: make better use of the nf_hook_state passed to the NF hooks\n  selinux: fix race condition when computing ocontext SIDs\n  selinux: remove unneeded ipv6 hook wrappers\n  selinux: remove the SELinux lockdown implementation\n  selinux: enable genfscon labeling for securityfs\n  Smack: Brutalist io_uring support\n  selinux: add support for the io_uring access controls\n  lsm,io_uring: add LSM hooks to io_uring\n  io_uring: convert io_uring to the secure anon inode interface\n  fs: add anon_inode_getfile_secure() similar to anon_inode_getfd_secure()\n  audit: add filtering for io_uring records\n  audit,io_uring,io-wq: add some basic audit support to io_uring\n  audit: prepare audit_context for use in calling contexts beyond syscalls",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-11-01 21:06:18 -0700 Merge tag 'selinux-pr-20211101' of git://git.kernel.org/pub/scm/linux/kernel/git/pcmoore/selinux"
    },
    {
        "commit": "8d1f01775f8ead7ee313403158be95bffdbb3638",
        "message": "Pull io_uring updates from Jens Axboe:\n \"Light on new features - basically just the hybrid mode support.\n\n  Outside of that it's just fixes, cleanups, and performance\n  improvements.\n\n  In detail:\n\n   - Add ring related information to the fdinfo output (Hao)\n\n   - Hybrid async mode (Hao)\n\n   - Support for batched issue on block (me)\n\n   - sqe error trace improvement (me)\n\n   - IOPOLL efficiency improvements (Pavel)\n\n   - submit state cleanups and improvements (Pavel)\n\n   - Completion side improvements (Pavel)\n\n   - Drain improvements (Pavel)\n\n   - Buffer selection cleanups (Pavel)\n\n   - Fixed file node improvements (Pavel)\n\n   - io-wq setup cancelation fix (Pavel)\n\n   - Various other performance improvements and cleanups (Pavel)\n\n   - Misc fixes (Arnd, Bixuan, Changcheng, Hao, me, Noah)\"\n\n* tag 'for-5.16/io_uring-2021-10-29' of git://git.kernel.dk/linux-block: (97 commits)\n  io-wq: remove worker to owner tw dependency\n  io_uring: harder fdinfo sq/cq ring iterating\n  io_uring: don't assign write hint in the read path\n  io_uring: clusterise ki_flags access in rw_prep\n  io_uring: kill unused param from io_file_supports_nowait\n  io_uring: clean up timeout async_data allocation\n  io_uring: don't try io-wq polling if not supported\n  io_uring: check if opcode needs poll first on arming\n  io_uring: clean iowq submit work cancellation\n  io_uring: clean io_wq_submit_work()'s main loop\n  io-wq: use helper for worker refcounting\n  io_uring: implement async hybrid mode for pollable requests\n  io_uring: Use ERR_CAST() instead of ERR_PTR(PTR_ERR())\n  io_uring: split logic of force_nonblock\n  io_uring: warning about unused-but-set parameter\n  io_uring: inform block layer of how many requests we are submitting\n  io_uring: simplify io_file_supports_nowait()\n  io_uring: combine REQ_F_NOWAIT_{READ,WRITE} flags\n  io_uring: arm poll for non-nowait files\n  fs/io_uring: Prioritise checking faster conditions first in io_write\n  ...",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-11-01 09:41:33 -0700 Merge tag 'for-5.16/io_uring-2021-10-29' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "f75d118349be055d47407b4ba4ceb98e6437e472",
        "message": "The ring iteration is racy, which isn't necessarily a problem except it\ncan cause us to iterate the whole thing. That isn't desired or ideal,\nand it can lead to excessive runtimes of reading fdinfo.\n\nCap the iteration at tail - head OR the ring size. While in there, clean\nup the ring masking and just dump the raw values along with the masks.\nThat provides more useful debug info.\n\nFixes: 83f84356bc8f (\"io_uring: add more uring info to fdinfo for debug\")\nReported-by: Eric Dumazet <edumazet@google.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-29 09:49:33 -0600 io_uring: harder fdinfo sq/cq ring iterating"
    },
    {
        "commit": "3884b83dff245e41def99ceacca8ed2056baf0a8",
        "message": "Move this out of the generic read/write prep path, and place it in the\nwrite specific kiocb setup instead.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-26 15:54:40 -0600 io_uring: don't assign write hint in the read path"
    },
    {
        "commit": "fb27274a90eac5a687fe73229775ad36df737d8b",
        "message": "ioprio setup doesn't depend on other fields that are modified in\nio_prep_rw() and we can move it down in the function without worrying\nabout performance. It's useful as it makes iocb->ki_flags\naccesses/modifications closer together, so it's more likely the compiler\nwill cache it in a register and avoid extra reloads.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8ee98779c06f1b59f6039b1e292db4332efd664b.1634987320.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-25 07:42:35 -0600 io_uring: clusterise ki_flags access in rw_prep"
    },
    {
        "commit": "b9a6b8f92f6feebf40609e4f5f22a3c0404afb60",
        "message": "io_file_supports_nowait() doesn't use rw argument anymore, remove it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4bd6709fc573d70c866ea656cb7a7dbe94be8026.1634987320.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-25 07:42:35 -0600 io_uring: kill unused param from io_file_supports_nowait"
    },
    {
        "commit": "d6a644a795451d5fd063a5c08d6bb3a91d021887",
        "message": "opcode prep functions are one of the first things that are called, we\ncan't have ->async_data allocated at this point and it's certainly a\nbug. Reflect this assumption in io_timeout_prep() and add a WARN_ONCE\njust in case.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/75a28ca7dbcc5af8b6cd9092819e8384c24dedd4.1634987320.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-25 07:42:35 -0600 io_uring: clean up timeout async_data allocation"
    },
    {
        "commit": "afb7f56fc624fb6ade8fde70a67eda4d831b4ed0",
        "message": "If an opcode doesn't support polling, just let it be executed\nsynchronously in iowq, otherwise it will do a nonblock attempt just to\nfail in io_arm_poll_handler() and return back to blocking execution.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6401256db01b88f448f15fcd241439cb76f5b940.1634987320.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-25 07:42:33 -0600 io_uring: don't try io-wq polling if not supported"
    },
    {
        "commit": "658d0a401637ffbc733b0f03723e9e1255289429",
        "message": "->pollout or ->pollin are set only for opcodes that need a file, so if\nio_arm_poll_handler() tests them first we can be sure that the request\nhas file set and the ->file check can be removed.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9adfe4f543d984875e516fce6da35348aab48668.1634987320.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-25 07:42:31 -0600 io_uring: check if opcode needs poll first on arming"
    },
    {
        "commit": "d01905db14eb6223dd1c375001f4daa26cb15c1f",
        "message": "If we've got IO_WQ_WORK_CANCEL in io_wq_submit_work(), handle the error\non the same lines as the check instead of having a weird code flow. The\nmain loop doesn't change but goes one indention left.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ff4a09cf41f7a22bbb294b6f1faea721e21fe615.1634987320.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-25 07:42:29 -0600 io_uring: clean iowq submit work cancellation"
    },
    {
        "commit": "255657d237042fd51673aef6f22463f662f9933f",
        "message": "Do a bit of cleaning for the main loop of io_wq_submit_work(). Get rid\nof switch, just replace it with a single if as we're retrying in both\nother cases. Kill issue_sqe label, Get rid of needs_poll nesting and\ndisambiguate a bit the comment.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ed12ce0c64e051f9a6b8a37a24f8ea554d299c29.1634987320.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-25 07:42:20 -0600 io_uring: clean io_wq_submit_work()'s main loop"
    },
    {
        "commit": "da4d34b669723508601a4c29daa22cdc669ee005",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two fixes for the max workers limit API that was introduced this\n  series: one fix for an issue with that code, and one fixing a linked\n  timeout regression in this series\"\n\n* tag 'io_uring-5.15-2021-10-22' of git://git.kernel.dk/linux-block:\n  io_uring: apply worker limits to previous users\n  io_uring: fix ltimeout unprep\n  io_uring: apply max_workers limit to all future users\n  io-wq: max_worker fixes",
        "kernel_version": "v5.15-rc7",
        "release_date": "2021-10-22 17:34:31 -1000 Merge tag 'io_uring-5.15-2021-10-22' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "90fa02883f063b971ebfd9f5b2184b38b83b7ee3",
        "message": "The current logic of requests with IOSQE_ASYNC is first queueing it to\nio-worker, then execute it in a synchronous way. For unbound works like\npollable requests(e.g. read/write a socketfd), the io-worker may stuck\nthere waiting for events for a long time. And thus other works wait in\nthe list for a long time too.\nLet's introduce a new way for unbound works (currently pollable\nrequests), with this a request will first be queued to io-worker, then\nexecuted in a nonblock try rather than a synchronous way. Failure of\nthat leads it to arm poll stuff and then the worker can begin to handle\nother works.\nThe detail process of this kind of requests is:\n\nstep1: original context:\n           queue it to io-worker\nstep2: io-worker context:\n           nonblock try(the old logic is a synchronous try here)\n               |\n               |--fail--> arm poll\n                            |\n                            |--(fail/ready)-->synchronous issue\n                            |\n                            |--(succeed)-->worker finish it's job, tw\n                                           take over the req\n\nThis works much better than the old IOSQE_ASYNC logic in cases where\nunbound max_worker is relatively small. In this case, number of\nio-worker eazily increments to max_worker, new worker cannot be created\nand running workers stuck there handling old works in IOSQE_ASYNC mode.\n\nIn my 64-core machine, set unbound max_worker to 20, run echo-server,\nturns out:\n(arguments: register_file, connetion number is 1000, message size is 12\nByte)\noriginal IOSQE_ASYNC: 76664.151 tps\nafter this patch: 166934.985 tps\n\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20211018133445.103438-1-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-22 19:20:57 -0600 io_uring: implement async hybrid mode for pollable requests"
    },
    {
        "commit": "b22fa62a35d7f2029d757a518d78041822b7c7c1",
        "message": "Another change to the API io-wq worker limitation API added in 5.15,\napply the limit to all prior users that already registered a tctx. It\nmay be confusing as it's now, in particular the change covers the\nfollowing 2 cases:\n\nTASK1                   | TASK2\n_________________________________________________\nring = create()         |\n                        | limit_iowq_workers()\n*not limited*           |\n\nTASK1                   | TASK2\n_________________________________________________\nring = create()         |\n                        | issue_requests()\nlimit_iowq_workers()    |\n                        | *not limited*\n\nA note on locking, it's safe to traverse ->tctx_list as we hold\n->uring_lock, but do that after dropping sqd->lock to avoid possible\nproblems. It's also safe to access tctx->io_wq there because tasks\nkill it only after removing themselves from tctx_list, see\nio_uring_cancel_generic() -> io_uring_clean_tctx()\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d6e09ecc3545e4dc56e43c906ee3d71b7ae21bed.1634818641.git.asml.silence@gmail.com\nReviewed-by: Hao Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc7",
        "release_date": "2021-10-21 11:19:38 -0600 io_uring: apply worker limits to previous users"
    },
    {
        "commit": "e94f68527a35271131cdf9d3fb4eb3c2513dc3d0",
        "message": "blk_try_enter_queue() already takes rcu_read_lock/unlock, so we can\navoid the second pair in percpu_ref_tryget_live(), use a newly added\npercpu_ref_tryget_live_rcu().\n\nAs rcu_read_lock/unlock imply barrier()s, it's pretty noticeable,\nespecially for for !CONFIG_PREEMPT_RCU (default for some distributions),\nwhere __rcu_read_lock/unlock() are not inlined.\n\n3.20%  io_uring  [kernel.vmlinux]  [k] __rcu_read_unlock\n3.05%  io_uring  [kernel.vmlinux]  [k] __rcu_read_lock\n\n2.52%  io_uring  [kernel.vmlinux]  [k] __rcu_read_unlock\n2.28%  io_uring  [kernel.vmlinux]  [k] __rcu_read_lock\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6b11c67ea495ed9d44f067622d852de4a510ce65.1634822969.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-21 08:37:26 -0600 block: kill extra rcu lock/unlock in queue enter"
    },
    {
        "commit": "4ea672ab694c23886b52e97cee10dea056e43e62",
        "message": "io_unprep_linked_timeout() is broken, first it needs to return back\nREQ_F_ARM_LTIMEOUT, so the linked timeout is enqueued and disarmed. But\nnow we refcounted it, and linked timeouts may get not executed at all,\nleaking a request.\n\nJust kill the unprep optimisation.\n\nFixes: 906c6caaf586 (\"io_uring: optimise io_prep_linked_timeout()\")\nReported-by: Beld Zhang <beldzhang@gmail.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/51b8e2bfc4bea8ee625cf2ba62b2a350cc9be031.1634719585.git.asml.silence@gmail.com\nLink: https://github.com/axboe/liburing/issues/460\nReported-by: Beld Zhang <beldzhang@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc7",
        "release_date": "2021-10-20 09:54:16 -0600 io_uring: fix ltimeout unprep"
    },
    {
        "commit": "e139a1ec92f8dbaaa7380d7e7ea17e148d473d06",
        "message": "Currently, IORING_REGISTER_IOWQ_MAX_WORKERS applies only to the task\nthat issued it, it's unexpected for users. If one task creates a ring,\nlimits workers and then passes it to another task the limit won't be\napplied to the other task.\n\nAnother pitfall is that a task should either create a ring or submit at\nleast one request for IORING_REGISTER_IOWQ_MAX_WORKERS to work at all,\nfurher complicating the picture.\n\nChange the API, save the limits and apply to all future users. Note, it\nshould be done first before giving away the ring or submitting new\nrequests otherwise the result is not guaranteed.\n\nFixes: 2e480058ddc2 (\"io-wq: provide a way to limit max number of workers\")\nLink: https://github.com/axboe/liburing/issues/460\nReported-by: Beld Zhang <beldzhang@gmail.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/51d0bae97180e08ab722c0d5c93e7439cfb6f697.1634683237.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc7",
        "release_date": "2021-10-20 09:54:06 -0600 io_uring: apply max_workers limit to all future users"
    },
    {
        "commit": "898df2447b9ee8d759e85d33087505d3905bf2f0",
        "message": "Use ERR_CAST() instead of ERR_PTR(PTR_ERR()).\nThis makes it more readable and also fix this warning detected by\nerr_cast.cocci:\n./fs/io_uring.c: WARNING: 3208: 11-18: ERR_CAST can be used with buf\n\nReported-by: Zeal Robot <zealci@zte.com.cn>\nSigned-off-by: Changcheng Deng <deng.changcheng@zte.com.cn>\nLink: https://lore.kernel.org/r/20211020084948.1038420-1-deng.changcheng@zte.com.cn\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-20 08:02:35 -0600 io_uring: Use ERR_CAST() instead of ERR_PTR(PTR_ERR())"
    },
    {
        "commit": "3b44b3712c5b19b0af11c25cd978abdc3680d5e7",
        "message": "Currently force_nonblock stands for three meanings:\n - nowait or not\n - in an io-worker or not(hold uring_lock or not)\n\nLet's split the logic to two flags, IO_URING_F_NONBLOCK and\nIO_URING_F_UNLOCKED for convenience of the next patch.\n\nSuggested-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20211018133431.103298-1-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 18:21:42 -0600 io_uring: split logic of force_nonblock"
    },
    {
        "commit": "a9a7e30fd918588bc312ba782426e3a1282df359",
        "message": "This memset in the fast path costs a lot of cycles on my setup. Here's a\ntop-of-profile of doing ~6.7M IOPS:\n\n+    5.90%  io_uring  [nvme]            [k] nvme_queue_rq\n+    5.32%  io_uring  [nvme_core]       [k] nvme_setup_cmd\n+    5.17%  io_uring  [kernel.vmlinux]  [k] io_submit_sqes\n+    4.97%  io_uring  [kernel.vmlinux]  [k] blkdev_direct_IO\n\nand a perf diff with this patch:\n\n     0.92%     +4.40%  [nvme_core]       [k] nvme_setup_cmd\n\nreducing it from 5.3% to only 0.9%. This takes it from the 2nd most\ncycle consumer to something that's mostly irrelevant.\n\nReviewed-by: Chaitanya Kulkarni <kch@nvidia.com>\nReviewed-by: Keith Busch <kbusch@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 12:41:09 -0600 nvme: don't memset() the normal read/write command"
    },
    {
        "commit": "00169246e6981752e53266c62d0ab0c827493634",
        "message": "When enabling -Wunused warnings by building with W=1, I get an\ninstance of the -Wunused-but-set-parameter warning in the io_uring code:\n\nfs/io_uring.c: In function 'io_queue_async_work':\nfs/io_uring.c:1445:61: error: parameter 'locked' set but not used [-Werror=unused-but-set-parameter]\n 1445 | static void io_queue_async_work(struct io_kiocb *req, bool *locked)\n      |                                                       ~~~~~~^~~~~~\n\nThere are very few warnings of this type, so it would be nice to enable\nthis by default and fix all the existing instances. As the assignment\nserves no purpose by itself other than to prevent developers from using\nthe variable, an easy workaround is to remove the assignment and just\nrename the argument to \"dont_use\".\n\nFixes: f237c30a5610 (\"io_uring: batch task work locking\")\nLink: https://lore.kernel.org/lkml/20210920121352.93063-1-arnd@kernel.org/\nSigned-off-by: Arnd Bergmann <arnd@arndb.de>\nLink: https://lore.kernel.org/r/20211019153507.348480-1-arnd@kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 09:50:18 -0600 io_uring: warning about unused-but-set parameter"
    },
    {
        "commit": "5ca7a8b3f698b111729ed8e133b14e30f961de0f",
        "message": "The block layer can use this knowledge to make smarter decisions on\nhow to handle the request, if it knows that N more may be coming. Switch\nto using blk_start_plug_nr_ios() to pass in that information.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:56 -0600 io_uring: inform block layer of how many requests we are submitting"
    },
    {
        "commit": "88459b50b42a4bd58e528006663afabd0b8652f2",
        "message": "Make sure that REQ_F_SUPPORT_NOWAIT is always set io_prep_rw(), and so\nwe can stop caring about setting it down the line simplifying\nio_file_supports_nowait().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/60c8f1f5e2cb45e00f4897b2cec10c5b3669da91.1634425438.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:56 -0600 io_uring: simplify io_file_supports_nowait()"
    },
    {
        "commit": "35645ac3c1853fbb54d8acd50fd12184f7905d5f",
        "message": "Merge REQ_F_NOWAIT_READ and REQ_F_NOWAIT_WRITE into one flag, i.e.\nREQ_F_SUPPORT_NOWAIT. First it gets rid of dependence on CONFIG_64BIT\nbut also simplifies the code.\n\nOne thing to consider is when we don't have ->{read,write}_iter and go\nthrough loop_rw_iter(). Just fail it with -EAGAIN if we expect nowait\nbehaviour but not sure whether it supports it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f832a20e5186c2e79c6519280c238f559a1d2bbc.1634425438.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:56 -0600 io_uring: combine REQ_F_NOWAIT_{READ,WRITE} flags"
    },
    {
        "commit": "e74ead135bc4459f7d40b1f8edab1333a28b54e8",
        "message": "Don't check if we can do nowait before arming apoll, there are several\nreasons for that. First, we don't care much about files that don't\nsupport nowait. Second, it may be useful -- we don't want to be taking\naway extra workers from io-wq when it can go in some async. Even if it\nwill go through io-wq eventually, it make difference in the numbers of\nworkers actually used. And the last one, it's needed to clean nowait in\nfuture commits.\n\n[kernel test robot: fix unused-var]\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9d06f3cb2c8b686d970269a87986f154edb83043.1634425438.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:56 -0600 io_uring: arm poll for non-nowait files"
    },
    {
        "commit": "b10841c98c8980ee50cc1e90640b04c935a24285",
        "message": "This commit reorders the conditions in a branch in io_write. The\nreorder to check 'ret2 == -EAGAIN' first as checking\n'(req->ctx->flags & IORING_SETUP_IOPOLL)' will likely be more\nexpensive due to 2x memory derefences.\n\nSigned-off-by: Noah Goldstein <goldstein.w.n@gmail.com>\nLink: https://lore.kernel.org/r/20211017013229.4124279-1-goldstein.w.n@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:56 -0600 fs/io_uring: Prioritise checking faster conditions first in io_write"
    },
    {
        "commit": "5cb03d63420bcf533111d30690141a5b28c172fa",
        "message": "We already store req->file in a variable in io_prep_rw(), just use it\ninstead of a couple of left references to kicob->ki_filp.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2f5889fc7ab670daefd5ccaedd99416d8355f0ad.1634314022.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:56 -0600 io_uring: clean io_prep_rw()"
    },
    {
        "commit": "578c0ee234e51d9746e526a26bdb2c1b7ad81dba",
        "message": "Move fixed rw io_req_set_rsrc_node() from rw prep into\nio_import_fixed(), if we're using fixed buffers it will always be called\nduring submission as we save the state in advance,\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/68c06f66d5aa9661f1e4b88d08c52d23528297ec.1634314022.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:56 -0600 io_uring: optimise fixed rw rsrc node setting"
    },
    {
        "commit": "caa8fe6e86fd32e26d05f724e07ec826807c6ad4",
        "message": "We pass iovec** into __io_import_iovec(), which should keep it,\ninitialise and modify accordingly. It's expensive, return it directly\nfrom __io_import_iovec encoding errors with ERR_PTR if needed.\n\nio_import_iovec keeps the old interface, but it's inline and so\neverything is optimised nicely.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6230e9769982f03a8f86fa58df24666088c44d3e.1634314022.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:56 -0600 io_uring: return iovec from __io_import_iovec"
    },
    {
        "commit": "d1d681b0846af8585904be562610bdfc70bf21aa",
        "message": "Delay loading req->rw.{addr,len} in io_import_iovec until it's really\nneeded, so removing extra loads for the fixed path, which doesn't use\nthem.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3cc48dd0c4f1a37c4ce9aab5784281a2d83ad8be.1634314022.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: optimise io_import_iovec fixed path"
    },
    {
        "commit": "9882131cd9de525d484de117644e5008c6557ac7",
        "message": "Don't decide about locking based on io_wq_current_is_worker(), it's not\nconsistent with all other code and is expensive, use issue_flags.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7546d5a58efa4360173541c6fe02ee6b8c7b4ea7.1634314022.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: kill io_wq_current_is_worker() in iopoll"
    },
    {
        "commit": "9983028e7660a2cc5e58403d8ce29569dbf3162d",
        "message": "Don't load req->ctx in advance, it takes an extra register and the field\nstays valid even after opcode handlers. It also optimises out req->ctx\nload in io_iopoll_req_issued() once it's inlined.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1e45ff671c44be0eb904f2e448a211734893fa0b.1634314022.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: optimise req->ctx reloads"
    },
    {
        "commit": "607b6fb8017a684f3f1567ccf776bdc0fe6d120e",
        "message": "Combine force_nonblock branches (which is already optimised by\ncompiler), flip branches so the most hot/common path is the first, e.g.\nas with non on-stack iov setup, and add extra likely/unlikely\nattributions for errror paths.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2c2536c5896d70994de76e387ea09a0402173a3f.1634144845.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: rearrange io_read()/write()"
    },
    {
        "commit": "5e49c973fc39ffce287ab0763cb6f1e607962689",
        "message": "Make io_import_iovec taking struct io_rw_state instead of an iter\npointer. First it takes care of initialising iovec pointer, which can be\nforgotten. Even more, we can not init it if not needed, e.g. in case of\nIORING_OP_READ_FIXED or IORING_OP_READ. Also hide saving iter_state\ninside of it by splitting out an inline function of it to avoid extra\nifs.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b1bbc213a95e5272d4da5867bb977d9acb6f2109.1634144845.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: clean up io_import_iovec"
    },
    {
        "commit": "51aac424aef980a6238d4907a5bb5a411fa926eb",
        "message": "First, change IO_URING_F_NONBLOCK to take sign bit of the int, so\nchecking for it can be turned into test + sign-based-jump, makes the\nbinary smaller and may be faster.\n\nThen, instead of passing need_lock boolean into io_import_iovec() just\ngive it issue_flags, which is already stored somewhere. Saves some space\non stack, a couple of test + cmov operations and other conversions.\n\nnote: we still leave\nforce_nonblock = issue_flags & IO_URING_F_NONBLOCK\nvariable, but it's optimised out by the compiler into testing\nissue_flags directly.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ee96547e692f6c975c229cd82fc721679571a734.1634144845.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: optimise io_import_iovec nonblock passing"
    },
    {
        "commit": "c88598a92a587109d915b4d97831bcea774c8b6f",
        "message": "Currently io_read() and io_write() keep separate pointers to an iter and\nto struct iov_iter_state, which is not great for register spilling and\nrequires more on-stack copies. They are both either on-stack or in\nreq->async_data at the same time, so use struct io_rw_state and keep a\npointer only to it, so having all the state with just one pointer.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5c5e7ffd7dc25fc35075c70411ba99df72f237fa.1634144845.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: optimise read/write iov state storing"
    },
    {
        "commit": "538941e2681c76ba6d6a7dd28ce691a9e02a9bdf",
        "message": "Add a new struct io_rw_state storing all iov related bits: fast iov,\niterator and iterator state. Not much changes here, simply convert\nstruct io_async_rw to use it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e8245ffcb568b228a009ec1eb79c993c813679f1.1634144845.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: encapsulate rw state"
    },
    {
        "commit": "258f3a7f84d11ce1d6cb1f60fe524bb37f7c1019",
        "message": "Don't override req->result in io_complete_rw_iopoll() when it's already\nof the same value, we have an if just above it, so move the assignment\nthere. Also, add one simle unlikely() in __io_complete_rw_common().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8dfeb4f84026a20172bcf82c05010abe955874ae.1634144845.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: optimise rw comletion handlers"
    },
    {
        "commit": "f80a50a632d6f5c64729563892b0deb95d563f6d",
        "message": "Rearrange io_read return handling so first we expect it completing\nsuccessfully and only then checking for errors, which is a colder path.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c91c7c2da11815ec8b04b5d872f60dc4cde662c5.1634144845.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: prioritise read success path over fails"
    },
    {
        "commit": "04f34081c5de35749274cf3bea8d8eb1c79b6ad1",
        "message": "Some of the functions keep issue_flags as int, change those to unsigned.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/04ad43797783bc9cc7567f287ab545518f8e8cf2.1634144845.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: consistent typing for issue_flags"
    },
    {
        "commit": "ab409402478462b5da007bfc46d165587c3adfc3",
        "message": "Apparently, percpu_ref_put/get() are expensive enough if done per\nrequest, get them in a batch and cache on the submission side to avoid\ngetting it over and over again. Also, if we're completing under\nuring_lock, return refs back into the cache instead of\nperfcpu_ref_put(). Pretty similar to how we do tctx->cached_refs\naccounting, but fall back to normal putting when we already changed a\nrsrc node by the time of free.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b40d8c5bc77d3c9550df8a319117a374ac85f8f4.1633817310.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: optimise rsrc referencing"
    },
    {
        "commit": "a46be971edb69fe4b1dcc4359c3ddf9127629dab",
        "message": "io_req_set_rsrc_node() reloads loads req->ctx, however it's already in\nregisters in all use cases, so better to pass it as a parameter.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/67a25557b8a51e90bfd578447a6f1671911b05ae.1633817310.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: optimise io_req_set_rsrc_node()"
    },
    {
        "commit": "def77acf4396a850544b41623d7720ea3a2674a9",
        "message": "[  158.514382] WARNING: CPU: 5 PID: 15251 at fs/io_uring.c:1141 io_free_batch_list+0x269/0x360\n[  158.514426] RIP: 0010:io_free_batch_list+0x269/0x360\n[  158.514437] Call Trace:\n[  158.514440]  __io_submit_flush_completions+0xde/0x180\n[  158.514444]  tctx_task_work+0x14a/0x220\n[  158.514447]  task_work_run+0x64/0xa0\n[  158.514448]  __do_sys_io_uring_enter+0x7c/0x970\n[  158.514450]  __x64_sys_io_uring_enter+0x22/0x30\n[  158.514451]  do_syscall_64+0x43/0x90\n[  158.514453]  entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nWe should not touch request internals including req->comp_list.next\nafter putting our ref if it's not final, e.g. we can start freeing\nrequests from the free cache.\n\nFixed: 62ca9cb93e7f8 (\"io_uring: optimise io_free_batch_list()\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b1f4df38fbb8f111f52911a02fd418d0283a4e6f.1634047298.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: fix io_free_batch_list races"
    },
    {
        "commit": "0cd3e3ddb4f60062c401929b2005eb7ff6399a4d",
        "message": "task_work_add() takes care of waking up the thread, remove useless\nwake_up_process().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/de9a71ee255112dcaed3b5d426be24934e74722c.1633532552.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: remove extra io_ring_exit_work wake up"
    },
    {
        "commit": "4a04d1d14831d31f2cd0e31eb1568cc9c1be0095",
        "message": "Looking at the assembly, the compiler decided to reload req->opcode in\nio_op_defs[opcode].needs_file instead of one it had in a register, so\nstore it in a temp variable so it can be optimised out. Also move the\npersonality block later, it's better for spilling/etc. as it only\ndepends on @sqe, which we're keeping anyway.\n\nBy the way, zero req->opcode if it over IORING_OP_LAST, not a problem,\nat the moment but is safer.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6ba869f5f8b7b0f991c87fdf089f0abf87cbe06b.1633532552.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: optimise out req->opcode reloading"
    },
    {
        "commit": "5a158c6b0d033893cc80c28b182e1207253768a5",
        "message": "struct io_submit_state's ->free_list and ->link are hotter and smaller\nthan ->plug, place them first.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6ad3c15849f50b27ad012c042c73e6e069d22df7.1633532552.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: reshuffle io_submit_state bits"
    },
    {
        "commit": "756ab7c0ec71e5e1e8e6ea11af53324b23d5efd2",
        "message": "Add extra wq flushing for fallback_work, that's not necessary but safer\nif invariants of io_fallback_req_func() change.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/24179419d6748516299600bc914f50b9e0b02275.1633532552.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: safer fallback_work free"
    },
    {
        "commit": "6d63416dc57eb27a3d35e7b7526e9915479d7eff",
        "message": "Plugging is only needed with requests that also need a file, so hide\nplugging under a ->needs_file check. Also, place ->needs_file and ->plug\nbits into the same byte of io_op_defs, it may matter for compilers, e.g.\nonly with the change a tested one decided to optimise two memory testb\ninto a more with two register testb.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1600d1287bb7d16451d4ef3343252787a5314927.1633532552.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: optimise plugging"
    },
    {
        "commit": "54daa9b2d80ab35824464b35a99f716e1cdf2ccb",
        "message": "CQE result is a 32-bit integer, so the functions generating CQEs are\nbetter to accept not long but ints. Convert io_cqring_fill_event() and\nother helpers.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7ca6f15255e9117eae28adcac272744cae29b113.1633373302.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: correct fill events helpers types"
    },
    {
        "commit": "eb6e6f0690c846f7de46181bab3954c12c96e11e",
        "message": "Inline io_poll_complete(), it's simple and doesn't have any particular\npurpose.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/933d7ee3e4450749a2d892235462c8f18d030293.1633373302.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:55 -0600 io_uring: inline io_poll_complete"
    },
    {
        "commit": "867f8fa5aeb7fa51290bf3567ce2bbc45580a469",
        "message": "There is only a single user of io_req_needs_clean() inline it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6111d0221ef4b439cad401e135dd6a5f990a0501.1633373302.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: inline io_req_needs_clean()"
    },
    {
        "commit": "d17e56eb4907c72054e63c71a2123d32b04ebd67",
        "message": "We keep struct io_completion only as a temporal storage of cflags, Place\nit in io_kiocb, it's cleaner, removes extra bits and even might be used\nfor future optimisations.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5299bd5c223204065464bd87a515d0e405316086.1633373302.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: remove struct io_completion"
    },
    {
        "commit": "d886e185a128a4f350c4df6471c0f403c2982ae8",
        "message": "->async_data is a slow path, so it won't matter much if we do the clean\nup inside io_clean_op(). Moreover, in many cases it's allocated together\nwith setting one or more of IO_REQ_CLEAN_FLAGS flags, so it'd go through\nio_clean_op() anyway.\n\nControl ->async_data allocation with a new flag REQ_F_ASYNC_DATA, so we\ncan do all the maintainence under io_req_needs_clean() fast check.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6892cf5883c459f36bda26f30ceb16742b20b84b.1633373302.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: control ->async_data with a REQ_F flag"
    },
    {
        "commit": "c1e53a6988b9c83dd8bbc759414bc0f13ff1fe0c",
        "message": "Delay reading the next node in io_free_batch_list(), allows the compiler\nto load the value a bit later improving register spilling in some cases.\nWith gcc 11.1 it helped to move @task_refs variable from the stack to a\nregister and optimises out a couple of per request instructions.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/cc9fdfb6f72a4e8bc9918a5e9f2d97869a263ae4.1633373302.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: optimise io_free_batch_list()"
    },
    {
        "commit": "c072481ded14bceb9a4163ac81ce73d907a43855",
        "message": "Attribute cold functions so compilers can optimise them for size. It\nshrinks the binary by 2.5-3%\n\n   text    data     bss     dec     hex filename\n  90670   14002       8  104680   198e8 ./fs/io_uring.o\n  88053   14002       8  102063   18eaf ./fs/io_uring.o\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b53d385f91dca45170b67d7f11c7abd787e821f6.1633373302.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: mark cold functions"
    },
    {
        "commit": "37f0e767e177af328fcaf74d3772495235e361a8",
        "message": "Currenlty, we allocate one ctx reference per request at submission time\nand put them at free. It's batched and not so expensive but it still\nbloats the kernel, adds 2 function calls for rcu and adds some overhead\nfor request counting in io_free_batch_list().\n\nAlways keep one reference with a request, even when it's freed and in\nio_uring request caches. There is extra work at ring exit / quiesce\npaths, which now need to put all cached requests. io_ring_exit_work() is\nalready looping, so it's not a problem. Add hybrid-busy waiting to\nio_ctx_quiesce() as well for now.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/99613fbe396e80777228cde39bbda1aa8938554e.1633373302.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: optimise ctx referencing by requests"
    },
    {
        "commit": "d60aa65ba221f038404b98d8484f562f72bb807b",
        "message": "->cq_wait and ->poll_wait and waken up in the same manner, use a single\nwaitqueue for both of them. CQ waiters are queued exclusively, so wake\nup should first go over all pollers and that's what we need.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/00fe603e50000365774cf8435ef5fe03f049c1c9.1633373302.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: merge CQ and poll waitqueues"
    },
    {
        "commit": "aede728aae355d4c1d38dd02747d415af011eea7",
        "message": "io_cqring_ev_posted() doesn't need to wake SQPOLL, it's either done by\nuserspace or with task_work, but no action is required on request\ncompletion. Rip off bits waking it up in io_cqring_ev_posted().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b49dab27b64cf11f4c50f2f90dcaac123430e05d.1633373302.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: don't wake sqpoll in io_cqring_ev_posted"
    },
    {
        "commit": "765ff496c781a982ed0a882a7784c93ce10d1155",
        "message": "The invariant of io_wq_work_list is that it's empty IFF ->first is NULL,\nso no need to initially set ->last. With now having more users of the\nlist it may play a role, i.e. used in each tw iteration and on every\ncompletion flushing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c464ab5cab6e46a858c6d39c107e92b3b5291f13.1633373302.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: optimise INIT_WQ_LIST"
    },
    {
        "commit": "a33ae9ce16a8ca62c5dffbe8909d185c6c5b4d77",
        "message": "Even after fully inlining io_alloc_req() my compiler does a NULL check\nin the path of successful allocation, no hacks like an empty dereference\nhelp it. Restructure io_alloc_req() by splitting out refilling part, so\nthe compiler generate a slightly better binary.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/eda17571bdc7248d8e617b23e7132a5416e4680b.1633373302.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: optimise request allocation"
    },
    {
        "commit": "fff4e40e3094972b119e38c8f0de4e1ca9fec654",
        "message": "io_req_complete_state() is inlined and used in lots of places, so we\nwant to keep it concise. Move adding a request into a completion batch\nlist from io_req_complete_state() into the consumer, i.e.\n__io_queue_sqe().\n\nbefore vs after\n   text    data     bss     dec     hex filename\n  91894   14002       8  105904   19db0 ./fs/io_uring.o\n  91046   14002       8  105056   19a60 ./fs/io_uring.o\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4afca4e11abfd4cc8e99777fdcaf4d34cf4d022d.1633373302.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: delay req queueing into compl-batch list"
    },
    {
        "commit": "51d48dab62ed9e44e0ccc723411b0d95286a821c",
        "message": "Add two extra unlikely() in io_submit_sqes() and one around\nio_req_needs_clean() to help the compiler to avoid extra jumps\nin hot paths.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/88e087afe657e7660194353aada9b00f11d480f9.1633373302.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: add more likely/unlikely() annotations"
    },
    {
        "commit": "7e3709d57651feab9c77d7a5a8024041f73f69f7",
        "message": "We want ->comp_list in the second cacheline, which is hotter comparing\nto the 3rd. Swap the field with ->link, which is not as hot and\ncontrolled by flags and so not accessed unless there is a link.\n\nBy the way add a couple of comments for io_kiocb fields.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9d9dde31f8f62279a5f48c575bbc27b8290edc0c.1633373302.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: optimise kiocb layout"
    },
    {
        "commit": "6224590d242fc8c6b26941328e02a40b4384949b",
        "message": "For some reason non-off IORING_OP_TIMEOUT always fails links, it's\npretty inconvenient and unnecessary limits chaining after it to hard\nlinking, which is far from ideal, e.g. doesn't pair well with timeout\ncancellation. Add a flag forcing it to not fail links on -ETIME.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/17c7ec0fb7a6113cc6be8cdaedcada0ba836ac0e.1633199723.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: add flag to not fail link after timeout"
    },
    {
        "commit": "30d51dd4ad2040d4c90497287b69635af7c67502",
        "message": "Hiding a pointer to a struct io_buffer in rw.addr is error prone. We\nhave some place in io_kiocb, so keep kbuf's in a separate field\nwithout aliasing and risks of it being misused.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3e63a6a953b04cad81d9ea827b12344dd57b37b4.1633107393.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: clean up buffer select"
    },
    {
        "commit": "fc0ae0244bbbedf309a835afb278e39a3fc9bb94",
        "message": "Move io_req_prep() call inside of io_init_req(), it simplifies a bit\nerror handling for callers.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a0f59291fd52da4672c323542fd56fd899e23f8f.1633107393.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: init opcode in io_init_req()"
    },
    {
        "commit": "e0eb71dcfc4b862261d99f7f90169142867beb0a",
        "message": "Never return from io_drain_req() but punt to tw if we've got there but\nit's a false positive and we shouldn't actually drain.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/93583cee51b8783706b76c73196c155b28d9e762.1633107393.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: don't return from io_drain_req()"
    },
    {
        "commit": "22b2ca310afcea319c72e051df0371f668192b10",
        "message": "Add a helper io_init_req_drain for initialising requests with\nIOSQE_DRAIN set. Also move bits from preambule of io_drain_req() in\nthere, because we already modify all the bits needed inside the helper.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/dcb412825b35b1cb8891245a387d7d69f8d14cef.1633107393.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: extra a helper for drain init"
    },
    {
        "commit": "5e371265ea1d3e0cd02236b1a6d79fe322523ae8",
        "message": "Clear ->drain_active in two more cases where we check for a need of\ndraining. It's not a bug, but still may lead to some extra requests\nbeing punted to io-wq, and that may be not desirable.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d20b265f77bb4e8860b15b9987252c7c711dfcba.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: disable draining earlier"
    },
    {
        "commit": "a1cdbb4cb5f7190c429b1127892f756b7cd32db4",
        "message": "io_req_complete_state() calls io_clean_op() and it may be not entirely\nobvious, leave a comment.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/21806f862151e223fdf439e5e8ed7178a8d66979.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: comment why inline complete calls io_clean_op()"
    },
    {
        "commit": "ef05d9ebcc927260f700a94436e7c9347657bbef",
        "message": "->inflight_entry is not used anymore after converting everything to\nsingle linked lists, remove it. Also adjust io_kiocb layout, so all hot\nbits are in first 3 cachelines.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/fd8d68087ede26c4e1707ce6b175aa1eb2381f2b.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: kill off ->inflight_entry field"
    },
    {
        "commit": "6962980947e2b967ab26bfd34004b6b573597513",
        "message": "Put an explicit check for number of requests to submit. First,\nwe can turn while into do-while and it generates better code, and second\nthat if can be cheaper, e.g. by using CPU flags after sub in\nio_sqring_entries().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5926baadd20c28feab7a5e1725fedf32e4553ff7.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: restructure submit sqes to_submit checks"
    },
    {
        "commit": "d9f9d2842c9156470b3f1d3dafe5684a3c036366",
        "message": "If a request completed inline the result should only be zero, it's a\ngrave error otherwise. So, when we see REQ_F_COMPLETE_INLINE it's not\neven necessary to check the return code, and the flag check can be moved\nearlier.\n\nIt's one \"if\" less for inline completions, and same two checks for it\nnormally completing (ret == 0). Those are two cases we care about the\nmost.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ebd4e397a9c26d96c99b24447acc309741041a83.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:54 -0600 io_uring: reshuffle queue_sqe completion handling"
    },
    {
        "commit": "d475a9a6226c86b7febe3863b900b820a0e6b71c",
        "message": "Extract slow paths from __io_queue_sqe() into a function and inline the\nhot path. With that we have everything completely inlined on the\nsubmission path up until io_issue_sqe().\n\n-> io_submit_sqes()\n  -> io_submit_sqe() (inlined)\n    -> io_queue_sqe() (inlined)\n       -> __io_queue_sqe() (inlined)\n         -> io_issue_sqe()\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f1606864d95d7f26dc28c7eec3dc6ed6ec32618a.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: inline hot path of __io_queue_sqe()"
    },
    {
        "commit": "4652fe3f10e57abc3fc6ac11c431b2ef39f78c03",
        "message": "We don't want the slow path of io_queue_sqe to be inlined, so extract a\nfunction from it.\n\n   text    data     bss     dec     hex filename\n  91950   13986       8  105944   19dd8 ./fs/io_uring.o\n  91758   13986       8  105752   19d18 ./fs/io_uring.o\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/fb01253911f8fb374268f65b1ba939b54ca6583f.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: split slow path from io_queue_sqe"
    },
    {
        "commit": "2a56a9bd64dbdff4fe5cbe00b20014da07694a78",
        "message": "req->ctx->active_drain is a bit too expensive, partially because of two\ndereferences. Do a trick, if we see it set in io_init_req(), set\nREQ_F_FORCE_ASYNC and it automatically goes through a slower path where\nwe can catch it. It's nearly free to do in io_init_req() because there\nis already ->restricted check and it's in the same byte of a bitmask.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d7e7ddc63c15e8a300833132abb3eb8fd3918aef.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: remove drain_active check from hot path"
    },
    {
        "commit": "f15a3431775a598ed89028acee334200160cc2d6",
        "message": "There are two call sites of io_queue_sqe() in io_submit_sqe(), combine\nthem into one, because io_queue_sqe() is inline and we don't want to\nbloat binary, and will become even bigger\n\n   text    data     bss     dec     hex filename\n  92126   13986       8  106120   19e88 ./fs/io_uring.o\n  91966   13986       8  105960   19de8 ./fs/io_uring.o\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/506124b8e767f0a4576f7a459f6aea3d13fb4dda.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: deduplicate io_queue_sqe() call sites"
    },
    {
        "commit": "553deffd0920fc52e8c91e7f8a42d1186a75760a",
        "message": "Submission state and ctx and coupled together, no need to passs\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e22d77a5786ef77e0c49b933ad74bae55cfb6ca6.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: don't pass state to io_submit_state_end"
    },
    {
        "commit": "1cce17aca621c38c657410dc278a48cda982dd2e",
        "message": "io_free_batch_list() iterates all requests in the passed in list,\nso we don't really need to know the tail but can keep iterating until\nmeet NULL. Just pass the first node into it and it will be enough.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4a12c84b6d887d980e05f417ba4172d04c64acae.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: don't pass tail into io_free_batch_list"
    },
    {
        "commit": "d4b7a5ef2b9c06def90d12db9b99bd12d75758fb",
        "message": "We now have a single function for batched put of requests, just inline\nstruct req_batch and all related helpers into it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/595a2917f80dd94288cd7203052c7934f5446580.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: inline completion batching helpers"
    },
    {
        "commit": "f5ed3bcd5b117ffe73b9dc2394bbbad26a68c086",
        "message": "First, convert rest of iopoll bits to single linked lists, and also\nreplace per-request list_add_tail() with splicing a part of slist.\n\nWith that, use io_free_batch_list() to put/free requests. The main\nadvantage of it is that it's now the only user of struct req_batch and\nfriends, and so they can be inlined. The main overhead there was\nper-request call to not-inlined io_req_free_batch(), which is expensive\nenough.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b37fc6d5954b241e025eead7ab92c6f44a42f229.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: optimise batch completion"
    },
    {
        "commit": "b3fa03fd1b17f557e2c43736440feed66fb86741",
        "message": "Convert explicit barrier around iopoll_completed to smp_load_acquire()\nand smp_store_release(). Similar on the callback side, but replaces a\nsingle smp_rmb() with per-request smp_load_acquire(), neither imply any\nextra CPU ordering for x86. Use READ_ONCE as usual where it doesn't\nmatter.\n\nUse it to move filling CQEs by iopoll earlier, that will be necessary\nto avoid traversing the list one extra time in the future.\n\nSuggested-by: Bart Van Assche <bvanassche@acm.org>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8bd663cb15efdc72d6247c38ee810964e744a450.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: convert iopoll_completed to store_release"
    },
    {
        "commit": "3aa83bfb6e5c3a89d0ec67e598ee04bfc1425b13",
        "message": "Add a helper io_free_batch_list(), which takes a single linked list and\nputs/frees all requests from it in an efficient manner. Will be reused\nlater.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4fc8306b542c6b1dd1d08e8021ef3bdb0ad15010.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: add a helper for batch free"
    },
    {
        "commit": "5eef4e87eb0b2b8482f6a77ebcad067636a867b3",
        "message": "Use single linked lists for keeping iopoll requests, takes less space,\nmay be faster, but mostly will be of benefit for further patches.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/314033676b100cd485518c3bc55e1b95a0dcd71f.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: use single linked list for iopoll"
    },
    {
        "commit": "e3f721e6f6d5d916edf71e5f26ac0547a4b28e1e",
        "message": "The main loop of io_do_iopoll() iterates and does ->iopoll() until it\nmeets a first completed request, then it continues from that position\nand splices requests to pass them through io_iopoll_complete().\n\nSplit the loop in two for clearness, iopolling and reaping completed\nrequests from the list.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a7f6fd27a94845e5dc925a47a4a9765a92e514fb.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: split iopoll loop"
    },
    {
        "commit": "c2b6c6bc4e0d3452ac4e208c198f6062e0f1d9df",
        "message": "Replace struct list_head free_list serving for caching requests with\nsingly linked stack, which is faster.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1bc942b82422fb2624b8353bd93aca183a022846.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: replace list with stack for req caches"
    },
    {
        "commit": "3ab665b74e59ad467b3eab5b2b95d378cea78b22",
        "message": "We have several of request allocation layers, remove the last one, which\nis the submit->reqs array, and always use submit->free_reqs instead.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8547095c35f7a87bab14f6447ecd30a273ed7500.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: remove allocation cache array"
    },
    {
        "commit": "6f33b0bc4ea43f5c5ce7b7c9ab66051f80837862",
        "message": "Currently we collect requests for completion batching in an array.\nReplace them with a singly linked list. It's as fast as arrays but\ndoesn't take some much space in ctx, and will be used in future patches.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a666826f2854d17e9fb9417fb302edfeb750f425.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: use slist for completion batching"
    },
    {
        "commit": "5ba3c874eb8acb09fd8aea78ded67e023931894a",
        "message": "Don't pass nr_events pointer around but return directly, it's less\nexpensive than pointer increments.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f771a8153a86f16f12ff4272524e9e549c5de40b.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: make io_do_iopoll return number of reqs"
    },
    {
        "commit": "87a115fb715bf1f6765e122babbcba566ada286e",
        "message": "We don't really need to pass the number of requests to complete into\nio_do_iopoll(), a flag whether to enforce non-spin mode is enough.\n\nShould be straightforward, maybe except io_iopoll_check(). We pass !min\nthere, because we do never enter with the number of already reaped\nrequests is larger than the specified @min, apart from the first\niteration, where nr_events is 0 and so the final check should be\nidentical.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/782b39d1d8ec584eae15bca0a1feb6f0571fe5b8.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: force_nonspin"
    },
    {
        "commit": "6878b40e7b28bd780dedb7d505c13dbf82b73290",
        "message": "Hint the compiler that it's not as likely to have creds different from\ncurrent attached to a request. The current code generation is far from\nideal, hopefully it can help to some compilers to remove duplicated jump\ntables and so.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e7815251ac4bf5a4a23d298c752f029ae19f3837.1632516769.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: mark having different creds unlikely"
    },
    {
        "commit": "8d4af6857c6fb5b1922218e93052bee29eb540f4",
        "message": "boolean value is good enough for io_alloc_async_data.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20210922101522.9179-1-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: return boolean value for io_alloc_async_data"
    },
    {
        "commit": "68fe256aadc0db70cb27132a6f5583819794d867",
        "message": "IOSQE_IO_DRAIN is quite marginal and we don't care too much about\nIOSQE_BUFFER_SELECT. Save to ifs and hide both of them under\nSQE_VALID_FLAGS check. Now we first check whether it uses a \"safe\"\nsubset, i.e. without DRAIN and BUFFER_SELECT, and only if it's not\ntrue we test the rest of the flags.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/dccfb9ab2ab0969a2d8dc59af88fa0ce44eeb1d5.1631703764.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: optimise io_req_init() sqe flags checks"
    },
    {
        "commit": "a3f349071eb0ae2aa18ac2da55060865d8190cf0",
        "message": "Now completions are done from task context, that means that it's either\nthe task itself, task_work or io-wq worker. In all those cases the ctx\nwill be staying alive by mutexing, explicit referencing or req references\nby iowq. Remove extra ctx pinning from io_req_complete_post().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/60a0e96434c16ab4fe587651448290d61ec9a113.1631703756.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:53 -0600 io_uring: remove ctx referencing from complete_post"
    },
    {
        "commit": "83f84356bc8f2dda9d0c9c7edb94decf71a36d26",
        "message": "Developers may need some uring info to help themselves debug and address\nissues in production. This includes sqring/cqring head/tail and the\ndetailed sqe/cqe info, which is very useful when an application is hung\non a ring.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20210913130854.38542-1-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:52 -0600 io_uring: add more uring info to fdinfo for debug"
    },
    {
        "commit": "d97ec6239ad8684e4d31b12b39cefca6782c09a3",
        "message": "TWA_SIGNAL already wakes the thread, no need in wake_up_process() after\nit.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7e90cf643f633e857443e0c9e72471b221735c50.1631115443.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:52 -0600 io_uring: kill extra wake_up_process in tw add"
    },
    {
        "commit": "c450178d9be9dc44327f3b526f82c36e4c5d362b",
        "message": "We don't do io_submit_flush_completions() when there is no requests\nenqueued, and every single caller checks for it. Hide that check into\nthe function not forgetting about inlining. That will make it much\neasier for changing the empty check condition in the future.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d7ff8cef5da1b38e8ea648f5aad9a315ddfc7b57.1631115443.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:52 -0600 io_uring: dedup CQE flushing non-empty checks"
    },
    {
        "commit": "d81499bfcd471e6b920683f8fcf06ce65f84286b",
        "message": "Inline part of __io_req_find_next() that returns a request but doesn't\nneed io_disarm_next(). It's just two places, but makes links a bit\nfaster.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4126d13f23d0e91b39b3558e16bd86cafa7fcef2.1631115443.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:52 -0600 io_uring: inline linked part of io_req_find_next"
    },
    {
        "commit": "6b639522f63f82437350038a4925633f769e4ec8",
        "message": "io_dismantle_req() is hot, and not _too_ huge. Inline it, there are 3\ncall sites, which hopefully will turn into 2 in the future.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bdd2dc30716cac270c2403e99bccd6286e4ae201.1631115443.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:52 -0600 io_uring: inline io_dismantle_req"
    },
    {
        "commit": "4b628aeb69cc49adf56b9c1fde43558143e810f5",
        "message": "->ios_left is only used to decide whether to plug or not, kill it to\navoid this extra accounting, just use the initial submission number.\nThere is no much difference in regards of enabling plugging, where this\none does it in a few more cases, but all major ones should be covered\nwell.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f13993bcf5b477f9a7d52881fc49f9457ea9870a.1631115443.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:52 -0600 io_uring: kill off ios_left"
    },
    {
        "commit": "a87acfde949140946456859eafa5f15175d0f960",
        "message": "I recently had to look at a production problem where a request ended\nup getting the dreaded -EINVAL error on submit. The most used and\nhence useless of error codes, as it just tells you that something\nwas wrong with your request, but not more than that.\n\nLet's dump the full sqe contents if we run into an issue failure,\nthat'll allow easier diagnosing of a wide variety of issues.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-19 05:49:52 -0600 io_uring: dump sqe contents if issue fails"
    },
    {
        "commit": "4f5022453acd0f7b28012e20b7d048470f129894",
        "message": "Trivial to do now, just need our own io_comp_batch on the stack and pass\nthat in to the usual command completion handling.\n\nI pondered making this dependent on how many entries we had to process,\nbut even for a single entry there's no discernable difference in\nperformance or latency. Running a sync workload over io_uring:\n\nt/io_uring -b512 -d1 -s1 -c1 -p0 -F1 -B1 -n2 /dev/nvme1n1 /dev/nvme2n1\n\nyields the below performance before the patch:\n\nIOPS=254820, BW=124MiB/s, IOS/call=1/1, inflight=(1 1)\nIOPS=251174, BW=122MiB/s, IOS/call=1/1, inflight=(1 1)\nIOPS=250806, BW=122MiB/s, IOS/call=1/1, inflight=(1 1)\n\nand the following after:\n\nIOPS=255972, BW=124MiB/s, IOS/call=1/1, inflight=(1 1)\nIOPS=251920, BW=123MiB/s, IOS/call=1/1, inflight=(1 1)\nIOPS=251794, BW=122MiB/s, IOS/call=1/1, inflight=(1 1)\n\nwhich definitely isn't slower, about the same if you factor in a bit of\nvariance. For peak performance workloads, benchmarking shows a 2%\nimprovement.\n\nReviewed-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-18 14:40:47 -0600 nvme: wire up completion batching for the IRQ path"
    },
    {
        "commit": "b688f11e86c9a22169a0e522530982735d2db19b",
        "message": "Wire up using an io_comp_batch for f_op->iopoll(). If the lower stack\nsupports it, we can handle high rates of polled IO more efficiently.\n\nThis raises the single core efficiency on my system from ~6.1M IOPS to\n~6.6M IOPS running a random read workload at depth 128 on two gen2\nOptane drives.\n\nReviewed-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-18 14:40:46 -0600 io_uring: utilize the io batching infrastructure for more efficient polled IO"
    },
    {
        "commit": "19416123ab3e1348b3532347af221d8f60838431",
        "message": "'struct bvec_iter' is embedded into 'struct bio', define it as packed\nso that we can get one extra 4bytes for other uses without expanding\nbio.\n\n'struct bvec_iter' is often allocated on stack, so making it packed\ndoesn't affect performance. Also I have run io_uring on both\nnvme/null_blk, and not observe performance effect in this way.\n\nSuggested-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nReviewed-by: Sagi Grimberg <sagi@grimberg.me>\nReviewed-by: Hannes Reinecke <hare@suse.de>\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nTested-by: Mark Wunderlich <mark.wunderlich@intel.com>\nLink: https://lore.kernel.org/r/20211012111226.760968-14-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-18 06:17:36 -0600 block: define 'struct bvec_iter' as packed"
    },
    {
        "commit": "d729cf9acb9311956c8a37113dcfa0160a2d9665",
        "message": "There is no point in sleeping for the expected I/O completion timeout\nin the io_uring async polling model as we never poll for a specific\nI/O.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nTested-by: Mark Wunderlich <mark.wunderlich@intel.com>\nLink: https://lore.kernel.org/r/20211012111226.760968-11-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-18 06:17:36 -0600 io_uring: don't sleep when polling for I/O"
    },
    {
        "commit": "ef99b2d37666b7a600baab9e1c4944436652b0a2",
        "message": "Switch the boolean spin argument to blk_poll to passing a set of flags\ninstead.  This will allow to control polling behavior in a more fine\ngrained way.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nTested-by: Mark Wunderlich <mark.wunderlich@intel.com>\nLink: https://lore.kernel.org/r/20211012111226.760968-10-hch@lst.de\n[axboe: adapt to changed io_uring iopoll]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-18 06:17:36 -0600 block: replace the spin argument to blk_iopoll with a flags argument"
    },
    {
        "commit": "30da1b45b130c70945b033900f45c9d61d6f3b4a",
        "message": "syscall-level code can't just poke into the details of the poll cookie,\nwhich is private information of the block layer.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nLink: https://lore.kernel.org/r/20211012111226.760968-5-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-18 06:17:35 -0600 io_uring: fix a layering violation in io_iopoll_req_issued"
    },
    {
        "commit": "94c2ed58d0d856a35c04365bdb39fee6e77547de",
        "message": "The polling support in the legacy direct-io support is a little crufty.\nIt already doesn't support the asynchronous polling needed for io_uring\npolling, and is hard to adopt to upcoming changes in the polling\ninterfaces.  Given that all the major file systems already use the iomap\ndirect I/O code, just drop the polling support.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nTested-by: Mark Wunderlich <mark.wunderlich@intel.com>\nLink: https://lore.kernel.org/r/20211012111226.760968-2-hch@lst.de\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-18 06:17:35 -0600 direct-io: remove blk_poll support"
    },
    {
        "commit": "d38a9c04c0d5637a828269dccb9703d42d40d42b",
        "message": "Currently we scan the entire plug list, which is potentially very\nexpensive. In an IOPS bound workload, we can drive about 5.6M IOPS with\nmerging enabled, and profiling shows that the plug merge check is the\n(by far) most expensive thing we're doing:\n\n  Overhead  Command   Shared Object     Symbol\n  +   20.89%  io_uring  [kernel.vmlinux]  [k] blk_attempt_plug_merge\n  +    4.98%  io_uring  [kernel.vmlinux]  [k] io_submit_sqes\n  +    4.78%  io_uring  [kernel.vmlinux]  [k] blkdev_direct_IO\n  +    4.61%  io_uring  [kernel.vmlinux]  [k] blk_mq_submit_bio\n\nInstead of browsing the whole list, just check the previously inserted\nentry. That is enough for a naive merge check and will catch most cases,\nand for devices that need full merging, the IO scheduler attached to\nsuch devices will do that anyway. The plug merge is meant to be an\ninexpensive check to avoid getting a request, but if we repeatedly\nscan the list for every single insert, it is very much not a cheap\ncheck.\n\nWith this patch, the workload instead runs at ~7.0M IOPS, providing\na 25% improvement. Disabling merging entirely yields another 5%\nimprovement.\n\nReviewed-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-18 06:17:35 -0600 block: only check previous entry for plug merge attempt"
    },
    {
        "commit": "cc0af0a95172db52db2ab41b1e8a9c9ac0930b63",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single fix for a wrong condition for grabbing a lock, a\n  regression in this merge window\"\n\n* tag 'io_uring-5.15-2021-10-17' of git://git.kernel.dk/linux-block:\n  io_uring: fix wrong condition to grab uring lock",
        "kernel_version": "v5.15-rc6",
        "release_date": "2021-10-17 19:20:13 -1000 Merge tag 'io_uring-5.15-2021-10-17' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "14cfbb7a7856f190035f8e53045bdbfa648fae41",
        "message": "Grab uring lock when we are in io-worker rather than in the original\nor system-wq context since we already hold it in these two situation.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nFixes: b66ceaf324b3 (\"io_uring: move iopoll reissue into regular IO path\")\nLink: https://lore.kernel.org/r/20211014140400.50235-1-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc6",
        "release_date": "2021-10-14 09:06:11 -0600 io_uring: fix wrong condition to grab uring lock"
    },
    {
        "commit": "c52e7b855b33ff2a3af57b1b1d114720cd39ec7e",
        "message": "Linux 5.15-rc4\n\n* tag 'v5.15-rc4': (320 commits)\n  Linux 5.15-rc4\n  elf: don't use MAP_FIXED_NOREPLACE for elf interpreter mappings\n  objtool: print out the symbol type when complaining about it\n  kvm: fix objtool relocation warning\n  cachefiles: Fix oops in trace_cachefiles_mark_buried due to NULL object\n  drm/i915: fix blank screen booting crashes\n  hwmon: (w83793) Fix NULL pointer dereference by removing unnecessary structure field\n  hwmon: (w83792d) Fix NULL pointer dereference by removing unnecessary structure field\n  hwmon: (w83791d) Fix NULL pointer dereference by removing unnecessary structure field\n  hwmon: (pmbus/mp2975) Add missed POUT attribute for page 1 mp2975 controller\n  hwmon: (pmbus/ibm-cffps) max_power_out swap changes\n  hwmon: (occ) Fix P10 VRM temp sensors\n  thermal: Update information in MAINTAINERS\n  io_uring: kill fasync\n  sched: Always inline is_percpu_thread()\n  sched/fair: Null terminate buffer when updating tunable_scaling\n  sched/fair: Add ancestors of unthrottled undecayed cfs_rq\n  perf/core: fix userpage->time_enabled of inactive events\n  perf/x86/intel: Update event constraints for ICX\n  perf/x86: Reset destroy callback on event init failure\n  ...",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-10-04 07:52:13 +0200 Merge tag 'v5.15-rc4' into media_tree"
    },
    {
        "commit": "65893b49d868bd2de5fbac41744d1eaecc739629",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two fixes in here:\n\n   - The signal issue that was discussed start of this week (me).\n\n   - Kill dead fasync support in io_uring. Looks like it was broken\n     since io_uring was initially merged, and given that nobody has ever\n     complained about it, let's just kill it (Pavel)\"\n\n* tag 'io_uring-5.15-2021-10-01' of git://git.kernel.dk/linux-block:\n  io_uring: kill fasync\n  io-wq: exclusively gate signal based exit on get_signal() return",
        "kernel_version": "v5.15-rc4",
        "release_date": "2021-10-02 10:26:19 -0700 Merge tag 'io_uring-5.15-2021-10-01' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "3f008385d46d3cea4a097d2615cd485f2184ba26",
        "message": "We have never supported fasync properly, it would only fire when there\nis something polling io_uring making it useless. The original support came\nin through the initial io_uring merge for 5.1. Since it's broken and\nnobody has reported it, get rid of the fasync bits.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2f7ca3d344d406d34fa6713824198915c41cea86.1633080236.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc4",
        "release_date": "2021-10-01 11:16:02 -0600 io_uring: kill fasync"
    },
    {
        "commit": "f6f360aef0e70a45cbf43db1dd9df5a5e96d9836",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"This one looks a bit bigger than it is, but that's mainly because 2/3\n  of it is enabling IORING_OP_CLOSE to close direct file descriptors.\n\n  We've had a few folks using them and finding it confusing that the way\n  to close them is through using -1 for file update, this just brings\n  API symmetry for direct descriptors. Hence I think we should just do\n  this now and have a better API for 5.15 release. There's some room for\n  de-duplicating the close code, but we're leaving that for the next\n  merge window.\n\n  Outside of that, just small fixes:\n\n   - Poll race fixes (Hao)\n\n   - io-wq core dump exit fix (me)\n\n   - Reschedule around potentially intensive tctx and buffer iterators\n     on teardown (me)\n\n   - Fix for always ending up punting files update to io-wq (me)\n\n   - Put the provided buffer meta data under memcg accounting (me)\n\n   - Tweak for io_write(), removing dead code that was added with the\n     iterator changes in this release (Pavel)\"\n\n* tag 'io_uring-5.15-2021-09-25' of git://git.kernel.dk/linux-block:\n  io_uring: make OP_CLOSE consistent with direct open\n  io_uring: kill extra checks in io_write()\n  io_uring: don't punt files update to io-wq unconditionally\n  io_uring: put provided buffer meta data under memcg accounting\n  io_uring: allow conditional reschedule for intensive iterators\n  io_uring: fix potential req refcount underflow\n  io_uring: fix missing set of EPOLLONESHOT for CQ ring overflow\n  io_uring: fix race between poll completion and cancel_hash insertion\n  io-wq: ensure we exit if thread group is exiting",
        "kernel_version": "v5.15-rc3",
        "release_date": "2021-09-25 15:51:08 -0700 Merge tag 'io_uring-5.15-2021-09-25' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "7df778be2f61e1a23002d1f2f5d6aaf702771eb8",
        "message": "From recently open/accept are now able to manipulate fixed file table,\nbut it's inconsistent that close can't. Close the gap, keep API same as\nwith open/accept, i.e. via sqe->file_slot.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc3",
        "release_date": "2021-09-24 14:07:54 -0600 io_uring: make OP_CLOSE consistent with direct open"
    },
    {
        "commit": "9f3a2cb228c28606895d15f13b30d1f7402dc745",
        "message": "We don't retry short writes and so we would never get to async setup in\nio_write() in that case. Thus ret2 > 0 is always false and\niov_iter_advance() is never used. Apparently, the same is found by\nCoverity, which complains on the code.\n\nFixes: cd65869512ab (\"io_uring: use iov_iter state save/restore helpers\")\nReported-by: Dave Jones <davej@codemonkey.org.uk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5b33e61034748ef1022766efc0fb8854cfcf749c.1632500058.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc3",
        "release_date": "2021-09-24 10:26:11 -0600 io_uring: kill extra checks in io_write()"
    },
    {
        "commit": "cdb31c29d397a8076d81fd1458d091c647ef94ba",
        "message": "There's no reason to punt it unconditionally, we just need to ensure that\nthe submit lock grabbing is conditional.\n\nFixes: 05f3fb3c5397 (\"io_uring: avoid ring quiesce for fixed file set unregister and update\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc3",
        "release_date": "2021-09-24 10:24:34 -0600 io_uring: don't punt files update to io-wq unconditionally"
    },
    {
        "commit": "9990da93d2bf9892c2c14c958bef050d4e461a1a",
        "message": "For each provided buffer, we allocate a struct io_buffer to hold the\ndata associated with it. As a large number of buffers can be provided,\naccount that data with memcg.\n\nFixes: ddf0322db79c (\"io_uring: add IORING_OP_PROVIDE_BUFFERS\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc3",
        "release_date": "2021-09-24 10:24:34 -0600 io_uring: put provided buffer meta data under memcg accounting"
    },
    {
        "commit": "8bab4c09f24ec8d4a7a78ab343620f89d3a24804",
        "message": "If we have a lot of threads and rings, the tctx list can get quite big.\nThis is especially true if we keep creating new threads and rings.\nLikewise for the provided buffers list. Be nice and insert a conditional\nreschedule point while iterating the nodes for deletion.\n\nLink: https://lore.kernel.org/io-uring/00000000000064b6b405ccb41113@google.com/\nReported-by: syzbot+111d2a03f51f5ae73775@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc3",
        "release_date": "2021-09-24 10:24:34 -0600 io_uring: allow conditional reschedule for intensive iterators"
    },
    {
        "commit": "5b7aa38d86f348847a48f71e9ac7715406de900e",
        "message": "For multishot mode, there may be cases like:\n\niowq                                 original context\nio_poll_add\n  _arm_poll()\n  mask = vfs_poll() is not 0\n  if mask\n(2)  io_poll_complete()\n  compl_unlock\n   (interruption happens\n    tw queued to original\n    context)\n                                     io_poll_task_func()\n                                     compl_lock\n                                 (3) done = io_poll_complete() is true\n                                     compl_unlock\n                                     put req ref\n(1) if (poll->flags & EPOLLONESHOT)\n      put req ref\n\nEPOLLONESHOT flag in (1) may be from (2) or (3), so there are multiple\ncombinations that can cause ref underfow.\nLet's address it by:\n- check the return value in (2) as done\n- change (1) to if (done)\n    in this way, we only do ref put in (1) if 'oneshot flag' is from\n    (2)\n- do poll.done check in io_poll_task_func(), so that we won't put ref\n  for the second time.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20210922101238.7177-4-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc3",
        "release_date": "2021-09-24 10:24:34 -0600 io_uring: fix potential req refcount underflow"
    },
    {
        "commit": "a62682f92eedb41c1cd8290fa875a4b85624fb9a",
        "message": "We should set EPOLLONESHOT if cqring_fill_event() returns false since\nio_poll_add() decides to put req or not by it.\n\nFixes: 5082620fb2ca (\"io_uring: terminate multishot poll for CQ ring overflow\")\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20210922101238.7177-3-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc3",
        "release_date": "2021-09-24 10:24:34 -0600 io_uring: fix missing set of EPOLLONESHOT for CQ ring overflow"
    },
    {
        "commit": "bd99c71bd14072ce2920f6d0c2fe43df072c653c",
        "message": "If poll arming and poll completion runs in parallel, there maybe races.\nFor instance, run io_poll_add in iowq and io_poll_task_func in original\ncontext, then:\n\n  iowq                                      original context\n  io_poll_add\n    vfs_poll\n     (interruption happens\n      tw queued to original\n      context)                              io_poll_task_func\n                                              generate cqe\n                                              del from cancel_hash[]\n    if !poll.done\n      insert to cancel_hash[]\n\nThe entry left in cancel_hash[], similar case for fast poll.\nFix it by set poll.done = true when del from cancel_hash[].\n\nFixes: 5082620fb2ca (\"io_uring: terminate multishot poll for CQ ring overflow\")\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20210922101238.7177-2-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc3",
        "release_date": "2021-09-24 10:24:34 -0600 io_uring: fix race between poll completion and cancel_hash insertion"
    },
    {
        "commit": "d9d8c93938c40e12de91650d04fceb99d92dad8a",
        "message": "Add Smack privilege checks for io_uring. Use CAP_MAC_OVERRIDE\nfor the override_creds case and CAP_MAC_ADMIN for creating a\npolling thread. These choices are based on conjecture regarding\nthe intent of the surrounding code.\n\nSigned-off-by: Casey Schaufler <casey@schaufler-ca.com>\n[PM: make the smack_uring_* funcs static, remove debug code]\nSigned-off-by: Paul Moore <paul@paul-moore.com>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-09-19 22:40:51 -0400 Smack: Brutalist io_uring support"
    },
    {
        "commit": "740b03414b20e7f1879cd99aae27d8c401bbcbf9",
        "message": "This patch implements two new io_uring access controls, specifically\nsupport for controlling the io_uring \"personalities\" and\nIORING_SETUP_SQPOLL.  Controlling the sharing of io_urings themselves\nis handled via the normal file/inode labeling and sharing mechanisms.\n\nThe io_uring { override_creds } permission restricts which domains\nthe subject domain can use to override it's own credentials.\nGranting a domain the io_uring { override_creds } permission allows\nit to impersonate another domain in io_uring operations.\n\nThe io_uring { sqpoll } permission restricts which domains can create\nasynchronous io_uring polling threads.  This is important from a\nsecurity perspective as operations queued by this asynchronous thread\ninherit the credentials of the thread creator by default; if an\nio_uring is shared across process/domain boundaries this could result\nin one domain impersonating another.  Controlling the creation of\nsqpoll threads, and the sharing of io_urings across processes, allow\npolicy authors to restrict the ability of one domain to impersonate\nanother via io_uring.\n\nAs a quick summary, this patch adds a new object class with two\npermissions:\n\n io_uring { override_creds sqpoll }\n\nThese permissions can be seen in the two simple policy statements\nbelow:\n\n  allow domA_t domB_t : io_uring { override_creds };\n  allow domA_t self : io_uring { sqpoll };\n\nSigned-off-by: Paul Moore <paul@paul-moore.com>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-09-19 22:40:32 -0400 selinux: add support for the io_uring access controls"
    },
    {
        "commit": "cdc1404a40461faba23c5a5ad40adcc7eecc1580",
        "message": "A full expalantion of io_uring is beyond the scope of this commit\ndescription, but in summary it is an asynchronous I/O mechanism\nwhich allows for I/O requests and the resulting data to be queued\nin memory mapped \"rings\" which are shared between the kernel and\nuserspace.  Optionally, io_uring offers the ability for applications\nto spawn kernel threads to dequeue I/O requests from the ring and\nsubmit the requests in the kernel, helping to minimize the syscall\noverhead.  Rings are accessed in userspace by memory mapping a file\ndescriptor provided by the io_uring_setup(2), and can be shared\nbetween applications as one might do with any open file descriptor.\nFinally, process credentials can be registered with a given ring\nand any process with access to that ring can submit I/O requests\nusing any of the registered credentials.\n\nWhile the io_uring functionality is widely recognized as offering a\nvastly improved, and high performing asynchronous I/O mechanism, its\nability to allow processes to submit I/O requests with credentials\nother than its own presents a challenge to LSMs.  When a process\ncreates a new io_uring ring the ring's credentials are inhertied\nfrom the calling process; if this ring is shared with another\nprocess operating with different credentials there is the potential\nto bypass the LSMs security policy.  Similarly, registering\ncredentials with a given ring allows any process with access to that\nring to submit I/O requests with those credentials.\n\nIn an effort to allow LSMs to apply security policy to io_uring I/O\noperations, this patch adds two new LSM hooks.  These hooks, in\nconjunction with the LSM anonymous inode support previously\nsubmitted, allow an LSM to apply access control policy to the\nsharing of io_uring rings as well as any io_uring credential changes\nrequested by a process.\n\nThe new LSM hooks are described below:\n\n * int security_uring_override_creds(cred)\n   Controls if the current task, executing an io_uring operation,\n   is allowed to override it's credentials with @cred.  In cases\n   where the current task is a user application, the current\n   credentials will be those of the user application.  In cases\n   where the current task is a kernel thread servicing io_uring\n   requests the current credentials will be those of the io_uring\n   ring (inherited from the process that created the ring).\n\n * int security_uring_sqpoll(void)\n   Controls if the current task is allowed to create an io_uring\n   polling thread (IORING_SETUP_SQPOLL).  Without a SQPOLL thread\n   in the kernel processes must submit I/O requests via\n   io_uring_enter(2) which allows us to compare any requested\n   credential changes against the application making the request.\n   With a SQPOLL thread, we can no longer compare requested\n   credential changes against the application making the request,\n   the comparison is made against the ring's credentials.\n\nSigned-off-by: Paul Moore <paul@paul-moore.com>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-09-19 22:37:21 -0400 lsm,io_uring: add LSM hooks to io_uring"
    },
    {
        "commit": "91a9ab7c942aaa40ac5957eebe71ddae30b2a49c",
        "message": "Converting io_uring's anonymous inode to the secure anon inode API\nenables LSMs to enforce policy on the io_uring anonymous inodes if\nthey chose to do so.  This is an important first step towards\nproviding the necessary mechanisms so that LSMs can apply security\npolicy to io_uring operations.\n\nSigned-off-by: Paul Moore <paul@paul-moore.com>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-09-19 22:36:24 -0400 io_uring: convert io_uring to the secure anon inode interface"
    },
    {
        "commit": "67daf270cebcf7aab4b3292b36f9adf357b23ddc",
        "message": "This patch adds basic audit io_uring filtering, using as much of the\nexisting audit filtering infrastructure as possible.  In order to do\nthis we reuse the audit filter rule's syscall mask for the io_uring\noperation and we create a new filter for io_uring operations as\nAUDIT_FILTER_URING_EXIT/audit_filter_list[7].\n\nThanks to Richard Guy Briggs for his review, feedback, and work on\nthe corresponding audit userspace changes.\n\nAcked-by: Richard Guy Briggs <rgb@redhat.com>\nSigned-off-by: Paul Moore <paul@paul-moore.com>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-09-19 22:34:38 -0400 audit: add filtering for io_uring records"
    },
    {
        "commit": "5bd2182d58e9d9c6279b7a8a2f9b41add0e7f9cb",
        "message": "This patch adds basic auditing to io_uring operations, regardless of\ntheir context.  This is accomplished by allocating audit_context\nstructures for the io-wq worker and io_uring SQPOLL kernel threads\nas well as explicitly auditing the io_uring operations in\nio_issue_sqe().  Individual io_uring operations can bypass auditing\nthrough the \"audit_skip\" field in the struct io_op_def definition for\nthe operation; although great care must be taken so that security\nrelevant io_uring operations do not bypass auditing; please contact\nthe audit mailing list (see the MAINTAINERS file) with any questions.\n\nThe io_uring operations are audited using a new AUDIT_URINGOP record,\nan example is shown below:\n\n  type=UNKNOWN[1336] msg=audit(1631800225.981:37289):\n    uring_op=19 success=yes exit=0 items=0 ppid=15454 pid=15681\n    uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0\n    subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023\n    key=(null)\n\nThanks to Richard Guy Briggs for review and feedback.\n\nSigned-off-by: Paul Moore <paul@paul-moore.com>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-09-19 22:10:44 -0400 audit,io_uring,io-wq: add some basic audit support to io_uring"
    },
    {
        "commit": "ddf21bd8ab984ccaa924f090fc7f515bb6d51414",
        "message": "Pull io_uring iov_iter retry fixes from Jens Axboe:\n \"This adds a helper to save/restore iov_iter state, and modifies\n  io_uring to use it.\n\n  After that is done, we can now kill the iter->truncated addition that\n  we added for this release. The io_uring change is being overly\n  cautious with the save/restore/advance, but better safe than sorry and\n  we can always improve that and reduce the overhead if it proves to be\n  of concern. The only case to be worried about in this regard is huge\n  IO, where iteration can take a while to iterate segments.\n\n  I spent some time writing test cases, and expanded the coverage quite\n  a bit from the last posting of this. liburing carries this regression\n  test case now:\n\n      https://git.kernel.dk/cgit/liburing/tree/test/file-verify.c\n\n  which exercises all of this. It now also supports provided buffers,\n  and explicitly tests for end-of-file/device truncation as well.\n\n  On top of that, Pavel sanitized the IOPOLL retry path to follow the\n  exact same pattern as normal IO\"\n\n* tag 'iov_iter.3-5.15-2021-09-17' of git://git.kernel.dk/linux-block:\n  io_uring: move iopoll reissue into regular IO path\n  Revert \"iov_iter: track truncated size\"\n  io_uring: use iov_iter state save/restore helpers\n  iov_iter: add helper to save iov_iter state",
        "kernel_version": "v5.15-rc2",
        "release_date": "2021-09-17 09:23:44 -0700 Merge tag 'iov_iter.3-5.15-2021-09-17' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "0bc7eb03cbd3e5d057cbe2ee15ddedf168f25a8d",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Mostly fixes for regressions in this cycle, but also a few fixes that\n  predate this release.\n\n  The odd one out is a tweak to the direct files added in this release,\n  where attempting to reuse a slot is allowed instead of needing an\n  explicit removal of that slot first. It's a considerable improvement\n  in usability to that API, hence I'm sending it for -rc2.\n\n   - io-wq race fix and cleanup (Hao)\n\n   - loop_rw_iter() type fix\n\n   - SQPOLL max worker race fix\n\n   - Allow poll arm for O_NONBLOCK files, fixing a case where it's\n     impossible to properly use io_uring if you cannot modify the file\n     flags\n\n   - Allow direct open to simply reuse a slot, instead of needing it\n     explicitly removed first (Pavel)\n\n   - Fix a case where we missed signal mask restoring in cqring_wait, if\n     we hit -EFAULT (Xiaoguang)\"\n\n* tag 'io_uring-5.15-2021-09-17' of git://git.kernel.dk/linux-block:\n  io_uring: allow retry for O_NONBLOCK if async is supported\n  io_uring: auto-removal for direct open/accept\n  io_uring: fix missing sigmask restore in io_cqring_wait()\n  io_uring: pin SQPOLL data before unlocking ring lock\n  io-wq: provide IO_WQ_* constants for IORING_REGISTER_IOWQ_MAX_WORKERS arg items\n  io-wq: fix potential race of acct->nr_workers\n  io-wq: code clean of io_wqe_create_worker()\n  io_uring: ensure symmetry in handling iter types in loop_rw_iter()",
        "kernel_version": "v5.15-rc2",
        "release_date": "2021-09-17 09:19:59 -0700 Merge tag 'io_uring-5.15-2021-09-17' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "b66ceaf324b394428bb47054140ddf03d8172e64",
        "message": "230d50d448acb (\"io_uring: move reissue into regular IO path\")\nmade non-IOPOLL I/O to not retry from ki_complete handler. Follow it\nsteps and do the same for IOPOLL. Same problems, same implementation,\nsame -EAGAIN assumptions.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f80dfee2d5fa7678f0052a8ab3cfca9496a112ca.1631699928.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc2",
        "release_date": "2021-09-15 09:22:35 -0600 io_uring: move iopoll reissue into regular IO path"
    },
    {
        "commit": "cd65869512ab5668a5d16f789bc4da1319c435c4",
        "message": "Get rid of the need to do re-expand and revert on an iterator when we\nencounter a short IO, or failure that warrants a retry. Use the new\nstate save/restore helpers instead.\n\nWe keep the iov_iter_state persistent across retries, if we need to\nrestart the read or write operation. If there's a pending retry, the\noperation will always exit with the state correctly saved.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc2",
        "release_date": "2021-09-15 09:22:32 -0600 io_uring: use iov_iter state save/restore helpers"
    },
    {
        "commit": "5d329e1286b0a040264e239b80257c937f6e685f",
        "message": "A common complaint is that using O_NONBLOCK files with io_uring can be a\nbit of a pain. Be a bit nicer and allow normal retry IFF the file does\nsupport async behavior. This makes it possible to use io_uring more\nreliably with O_NONBLOCK files, for use cases where it either isn't\npossible or feasible to modify the file flags.\n\nCc: stable@vger.kernel.org\nReported-and-tested-by: Dan Melnic <dmm@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc2",
        "release_date": "2021-09-14 11:09:42 -0600 io_uring: allow retry for O_NONBLOCK if async is supported"
    },
    {
        "commit": "9c7b0ba887513b6681e7883d306df0a0fd580afa",
        "message": "It might be inconvenient that direct open/accept deviates from the\nupdate semantics and fails if the slot is taken instead of removing a\nfile sitting there. Implement this auto-removal.\n\nNote that removal might need to allocate and so may fail. However, if an\nempty slot is specified, it's guaraneed to not fail on the fd\ninstallation side for valid userspace programs. It's needed for users\nwho can't tolerate such failures, e.g. accept where the other end\nnever retries.\n\nSuggested-by: Franz-B. Tuneke <franz-bernhard.tuneke@tu-dortmund.de>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c896f14ea46b0eaa6c09d93149e665c2c37979b4.1631632300.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc2",
        "release_date": "2021-09-14 09:50:56 -0600 io_uring: auto-removal for direct open/accept"
    },
    {
        "commit": "44df58d441a94de40d52fca67dc60790daee4266",
        "message": "Move get_timespec() section in io_cqring_wait() before the sigmask\nsaving, otherwise we'll fail to restore sigmask once get_timespec()\nreturns error.\n\nFixes: c73ebb685fb6 (\"io_uring: add timeout support for io_uring_enter()\")\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20210914143852.9663-1-xiaoguang.wang@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc2",
        "release_date": "2021-09-14 08:47:00 -0600 io_uring: fix missing sigmask restore in io_cqring_wait()"
    },
    {
        "commit": "e5f71d60ff167d0caa491659d65551a55ea6b406",
        "message": "Make read_iter_zero() to honor IOCB_NOWAIT, so /dev/zero can be\nadvertised as FMODE_NOWAIT. It's useful for io_uring, which needs it to\napply certain optimisations when doing I/O against the device.\n\nSet FMODE_NOWAIT for /dev/null as well, it never waits and therefore\ntrivially meets the criteria.\n\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f11090f97ddc2b2ce49ea1211258658ddfbc5563.1631127867.git.asml.silence@gmail.com\nSigned-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>",
        "kernel_version": "v5.16-rc1",
        "release_date": "2021-09-14 10:46:19 +0200 /dev/mem: nowait zero/null ops"
    },
    {
        "commit": "41d3a6bd1d37149b18331fc4bb789c5456a7aeb0",
        "message": "We need to re-check sqd->thread after we've dropped the lock. Pin\nthe sqd before doing the lockdep lock dance, and check if the thread\nis alive after that. It's either NULL or alive, as the SQPOLL thread\ncannot exit without holding the same sqd->lock.\n\nReported-and-tested-by: syzbot+337de45f13a4fd54d708@syzkaller.appspotmail.com\nFixes: fa84693b3c89 (\"io_uring: ensure IORING_REGISTER_IOWQ_MAX_WORKERS works with SQPOLL\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc2",
        "release_date": "2021-09-13 19:44:29 -0600 io_uring: pin SQPOLL data before unlocking ring lock"
    },
    {
        "commit": "dd47c104533dedb90434a3f142e94a671ac623a6",
        "message": "The items passed in the array pointed by the arg parameter\nof IORING_REGISTER_IOWQ_MAX_WORKERS io_uring_register operation\ncarry certain semantics: they refer to different io-wq worker categories;\nprovide IO_WQ_* constants in the UAPI, so these categories can be referenced\nin the user space code.\n\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nComplements: 2e480058ddc21ec5 (\"io-wq: provide a way to limit max number of workers\")\nSigned-off-by: Eugene Syromiatnikov <esyr@redhat.com>\nLink: https://lore.kernel.org/r/20210913154415.GA12890@asgard.redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc2",
        "release_date": "2021-09-13 10:38:13 -0600 io-wq: provide IO_WQ_* constants for IORING_REGISTER_IOWQ_MAX_WORKERS arg items"
    },
    {
        "commit": "16c8d2df7ec0eed31b7d3b61cb13206a7fb930cc",
        "message": "When setting up the next segment, we check what type the iter is and\nhandle it accordingly. However, when incrementing and processed amount\nwe do not, and both iter advance and addr/len are adjusted, regardless\nof type. Split the increment side just like we do on the setup side.\n\nFixes: 4017eb91a9e7 (\"io_uring: make loop_rw_iter() use original user supplied pointers\")\nCc: stable@vger.kernel.org\nReported-by: Valentina Palmiotti <vpalmiotti@gmail.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc2",
        "release_date": "2021-09-12 19:27:47 -0600 io_uring: ensure symmetry in handling iter types in loop_rw_iter()"
    },
    {
        "commit": "fdfc346302a7b63e3d5b9168be74bb12b1975999",
        "message": "Pull namei updates from Al Viro:\n \"Clearing fallout from mkdirat in io_uring series. The fix in the\n  kern_path_locked() patch plus associated cleanups\"\n\n* 'misc.namei' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:\n  putname(): IS_ERR_OR_NULL() is wrong here\n  namei: Standardize callers of filename_create()\n  namei: Standardize callers of filename_lookup()\n  rename __filename_parentat() to filename_parentat()\n  namei: Fix use after free in kern_path_locked",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-12 10:43:51 -0700 Merge branch 'misc.namei' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs"
    },
    {
        "commit": "c605c39677b9842b0566013e0cf30bc13e90bdbc",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix an off-by-one in a BUILD_BUG_ON() check. Not a real issue right\n   now as we have plenty of flags left, but could become one. (Hao)\n\n - Fix lockdep issue introduced in this merge window (me)\n\n - Fix a few issues with the worker creation (me, Pavel, Qiang)\n\n - Fix regression with wq_has_sleeper() for IOPOLL (Pavel)\n\n - Timeout link error propagation fix (Pavel)\n\n* tag 'io_uring-5.15-2021-09-11' of git://git.kernel.dk/linux-block:\n  io_uring: fix off-by-one in BUILD_BUG_ON check of __REQ_F_LAST_BIT\n  io_uring: fail links of cancelled timeouts\n  io-wq: fix memory leak in create_io_worker()\n  io-wq: fix silly logic error in io_task_work_match()\n  io_uring: drop ctx->uring_lock before acquiring sqd->lock\n  io_uring: fix missing mb() before waitqueue_active\n  io-wq: fix cancellation on create-worker failure",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-11 10:28:14 -0700 Merge tag 'io_uring-5.15-2021-09-11' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "32c2d33e0b7c4ea53284d5d9435dd022b582c8cf",
        "message": "Build check of __REQ_F_LAST_BIT should be larger than, not equal or larger\nthan. It's perfectly valid to have __REQ_F_LAST_BIT be 32, as that means\nthat the last valid bit is 31 which does fit in the type.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20210907032243.114190-1-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-10 06:24:51 -0600 io_uring: fix off-by-one in BUILD_BUG_ON check of __REQ_F_LAST_BIT"
    },
    {
        "commit": "7b7699c09f66f180b9a8a5010df352acb8683bfa",
        "message": "Pull iov_iter fixes from Al Viro:\n \"Fixes for io-uring handling of iov_iter reexpands\"\n\n* 'work.iov_iter' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:\n  io_uring: reexpand under-reexpanded iters\n  iov_iter: track truncated size",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-09 12:13:46 -0700 Merge branch 'work.iov_iter' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs"
    },
    {
        "commit": "2ae2eb9dde18979b40629dd413b9adbd6c894cdf",
        "message": "When we cancel a timeout we should mark it with REQ_F_FAIL, so\nlinked requests are cancelled as well, but not queued for further\nexecution.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/fff625b44eeced3a5cae79f60e6acf3fbdf8f990.1631192135.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-09 09:41:02 -0600 io_uring: fail links of cancelled timeouts"
    },
    {
        "commit": "66e70be722886e4f134350212baa13f217e39e42",
        "message": "BUG: memory leak\nunreferenced object 0xffff888126fcd6c0 (size 192):\n  comm \"syz-executor.1\", pid 11934, jiffies 4294983026 (age 15.690s)\n  backtrace:\n    [<ffffffff81632c91>] kmalloc_node include/linux/slab.h:609 [inline]\n    [<ffffffff81632c91>] kzalloc_node include/linux/slab.h:732 [inline]\n    [<ffffffff81632c91>] create_io_worker+0x41/0x1e0 fs/io-wq.c:739\n    [<ffffffff8163311e>] io_wqe_create_worker fs/io-wq.c:267 [inline]\n    [<ffffffff8163311e>] io_wqe_enqueue+0x1fe/0x330 fs/io-wq.c:866\n    [<ffffffff81620b64>] io_queue_async_work+0xc4/0x200 fs/io_uring.c:1473\n    [<ffffffff8162c59c>] __io_queue_sqe+0x34c/0x510 fs/io_uring.c:6933\n    [<ffffffff8162c7ab>] io_req_task_submit+0x4b/0xa0 fs/io_uring.c:2233\n    [<ffffffff8162cb48>] io_async_task_func+0x108/0x1c0 fs/io_uring.c:5462\n    [<ffffffff816259e3>] tctx_task_work+0x1b3/0x3a0 fs/io_uring.c:2158\n    [<ffffffff81269b43>] task_work_run+0x73/0xb0 kernel/task_work.c:164\n    [<ffffffff812dcdd1>] tracehook_notify_signal include/linux/tracehook.h:212 [inline]\n    [<ffffffff812dcdd1>] handle_signal_work kernel/entry/common.c:146 [inline]\n    [<ffffffff812dcdd1>] exit_to_user_mode_loop kernel/entry/common.c:172 [inline]\n    [<ffffffff812dcdd1>] exit_to_user_mode_prepare+0x151/0x180 kernel/entry/common.c:209\n    [<ffffffff843ff25d>] __syscall_exit_to_user_mode_work kernel/entry/common.c:291 [inline]\n    [<ffffffff843ff25d>] syscall_exit_to_user_mode+0x1d/0x40 kernel/entry/common.c:302\n    [<ffffffff843fa4a2>] do_syscall_64+0x42/0xb0 arch/x86/entry/common.c:86\n    [<ffffffff84600068>] entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nwhen create_io_thread() return error, and not retry, the worker object\nneed to be freed.\n\nReported-by: syzbot+65454c239241d3d647da@syzkaller.appspotmail.com\nSigned-off-by: Qiang.zhang <qiang.zhang@windriver.com>\nLink: https://lore.kernel.org/r/20210909115822.181188-1-qiang.zhang@windriver.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-09 06:57:04 -0600 io-wq: fix memory leak in create_io_worker()"
    },
    {
        "commit": "3b33e3f4a6c0296812a7ee757bdae83290e64aed",
        "message": "We check for the func with an OR condition, which means it always ends\nup being false and we never match the task_work we want to cancel. In\nthe unexpected case that we do exit with that pending, we can trigger\na hang waiting for a worker to exit, but it was never created. syzbot\nreports that as such:\n\nINFO: task syz-executor687:8514 blocked for more than 143 seconds.\n      Not tainted 5.14.0-syzkaller #0\n\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\ntask:syz-executor687 state:D stack:27296 pid: 8514 ppid:  8479 flags:0x00024004\nCall Trace:\n context_switch kernel/sched/core.c:4940 [inline]\n __schedule+0x940/0x26f0 kernel/sched/core.c:6287\n schedule+0xd3/0x270 kernel/sched/core.c:6366\n schedule_timeout+0x1db/0x2a0 kernel/time/timer.c:1857\n do_wait_for_common kernel/sched/completion.c:85 [inline]\n __wait_for_common kernel/sched/completion.c:106 [inline]\n wait_for_common kernel/sched/completion.c:117 [inline]\n wait_for_completion+0x176/0x280 kernel/sched/completion.c:138\n io_wq_exit_workers fs/io-wq.c:1162 [inline]\n io_wq_put_and_exit+0x40c/0xc70 fs/io-wq.c:1197\n io_uring_clean_tctx fs/io_uring.c:9607 [inline]\n io_uring_cancel_generic+0x5fe/0x740 fs/io_uring.c:9687\n io_uring_files_cancel include/linux/io_uring.h:16 [inline]\n do_exit+0x265/0x2a30 kernel/exit.c:780\n do_group_exit+0x125/0x310 kernel/exit.c:922\n get_signal+0x47f/0x2160 kernel/signal.c:2868\n arch_do_signal_or_restart+0x2a9/0x1c40 arch/x86/kernel/signal.c:865\n handle_signal_work kernel/entry/common.c:148 [inline]\n exit_to_user_mode_loop kernel/entry/common.c:172 [inline]\n exit_to_user_mode_prepare+0x17d/0x290 kernel/entry/common.c:209\n __syscall_exit_to_user_mode_work kernel/entry/common.c:291 [inline]\n syscall_exit_to_user_mode+0x19/0x60 kernel/entry/common.c:302\n do_syscall_64+0x42/0xb0 arch/x86/entry/common.c:86\n entry_SYSCALL_64_after_hwframe+0x44/0xae\nRIP: 0033:0x445cd9\nRSP: 002b:00007fc657f4b308 EFLAGS: 00000246 ORIG_RAX: 00000000000000ca\nRAX: 0000000000000001 RBX: 00000000004cb448 RCX: 0000000000445cd9\nRDX: 00000000000f4240 RSI: 0000000000000081 RDI: 00000000004cb44c\nRBP: 00000000004cb440 R08: 000000000000000e R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000246 R12: 000000000049b154\nR13: 0000000000000003 R14: 00007fc657f4b400 R15: 0000000000022000\n\nWhile in there, also decrement accr->nr_workers. This isn't strictly\nneeded as we're exiting, but let's make sure the accounting matches up.\n\nFixes: 3146cba99aa2 (\"io-wq: make worker creation resilient against signals\")\nReported-by: syzbot+f62d3e0a4ea4f38f5326@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-08 19:57:26 -0600 io-wq: fix silly logic error in io_task_work_match()"
    },
    {
        "commit": "009ad9f0c6eed0caa7943bc46aa1ae2cb8c382fb",
        "message": "The SQPOLL thread dictates the lock order, and we hold the ctx->uring_lock\nfor all the registration opcodes. We also hold a ref to the ctx, and we\ndo drop the lock for other reasons to quiesce, so it's fine to drop the\nctx lock temporarily to grab the sqd->lock. This fixes the following\nlockdep splat:\n\n======================================================\nWARNING: possible circular locking dependency detected\n5.14.0-syzkaller #0 Not tainted\n------------------------------------------------------\nsyz-executor.5/25433 is trying to acquire lock:\nffff888023426870 (&sqd->lock){+.+.}-{3:3}, at: io_register_iowq_max_workers fs/io_uring.c:10551 [inline]\nffff888023426870 (&sqd->lock){+.+.}-{3:3}, at: __io_uring_register fs/io_uring.c:10757 [inline]\nffff888023426870 (&sqd->lock){+.+.}-{3:3}, at: __do_sys_io_uring_register+0x10aa/0x2e70 fs/io_uring.c:10792\n\nbut task is already holding lock:\nffff8880885b40a8 (&ctx->uring_lock){+.+.}-{3:3}, at: __do_sys_io_uring_register+0x2e1/0x2e70 fs/io_uring.c:10791\n\nwhich lock already depends on the new lock.\n\nthe existing dependency chain (in reverse order) is:\n\n-> #1 (&ctx->uring_lock){+.+.}-{3:3}:\n       __mutex_lock_common kernel/locking/mutex.c:596 [inline]\n       __mutex_lock+0x131/0x12f0 kernel/locking/mutex.c:729\n       __io_sq_thread fs/io_uring.c:7291 [inline]\n       io_sq_thread+0x65a/0x1370 fs/io_uring.c:7368\n       ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295\n\n-> #0 (&sqd->lock){+.+.}-{3:3}:\n       check_prev_add kernel/locking/lockdep.c:3051 [inline]\n       check_prevs_add kernel/locking/lockdep.c:3174 [inline]\n       validate_chain kernel/locking/lockdep.c:3789 [inline]\n       __lock_acquire+0x2a07/0x54a0 kernel/locking/lockdep.c:5015\n       lock_acquire kernel/locking/lockdep.c:5625 [inline]\n       lock_acquire+0x1ab/0x510 kernel/locking/lockdep.c:5590\n       __mutex_lock_common kernel/locking/mutex.c:596 [inline]\n       __mutex_lock+0x131/0x12f0 kernel/locking/mutex.c:729\n       io_register_iowq_max_workers fs/io_uring.c:10551 [inline]\n       __io_uring_register fs/io_uring.c:10757 [inline]\n       __do_sys_io_uring_register+0x10aa/0x2e70 fs/io_uring.c:10792\n       do_syscall_x64 arch/x86/entry/common.c:50 [inline]\n       do_syscall_64+0x35/0xb0 arch/x86/entry/common.c:80\n       entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nother info that might help us debug this:\n\n Possible unsafe locking scenario:\n\n       CPU0                    CPU1\n       ----                    ----\n  lock(&ctx->uring_lock);\n                               lock(&sqd->lock);\n                               lock(&ctx->uring_lock);\n  lock(&sqd->lock);\n\n *** DEADLOCK ***\n\nFixes: 2e480058ddc2 (\"io-wq: provide a way to limit max number of workers\")\nReported-by: syzbot+97fa56483f69d677969f@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-08 19:07:26 -0600 io_uring: drop ctx->uring_lock before acquiring sqd->lock"
    },
    {
        "commit": "c57a91fb1ccfa203ba3e31e5a389cb04de5b0561",
        "message": "In case of !SQPOLL, io_cqring_ev_posted_iopoll() doesn't provide a\nmemory barrier required by waitqueue_active(&ctx->poll_wait). There is\na wq_has_sleeper(), which does smb_mb() inside, but it's called only for\nSQPOLL.\n\nFixes: 5fd4617840596 (\"io_uring: be smarter about waking multiple CQ ring waiters\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2982e53bcea2274006ed435ee2a77197107d8a29.1631130542.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-08 13:57:56 -0600 io_uring: fix missing mb() before waitqueue_active"
    },
    {
        "commit": "713b9825a4c47897f66ad69409581e7734a8728e",
        "message": "WARNING: CPU: 0 PID: 10392 at fs/io_uring.c:1151 req_ref_put_and_test\nfs/io_uring.c:1151 [inline]\nWARNING: CPU: 0 PID: 10392 at fs/io_uring.c:1151 req_ref_put_and_test\nfs/io_uring.c:1146 [inline]\nWARNING: CPU: 0 PID: 10392 at fs/io_uring.c:1151\nio_req_complete_post+0xf5b/0x1190 fs/io_uring.c:1794\nModules linked in:\nCall Trace:\n tctx_task_work+0x1e5/0x570 fs/io_uring.c:2158\n task_work_run+0xe0/0x1a0 kernel/task_work.c:164\n tracehook_notify_signal include/linux/tracehook.h:212 [inline]\n handle_signal_work kernel/entry/common.c:146 [inline]\n exit_to_user_mode_loop kernel/entry/common.c:172 [inline]\n exit_to_user_mode_prepare+0x232/0x2a0 kernel/entry/common.c:209\n __syscall_exit_to_user_mode_work kernel/entry/common.c:291 [inline]\n syscall_exit_to_user_mode+0x19/0x60 kernel/entry/common.c:302\n do_syscall_64+0x42/0xb0 arch/x86/entry/common.c:86\n entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nWhen io_wqe_enqueue() -> io_wqe_create_worker() fails, we can't just\ncall io_run_cancel() to clean up the request, it's already enqueued via\nio_wqe_insert_work() and will be executed either by some other worker\nduring cancellation (e.g. in io_wq_put_and_exit()).\n\nReported-by: Hao Sun <sunhao.th@gmail.com>\nFixes: 3146cba99aa28 (\"io-wq: make worker creation resilient against signals\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/93b9de0fcf657affab0acfd675d4abcd273ee863.1631092071.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-08 06:34:57 -0600 io-wq: fix cancellation on create-worker failure"
    },
    {
        "commit": "60f8fbaa954452104a1914e21c5cc109f7bf276a",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"As sometimes happens, two reports came in around the merge window open\n  that led to some fixes. Hence this one is a bit bigger than usual\n  followup fixes, but most of it will be going towards stable, outside\n  of the fixes that are addressing regressions from this merge window.\n\n  In detail:\n\n   - postgres is a heavy user of signals between tasks, and if we're\n     unlucky this can interfere with io-wq worker creation. Make sure\n     we're resilient against unrelated signal handling. This set of\n     changes also includes hardening against allocation failures, which\n     could previously had led to stalls.\n\n   - Some use cases that end up having a mix of bounded and unbounded\n     work would have starvation issues related to that. Split the\n     pending work lists to handle that better.\n\n   - Completion trace int -> unsigned -> long fix\n\n   - Fix issue with REGISTER_IOWQ_MAX_WORKERS and SQPOLL\n\n   - Fix regression with hash wait lock in this merge window\n\n   - Fix retry issued on block devices (Ming)\n\n   - Fix regression with links in this merge window (Pavel)\n\n   - Fix race with multi-shot poll and completions (Xiaoguang)\n\n   - Ensure regular file IO doesn't inadvertently skip completion\n     batching (Pavel)\n\n   - Ensure submissions are flushed after running task_work (Pavel)\"\n\n* tag 'for-5.15/io_uring-2021-09-04' of git://git.kernel.dk/linux-block:\n  io_uring: io_uring_complete() trace should take an integer\n  io_uring: fix possible poll event lost in multi shot mode\n  io_uring: prolong tctx_task_work() with flushing\n  io_uring: don't disable kiocb_done() CQE batching\n  io_uring: ensure IORING_REGISTER_IOWQ_MAX_WORKERS works with SQPOLL\n  io-wq: make worker creation resilient against signals\n  io-wq: get rid of FIXED worker flag\n  io-wq: only exit on fatal signals\n  io-wq: split bounded and unbounded work into separate lists\n  io-wq: fix queue stalling race\n  io_uring: don't submit half-prepared drain request\n  io_uring: fix queueing half-created requests\n  io-wq: ensure that hash wait lock is IRQ disabling\n  io_uring: retry in case of short read on block device\n  io_uring: IORING_OP_WRITE needs hash_reg_file set\n  io-wq: fix race between adding work and activating a free worker",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-06 09:26:07 -0700 Merge tag 'for-5.15/io_uring-2021-09-04' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "89c2b3b74918200e46699338d7bcc19b1ea12110",
        "message": "[   74.211232] BUG: KASAN: stack-out-of-bounds in iov_iter_revert+0x809/0x900\n[   74.212778] Read of size 8 at addr ffff888025dc78b8 by task\nsyz-executor.0/828\n[   74.214756] CPU: 0 PID: 828 Comm: syz-executor.0 Not tainted\n5.14.0-rc3-next-20210730 #1\n[   74.216525] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996),\nBIOS rel-1.14.0-0-g155821a1990b-prebuilt.qemu.org 04/01/2014\n[   74.219033] Call Trace:\n[   74.219683]  dump_stack_lvl+0x8b/0xb3\n[   74.220706]  print_address_description.constprop.0+0x1f/0x140\n[   74.224226]  kasan_report.cold+0x7f/0x11b\n[   74.226085]  iov_iter_revert+0x809/0x900\n[   74.227960]  io_write+0x57d/0xe40\n[   74.232647]  io_issue_sqe+0x4da/0x6a80\n[   74.242578]  __io_queue_sqe+0x1ac/0xe60\n[   74.245358]  io_submit_sqes+0x3f6e/0x76a0\n[   74.248207]  __do_sys_io_uring_enter+0x90c/0x1a20\n[   74.257167]  do_syscall_64+0x3b/0x90\n[   74.257984]  entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nold_size = iov_iter_count();\n...\niov_iter_revert(old_size - iov_iter_count());\n\nIf iov_iter_revert() is done base on the initial size as above, and the\niter is truncated and not reexpanded in the middle, it miscalculates\nborders causing problems. This trace is due to no one reexpanding after\ngeneric_write_checks().\n\nNow iters store how many bytes has been truncated, so reexpand them to\nthe initial state right before reverting.\n\nCc: stable@vger.kernel.org\nReported-by: Palash Oswal <oswalpalash@gmail.com>\nReported-by: Sudip Mukherjee <sudipm.mukherjee@gmail.com>\nReported-and-tested-by: syzbot+9671693590ef5aad8953@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Al Viro <viro@zeniv.linux.org.uk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-03 19:31:33 -0400 io_uring: reexpand under-reexpanded iters"
    },
    {
        "commit": "2fc2a7a62eb58650e71b4550cf6fa6cc0a75b2d2",
        "message": "It currently takes a long, and while that's normally OK, the io_uring\nlimit is an int. Internally in io_uring it's an int, but sometimes it's\npassed as a long. That can yield confusing results where a completions\nseems to generate a huge result:\n\nou-sqp-1297-1298    [001] ...1   788.056371: io_uring_complete: ring 000000000e98e046, user_data 0x0, result 4294967171, cflags 0\n\nwhich is due to -ECANCELED being stored in an unsigned, and then passed\nin as a long. Using the right int type, the trace looks correct:\n\niou-sqp-338-339     [002] ...1    15.633098: io_uring_complete: ring 00000000e0ac60cf, user_data 0x0, result -125, cflags 0\n\nCc: stable@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-03 16:59:06 -0600 io_uring: io_uring_complete() trace should take an integer"
    },
    {
        "commit": "31efe48eb5dc4de3e31e84b54f287e9665410ab3",
        "message": "IIUC, IORING_POLL_ADD_MULTI is similar to epoll's edge-triggered mode,\nthat means once one pure poll request returns one event(cqe), we'll\nneed to read or write continually until EAGAIN is returned, then I think\nthere is a possible poll event lost race in multi shot mode:\n\nt1  poll request add |                         |\nt2                   |                         |\nt3  event happens    |                         |\nt4  task work add    |                         |\nt5                   | task work run           |\nt6                   |   commit one cqe        |\nt7                   |                         | user app handles cqe\nt8                   |   new event happen      |\nt9                   |   add back to waitqueue |\nt10                  |\n\nAfter t6 but before t9, if new event happens, there'll be no wakeup\noperation, and if user app has picked up this cqe in t7, read or write\nuntil EAGAIN is returned. In t8, new event happens and will be lost,\nthough this race window maybe small.\n\nTo fix this possible race, add poll request back to waitqueue before\ncommitting cqe.\n\nFixes: 88e41cf928a6 (\"io_uring: add multishot mode for IORING_OP_POLL_ADD\")\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20210903142436.5767-1-xiaoguang.wang@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-03 08:27:49 -0600 io_uring: fix possible poll event lost in multi shot mode"
    },
    {
        "commit": "8d4ad41e3e8e4b907f088f25aee4a92f3f864027",
        "message": "io_submit_flush_completions() may enqueue linked requests for task_work\nexecution, so don't leave tctx_task_work() right after the tw list is\nexhausted, but try to flush and then retry.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0755d4c2c36301447c63bdd4146c10477cea4249.1630539342.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-03 06:16:15 -0600 io_uring: prolong tctx_task_work() with flushing"
    },
    {
        "commit": "636378535afb837f165beb7de3907896480cf3b2",
        "message": "Not passing issue_flags from kiocb_done() into __io_complete_rw() means\nthat completion batching for this case is disabled, e.g. for most of\nbuffered reads.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b2689462835c3ee28a5999ef4f9a581e24be04a2.1630539342.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-03 06:16:14 -0600 io_uring: don't disable kiocb_done() CQE batching"
    },
    {
        "commit": "fa84693b3c896460831fe0750554121121a23da8",
        "message": "SQPOLL has a different thread doing submissions, we need to check for\nthat and use the right task context when updating the worker values.\nJust hold the sqd->lock across the operation, this ensures that the\nthread cannot go away while we poke at ->io_uring.\n\nLink: https://github.com/axboe/liburing/issues/420\nFixes: 2e480058ddc2 (\"io-wq: provide a way to limit max number of workers\")\nReported-by: Johannes Lundberg <johalun0@gmail.com>\nTested-by: Johannes Lundberg <johalun0@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-03 06:16:11 -0600 io_uring: ensure IORING_REGISTER_IOWQ_MAX_WORKERS works with SQPOLL"
    },
    {
        "commit": "15e20db2e0cecce0bfc6a67b69e55020fe9cda00",
        "message": "If the application uses io_uring and also relies heavily on signals\nfor communication, that can cause io-wq workers to spuriously exit\njust because the parent has a signal pending. Just ignore signals\nunless they are fatal.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-09-01 12:35:32 -0600 io-wq: only exit on fatal signals"
    },
    {
        "commit": "b8ce1b9d25ccf81e1bbabd45b963ed98b2222df8",
        "message": "[ 3784.910888] BUG: kernel NULL pointer dereference, address: 0000000000000020\n[ 3784.910904] RIP: 0010:__io_file_supports_nowait+0x5/0xc0\n[ 3784.910926] Call Trace:\n[ 3784.910928]  ? io_read+0x17c/0x480\n[ 3784.910945]  io_issue_sqe+0xcb/0x1840\n[ 3784.910953]  __io_queue_sqe+0x44/0x300\n[ 3784.910959]  io_req_task_submit+0x27/0x70\n[ 3784.910962]  tctx_task_work+0xeb/0x1d0\n[ 3784.910966]  task_work_run+0x61/0xa0\n[ 3784.910968]  io_run_task_work_sig+0x53/0xa0\n[ 3784.910975]  __x64_sys_io_uring_enter+0x22/0x30\n[ 3784.910977]  do_syscall_64+0x3d/0x90\n[ 3784.910981]  entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nio_drain_req() goes before checks for REQ_F_FAIL, which protect us from\nsubmitting under-prepared request (e.g. failed in io_init_req(). Fail\nsuch drained requests as well.\n\nFixes: a8295b982c46d (\"io_uring: fix failed linkchain code logic\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e411eb9924d47a131b1e200b26b675df0c2b7627.1630415423.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-31 11:45:31 -0600 io_uring: don't submit half-prepared drain request"
    },
    {
        "commit": "c6d3d9cbd659de8f2176b4e4721149c88ac096d4",
        "message": "[   27.259845] general protection fault, probably for non-canonical address 0xdffffc0000000005: 0000 [#1] SMP KASAN PTI\n[   27.261043] KASAN: null-ptr-deref in range [0x0000000000000028-0x000000000000002f]\n[   27.263730] RIP: 0010:sock_from_file+0x20/0x90\n[   27.272444] Call Trace:\n[   27.272736]  io_sendmsg+0x98/0x600\n[   27.279216]  io_issue_sqe+0x498/0x68d0\n[   27.281142]  __io_queue_sqe+0xab/0xb50\n[   27.285830]  io_req_task_submit+0xbf/0x1b0\n[   27.286306]  tctx_task_work+0x178/0xad0\n[   27.288211]  task_work_run+0xe2/0x190\n[   27.288571]  exit_to_user_mode_prepare+0x1a1/0x1b0\n[   27.289041]  syscall_exit_to_user_mode+0x19/0x50\n[   27.289521]  do_syscall_64+0x48/0x90\n[   27.289871]  entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nio_req_complete_failed() -> io_req_complete_post() ->\nio_req_task_queue() still would try to enqueue hard linked request,\nwhich can be half prepared (e.g. failed init), so we can't allow\nthat to happen.\n\nFixes: a8295b982c46d (\"io_uring: fix failed linkchain code logic\")\nReported-by: syzbot+f9704d1878e290eddf73@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/70b513848c1000f88bd75965504649c6bb1415c0.1630415423.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-31 11:45:31 -0600 io_uring: fix queueing half-created requests"
    },
    {
        "commit": "7db304375e11741e5940f9bc549155035bfb4dc1",
        "message": "In case of buffered reading from block device, when short read happens,\nwe should retry to read more, otherwise the IO will be completed\npartially, for example, the following fio expects to read 2MB, but it\ncan only read 1M or less bytes:\n\n    fio --name=onessd --filename=/dev/nvme0n1 --filesize=2M \\\n\t--rw=randread --bs=2M --direct=0 --overwrite=0 --numjobs=1 \\\n\t--iodepth=1 --time_based=0 --runtime=2 --ioengine=io_uring \\\n\t--registerfiles --fixedbufs --gtod_reduce=1 --group_reporting\n\nFix the issue by allowing short read retry for block device, which sets\nFMODE_BUF_RASYNC really.\n\nFixes: 9a173346bd9e (\"io_uring: fix short read retries for non-reg files\")\nCc: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20210821150751.1290434-1-ming.lei@redhat.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-31 11:45:30 -0600 io_uring: retry in case of short read on block device"
    },
    {
        "commit": "7b3188e7ed54102a5dcc73d07727f41fb528f7c8",
        "message": "During some testing, it became evident that using IORING_OP_WRITE doesn't\nhash buffered writes like the other writes commands do. That's simply\nan oversight, and can cause performance regressions when doing buffered\nwrites with this command.\n\nCorrect that and add the flag, so that buffered writes are correctly\nhashed when using the non-iovec based write command.\n\nCc: stable@vger.kernel.org\nFixes: 3a6820f2bb8a (\"io_uring: add non-vectored read/write commands\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-31 11:45:30 -0600 io_uring: IORING_OP_WRITE needs hash_reg_file set"
    },
    {
        "commit": "b91db6a0b52e019b6bdabea3f1dbe36d85c7e52c",
        "message": "Pull io_uring mkdirat/symlinkat/linkat support from Jens Axboe:\n \"This adds io_uring support for mkdirat, symlinkat, and linkat\"\n\n* tag 'for-5.15/io_uring-vfs-2021-08-30' of git://git.kernel.dk/linux-block:\n  io_uring: add support for IORING_OP_LINKAT\n  io_uring: add support for IORING_OP_SYMLINKAT\n  io_uring: add support for IORING_OP_MKDIRAT\n  namei: update do_*() helpers to return ints\n  namei: make do_linkat() take struct filename\n  namei: add getname_uflags()\n  namei: make do_symlinkat() take struct filename\n  namei: make do_mknodat() take struct filename\n  namei: make do_mkdirat() take struct filename\n  namei: change filename_parentat() calling conventions\n  namei: ignore ERR/NULL names in putname()",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-30 19:39:59 -0700 Merge tag 'for-5.15/io_uring-vfs-2021-08-30' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "3b629f8d6dc04d3af94429c18fe17239d6fbe2c3",
        "message": "Pull support for struct bio recycling from Jens Axboe:\n \"This adds bio recycling support for polled IO, allowing quick reuse of\n  a bio for high IOPS scenarios via a percpu bio_set list.\n\n  It's good for almost a 10% improvement in performance, bumping our\n  per-core IO limit from ~3.2M IOPS to ~3.5M IOPS\"\n\n* tag 'io_uring-bio-cache.5-2021-08-30' of git://git.kernel.dk/linux-block:\n  bio: improve kerneldoc documentation for bio_alloc_kiocb()\n  block: provide bio_clear_hipri() helper\n  block: use the percpu bio cache in __blkdev_direct_IO\n  io_uring: enable use of bio alloc cache\n  block: clear BIO_PERCPU_CACHE flag if polling isn't supported\n  bio: add allocation cache abstraction\n  fs: add kiocb alloc cache flag\n  bio: optimize initialization of a bio",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-30 19:30:30 -0700 Merge tag 'io_uring-bio-cache.5-2021-08-30' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "c547d89a9a445f6bb757b93247de43d312e722da",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - cancellation cleanups (Hao, Pavel)\n\n - io-wq accounting cleanup (Hao)\n\n - io_uring submit locking fix (Hao)\n\n - io_uring link handling fixes (Hao)\n\n - fixed file improvements (wangyangbo, Pavel)\n\n - allow updates of linked timeouts like regular timeouts (Pavel)\n\n - IOPOLL fix (Pavel)\n\n - remove batched file get optimization (Pavel)\n\n - improve reference handling (Pavel)\n\n - IRQ task_work batching (Pavel)\n\n - allow pure fixed file, and add support for open/accept (Pavel)\n\n - GFP_ATOMIC RT kernel fix\n\n - multiple CQ ring waiter improvement\n\n - funnel IRQ completions through task_work\n\n - add support for limiting async workers explicitly\n\n - add different clocksource support for timeouts\n\n - io-wq wakeup race fix\n\n - lots of cleanups and improvement (Pavel et al)\n\n* tag 'for-5.15/io_uring-2021-08-30' of git://git.kernel.dk/linux-block: (87 commits)\n  io-wq: fix wakeup race when adding new work\n  io-wq: wqe and worker locks no longer need to be IRQ safe\n  io-wq: check max_worker limits if a worker transitions bound state\n  io_uring: allow updating linked timeouts\n  io_uring: keep ltimeouts in a list\n  io_uring: support CLOCK_BOOTTIME/REALTIME for timeouts\n  io-wq: provide a way to limit max number of workers\n  io_uring: add build check for buf_index overflows\n  io_uring: clarify io_req_task_cancel() locking\n  io_uring: add task-refs-get helper\n  io_uring: fix failed linkchain code logic\n  io_uring: remove redundant req_set_fail()\n  io_uring: don't free request to slab\n  io_uring: accept directly into fixed file table\n  io_uring: hand code io_accept() fd installing\n  io_uring: openat directly into fixed fd table\n  net: add accept helper not installing fd\n  io_uring: fix io_try_cancel_userdata race for iowq\n  io_uring: IRQ rw completion batching\n  io_uring: batch task work locking\n  ...",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-30 19:22:52 -0700 Merge tag 'for-5.15/io_uring-2021-08-30' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "a9a4aa9fbfc5b87f315c63d9a317648774a46879",
        "message": "io_uring no longer queues async work off completion handlers that run in\nhard or soft interrupt context, and that use case was the only reason that\nio-wq had to use IRQ safe locks for wqe and worker locks.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-30 07:32:17 -0600 io-wq: wqe and worker locks no longer need to be IRQ safe"
    },
    {
        "commit": "f1042b6ccb887f07301f6b096b3d0cfcf9189323",
        "message": "We allow updating normal timeouts, add support for adjusting timings of\nlinked timeouts as well.\n\nReported-by: Victor Stewart <v@nametag.social>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-29 16:12:21 -0600 io_uring: allow updating linked timeouts"
    },
    {
        "commit": "ef9dd637084d437463f5e9efa153dfc94e7e5f08",
        "message": "A preparation patch. Keep all queued linked timeout in a list, so they\nmay be found and updated.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-29 16:12:11 -0600 io_uring: keep ltimeouts in a list"
    },
    {
        "commit": "50c1df2b56e0f581b1dbf334dbf807d6fb8f77b2",
        "message": "Certain use cases want to use CLOCK_BOOTTIME or CLOCK_REALTIME rather than\nCLOCK_MONOTONIC, instead of the default CLOCK_MONOTONIC.\n\nAdd an IORING_TIMEOUT_BOOTTIME and IORING_TIMEOUT_REALTIME flag that\nallows timeouts and linked timeouts to use the selected clock source.\n\nOnly one clock source may be selected, and we -EINVAL the request if more\nthan one is given. If neither BOOTIME nor REALTIME are selected, the\nprevious default of MONOTONIC is used.\n\nLink: https://github.com/axboe/liburing/issues/369\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-29 07:57:23 -0600 io_uring: support CLOCK_BOOTTIME/REALTIME for timeouts"
    },
    {
        "commit": "90499ad00ca59320b5bb43392b7931e1bd84cad2",
        "message": "req->buf_index is u16 and so we rely on registered buffers indexes\nfitting into it. Add a build check, so when the upper limit for the\nnumber of buffers is lifted we get a compliation fail but not lurking\nproblems.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/787e8e1a17cea51ca6301426b1c4c4887b8bd676.1629920396.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-27 09:23:11 -0600 io_uring: add build check for buf_index overflows"
    },
    {
        "commit": "b18a1a4574d2d15f1b0c84658d4549ccbf241fee",
        "message": "It's too easy to forget and misjudge about synchronisation in\nio_req_task_cancel(), add a comment clarifying it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/71099083835f983a1fd73d5a3da6391924da8300.1629920396.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-27 09:23:11 -0600 io_uring: clarify io_req_task_cancel() locking"
    },
    {
        "commit": "9a10867ae54e02a0f204d2eebea5a446fb7a86f9",
        "message": "As we have a more complicated task referencing, which apart from normal\ntask references includes taking tctx->inflight and caching all that, it\nwould be a good idea to have all that isolated in helpers.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d9114d037f1c195897aa13f38a496078eca2afdb.1630023531.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-27 07:29:41 -0600 io_uring: add task-refs-get helper"
    },
    {
        "commit": "a8295b982c46d4a7c259a4cdd58a2681929068a9",
        "message": "Given a linkchain like this:\nreq0(link_flag)-->req1(link_flag)-->...-->reqn(no link_flag)\n\nThere is a problem:\n - if some intermediate linked req like req1 's submittion fails, reqs\n   after it won't be cancelled.\n\n   - sqpoll disabled: maybe it's ok since users can get the error info\n     of req1 and stop submitting the following sqes.\n\n   - sqpoll enabled: definitely a problem, the following sqes will be\n     submitted in the next round.\n\nThe solution is to refactor the code logic to:\n - if a linked req's submittion fails, just mark it and the head(if it\n   exists) as REQ_F_FAIL. Leverage req->result to indicate whether it\n   is failed or cancelled.\n - submit or fail the whole chain when we come to the end of it.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20210827094609.36052-3-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-27 07:27:24 -0600 io_uring: fix failed linkchain code logic"
    },
    {
        "commit": "14afdd6ee3a0db7bcae887d1951ed21c4d1539cd",
        "message": "req_set_fail() in io_submit_sqe() is redundant, remove it.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20210827094609.36052-2-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-27 07:27:24 -0600 io_uring: remove redundant req_set_fail()"
    },
    {
        "commit": "0c6e1d7fd5e7560fdc4bb3418c2c0f0d7a95bf76",
        "message": "It's not necessary to free the request back to slab when we fail to\nget sqe, just move it to state->free_list.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20210825175856.194299-1-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-25 13:04:26 -0600 io_uring: don't free request to slab"
    },
    {
        "commit": "aaa4db12ef7bdc3e343580d1d3c0b2a8874fc1fb",
        "message": "As done with open opcodes, allow accept to skip installing fd into\nprocesses' file tables and put it directly into io_uring's fixed file\ntable. Same restrictions and design as for open.\n\nSuggested-by: Josh Triplett <josh@joshtriplett.org>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nLink: https://lore.kernel.org/r/6d16163f376fac7ac26a656de6b42199143e9721.1629888991.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-25 06:36:56 -0600 io_uring: accept directly into fixed file table"
    },
    {
        "commit": "a7083ad5e30767ede4ff49d7471ea9c078702db2",
        "message": "Make io_accept() to handle file descriptor allocations and installation.\nA preparation patch for bypassing file tables.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nLink: https://lore.kernel.org/r/5b73d204caa0ce979ccb98136695b60f52a3d98c.1629888991.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-25 06:36:56 -0600 io_uring: hand code io_accept() fd installing"
    },
    {
        "commit": "b9445598d8c60a1379887b957024b71343965f74",
        "message": "Instead of opening a file into a process's file table as usual and then\nregistering the fd within io_uring, some users may want to skip the\nfirst step and place it directly into io_uring's fixed file table.\nThis patch adds such a capability for IORING_OP_OPENAT and\nIORING_OP_OPENAT2.\n\nThe behaviour is controlled by setting sqe->file_index, where 0 implies\nthe old behaviour using normal file tables. If non-zero value is\nspecified, then it will behave as described and place the file into a\nfixed file slot sqe->file_index - 1. A file table should be already\ncreated, the slot should be valid and empty, otherwise the operation\nwill fail.\n\nKeep the error codes consistent with IORING_OP_FILES_UPDATE, ENXIO and\nEINVAL on inappropriate fixed tables, and return EBADF on collision with\nalready registered file.\n\nNote: IOSQE_FIXED_FILE can't be used to switch between modes, because\naccept takes a file, and it already uses the flag with a different\nmeaning.\n\nSuggested-by: Josh Triplett <josh@joshtriplett.org>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nLink: https://lore.kernel.org/r/e9b33d1163286f51ea707f87d95bd596dada1e65.1629888991.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-25 06:36:56 -0600 io_uring: openat directly into fixed fd table"
    },
    {
        "commit": "d32f89da7fa8ccc8b3fb8f909d61e42b9bc39329",
        "message": "Introduce and reuse a helper that acts similarly to __sys_accept4_file()\nbut returns struct file instead of installing file descriptor. Will be\nused by io_uring.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nAcked-by: Jakub Kicinski <kuba@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nAcked-by: David S. Miller <davem@davemloft.net>\nLink: https://lore.kernel.org/r/c57b9e8e818d93683a3d24f8ca50ca038d1da8c4.1629888991.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-25 06:36:56 -0600 net: add accept helper not installing fd"
    },
    {
        "commit": "cf30da90bc3a26911d369f199411f38b701394de",
        "message": "IORING_OP_LINKAT behaves like linkat(2) and takes the same flags and\narguments.\n\nIn some internal places 'hardlink' is used instead of 'link' to avoid\nconfusion with the SQE links. Name 'link' conflicts with the existing\n'link' member of io_kiocb.\n\nAcked-by: Linus Torvalds <torvalds@linux-foundation.org>\nSuggested-by: Christian Brauner <christian.brauner@ubuntu.com>\nLink: https://lore.kernel.org/io-uring/20210514145259.wtl4xcsp52woi6ab@wittgenstein/\nSigned-off-by: Dmitry Kadashev <dkadashev@gmail.com>\nAcked-by: Christian Brauner <christian.brauner@ubuntu.com>\nLink: https://lore.kernel.org/r/20210708063447.3556403-12-dkadashev@gmail.com\n[axboe: add splice_fd_in check]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:48:52 -0600 io_uring: add support for IORING_OP_LINKAT"
    },
    {
        "commit": "7a8721f84fcb3b2946a92380b6fc311e017ff02c",
        "message": "IORING_OP_SYMLINKAT behaves like symlinkat(2) and takes the same flags\nand arguments.\n\nAcked-by: Linus Torvalds <torvalds@linux-foundation.org>\nSuggested-by: Christian Brauner <christian.brauner@ubuntu.com>\nLink: https://lore.kernel.org/io-uring/20210514145259.wtl4xcsp52woi6ab@wittgenstein/\nSigned-off-by: Dmitry Kadashev <dkadashev@gmail.com>\nAcked-by: Christian Brauner <christian.brauner@ubuntu.com>\nLink: https://lore.kernel.org/r/20210708063447.3556403-11-dkadashev@gmail.com\n[axboe: add splice_fd_in check]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:48:33 -0600 io_uring: add support for IORING_OP_SYMLINKAT"
    },
    {
        "commit": "394918ebb889f99d89db6843bcc93279b2b745f9",
        "message": "Mark polled IO as being safe for dipping into the bio allocation\ncache, in case the targeted bio_set has it enabled.\n\nThis brings an IOPOLL gen2 Optane QD=128 workload from ~3.2M IOPS to\n~3.5M IOPS.\n\nReviewed-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:44:55 -0600 io_uring: enable use of bio alloc cache"
    },
    {
        "commit": "dadebc350da2bef62593b1df007a6e0b90baf42a",
        "message": "WARNING: CPU: 1 PID: 5870 at fs/io_uring.c:5975 io_try_cancel_userdata+0x30f/0x540 fs/io_uring.c:5975\nCPU: 0 PID: 5870 Comm: iou-wrk-5860 Not tainted 5.14.0-rc6-next-20210820-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nRIP: 0010:io_try_cancel_userdata+0x30f/0x540 fs/io_uring.c:5975\nCall Trace:\n io_async_cancel fs/io_uring.c:6014 [inline]\n io_issue_sqe+0x22d5/0x65a0 fs/io_uring.c:6407\n io_wq_submit_work+0x1dc/0x300 fs/io_uring.c:6511\n io_worker_handle_work+0xa45/0x1840 fs/io-wq.c:533\n io_wqe_worker+0x2cc/0xbb0 fs/io-wq.c:582\n ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295\n\nio_try_cancel_userdata() can be called from io_async_cancel() executing\nin the io-wq context, so the warning fires, which is there to alert\nanyone accessing task->io_uring->io_wq in a racy way. However,\nio_wq_put_and_exit() always first waits for all threads to complete,\nso the only detail left is to zero tctx->io_wq after the context is\nremoved.\n\nnote: one little assumption is that when IO_WQ_WORK_CANCEL, the executor\nwon't touch ->io_wq, because io_wq_destroy() might cancel left pending\nrequests in such a way.\n\nCc: stable@vger.kernel.org\nReported-by: syzbot+b0c9d1588ae92866515f@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/dfdd37a80cfa9ffd3e59538929c99cdd55d8699e.1629721757.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:41:56 -0600 io_uring: fix io_try_cancel_userdata race for iowq"
    },
    {
        "commit": "e34a02dc40c95d126bb6486dcf802bbb8d1624a0",
        "message": "IORING_OP_MKDIRAT behaves like mkdirat(2) and takes the same flags\nand arguments.\n\nAcked-by: Linus Torvalds <torvalds@linux-foundation.org>\nSigned-off-by: Dmitry Kadashev <dkadashev@gmail.com>\nAcked-by: Christian Brauner <christian.brauner@ubuntu.com>\nLink: https://lore.kernel.org/r/20210708063447.3556403-10-dkadashev@gmail.com\n[axboe: add splice_fd_in check]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:41:26 -0600 io_uring: add support for IORING_OP_MKDIRAT"
    },
    {
        "commit": "8228e2c313194f13f1d1806ed5734a26c38d49ac",
        "message": "There are a couple of places where we already open-code the (flags &\nAT_EMPTY_PATH) check and io_uring will likely add another one in the\nfuture.  Let's just add a simple helper getname_uflags() that handles\nthis directly and use it.\n\nCc: Al Viro <viro@zeniv.linux.org.uk>\nCc: Christian Brauner <christian.brauner@ubuntu.com>\nAcked-by: Linus Torvalds <torvalds@linux-foundation.org>\nLink: https://lore.kernel.org/io-uring/20210415100815.edrn4a7cy26wkowe@wittgenstein/\nSigned-off-by: Christian Brauner <christian.brauner@ubuntu.com>\nSigned-off-by: Dmitry Kadashev <dkadashev@gmail.com>\nAcked-by: Christian Brauner <christian.brauner@ubuntu.com>\nLink: https://lore.kernel.org/r/20210708063447.3556403-7-dkadashev@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:41:26 -0600 namei: add getname_uflags()"
    },
    {
        "commit": "126180b95f27ef6cc536da57115e06665254b0d7",
        "message": "Employ inline completion logic for read/write completions done via\nio_req_task_complete(). If ->uring_lock is contended, just do normal\nrequest completion, but if not, make tctx_task_work() to grab the lock\nand do batched inline completions in io_req_task_complete().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/94589c3ce69eaed86a21bb1ec696407a54fab1aa.1629286357.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:13:04 -0600 io_uring: IRQ rw completion batching"
    },
    {
        "commit": "f237c30a5610d35a584f3296d397b93d80ce374e",
        "message": "Many task_work handlers either grab ->uring_lock, or may benefit from\nhaving it. Move locking logic out of individual handlers to a lazy\napproach controlled by tctx_task_work(), so we don't keep doing\ntons of mutex lock/unlock.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d6a34e147f2507a2f3e2fa1e38a9c541dcad3929.1629286357.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:13:04 -0600 io_uring: batch task work locking"
    },
    {
        "commit": "5636c00d3e8ef1f6d1291e71edb48f727ba5a999",
        "message": "io_fallback_req_func() doesn't expect anyone creating inline\ncompletions, and no one currently does that. Teach the function to flush\ncompletions preparing for further changes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8b941516921f72e1a64d58932d671736892d7fff.1629286357.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:13:04 -0600 io_uring: flush completions for fallbacks"
    },
    {
        "commit": "26578cda3db983b17cabe4e577af26306beb9987",
        "message": "->splice_fd_in is used only by splice/tee, but no other request checks\nit for validity. Add the check for most of request types excluding\nreads/writes/sends/recvs, we don't want overhead for them and can leave\nthem be as is until the field is actually used.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f44bc2acd6777d932de3d71a5692235b5b2b7397.1629451684.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:13:00 -0600 io_uring: add ->splice_fd_in checks"
    },
    {
        "commit": "2c5d763c1939fbd130452ee0d4d1a44b5dd97bb7",
        "message": "We've previously had an issue where overflow flush unconditionally calls\nio_cqring_ev_posted() even if it didn't flush any events to the ring,\ncausing wake and eventfd increment where no new events are available.\nSome applications don't like that, see commit b18032bb0a88 for details.\n\nThis came up in discussion for another patch recently, hence add a\ncomment detailing what the relationship between calling the events\nposted helper and CQ ring entries is.\n\nLink: https://lore.kernel.org/io-uring/77a44fce-c831-16a6-8e80-9aee77f496a2@kernel.dk/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:47 -0600 io_uring: add clarifying comment for io_cqring_ev_posted()"
    },
    {
        "commit": "0bea96f59ba40e63c0ae93ad6a02417b95f22f4d",
        "message": "Fixed tables may be large enough, place all of them together with\nallocated tags under memcg limits.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b3ac9f5da9821bb59837b5fe25e8ef4be982218c.1629451684.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:47 -0600 io_uring: place fixed tables under memcg limits"
    },
    {
        "commit": "3a1b8a4e843f96b636431450d8d79061605cf74b",
        "message": "Limit the number of files in io_uring fixed tables by RLIMIT_NOFILE,\nthat's the first and the simpliest restriction that we should impose.\n\nCc: stable@vger.kernel.org\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b2756c340aed7d6c0b302c26dab50c6c5907f4ce.1629451684.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:46 -0600 io_uring: limit fixed table size by RLIMIT_NOFILE"
    },
    {
        "commit": "99c8bc52d1321ab3a711eba2941eadbe7425230f",
        "message": "coml_nr in ctx_flush_and_put() is not protected by uring_lock, this\nmay cause problems when accessing in parallel:\n\nsay coml_nr > 0\n\n  ctx_flush_and put                  other context\n   if (compl_nr)                      get mutex\n                                      coml_nr > 0\n                                      do flush\n                                          coml_nr = 0\n                                      release mutex\n        get mutex\n           do flush (*)\n        release mutex\n\nin (*) place, we call io_cqring_ev_posted() and users likely get\nno events there. To avoid spurious events, re-check the value when\nunder the lock.\n\nFixes: 2c32395d8111 (\"io_uring: fix __tctx_task_work() ctx race\")\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20210820221954.61815-1-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:46 -0600 io_uring: fix lack of protection for compl_nr"
    },
    {
        "commit": "187f08c12cd1d81f000cdc9c0119ef6e0a6f47e3",
        "message": "Now allocated rsrc table uses PAGE_SIZE as the size of 2nd-level, and\naccessing this table relies on each level index from fixed TABLE_SHIFT\n(12 - 3) in 4k page case. In order to correctly work in non-4k page,\ndefine TABLE_SHIFT as non-fixed (PAGE_SHIFT - shift of data) for\n2nd-level table entry number.\n\nSigned-off-by: wangyangbo <wangyangbo@uniontech.com>\nLink: https://lore.kernel.org/r/20210819055657.27327-1-wangyangbo@uniontech.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:46 -0600 io_uring: Add register support for non-4k PAGE_SIZE"
    },
    {
        "commit": "e98e49b2bbf777f91732dc916d7ad33876c663c9",
        "message": "Now with IRQ completions done via IRQ, almost all requests freeing\nare done from the context of submitter task, so it makes sense to\nextend task_put optimisation from io_req_free_batch_finish() to cover\nall the cases including task_work by moving it into io_put_task().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/824a7cbd745ddeee4a0f3ff85c558a24fd005872.1629302453.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:46 -0600 io_uring: extend task put optimisations"
    },
    {
        "commit": "316319e82f7342ef327223a23199648bfabeadcd",
        "message": "We have two checks of task->flags & PF_EXITING left:\n\n1) In io_req_task_submit(), which is called in task_work and hence always\n   in the context of the original task. That means that\n   req->task == current, and hence checking ->flags is totally fine.\n\n2) In io_poll_rewait(), where we need to stop re-arming poll to prevent\n   it interfering with cancelation. This is only run from task_work as\n   well, and hence for this case too req->task == current.\n\nAdd a comment to both spots detailing that.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:43 -0600 io_uring: add comments on why PF_EXITING checking is safe"
    },
    {
        "commit": "ec3c3d0f3a271b5c7422449262970e7eb98f2126",
        "message": "io_timeout_cancel() posts CQEs so needs ->completion_lock to be held,\nso grab it in io_timeout_remove().\n\nFixes: 48ecb6369f1f2 (\"io_uring: run timeouts from task_work\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d6f03d653a4d7bf693ef6f39b6a426b6d97fd96f.1629280204.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:43 -0600 io_uring: fix io_timeout_remove locking"
    },
    {
        "commit": "23a65db83b3f4549e5eee1fb5517c3365f627699",
        "message": "Move earlier the check for whether __io_queue_proc() tries to poll\nalready polled waitqueue, and do the same for the second poll entry, if\nany. Shouldn't really matter, but at least it would have a more\npredictable behaviour.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8cb428cfe8ade0fd055859fabb878db8777d4c2f.1629228203.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:43 -0600 io_uring: improve same wq polling"
    },
    {
        "commit": "505657bc6c52b01304d8a7c79b2f98878e3d83db",
        "message": "We have io_req_complete_post() to post a CQE and put the request. It\ntakes care of all synchronisation and is more concise and efficent, so\nreplace all hancoded occurrences of\n\"lock; post CQE; unlock; + put_req()\" with io_req_complete_post().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2c83463458a613f9d870e5147eb134da2aa70779.1629228203.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:43 -0600 io_uring: reuse io_req_complete_post()"
    },
    {
        "commit": "ae421d9350b51cba1daa28ee6eb14fbce7517eca",
        "message": "Make io_put_rw_kbuf() to do the REQ_F_BUFFER_SELECTED check, so all the\ncallers don't need to hand code it. The number of places where we call\nio_put_rw_kbuf() is growing, so saves some pain.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3df3919e5e7efe03420c44ab4d9317a81a9cf398.1629228203.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:43 -0600 io_uring: better encapsulate buffer select for rw"
    },
    {
        "commit": "906c6caaf586180261ea581915e1cf8bc466bd69",
        "message": "Linked timeout handling during issuing is heavy, it adds extra\ninstructions and forces to save the next linked timeout before\nio_issue_sqe().\n\nFollwing the same reasoning as in refcounting patches, a request can't\nbe freed by the time it returns from io_issue_sqe(), so now we don't\nneed to do io_prep_linked_timeout() in advance, and it can be delayed to\ncolder paths optimising the generic path.\n\nAlso, it should also save quite a lot for requests with linked timeouts\nand completed inline on timeout spinlocking + hrtimer_start() +\nhrtimer_try_to_cancel() and so on.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/19bfc9a0d26c5c5f1e359f7650afe807ca8ef879.1628981736.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:43 -0600 io_uring: optimise io_prep_linked_timeout()"
    },
    {
        "commit": "0756a8691017518ceeca4c083e7a359107186498",
        "message": "Adjust io_disarm_next(), so it can detect if there is a linked but\nnot-yet-armed timeout and complete/cancel it separately. Will be used in\nthe following patch.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ae228cde2c0df3d92d29d5e4852ed9fa8a2a97db.1628981736.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:43 -0600 io_uring: cancel not-armed linked touts separately"
    },
    {
        "commit": "4d13d1a4d1e1807e04b846b48934e87016027f90",
        "message": "The link test in io_prep_linked_timeout() is pretty bulky, replace it\nwith a flag. It's better for normal path and linked requests, and also\nwill be used further for request failing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3703770bfae8bc1ff370e43ef5767940202cab42.1628981736.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:43 -0600 io_uring: simplify io_prep_linked_timeout"
    },
    {
        "commit": "b97e736a4b553ff18963019c7ca91cd684f83709",
        "message": "Instead of handling double consecutive linked timeouts through tricky\nflag combinations, just check the submit_state.link during timeout_prep\nand fail that case in advance.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/04150760b0dc739522264b8abd309409f7421a06.1628981736.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:43 -0600 io_uring: kill REQ_F_LTIMEOUT_ACTIVE"
    },
    {
        "commit": "fd08e5309bba8672c1190362dff6c92bfd59218d",
        "message": "io_prep_linked_timeout() grew too heavy and compiler now refuse to\ninline the function. Help it by splitting in two and annotating with\ninline.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/560636717a32e9513724f09b9ecaace942dde4d4.1628705069.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:37 -0600 io_uring: optimise hot path of ltimeout prep"
    },
    {
        "commit": "8cb01fac982a3f8622a46821af1eb68136f936ca",
        "message": "IORING_OP_ASYNC_CANCEL and IORING_OP_LINK_TIMEOUT have enough of\noverlap, so extract a helper for request cancellation and use in both.\nAlso, removes some amount of ugliness because of success_ret.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/900122b588e65b637e71bfec80a260726c6a54d6.1628981736.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:37 -0600 io_uring: deduplicate cancellation code"
    },
    {
        "commit": "a8576af9d1b03a1b8aba7228e938ab0817fdbda6",
        "message": "773af69121ecc (\"io_uring: always reissue from task_work context\") makes\nall resubmission to be made from task_work, so we don't need that hack\nwith resubmit/not-resubmit switch anymore.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/47fa177cca04e5ffd308a35227966c8e15d8525b.1628981736.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:37 -0600 io_uring: kill not necessary resubmit switch"
    },
    {
        "commit": "fb6820998f57a3e63a382a322530fa28522a2bba",
        "message": "Linked timeouts are never refcounted when it comes to the first call to\n__io_prep_linked_timeout(), so save an io_ref_get() and set the desired\nvalue directly.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/177b24cc62ffbb42d915d6eb9e8876266e4c0d5a.1628981736.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:37 -0600 io_uring: optimise initial ltimeout refcounting"
    },
    {
        "commit": "761bcac1573efc99042d59add94d468bf17127f0",
        "message": "Tracking linked timeouts as infligh was needed to make sure that io-wq\nis not destroyed by io_uring_cancel_generic() racing with\nio_async_cancel_one() accessing it. Now, cancellations issued by linked\ntimeouts are done in the task context, so it's already synchronised.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e1b05cf47cb69df2305efdbee8cf7ba36f46c1a3.1628981736.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:37 -0600 io_uring: don't inflight-track linked timeouts"
    },
    {
        "commit": "48dcd38d73c22b22bf9dc1c01b0ca0b8414b31da",
        "message": "If a requests is forwarded into io-wq, there is a good chance it hasn't\nbeen refcounted yet and we can save one req_ref_get() by setting the\nrefcount number to the right value directly.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2d53f4449faaf73b4a4c5de667fc3c176d974860.1628981736.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:37 -0600 io_uring: optimise iowq refcounting"
    },
    {
        "commit": "a141dd896f544df9627502cfb3fc1a73fb6587e4",
        "message": "io_req_free_batch() has a __must_hold annotation referencing a\nrequest being passed in, but we're passing in the context.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:37 -0600 io_uring: correct __must_hold annotation"
    },
    {
        "commit": "41a5169c23ebe85fdd0b64a0b6381f486a34ef3c",
        "message": "We can merge two spin_unlock() operations to one since we removed some\ncode not long ago.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:37 -0600 io_uring: code clean for completion_lock in io_arm_poll_handler()"
    },
    {
        "commit": "f552a27afe67f05c47bb0c33b92af2a23b684c31",
        "message": "When doing cancellation, we use a parameter to indicate where it's from\ndo_exit or exec. So a boolean value is good enough for this, remove the\nstruct files* as it is not necessary.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\n[axboe: fixup io_uring_files_cancel for !CONFIG_IO_URING]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:37 -0600 io_uring: remove files pointer in cancellation functions"
    },
    {
        "commit": "a4aadd11ea4932588e6530ecd021ffe39f9d5adf",
        "message": "Extract io_uring_files_cancel() call in io_uring_task_cancel() to make\nio_uring_files_cancel() and io_uring_task_cancel() coherent and easy to\nread.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:37 -0600 io_uring: extract io_uring_files_cancel() in io_uring_task_cancel()"
    },
    {
        "commit": "20e60a3832089741d6b25c13d291050c5d00b4e7",
        "message": "As submission references are gone, there is only one initial reference\nleft. Instead of actually doing atomic refcounting, add a flag\nindicating whether we're going to take more refs or doing any other sync\nmagic. The flag should be set before the request may get used in\nparallel.\n\nTogether with the previous patch it saves 2 refcount atomics per request\nfor IOPOLL and IRQ completions, and 1 atomic per req for inline\ncompletions, with some exceptions. In particular, currently, there are\nthree cases, when the refcounting have to be enabled:\n- Polling, including apoll. Because double poll entries takes a ref.\n  Might get relaxed in the near future.\n- Link timeouts, enabled for both, the timeout and the request it's\n  bound to, because they work in-parallel and we need to synchronise\n  to cancel one of them on completion.\n- When a request gets in io-wq, because it doesn't hold uring_lock and\n  we need guarantees of submission references.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8b204b6c5f6643062270a1913d6d3a7f8f795fd9.1628705069.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:32 -0600 io_uring: skip request refcounting"
    },
    {
        "commit": "5d5901a3434064e98c1dbb3047b9f9793825ea42",
        "message": "Requests are by default given with two references, submission and\ncompletion. Completion references are straightforward, they represent\nrequest ownership and are put when a request is completed or so.\nSubmission references are a bit more trickier. They're needed when\nio_issue_sqe() followed deep into the submission stack (e.g. in fs,\nblock, drivers, etc.), request may have given away for concurrent\nexecution or already completed, and the code unwinding back to\nio_issue_sqe() may be accessing some pieces of our requests, e.g.\nfile or iov.\n\nNow, we prevent such async/in-depth completions by pushing requests\nthrough task_work. Punting to io-wq is also done through task_works,\napart from a couple of cases with a pretty well known context. So,\nthere're two cases:\n1) io_issue_sqe() from the task context and protected by ->uring_lock.\nEither requests return back to io_uring or handed to task_work, which\nwon't be executed because we're currently controlling that task. So,\nwe can be sure that requests are staying alive all the time and we don't\nneed submission references to pin them.\n\n2) io_issue_sqe() from io-wq, which doesn't hold the mutex. The role of\nsubmission reference is played by io-wq reference, which is put by\nio_wq_submit_work(). Hence, it should be fine.\n\nConsidering that, we can carefully kill the submission reference.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6b68f1c763229a590f2a27148aee77767a8d7750.1628705069.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:32 -0600 io_uring: remove submission references"
    },
    {
        "commit": "91c2f6978311afe1f49094fdd90fd6ab29b66223",
        "message": "Soon, we won't need to put several references at once, remove\nreq_ref_sub_and_test() and @nr argument from io_put_req_deferred(),\nand put the rest of the references by hand.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1868c7554108bff9194fb5757e77be23fadf7fc0.1628705069.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:32 -0600 io_uring: remove req_ref_sub_and_test()"
    },
    {
        "commit": "21c843d5825b949332fe58495007ca531ef6ae91",
        "message": "Move all request refcount helpers to avoid forward declarations in the\nfuture.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/89fd36f6f3fe5b733dfe4546c24725eee40df605.1628705069.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:32 -0600 io_uring: move req_ref_get() and friends"
    },
    {
        "commit": "79ebeaee8a21a00417d89f1a02019f79840d9bad",
        "message": "We have no hard/soft IRQ users of this lock left, remove any IRQ\ndisabling/saving and restoring when grabbing this lock.\n\nThis is straight forward with no users entering with IRQs disabled\nanymore, the only thing to look out for is the waitqueue poll head\nlock which nests inside the completion lock. That needs IRQs disabled,\nand hence we have to do that now instead of relying on the outer lock\ndoing so.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:32 -0600 io_uring: remove IRQ aspect of io_ring_ctx completion lock"
    },
    {
        "commit": "8ef12efe26c8e44323011e57753b8c0e87af1582",
        "message": "This is in preparation to making the completion lock work outside of\nhard/soft IRQ context.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:32 -0600 io_uring: run regular file completions from task_work"
    },
    {
        "commit": "89b263f6d56e683ddcf7643140271ef6e36c72b9",
        "message": "This is in preparation to making the completion lock work outside of\nhard/soft IRQ context.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:32 -0600 io_uring: run linked timeouts from task_work"
    },
    {
        "commit": "89850fce16a1a75caacca77cfa0c829aeea4f886",
        "message": "This is in preparation to making the completion lock work outside of\nhard/soft IRQ context.\n\nAdd a timeout_lock to handle the ordering of timeout completions or\ncancelations with the timeouts actually triggering.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:32 -0600 io_uring: run timeouts from task_work"
    },
    {
        "commit": "62906e89e63ba497105c0e3558089a10365f4f33",
        "message": "For requests with non-fixed files, instead of grabbing just one\nreference, we get by the number of left requests, so the following\nrequests using the same file can take it without atomics.\n\nHowever, it's not all win. If there is one request in the middle\nnot using files or having a fixed file, we'll need to put back the left\nreferences. Even worse if an application submits requests dealing with\ndifferent files, it will do a put for each new request, so doubling the\nnumber of atomics needed. Also, even if not used, it's still takes some\ncycles in the submission path.\n\nIf a file used many times, it rather makes sense to pre-register it, if\nnot, we may fall in the described pitfall. So, this optimisation is a\nmatter of use case. Go with the simpliest code-wise way, remove it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:32 -0600 io_uring: remove file batch-get optimisation"
    },
    {
        "commit": "6294f3686b4d77771ab8b161304ada546e71d36a",
        "message": "After recent fixes, tctx_task_work() always does proper spinlocking\nbefore looking into ->task_list, so now we don't need atomics for\n->task_state, replace it with non-atomic task_running using the critical\nsection.\n\nTide it up, combine two separate block with spinlocking, and always try\nto splice in there, so we do less locking when new requests are arriving\nduring the function execution.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\n[axboe: fix missing ->task_running reset on task_work_add() failure]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:32 -0600 io_uring: clean up tctx_task_work()"
    },
    {
        "commit": "5d70904367b45b74dab9da5c023b6629f511e48f",
        "message": "Inline io_poll_remove_waitqs() into its only user and clean it up.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2f1a91a19ffcd591531dc4c61e2f11c64a2d6a6d.1628536684.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:26 -0600 io_uring: inline io_poll_remove_waitqs"
    },
    {
        "commit": "90f67366cb8871951399fb5bcf182e902b896615",
        "message": "Unlike __io_cqring_overflow_flush(), nobody does forced flushing with\nio_cqring_overflow_flush(), so removed the argument from it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7594f869ca41b7cfb5a35a3c7c2d402242834e9e.1628536684.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:10:19 -0600 io_uring: remove extra argument for overflow flush"
    },
    {
        "commit": "cd0ca2e048dc0ddea4f59354b0b8ce4548a76a91",
        "message": "Inline struct io_comp_state into struct io_submit_state. They are\nalready coupled tightly, together with mixed responsibilities it\nonly brings confusion having them separately.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e55bba77426b399e3a2e54e3c6c267c6a0fc4b57.1628536684.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:09:48 -0600 io_uring: inline struct io_comp_state"
    },
    {
        "commit": "bb943b8265c84e9553903161bc39ff45f427d00d",
        "message": "req->compl.list is used to cache freed requests, and so can't overlap in\ntime with req->inflight_entry. So, use inflight_entry to link requests\nand remove compl.list.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e430e79d22d70a190d718831bda7bfed1daf8976.1628536684.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:09:43 -0600 io_uring: use inflight_entry instead of compl.list"
    },
    {
        "commit": "7255834ed6ef9658b9e7fb192da6a323a64eac98",
        "message": "We don't use @tsk argument of io_req_cache_free(), remove it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6a28b4a58ee0aaf0db98e2179b9c9f06f9b0cca1.1628536684.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:09:43 -0600 io_uring: remove redundant args from cache_free"
    },
    {
        "commit": "c34b025f2d2149d4351b994a923fa687a32478f8",
        "message": "Don't kfree requests in __io_free_req() but put them back into the\ninternal request cache. That makes allocations more sustainable and will\nbe used for refcounting optimisations.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9f4950fbe7771c8d41799366d0a3a08ac3040236.1628536684.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:09:43 -0600 io_uring: cache __io_free_req()'d requests"
    },
    {
        "commit": "f56165e62fae78200292857628e4f1d8d12a0ed0",
        "message": "Move io_fallback_req_func() to kill yet another forward declaration.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d0a8f9d9a0057ed761d6237167d51c9378798d2d.1628536684.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:09:22 -0600 io_uring: move io_fallback_req_func()"
    },
    {
        "commit": "e9dbe221f5d1c974c853da94eee456803239cab5",
        "message": "We cache all the reference to task + tctx, so if io_put_task() is\ncalled by the corresponding task itself, we can save on atomics and\nreturn the refs right back into the cache.\n\nIt's beneficial for all inline completions, and also iopolling, when\npolling and submissions are done by the same task, including\nSQPOLL|IOPOLL.\n\nNote: io_uring_cancel_generic() can return refs to the cache as well,\nso those should be flushed in the loop for tctx_inflight() to work\nright.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6fe9646b3cb70e46aca1f58426776e368c8926b3.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:08:06 -0600 io_uring: optimise putting task struct"
    },
    {
        "commit": "af066f31eb3dac2a11516315d47a286a7b3b07df",
        "message": "In case of on-exec io_uring cancellations, tasks already wait for all\nsubmitted requests to get completed/cancelled, so we don't need to check\nfor ->in_execve separately.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/be8707049f10df9d20ca03dc4ca3316239b5e8e0.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:08:06 -0600 io_uring: drop exec checks from io_req_task_submit"
    },
    {
        "commit": "bbbca0948989aa1a8a75b99bcdece677ad06dfe6",
        "message": "IO_IOPOLL_BATCH is not used, delete it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b2bdf19dbee2c9fc8865bbab9412135a14e24a64.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:08:06 -0600 io_uring: kill unused IO_IOPOLL_BATCH"
    },
    {
        "commit": "58d3be2c60d2cf4e6bb65bb6200fa39a7bc477f9",
        "message": "If io_ring_exit_work() can't get it done in 5 minutes, something is\ngoing very wrong, don't keep spinning at HZ / 20 rate, it doesn't help\nand it may take much of CPU time if there is a lot of workers stuck as\nsuch.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9e2d1ca81d569f6bc628af1a42ff6663bff7ce9c.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:59 -0600 io_uring: improve ctx hang handling"
    },
    {
        "commit": "d3fddf6dddd84432161eb070ed8e34d14c8bf56a",
        "message": "Move IORING_SETUP_IOPOLL check into __io_openat_prep(), so both openat\nand openat2 reuse it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9a73ce83e4ee60d011180ef177eecef8e87ff2a2.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:59 -0600 io_uring: deduplicate open iopoll check"
    },
    {
        "commit": "543af3a13da308f2cea954644b43c2c9f864c350",
        "message": "Inline io_free_req_deferred(), there is no reason to keep it separated.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ce04b7180d4eac0d69dd00677b227eefe80c2cc5.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:59 -0600 io_uring: inline io_free_req_deferred"
    },
    {
        "commit": "b9bd2bea0f22f502019266dce368a9cd477ac721",
        "message": "Move the function together with io_rsrc_node_ref_zero() in the source\nfile as it is to get rid of forward declarations.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4d81f6f833e7d017860b24463a9a68b14a8a5ed2.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:59 -0600 io_uring: move io_rsrc_node_alloc() definition"
    },
    {
        "commit": "6a290a1442b45afb55d6a87619b716e5031d7c3e",
        "message": "Move the function in the source file as it is to get rid of forward\ndeclarations.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/33d917d69e4206557c75a5b98fe22bcdf77ce47d.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:59 -0600 io_uring: move io_put_task() definition"
    },
    {
        "commit": "e73c5c7cd3e21bb95032a9ed3593c000f17f9ab8",
        "message": "Refactor __io_uring_register() by extracting a helper responsible for\nctx queisce. Looks better and will make it easier to add more\noptimisations.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0339e0027504176be09237eefa7945bf9a6f153d.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:59 -0600 io_uring: extract a helper for ctx quiesce"
    },
    {
        "commit": "90291099f24a82863e00de136d95ad7e73560107",
        "message": "Turns out we always init struct io_wait_queue in io_cqring_wait(), even\nif it's not used after, i.e. there are already enough of CQEs. And often\nit's exactly what happens, for instance, requests may have been\ncompleted inline, or in case of io_uring_enter(submit=N, wait=1).\n\nIt shows up in my profiler, so optimise it by delaying the struct init.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/6f1b81c60b947d165583dc333947869c3d85d037.1628471125.git.asml.silence@gmail.com\n[axboe: fixed up for new cqring wait]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:59 -0600 io_uring: optimise io_cqring_wait() hot path"
    },
    {
        "commit": "282cdc86937bd31cf0ea49978ad7a42cfe12ea35",
        "message": "Add more annotations for submission path functions holding ->uring_lock.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/128ec4185e26fbd661dd3a424aa66108ee8ff951.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:59 -0600 io_uring: add more locking annotations for submit"
    },
    {
        "commit": "a2416e1ec23c6b79010d03d69c0e4e035339b4ad",
        "message": "IOPOLL users should care more about getting completions for requests\nthey submitted, but not in \"device did/completed something\". Currently,\nio_do_iopoll() may return a positive number, which will instruct\nio_iopoll_check() to break the loop and end the syscall, even if there\nis not enough CQEs or none at all.\n\nDon't return positive numbers, so io_iopoll_check() exits only when it\ngets an actual error, need reschedule or got enough CQEs.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/641a88f751623b6758303b3171f0a4141f06726e.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:59 -0600 io_uring: don't halt iopoll too early"
    },
    {
        "commit": "864ea921b0300fe5a4db9136b7e307e94b369530",
        "message": "Replace the main if of io_flush_cached_reqs() with inverted condition +\ngoto, so all the cases are handled in the same way. And also extract\nio_preinit_req() to make it cleaner and easier to refer to.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1abcba1f7b55dc53bf1dbe95036e345ffb1d5b01.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:59 -0600 io_uring: refactor io_alloc_req"
    },
    {
        "commit": "2215bed9246dbb95df50fcef788b0765c7c2aac0",
        "message": "We prefer nornal task_works even if it would fail requests inside. Kill\na PF_EXITING check in io_req_task_work_add(), task_work_add() handles\nwell dying tasks, i.e. return error when can't enqueue due to late\nstages of do_exit().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/fc14297e8441cd8f5d1743a2488cf0df09bf48ac.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:56 -0600 io_uring: remove unnecessary PF_EXITING check"
    },
    {
        "commit": "ebc11b6c6b87da5c83b4d934893a893f49160bc3",
        "message": "Move io-wq callbacks closer to each other, so it's easier to work with\nthem, and rename io_free_work() into io_wq_free_work() for consistency.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/851bbc7f0f86f206d8c1333efee8bcb9c26e419f.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:56 -0600 io_uring: clean io-wq callbacks"
    },
    {
        "commit": "c97d8a0f68b30960e9c8089bc37cc3b96a96f84d",
        "message": "If we use fixed files, we can be sure (almost) that REQ_F_ISREG is set.\nHowever, for non-reg files io_prep_rw() still will look into inode to\ndouble check, and that's expensive and can be avoided.\n\nThe only caveat is that it only currently works with 64+ bit\narchitectures, see FFS_ISREG, so we should consider that.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0a62780c491ca2522cd52db4ae3f16e03aafed0f.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:56 -0600 io_uring: avoid touching inode in rw prep"
    },
    {
        "commit": "b191e2dfe5955b392bc8c0ae546dfa5a13649c38",
        "message": "io_file_supports_async() checks whether a file supports nowait\noperations, so \"async\" in the name is misleading. Rename it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/33d55b5ce43aa1884c637c1957f1e30d30dc3bec.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:56 -0600 io_uring: rename io_file_supports_async()"
    },
    {
        "commit": "ac177053bb2cb1f3c4c8bf89bce34c3f2c4823a7",
        "message": "Optimise io_file_get() with registered files, which is in a hot path,\nby inlining parts of the function. Saves a function call, and\ninefficiencies of passing arguments, e.g. evaluating\n(sqe_flags & IOSQE_FIXED_FILE).\n\nIt couldn't have been done before as compilers were refusing to inline\nit because of the function size.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/52115cd6ce28f33bd0923149c0e6cb611084a0b1.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:56 -0600 io_uring: inline fixed part of io_file_get()"
    },
    {
        "commit": "042b0d85eabb79909ef29063fb45d363cbc0a85d",
        "message": "Instead of hand-coded two-level tables for registered files, allocate\nthem with kvmalloc(). In many cases small enough tables are enough, and\nso can be kmalloc()'ed removing an extra memory load and a bunch of bit\nlogic instructions from the hot path. If the table is larger, we trade\noff all the pros with a TLB-assisted memory lookup.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/280421d3b48775dabab773006bb5588c7b2dabc0.1628471125.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:56 -0600 io_uring: use kvmalloc for fixed files"
    },
    {
        "commit": "5fd4617840596884334332f36cabfe0deabe85c8",
        "message": "Currently we only wake the first waiter, even if we have enough entries\nposted to satisfy multiple waiters. Improve that situation so that\nevery waiter knows how much the CQ tail has to advance before they can\nbe safely woken up.\n\nWith this change, if we have N waiters each asking for 1 event and we get\n4 completions, then we wake up 4 waiters. If we have N waiters asking\nfor 2 completions and we get 4 completions, then we wake up the first\ntwo. Previously, only the first waiter would've been woken up.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-23 13:07:56 -0600 io_uring: be smarter about waking multiple CQ ring waiters"
    },
    {
        "commit": "1e6907d58cf03fc808009681b8ef178affbf96aa",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A few small fixes that should go into this release:\n\n   - Fix never re-assigning an initial error value for io_uring_enter()\n     for SQPOLL, if asked to do nothing\n\n   - Fix xa_alloc_cycle() return value checking, for cases where we have\n     wrapped around\n\n   - Fix for a ctx pin issue introduced in this cycle (Pavel)\"\n\n* tag 'io_uring-5.14-2021-08-20' of git://git.kernel.dk/linux-block:\n  io_uring: fix xa_alloc_cycle() error return value check\n  io_uring: pin ctx on fallback execution\n  io_uring: only assign io_uring_enter() SQPOLL error in actual error case",
        "kernel_version": "v5.14-rc7",
        "release_date": "2021-08-21 08:06:26 -0700 Merge tag 'io_uring-5.14-2021-08-20' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "a30f895ad3239f45012e860d4f94c1a388b36d14",
        "message": "We currently check for ret != 0 to indicate error, but '1' is a valid\nreturn and just indicates that the allocation succeeded with a wrap.\nCorrect the check to be for < 0, like it was before the xarray\nconversion.\n\nCc: stable@vger.kernel.org\nFixes: 61cf93700fe6 (\"io_uring: Convert personality_idr to XArray\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc7",
        "release_date": "2021-08-20 14:59:58 -0600 io_uring: fix xa_alloc_cycle() error return value check"
    },
    {
        "commit": "9cb0073b302a6b8a8c1015ff31b2b3ab4900f866",
        "message": "Pin ring in io_fallback_req_func() by briefly elevating ctx->refs in\ncase any task_work handler touches ctx after releasing a request.\n\nFixes: 9011bf9a13e3b (\"io_uring: fix stuck fallback reqs\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/833a494713d235ec144284a9bbfe418df4f6b61c.1629235576.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc7",
        "release_date": "2021-08-17 16:06:14 -0600 io_uring: pin ctx on fallback execution"
    },
    {
        "commit": "21f965221e7c42609521342403e8fb91b8b3e76e",
        "message": "If an SQPOLL based ring is newly created and an application issues an\nio_uring_enter(2) system call on it, then we can return a spurious\n-EOWNERDEAD error. This happens because there's nothing to submit, and\nif the caller doesn't specify any other action, the initial error\nassignment of -EOWNERDEAD never gets overwritten. This causes us to\nreturn it directly, even if it isn't valid.\n\nMove the error assignment into the actual failure case instead.\n\nCc: stable@vger.kernel.org\nFixes: d9d05217cb69 (\"io_uring: stop SQPOLL submit on creator's death\")\nReported-by: Sherlock Holo sherlockya@gmail.com\nLink: https://github.com/axboe/liburing/issues/413\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc7",
        "release_date": "2021-08-14 12:38:21 -0600 io_uring: only assign io_uring_enter() SQPOLL error in actual error case"
    },
    {
        "commit": "42995cee61f842c4e275e4902459f8a951fe4607",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A bit bigger than the previous weeks, but mostly just a few stable\n  bound fixes. In detail:\n\n   - Followup fixes to patches from last week for io-wq, turns out they\n     weren't complete (Hao)\n\n   - Two lockdep reported fixes out of the RT camp (me)\n\n   - Sync the io_uring-cp example with liburing, as a few bug fixes\n     never made it to the kernel carried version (me)\n\n   - SQPOLL related TIF_NOTIFY_SIGNAL fix (Nadav)\n\n   - Use WRITE_ONCE() when writing sq flags (Nadav)\n\n   - io_rsrc_put_work() deadlock fix (Pavel)\"\n\n* tag 'io_uring-5.14-2021-08-13' of git://git.kernel.dk/linux-block:\n  tools/io_uring/io_uring-cp: sync with liburing example\n  io_uring: fix ctx-exit io_rsrc_put_work() deadlock\n  io_uring: drop ctx->uring_lock before flushing work item\n  io-wq: fix IO_WORKER_F_FIXED issue in create_io_worker()\n  io-wq: fix bug of creating io-wokers unconditionally\n  io_uring: rsrc ref lock needs to be IRQ safe\n  io_uring: Use WRITE_ONCE() when writing to sq_flags\n  io_uring: clear TIF_NOTIFY_SIGNAL when running task work",
        "kernel_version": "v5.14-rc6",
        "release_date": "2021-08-13 13:25:08 -1000 Merge tag 'io_uring-5.14-2021-08-13' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "8f40d0370795313b6f1b1782035919cfc76b159f",
        "message": "This example is missing a few fixes that are in the liburing version,\nsynchronize with the upstream version.\n\nReported-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc6",
        "release_date": "2021-08-13 08:58:11 -0600 tools/io_uring/io_uring-cp: sync with liburing example"
    },
    {
        "commit": "432bc7caef4eaacc1101ee2569bb870bdfeed7ce",
        "message": "Enable the driver to work in non-IRQ mode, i.e. there will not be any MSI-X\nvectors associated with queues dedicated to polling. The IOC hardware is\nsingle submission queue and multiple reply queue. However, using the shared\nhost tagset support it is possible to simulate multiple hardware queues.\n\nWhen poll_queues are enabled through the module parameter, the driver will\nallocate extra reply queues without an MSI-X association. All I/O\ncompletion on these queues will be done through the iopoll interface.\n\nLink: https://lore.kernel.org/r/20210727081212.2742-1-sreekanth.reddy@broadcom.com\nSigned-off-by: Sreekanth Reddy <sreekanth.reddy@broadcom.com>\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>",
        "kernel_version": "v5.15-rc1",
        "release_date": "2021-08-09 22:55:50 -0400 scsi: mpt3sas: Add io_uring iopoll support"
    },
    {
        "commit": "43597aac1f87230cb565ab354d331682f13d3c7a",
        "message": "__io_rsrc_put_work() might need ->uring_lock, so nobody should wait for\nrsrc nodes holding the mutex. However, that's exactly what\nio_ring_ctx_free() does with io_wait_rsrc_data().\n\nSplit it into rsrc wait + dealloc, and move the first one out of the\nlock.\n\nCc: stable@vger.kernel.org\nFixes: b60c8dce33895 (\"io_uring: preparation for rsrc tagging\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0130c5c2693468173ec1afab714e0885d2c9c363.1628559783.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc6",
        "release_date": "2021-08-09 19:59:28 -0600 io_uring: fix ctx-exit io_rsrc_put_work() deadlock"
    },
    {
        "commit": "c018db4a57f3e31a9cb24d528e9f094eda89a499",
        "message": "Ammar reports that he's seeing a lockdep splat on running test/rsrc_tags\nfrom the regression suite:\n\n======================================================\nWARNING: possible circular locking dependency detected\n5.14.0-rc3-bluetea-test-00249-gc7d102232649 #5 Tainted: G           OE\n------------------------------------------------------\nkworker/2:4/2684 is trying to acquire lock:\nffff88814bb1c0a8 (&ctx->uring_lock){+.+.}-{3:3}, at: io_rsrc_put_work+0x13d/0x1a0\n\nbut task is already holding lock:\nffffc90001c6be70 ((work_completion)(&(&ctx->rsrc_put_work)->work)){+.+.}-{0:0}, at: process_one_work+0x1bc/0x530\n\nwhich lock already depends on the new lock.\n\nthe existing dependency chain (in reverse order) is:\n\n-> #1 ((work_completion)(&(&ctx->rsrc_put_work)->work)){+.+.}-{0:0}:\n       __flush_work+0x31b/0x490\n       io_rsrc_ref_quiesce.part.0.constprop.0+0x35/0xb0\n       __do_sys_io_uring_register+0x45b/0x1060\n       do_syscall_64+0x35/0xb0\n       entry_SYSCALL_64_after_hwframe+0x44/0xae\n\n-> #0 (&ctx->uring_lock){+.+.}-{3:3}:\n       __lock_acquire+0x119a/0x1e10\n       lock_acquire+0xc8/0x2f0\n       __mutex_lock+0x86/0x740\n       io_rsrc_put_work+0x13d/0x1a0\n       process_one_work+0x236/0x530\n       worker_thread+0x52/0x3b0\n       kthread+0x135/0x160\n       ret_from_fork+0x1f/0x30\n\nother info that might help us debug this:\n\n Possible unsafe locking scenario:\n\n       CPU0                    CPU1\n       ----                    ----\n  lock((work_completion)(&(&ctx->rsrc_put_work)->work));\n                               lock(&ctx->uring_lock);\n                               lock((work_completion)(&(&ctx->rsrc_put_work)->work));\n  lock(&ctx->uring_lock);\n\n *** DEADLOCK ***\n\n2 locks held by kworker/2:4/2684:\n #0: ffff88810004d938 ((wq_completion)events){+.+.}-{0:0}, at: process_one_work+0x1bc/0x530\n #1: ffffc90001c6be70 ((work_completion)(&(&ctx->rsrc_put_work)->work)){+.+.}-{0:0}, at: process_one_work+0x1bc/0x530\n\nstack backtrace:\nCPU: 2 PID: 2684 Comm: kworker/2:4 Tainted: G           OE     5.14.0-rc3-bluetea-test-00249-gc7d102232649 #5\nHardware name: Acer Aspire ES1-421/OLVIA_BE, BIOS V1.05 07/02/2015\nWorkqueue: events io_rsrc_put_work\nCall Trace:\n dump_stack_lvl+0x6a/0x9a\n check_noncircular+0xfe/0x110\n __lock_acquire+0x119a/0x1e10\n lock_acquire+0xc8/0x2f0\n ? io_rsrc_put_work+0x13d/0x1a0\n __mutex_lock+0x86/0x740\n ? io_rsrc_put_work+0x13d/0x1a0\n ? io_rsrc_put_work+0x13d/0x1a0\n ? io_rsrc_put_work+0x13d/0x1a0\n ? process_one_work+0x1ce/0x530\n io_rsrc_put_work+0x13d/0x1a0\n process_one_work+0x236/0x530\n worker_thread+0x52/0x3b0\n ? process_one_work+0x530/0x530\n kthread+0x135/0x160\n ? set_kthread_struct+0x40/0x40\n ret_from_fork+0x1f/0x30\n\nwhich is due to holding the ctx->uring_lock when flushing existing\npending work, while the pending work flushing may need to grab the uring\nlock if we're using IOPOLL.\n\nFix this by dropping the uring_lock a bit earlier as part of the flush.\n\nCc: stable@vger.kernel.org\nLink: https://github.com/axboe/liburing/issues/404\nTested-by: Ammar Faizi <ammarfaizi2@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc6",
        "release_date": "2021-08-09 19:59:06 -0600 io_uring: drop ctx->uring_lock before flushing work item"
    },
    {
        "commit": "4956b9eaad456a88b0d56947bef036e086250beb",
        "message": "Nadav reports running into the below splat on re-enabling softirqs:\n\nWARNING: CPU: 2 PID: 1777 at kernel/softirq.c:364 __local_bh_enable_ip+0xaa/0xe0\nModules linked in:\nCPU: 2 PID: 1777 Comm: umem Not tainted 5.13.1+ #161\nHardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/22/2020\nRIP: 0010:__local_bh_enable_ip+0xaa/0xe0\nCode: a9 00 ff ff 00 74 38 65 ff 0d a2 21 8c 7a e8 ed 1a 20 00 fb 66 0f 1f 44 00 00 5b 41 5c 5d c3 65 8b 05 e6 2d 8c 7a 85 c0 75 9a <0f> 0b eb 96 e8 2d 1f 20 00 eb a5 4c 89 e7 e8 73 4f 0c 00 eb ae 65\nRSP: 0018:ffff88812e58fcc8 EFLAGS: 00010046\nRAX: 0000000000000000 RBX: 0000000000000201 RCX: dffffc0000000000\nRDX: 0000000000000007 RSI: 0000000000000201 RDI: ffffffff8898c5ac\nRBP: ffff88812e58fcd8 R08: ffffffff8575dbbf R09: ffffed1028ef14f9\nR10: ffff88814778a7c3 R11: ffffed1028ef14f8 R12: ffffffff85c9e9ae\nR13: ffff88814778a000 R14: ffff88814778a7b0 R15: ffff8881086db890\nFS:  00007fbcfee17700(0000) GS:ffff8881e0300000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 000000c0402a5008 CR3: 000000011c1ac003 CR4: 00000000003706e0\nCall Trace:\n _raw_spin_unlock_bh+0x31/0x40\n io_rsrc_node_ref_zero+0x13e/0x190\n io_dismantle_req+0x215/0x220\n io_req_complete_post+0x1b8/0x720\n __io_complete_rw.isra.0+0x16b/0x1f0\n io_complete_rw+0x10/0x20\n\nwhere it's clear we end up calling the percpu count release directly\nfrom the completion path, as it's in atomic mode and we drop the last\nref. For file/block IO, this can be from IRQ context already, and the\nsoftirq locking for rsrc isn't enough.\n\nJust make the lock fully IRQ safe, and ensure we correctly safe state\nfrom the release path as we don't know the full context there.\n\nReported-by: Nadav Amit <nadav.amit@gmail.com>\nTested-by: Nadav Amit <nadav.amit@gmail.com>\nLink: https://lore.kernel.org/io-uring/C187C836-E78B-4A31-B24C-D16919ACA093@gmail.com/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc6",
        "release_date": "2021-08-09 19:58:59 -0600 io_uring: rsrc ref lock needs to be IRQ safe"
    },
    {
        "commit": "20c0b380f971e7d48f5d978bc27d827f7eabb21a",
        "message": "The compiler should be forbidden from any strange optimization for async\nwrites to user visible data-structures. Without proper protection, the\ncompiler can cause write-tearing or invent writes that would confuse the\nuserspace.\n\nHowever, there are writes to sq_flags which are not protected by\nWRITE_ONCE(). Use WRITE_ONCE() for these writes.\n\nThis is purely a theoretical issue. Presumably, any compiler is very\nunlikely to do such optimizations.\n\nFixes: 75b28affdd6a (\"io_uring: allocate the two rings together\")\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Nadav Amit <namit@vmware.com>\nLink: https://lore.kernel.org/r/20210808001342.964634-3-namit@vmware.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc6",
        "release_date": "2021-08-08 21:21:11 -0600 io_uring: Use WRITE_ONCE() when writing to sq_flags"
    },
    {
        "commit": "ef98eb0409c31c39ab55ff46b2721c3b4f84c122",
        "message": "When using SQPOLL, the submission queue polling thread calls\ntask_work_run() to run queued work. However, when work is added with\nTWA_SIGNAL - as done by io_uring itself - the TIF_NOTIFY_SIGNAL remains\nset afterwards and is never cleared.\n\nConsequently, when the submission queue polling thread checks whether\nsignal_pending(), it may always find a pending signal, if\ntask_work_add() was ever called before.\n\nThe impact of this bug might be different on different kernel versions.\nIt appears that on 5.14 it would only cause unnecessary calculation and\nprevent the polling thread from sleeping. On 5.13, where the bug was\nfound, it stops the polling thread from finding newly submitted work.\n\nInstead of task_work_run(), use tracehook_notify_signal() that clears\nTIF_NOTIFY_SIGNAL. Test for TIF_NOTIFY_SIGNAL in addition to\ncurrent->task_works to avoid a race in which task_works is cleared but\nthe TIF_NOTIFY_SIGNAL is set.\n\nFixes: 685fe7feedb96 (\"io-wq: eliminate the need for a manager thread\")\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Nadav Amit <namit@vmware.com>\nLink: https://lore.kernel.org/r/20210808001342.964634-2-namit@vmware.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc6",
        "release_date": "2021-08-08 21:21:11 -0600 io_uring: clear TIF_NOTIFY_SIGNAL when running task work"
    },
    {
        "commit": "85a90500f9a1717c4e142ce92e6c1cb1a339ec78",
        "message": "Pull io_uring from Jens Axboe:\n \"A few io-wq related fixes:\n\n   - Fix potential nr_worker race and missing max_workers check from one\n     path (Hao)\n\n   - Fix race between worker exiting and new work queue (me)\"\n\n* tag 'io_uring-5.14-2021-08-07' of git://git.kernel.dk/linux-block:\n  io-wq: fix lack of acct->nr_workers < acct->max_workers judgement\n  io-wq: fix no lock protection of acct->nr_worker\n  io-wq: fix race between worker exiting and activating free worker",
        "kernel_version": "v5.14-rc5",
        "release_date": "2021-08-07 10:34:26 -0700 Merge tag 'io_uring-5.14-2021-08-07' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "27eb687bcdb987d978da842ede944bee335b3524",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - A fix for block backed reissue (me)\n\n - Reissue context hardening (me)\n\n - Async link locking fix (Pavel)\n\n* tag 'io_uring-5.14-2021-07-30' of git://git.kernel.dk/linux-block:\n  io_uring: fix poll requests leaking second poll entries\n  io_uring: don't block level reissue off completion path\n  io_uring: always reissue from task_work context\n  io_uring: fix race in unified task_work running\n  io_uring: fix io_prep_async_link locking",
        "kernel_version": "v5.14-rc4",
        "release_date": "2021-07-30 11:01:47 -0700 Merge tag 'io_uring-5.14-2021-07-30' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "a890d01e4ee016978776e45340e521b3bbbdf41f",
        "message": "For pure poll requests, it doesn't remove the second poll wait entry\nwhen it's done, neither after vfs_poll() or in the poll completion\nhandler. We should remove the second poll wait entry.\nAnd we use io_poll_remove_double() rather than io_poll_remove_waitqs()\nsince the latter has some redundant logic.\n\nFixes: 88e41cf928a6 (\"io_uring: add multishot mode for IORING_OP_POLL_ADD\")\nCc: stable@vger.kernel.org # 5.13+\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20210728030322.12307-1-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc4",
        "release_date": "2021-07-28 07:24:57 -0600 io_uring: fix poll requests leaking second poll entries"
    },
    {
        "commit": "ef04688871f3386b6d40ade8f5c664290420f819",
        "message": "Some setups, like SCSI, can throw spurious -EAGAIN off the softirq\ncompletion path. Normally we expect this to happen inline as part\nof submission, but apparently SCSI has a weird corner case where it\ncan happen as part of normal completions.\n\nThis should be solved by having the -EAGAIN bubble back up the stack\nas part of submission, but previous attempts at this failed and we're\nnot just quite there yet. Instead we currently use REQ_F_REISSUE to\nhandle this case.\n\nFor now, catch it in io_rw_should_reissue() and prevent a reissue\nfrom a bogus path.\n\nCc: stable@vger.kernel.org\nReported-by: Fabian Ebner <f.ebner@proxmox.com>\nTested-by: Fabian Ebner <f.ebner@proxmox.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc4",
        "release_date": "2021-07-28 07:24:38 -0600 io_uring: don't block level reissue off completion path"
    },
    {
        "commit": "773af69121ecc6c53d192661af8d53bb3db028ae",
        "message": "As a safeguard, if we're going to queue async work, do it from task_work\nfrom the original task. This ensures that we can always sanely create\nthreads, regards of what the reissue context may be.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc4",
        "release_date": "2021-07-27 10:49:48 -0600 io_uring: always reissue from task_work context"
    },
    {
        "commit": "110aa25c3ce417a44e35990cf8ed22383277933a",
        "message": "We use a bit to manage if we need to add the shared task_work, but\na list + lock for the pending work. Before aborting a current run\nof the task_work we check if the list is empty, but we do so without\ngrabbing the lock that protects it. This can lead to races where\nwe think we have nothing left to run, where in practice we could be\nracing with a task adding new work to the list. If we do hit that\nrace condition, we could be left with work items that need processing,\nbut the shared task_work is not active.\n\nEnsure that we grab the lock before checking if the list is empty,\nso we know if it's safe to exit the run or not.\n\nLink: https://lore.kernel.org/io-uring/c6bd5987-e9ae-cd02-49d0-1b3ac1ef65b1@tnonline.net/\nCc: stable@vger.kernel.org # 5.11+\nReported-by: Forza <forza@tnonline.net>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc4",
        "release_date": "2021-07-26 10:42:56 -0600 io_uring: fix race in unified task_work running"
    },
    {
        "commit": "44eff40a32e8f5228ae041006352e32638ad2368",
        "message": "io_prep_async_link() may be called after arming a linked timeout,\nautomatically making it unsafe to traverse the linked list. Guard\nwith completion_lock if there was a linked timeout.\n\nCc: stable@vger.kernel.org # 5.9+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/93f7c617e2b4f012a2a175b3dab6bc2f27cebc48.1627304436.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc4",
        "release_date": "2021-07-26 08:58:04 -0600 io_uring: fix io_prep_async_link locking"
    },
    {
        "commit": "0ee818c393dce98340bff2b08573d4d2d8650eb7",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix a memory leak due to a race condition in io_init_wq_offload\n   (Yang)\n\n - Poll error handling fixes (Pavel)\n\n - Fix early fdput() regression (me)\n\n - Don't reissue iopoll requests off release path (me)\n\n - Add a safety check for io-wq queue off wrong path (me)\n\n* tag 'io_uring-5.14-2021-07-24' of git://git.kernel.dk/linux-block:\n  io_uring: explicitly catch any illegal async queue attempt\n  io_uring: never attempt iopoll reissue from release path\n  io_uring: fix early fdput() of file\n  io_uring: fix memleak in io_init_wq_offload()\n  io_uring: remove double poll entry on arm failure\n  io_uring: explicitly count entries for poll reqs",
        "kernel_version": "v5.14-rc3",
        "release_date": "2021-07-24 13:03:40 -0700 Merge tag 'io_uring-5.14-2021-07-24' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "991468dcf198bb87f24da330676724a704912b47",
        "message": "Catch an illegal case to queue async from an unrelated task that got\nthe ring fd passed to it. This should not be possible to hit, but\nbetter be proactive and catch it explicitly. io-wq is extended to\ncheck for early IO_WQ_WORK_CANCEL being set on a work item as well,\nso it can run the request through the normal cancelation path.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc3",
        "release_date": "2021-07-23 16:44:51 -0600 io_uring: explicitly catch any illegal async queue attempt"
    },
    {
        "commit": "3c30ef0f78cfb36fdb13753794b0384cf7e37cc9",
        "message": "There are two reasons why this shouldn't be done:\n\n1) Ring is exiting, and we're canceling requests anyway. Any request\n   should be canceled anyway. In theory, this could iterate for a\n   number of times if someone else is also driving the target block\n   queue into request starvation, however the likelihood of this\n   happening is miniscule.\n\n2) If the original task decided to pass the ring to another task, then\n   we don't want to be reissuing from this context as it may be an\n   unrelated task or context. No assumptions should be made about\n   the context in which ->release() is run. This can only happen for pure\n   read/write, and we'll get -EFAULT on them anyway.\n\nLink: https://lore.kernel.org/io-uring/YPr4OaHv0iv0KTOc@zeniv-ca.linux.org.uk/\nReported-by: Al Viro <viro@zeniv.linux.org.uk>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc3",
        "release_date": "2021-07-23 16:32:48 -0600 io_uring: never attempt iopoll reissue from release path"
    },
    {
        "commit": "0cc936f74bcacb039b7533aeac0a887dfc896bf6",
        "message": "A previous commit shuffled some code around, and inadvertently used\nstruct file after fdput() had been called on it. As we can't touch\nthe file post fdput() dropping our reference, move the fdput() to\nafter that has been done.\n\nCc: Pavel Begunkov <asml.silence@gmail.com>\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/io-uring/YPnqM0fY3nM5RdRI@zeniv-ca.linux.org.uk/\nFixes: f2a48dd09b8e (\"io_uring: refactor io_sq_offload_create()\")\nReported-by: Al Viro <viro@zeniv.linux.org.uk>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc3",
        "release_date": "2021-07-22 17:11:46 -0600 io_uring: fix early fdput() of file"
    },
    {
        "commit": "362a9e65289284f36403058eea2462d0330c1f24",
        "message": "I got memory leak report when doing fuzz test:\n\nBUG: memory leak\nunreferenced object 0xffff888107310a80 (size 96):\ncomm \"syz-executor.6\", pid 4610, jiffies 4295140240 (age 20.135s)\nhex dump (first 32 bytes):\n01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................\n00 00 00 00 ad 4e ad de ff ff ff ff 00 00 00 00 .....N..........\nbacktrace:\n[<000000001974933b>] kmalloc include/linux/slab.h:591 [inline]\n[<000000001974933b>] kzalloc include/linux/slab.h:721 [inline]\n[<000000001974933b>] io_init_wq_offload fs/io_uring.c:7920 [inline]\n[<000000001974933b>] io_uring_alloc_task_context+0x466/0x640 fs/io_uring.c:7955\n[<0000000039d0800d>] __io_uring_add_tctx_node+0x256/0x360 fs/io_uring.c:9016\n[<000000008482e78c>] io_uring_add_tctx_node fs/io_uring.c:9052 [inline]\n[<000000008482e78c>] __do_sys_io_uring_enter fs/io_uring.c:9354 [inline]\n[<000000008482e78c>] __se_sys_io_uring_enter fs/io_uring.c:9301 [inline]\n[<000000008482e78c>] __x64_sys_io_uring_enter+0xabc/0xc20 fs/io_uring.c:9301\n[<00000000b875f18f>] do_syscall_x64 arch/x86/entry/common.c:50 [inline]\n[<00000000b875f18f>] do_syscall_64+0x3b/0x90 arch/x86/entry/common.c:80\n[<000000006b0a8484>] entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nCPU0                          CPU1\nio_uring_enter                io_uring_enter\nio_uring_add_tctx_node        io_uring_add_tctx_node\n__io_uring_add_tctx_node      __io_uring_add_tctx_node\nio_uring_alloc_task_context   io_uring_alloc_task_context\nio_init_wq_offload            io_init_wq_offload\nhash = kzalloc                hash = kzalloc\nctx->hash_map = hash          ctx->hash_map = hash <- one of the hash is leaked\n\nWhen calling io_uring_enter() in parallel, the 'hash_map' will be leaked,\nadd uring_lock to protect 'hash_map'.\n\nFixes: e941894eae31 (\"io-wq: make buffered file write hashed work map per-ctx\")\nReported-by: Hulk Robot <hulkci@huawei.com>\nSigned-off-by: Yang Yingliang <yangyingliang@huawei.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20210720083805.3030730-1-yangyingliang@huawei.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc3",
        "release_date": "2021-07-20 07:51:47 -0600 io_uring: fix memleak in io_init_wq_offload()"
    },
    {
        "commit": "46fee9ab02cb24979bbe07631fc3ae95ae08aa3e",
        "message": "__io_queue_proc() can enqueue both poll entries and still fail\nafterwards, so the callers trying to cancel it should also try to remove\nthe second poll entry (if any).\n\nFor example, it may leave the request alive referencing a io_uring\ncontext but not accessible for cancellation:\n\n[  282.599913][ T1620] task:iou-sqp-23145   state:D stack:28720 pid:23155 ppid:  8844 flags:0x00004004\n[  282.609927][ T1620] Call Trace:\n[  282.613711][ T1620]  __schedule+0x93a/0x26f0\n[  282.634647][ T1620]  schedule+0xd3/0x270\n[  282.638874][ T1620]  io_uring_cancel_generic+0x54d/0x890\n[  282.660346][ T1620]  io_sq_thread+0xaac/0x1250\n[  282.696394][ T1620]  ret_from_fork+0x1f/0x30\n\nCc: stable@vger.kernel.org\nFixes: 18bceab101add (\"io_uring: allow POLL_ADD with double poll_wait() users\")\nReported-and-tested-by: syzbot+ac957324022b7132accf@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0ec1228fc5eda4cb524eeda857da8efdc43c331c.1626774457.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc3",
        "release_date": "2021-07-20 07:50:42 -0600 io_uring: remove double poll entry on arm failure"
    },
    {
        "commit": "68b11e8b1562986c134764433af64e97d30c9fc0",
        "message": "If __io_queue_proc() fails to add a second poll entry, e.g. kmalloc()\nfailed, but it goes on with a third waitqueue, it may succeed and\noverwrite the error status. Count the number of poll entries we added,\nso we can set pt->error to zero at the beginning and find out when the\nmentioned scenario happens.\n\nCc: stable@vger.kernel.org\nFixes: 18bceab101add (\"io_uring: allow POLL_ADD with double poll_wait() users\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9d6b9e561f88bcc0163623b74a76c39f712151c3.1626774457.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc3",
        "release_date": "2021-07-20 07:50:42 -0600 io_uring: explicitly count entries for poll reqs"
    },
    {
        "commit": "13fdaf041067a7827b8c3cae095b661aabbc6b65",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two small fixes: one fixing the process target of a check, and the\n  other a minor issue with the drain error handling\"\n\n* tag 'io_uring-5.14-2021-07-16' of git://git.kernel.dk/linux-block:\n  io_uring: fix io_drain_req()\n  io_uring: use right task for exiting checks",
        "kernel_version": "v5.14-rc2",
        "release_date": "2021-07-16 12:27:33 -0700 Merge tag 'io_uring-5.14-2021-07-16' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "1b48773f9fd09f311d1166ce1dd50652ebe05218",
        "message": "io_drain_req() return whether the request has been consumed or not, not\nan error code. Fix a stupid mistake slipped from optimisation patches.\n\nReported-by: syzbot+ba6fcd859210f4e9e109@syzkaller.appspotmail.com\nFixes: 76cc33d79175a (\"io_uring: refactor io_req_defer()\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4d3c53c4274ffff307c8ae062fc7fda63b978df2.1626039606.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc2",
        "release_date": "2021-07-11 16:39:06 -0600 io_uring: fix io_drain_req()"
    },
    {
        "commit": "9c6882608bce249a8918744ecdb65748534e3f17",
        "message": "When we use delayed_work for fallback execution of requests, current\nwill be not of the submitter task, and so checks in io_req_task_submit()\nmay not behave as expected. Currently, it leaves inline completions not\nflushed, so making io_ring_exit_work() to hang. Use the submitter task\nfor all those checks.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/cb413c715bed0bc9c98b169059ea9c8a2c770715.1625881431.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc2",
        "release_date": "2021-07-11 16:39:06 -0600 io_uring: use right task for exiting checks"
    },
    {
        "commit": "50be9417e23af5a8ac860d998e1e3f06b8fd79d7",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A few fixes that should go into this merge.\n\n  One fixes a regression introduced in this release, others are just\n  generic fixes, mostly related to handling fallback task_work\"\n\n* tag 'io_uring-5.14-2021-07-09' of git://git.kernel.dk/linux-block:\n  io_uring: remove dead non-zero 'poll' check\n  io_uring: mitigate unlikely iopoll lag\n  io_uring: fix drain alloc fail return code\n  io_uring: fix exiting io_req_task_work_add leaks\n  io_uring: simplify task_work func\n  io_uring: fix stuck fallback reqs",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-07-09 12:17:38 -0700 Merge tag 'io_uring-5.14-2021-07-09' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "9ce85ef2cb5c738754837a6937e120694cde33c9",
        "message": "Colin reports that Coverity complains about checking for poll being\nnon-zero after having dereferenced it multiple times. This is a valid\ncomplaint, and actually a leftover from back when this code was based\non the aio poll code.\n\nKill the redundant check.\n\nLink: https://lore.kernel.org/io-uring/fe70c532-e2a7-3722-58a1-0fa4e5c5ff2c@canonical.com/\nReported-by: Colin Ian King <colin.king@canonical.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-07-09 08:20:28 -0600 io_uring: remove dead non-zero 'poll' check"
    },
    {
        "commit": "8f487ef2cbb2d4f6ca8c113d70da63baaf68c91a",
        "message": "We have requests like IORING_OP_FILES_UPDATE that don't go through\n->iopoll_list but get completed in place under ->uring_lock, and so\nafter dropping the lock io_iopoll_check() should expect that some CQEs\nmight have get completed in a meanwhile.\n\nCurrently such events won't be accounted in @nr_events, and the loop\nwill continue to poll even if there is enough of CQEs. It shouldn't be a\nproblem as it's not likely to happen and so, but not nice either. Just\nreturn earlier in this case, it should be enough.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/66ef932cc66a34e3771bbae04b2953a8058e9d05.1625747741.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-07-08 14:07:43 -0600 io_uring: mitigate unlikely iopoll lag"
    },
    {
        "commit": "c32aace0cf93383fde48c60ce0ae0c9073b6d360",
        "message": "After a recent change io_drain_req() started to fail requests with\nresult=0 in case of allocation failure, where it should be and have\nbeen -ENOMEM.\n\nFixes: 76cc33d79175a (\"io_uring: refactor io_req_defer()\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e068110ac4293e0c56cfc4d280d0f22b9303ec08.1625682153.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-07-07 12:49:32 -0600 io_uring: fix drain alloc fail return code"
    },
    {
        "commit": "e09ee510600b941c62e94f6b59878cf53ba0e447",
        "message": "If one entered io_req_task_work_add() not seeing PF_EXITING, it will set\na ->task_state bit and try task_work_add(), which may fail by that\nmoment. If that happens the function would try to cancel the request.\n\nHowever, in a meanwhile there might come other io_req_task_work_add()\ncallers, which will see the bit set and leave their requests in the\nlist, which will never be executed.\n\nDon't propagate an error, but clear the bit first and then fallback\nall requests that we can splice from the list. The callback functions\nhave to be able to deal with PF_EXITING, so poll and apoll was modified\nvia changing io_poll_rewait().\n\nFixes: 7cbf1722d5fc (\"io_uring: provide FIFO ordering for task_work\")\nReported-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/060002f19f1fdbd130ba24aef818ea4d3080819b.1625142209.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-07-01 13:40:32 -0600 io_uring: fix exiting io_req_task_work_add leaks"
    },
    {
        "commit": "5b0a6acc73fcac5f7d17badd09275bf7b9b46603",
        "message": "Since we don't really use req->task_work anymore, get rid of it together\nwith the nasty ->func aliasing between ->io_task_work and ->task_work,\nand hide ->fallback_node inside of io_task_work.\n\nAlso, as task_work is gone now, replace the callback type from\ntask_work_func_t to a function taking io_kiocb to avoid casting and\nsimplify code.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-07-01 13:40:23 -0600 io_uring: simplify task_work func"
    },
    {
        "commit": "9011bf9a13e3b5710c3cfc330da829ee25b5a029",
        "message": "When task_work_add() fails, we use ->exit_task_work to queue the work.\nThat will be run only in the cancellation path, which happens either\nwhen the ctx is dying or one of tasks with inflight requests is exiting\nor executing. There is a good chance that such a request would just get\nstuck in the list potentially hodling a file, all io_uring rsrc\nrecycling or some other resources. Nothing terrible, it'll go away at\nsome point, but we don't want to lock them up for longer than needed.\n\nReplace that hand made ->exit_task_work with delayed_work + llist\ninspired by fput_many().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-07-01 13:40:17 -0600 io_uring: fix stuck fallback reqs"
    },
    {
        "commit": "c288d9cd710433e5991d58a0764c4d08a933b871",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - Multi-queue iopoll improvement (Fam)\n\n - Allow configurable io-wq CPU masks (me)\n\n - renameat/linkat tightening (me)\n\n - poll re-arm improvement (Olivier)\n\n - SQPOLL race fix (Olivier)\n\n - Cancelation unification (Pavel)\n\n - SQPOLL cleanups (Pavel)\n\n - Enable file backed buffers for shmem/memfd (Pavel)\n\n - A ton of cleanups and performance improvements (Pavel)\n\n - Followup and misc fixes (Colin, Fam, Hao, Olivier)\n\n* tag 'for-5.14/io_uring-2021-06-30' of git://git.kernel.dk/linux-block: (83 commits)\n  io_uring: code clean for kiocb_done()\n  io_uring: spin in iopoll() only when reqs are in a single queue\n  io_uring: pre-initialise some of req fields\n  io_uring: refactor io_submit_flush_completions\n  io_uring: optimise hot path restricted checks\n  io_uring: remove not needed PF_EXITING check\n  io_uring: mainstream sqpoll task_work running\n  io_uring: refactor io_arm_poll_handler()\n  io_uring: reduce latency by reissueing the operation\n  io_uring: add IOPOLL and reserved field checks to IORING_OP_UNLINKAT\n  io_uring: add IOPOLL and reserved field checks to IORING_OP_RENAMEAT\n  io_uring: refactor io_openat2()\n  io_uring: simplify struct io_uring_sqe layout\n  io_uring: update sqe layout build checks\n  io_uring: fix code style problems\n  io_uring: refactor io_sq_thread()\n  io_uring: don't change sqpoll creds if not needed\n  io_uring: Create define to modify a SQPOLL parameter\n  io_uring: Fix race condition when sqp thread goes to sleep\n  io_uring: improve in tctx_task_work() resubmission\n  ...",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-07-01 12:16:24 -0700 Merge tag 'for-5.14/io_uring-2021-06-30' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "5c874a5b29c264f88fafb323e8df7da7b214b6a9",
        "message": "Pull smack updates from Casey Schaufler:\n \"There is nothing more significant than an improvement to a byte count\n  check in smackfs.\n\n  All changes have been in next for weeks\"\n\n* tag 'Smack-for-5.14' of git://github.com/cschaufler/smack-next:\n  Smack: fix doc warning\n  Revert \"Smack: Handle io_uring kernel thread privileges\"\n  smackfs: restrict bytes count in smk_set_cipso()\n  security/smack/: fix misspellings using codespell tool",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 15:28:43 -0700 Merge tag 'Smack-for-5.14' of git://github.com/cschaufler/smack-next"
    },
    {
        "commit": "e149bd742b2db6a63fc078b1ea6843dc9b22678d",
        "message": "A simple code clean for kiocb_done()\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:40 -0600 io_uring: code clean for kiocb_done()"
    },
    {
        "commit": "915b3dde9b72cb4f531b04208daafcd0a257b847",
        "message": "We currently spin in iopoll() when requests to be iopolled are for\nsame file(device), while one device may have multiple hardware queues.\ngiven an example:\n\nhw_queue_0     |    hw_queue_1\nreq(30us)           req(10us)\n\nIf we first spin on iopolling for the hw_queue_0. the avg latency would\nbe (30us + 30us) / 2 = 30us. While if we do round robin, the avg\nlatency would be (30us + 10us) / 2 = 20us since we reap the request in\nhw_queue_1 in time. So it's better to do spinning only when requests\nare in same hardware queue.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:40 -0600 io_uring: spin in iopoll() only when reqs are in a single queue"
    },
    {
        "commit": "99ebe4efbd3882422db1fd6a1b477291ea8bdab7",
        "message": "Most of requests are allocated from an internal cache, so it's waste of\ntime fully initialising them every time. Instead, let's pre-init some of\nthe fields we can during initial allocation (e.g. kmalloc(), see\nio_alloc_req()) and keep them valid on request recycling. There are four\nof them in this patch:\n\n->ctx is always stays the same\n->link is NULL on free, it's an invariant\n->result is not even needed to init, just a precaution\n->async_data we now clean in io_dismantle_req() as it's likely to\n   never be allocated.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/892ba0e71309bba9fe9e0142472330bbf9d8f05d.1624739600.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:40 -0600 io_uring: pre-initialise some of req fields"
    },
    {
        "commit": "5182ed2e332e8e11fa3c1649ef6d6546ccca64d0",
        "message": "Don't init req_batch before we actually need it. Also, add a small clean\nup for req declaration.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ad85512e12bd3a20d521e9782750300970e5afc8.1624739600.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:40 -0600 io_uring: refactor io_submit_flush_completions"
    },
    {
        "commit": "4cfb25bf8877c947e5ae4875e387babe87e12afa",
        "message": "Move likely/unlikely from io_check_restriction() to specifically\nctx->restricted check, because doesn't do what it supposed to and make\nthe common path take an extra jump.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/22bf70d0a543dfc935d7276bdc73081784e30698.1624739600.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:40 -0600 io_uring: optimise hot path restricted checks"
    },
    {
        "commit": "e5dc480d4ed9884274e95c757fa2d2e9cc1047ee",
        "message": "Since cancellation got moved before exit_signals(), there is no one left\nwho can call io_run_task_work() with PF_EXIING set, so remove the check.\nNote that __io_req_task_submit() still needs a similar check.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f7f305ececb1e6044ea649fb983ca754805bb884.1624739600.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:40 -0600 io_uring: remove not needed PF_EXITING check"
    },
    {
        "commit": "dd432ea5204eeb92a2abf246ce518e68679da772",
        "message": "task_works are widely used, so place io_run_task_work() directly into\nthe main path of io_sq_thread(), and remove it from other places where\nit's not needed anymore.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/24eb5e35d519c590d3dffbd694b4c61a5fe49029.1624739600.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:39 -0600 io_uring: mainstream sqpoll task_work running"
    },
    {
        "commit": "b2d9c3da77115b5172749dec20312651e67e0adf",
        "message": "gcc 11 goes a weird path and duplicates most of io_arm_poll_handler()\nfor READ and WRITE cases. Help it and move all pollin vs pollout\nspecific bits under a single if-else, so there is no temptation for this\nkind of unfolding.\n\nbefore vs after:\n   text    data     bss     dec     hex filename\n  85362   12650       8   98020   17ee4 ./fs/io_uring.o\n  85186   12650       8   97844   17e34 ./fs/io_uring.o\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1deea0037293a922a0358e2958384b2e42437885.1624739600.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:39 -0600 io_uring: refactor io_arm_poll_handler()"
    },
    {
        "commit": "59b735aeeb0f23a760bc21f1c5a1ab6c79e9fe0e",
        "message": "It is quite frequent that when an operation fails and returns EAGAIN,\nthe data becomes available between that failure and the call to\nvfs_poll() done by io_arm_poll_handler().\n\nDetecting the situation and reissuing the operation is much faster\nthan going ahead and push the operation to the io-wq.\n\nPerformance improvement testing has been performed with:\nSingle thread, 1 TCP connection receiving a 5 Mbps stream, no sqpoll.\n\n4 measurements have been taken:\n1. The time it takes to process a read request when data is already available\n2. The time it takes to process by calling twice io_issue_sqe() after vfs_poll() indicated that data was available\n3. The time it takes to execute io_queue_async_work()\n4. The time it takes to complete a read request asynchronously\n\n2.25% of all the read operations did use the new path.\n\nready data (baseline)\navg\t3657.94182918628\nmin\t580\nmax\t20098\nstddev\t1213.15975908162\n\nreissue\tcompletion\naverage\t7882.67567567568\nmin\t2316\nmax\t28811\nstddev\t1982.79172973284\n\ninsert io-wq time\naverage\t8983.82276995305\nmin\t3324\nmax\t87816\nstddev\t2551.60056552038\n\nasync time completion\naverage\t24670.4758861127\nmin\t10758\nmax\t102612\nstddev\t3483.92416873804\n\nConclusion:\nOn average reissuing the sqe with the patch code is 1.1uSec faster and\nin the worse case scenario 59uSec faster than placing the request on\nio-wq\n\nOn average completion time by reissuing the sqe with the patch code is\n16.79uSec faster and in the worse case scenario 73.8uSec faster than\nasync completion.\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9e8441419bb1b8f3c3fcc607b2713efecdef2136.1624364038.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:39 -0600 io_uring: reduce latency by reissueing the operation"
    },
    {
        "commit": "22634bc5620d29765e5199c7b230a372c7ddcda2",
        "message": "We can't support IOPOLL with non-pollable request types, and we should\ncheck for unused/reserved fields like we do for other request types.\n\nFixes: 14a1143b68ee (\"io_uring: add support for IORING_OP_UNLINKAT\")\nCc: stable@vger.kernel.org\nReported-by: Dmitry Kadashev <dkadashev@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:39 -0600 io_uring: add IOPOLL and reserved field checks to IORING_OP_UNLINKAT"
    },
    {
        "commit": "ed7eb2592286ead7d3bfdf8adf65e65392167cc4",
        "message": "We can't support IOPOLL with non-pollable request types, and we should\ncheck for unused/reserved fields like we do for other request types.\n\nFixes: 80a261fd0032 (\"io_uring: add support for IORING_OP_RENAMEAT\")\nCc: stable@vger.kernel.org\nReported-by: Dmitry Kadashev <dkadashev@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:39 -0600 io_uring: add IOPOLL and reserved field checks to IORING_OP_RENAMEAT"
    },
    {
        "commit": "12dcb58ac785ee678f577e1502d966b538375aae",
        "message": "Put do_filp_open() fail path of io_openat2() under a single if,\ndeduplicating put_unused_fd(), making it look better and helping\nthe hot path.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f4c84d25c049d0af2adc19c703bbfef607200209.1624543113.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:39 -0600 io_uring: refactor io_openat2()"
    },
    {
        "commit": "9ba6a1c06279ce499fcf755d8134d679a1f3b4ed",
        "message": "Flatten struct io_uring_sqe, the last union is exactly 64B, so move them\nout of union { struct { ... }}, and decrease __pad2 size.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2e21ef7aed136293d654450bc3088973a8adc730.1624543113.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:39 -0600 io_uring: simplify struct io_uring_sqe layout"
    },
    {
        "commit": "16340eab61a3ed1b5c983c19cfa9f51929b2beeb",
        "message": "Add missing BUILD_BUG_SQE_ELEM() for ->buf_group verifying that SQE\nlayout doesn't change.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1f9d21bd74599b856b3a632be4c23ffa184a3ef0.1624543113.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:39 -0600 io_uring: update sqe layout build checks"
    },
    {
        "commit": "fe7e325750299126b9cc86d3071af594b46c4518",
        "message": "Fix a bunch of problems mostly found by checkpatch.pl\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/cfaf9a2f27b43934144fe9422a916bd327099f44.1624543113.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:39 -0600 io_uring: fix code style problems"
    },
    {
        "commit": "1a924a808208c1880ef9f36b6bf98d27af045f06",
        "message": "Move needs_sched declaration into the block where it's used, so it's\nharder to misuse/wrongfully reuse.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e4a07db1353ee38b924dd1b45394cf8e746130b4.1624543113.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:39 -0600 io_uring: refactor io_sq_thread()"
    },
    {
        "commit": "948e19479cb649587165243c6cc12d113c9cbbe0",
        "message": "SQPOLL doesn't need to change creds if it's not submitting requests.\nMove creds overriding into __io_sq_thread() after checking if there are\nSQEs pending.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c54368da2357ac539e0a333f7cfff70d5fb045b2.1624543113.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-30 14:15:38 -0600 io_uring: don't change sqpoll creds if not needed"
    },
    {
        "commit": "4ce8ad95f0afe927e9a29e7ad491274ebe3a8a7b",
        "message": "The magic number used to cap the number of entries extracted from an\nio_uring instance SQ before moving to the other instances is an\ninteresting parameter to experiment with.\n\nA define has been created to make it easy to change its value from a\nsingle location.\n\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b401640063e77ad3e9f921e09c9b3ac10a8bb923.1624473200.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-23 20:38:21 -0600 io_uring: Create define to modify a SQPOLL parameter"
    },
    {
        "commit": "997135017716c33f3405e86cca5da9567b40a08e",
        "message": "If an asynchronous completion happens before the task is preparing\nitself to wait and set its state to TASK_INTERRUPTIBLE, the completion\nwill not wake up the sqp thread.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d1419dc32ec6a97b453bee34dc03fa6a02797142.1624473200.git.olivier@trillion01.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-23 20:38:21 -0600 io_uring: Fix race condition when sqp thread goes to sleep"
    },
    {
        "commit": "7a778f9dc32deae4f748903f6f9169dc01cbcd28",
        "message": "If task_state is cleared, io_req_task_work_add() will go the slow path\nadding a task_work, setting the task_state, waking up the task and so\non. Not to mention it's expensive. tctx_task_work() first clears the\nstate and then executes all the work items queued, so if any of them\nresubmits or adds new task_work items, it would unnecessarily go through\nthe slow path of io_req_task_work_add().\n\nLet's clear the ->task_state at the end. We still have to check\n->task_list for emptiness afterward to synchronise with\nio_req_task_work_add(), do that, and set the state back if we're going\nto retry, because clearing not-ours task_state on the next iteration\nwould be buggy.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1ef72cdac7022adf0cd7ce4bfe3bb5c82a62eb93.1623949695.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-18 09:22:02 -0600 io_uring: improve in tctx_task_work() resubmission"
    },
    {
        "commit": "16f72070386fca59312bde696cff917bb04b183e",
        "message": "Entering tctx_task_work() with empty task_list is a strange scenario,\nthat can happen only on rare occasion during task exit, so let's not\ncheck for task_list emptiness in advance and do it do-while style. The\ncode still correct for the empty case, just would do extra work about\nwhich we don't care.\n\nDo extra step and do the check before cond_resched(), so we don't\nresched if have nothing to execute.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c4173e288e69793d03c7d7ce826f9d28afba718a.1623949695.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-18 09:22:02 -0600 io_uring: don't resched with empty task_list"
    },
    {
        "commit": "c6538be9e4883d1371adaff45712b1b2172773dd",
        "message": "We don't need a full copy of tctx->task_list in tctx_task_work(), but\nonly a first one, so just assign node directly.\n\nTaking into account that task_works are run in a context of a task,\nit's very unlikely to first see non-empty tctx->task_list and then\nsplice it empty, can only happen with task_work cancellations that is\nnot-normal slow path anyway. Hence, get rid of the check in the end,\nit's there not for validity but \"performance\" purposes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d076c83fedb8253baf43acb23b8fafd7c5da1714.1623949695.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-18 09:22:02 -0600 io_uring: refactor tctx task_work list splicing"
    },
    {
        "commit": "ebd0df2e63426bbd9ed50966e888c87eac88fc30",
        "message": "tctx_task_work() tries to fetch a next batch of requests, but before it\nwould flush completions from the previous batch that may be sub-optimal.\nE.g. io_req_task_queue() executes a head of the link where all the\nlinked may be enqueued through the same io_req_task_queue(). And there\nare more cases for that.\n\nDo the flushing at the end, so it can cache completions of several waves\nof a single tctx_task_work(), and do the flush at the very end.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3cac83934e4fbce520ff8025c3524398b3ae0270.1623949695.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-18 09:22:02 -0600 io_uring: optimise task_work submit flushing"
    },
    {
        "commit": "3f18407dc6f2db0968daaa36c39a772c2c9f8ea7",
        "message": "Inline __tctx_task_work() into tctx_task_work() in preparation for\nfurther optimisations.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f9c05c4bc9763af7bd8e25ebc3c5f7b6f69148f8.1623949695.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-18 09:22:02 -0600 io_uring: inline __tctx_task_work()"
    },
    {
        "commit": "a3dbdf54da80326fd12bc11ad75ecd699a82374f",
        "message": "Clean up io_get_sequence() and add a comment describing the magic around\nsequence correction.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f55dc409936b8afa4698d24b8677a34d31077ccb.1623949695.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-18 09:22:02 -0600 io_uring: refactor io_get_sequence()"
    },
    {
        "commit": "c854357bc1b965e1e261c612d5be1297dfb3e406",
        "message": "Clean all flags in io_clean_op() in the end in one operation, will save\nus a couple of operation and binary size.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b8efe1f022a037f74e7fe497c69fb554d59bfeaf.1623949695.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-18 09:22:02 -0600 io_uring: clean all flags in io_clean_op() at once"
    },
    {
        "commit": "1dacb4df4ebe61ec2005d7ab82ee38ffa7125ee7",
        "message": "We don't get REQ_F_NEED_CLEANUP for rw unless there is ->free_iovec set,\nso remove the optimisation of NULL checking it inline, kfree() will take\ncare if that would ever be the case.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a233dc655d3d45bd4f69b73d55a61de46d914415.1623949695.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-18 09:22:02 -0600 io_uring: simplify iovec freeing in io_clean_op()"
    },
    {
        "commit": "b8e64b530011162adda0e176150774d22326c50c",
        "message": "Currently, if req->creds is not NULL, then there are creds assigned.\nTrack the invariant with a new flag in req->flags. No need to clear the\nfield at init, and also cleanup can be efficiently moved into\nio_clean_op().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5f8baeb8d3b909487f555542350e2eac97005556.1623949695.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-18 09:22:02 -0600 io_uring: track request creds with a flag"
    },
    {
        "commit": "c10d1f986b4e2a906862148c77a97f186cc08b9e",
        "message": "io-wq now doesn't have anything to do with creds now, so move ->creds\nfrom struct io_wq_work into request (aka struct io_kiocb).\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8520c72ab8b8f4b96db12a228a2ab4c094ae64e1.1623949695.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-18 09:22:02 -0600 io_uring: move creds from io-wq work to io_kiocb"
    },
    {
        "commit": "2a2758f26df519fab011f49d53440382dda8e1a5",
        "message": "struct io_comp_state is always contained in struct io_ring_ctx, don't\npass them into io_submit_flush_completions() separately, it makes the\ninterface cleaner and simplifies it for the compiler.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/44d6ca57003a82484338e95197024dbd65a1b376.1623949695.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-18 09:22:02 -0600 io_uring: refactor io_submit_flush_completions()"
    },
    {
        "commit": "e6ab8991c5d0b0deae0961dc22c0edd1dee328f5",
        "message": "WARNING: CPU: 1 PID: 11749 at fs/io-wq.c:244 io_wqe_wake_worker fs/io-wq.c:244 [inline]\nWARNING: CPU: 1 PID: 11749 at fs/io-wq.c:244 io_wqe_enqueue+0x7f6/0x910 fs/io-wq.c:751\n\nA WARN_ON_ONCE() in io_wqe_wake_worker() can be triggered by a valid\nuserspace setup. Replace it with pr_warn.\n\nReported-by: syzbot+ea2f1484cffe5109dc10@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f7ede342c3342c4c26668f5168e2993e38bbd99c.1623949695.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-18 09:22:02 -0600 io_uring: fix false WARN_ONCE"
    },
    {
        "commit": "fe76421d1da1dcdb3a2cd8428ac40106bff28bc0",
        "message": "io-wq defaults to per-node masks for IO workers. This works fine by\ndefault, but isn't particularly handy for workloads that prefer more\nspecific affinities, for either performance or isolation reasons.\n\nThis adds IORING_REGISTER_IOWQ_AFF that allows the user to pass in a CPU\nmask that is then applied to IO thread workers, and an\nIORING_UNREGISTER_IOWQ_AFF that simply resets the masks back to the\ndefault of per-node.\n\nNote that no care is given to existing IO threads, they will need to go\nthrough a reschedule before the affinity is correct if they are already\nrunning or sleeping.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-17 10:25:50 -0600 io_uring: allow user configurable IO thread CPU affinity"
    },
    {
        "commit": "3d7b7b5285f0a8e73e332f3d7c7b2ca1e46309d7",
        "message": "Fix tabulation to make nice columns\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-16 06:41:48 -0600 io_uring: minor clean up in trace events definition"
    },
    {
        "commit": "236daeae3616b1c62ce1a9f8a348d576ec9e22d9",
        "message": "The req pointer uniquely identify a specific request.\nHaving it in traces can provide valuable insights that is not possible\nto have if the calling process is reusing the same user_data value.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Olivier Langlois <olivier@trillion01.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-16 06:41:46 -0600 io_uring: Add to traces the req pointer when available"
    },
    {
        "commit": "2335f6f5ddf2f4621395fac5fa4b53d075828cc1",
        "message": "In most cases io_commit_cqring() is just an smp_store_release(), and\nit's hot enough, especially for IRQ rw, to want it to save on a function\ncall. Mark it inline and extract a non-inlined slow path doing drain\nand timeout flushing. The inlined part is pretty slim to not cause\nbinary bloating.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7350f8b6b92caa50a48a80be39909f0d83eddd93.1623772051.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:44:34 -0600 io_uring: optimise io_commit_cqring()"
    },
    {
        "commit": "3c19966d3710dbe5a44658c532052f11d797aecb",
        "message": "Place all drain_next logic into io_drain_req(), so it's never executed\nif there was no drained requests before. The only thing we need is to\nset ->drain_active if we see a request with IOSQE_IO_DRAIN, do that in\nio_init_req() where flags are definitely in registers.\n\nAlso, all drain-related code is encapsulated in io_drain_req(), makes it\ncleaner.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/68bf4f7395ddaafbf1a26bd97b57d57d45a9f900.1623772051.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:44:34 -0600 io_uring: shove more drain bits out of hot path"
    },
    {
        "commit": "10c669040e9b3538e1732c8d40729636b17ce9dd",
        "message": "->drain_used is one way, which is not optimal if users use DRAIN but\nvery rarely. However, we can just clear it in io_drain_req() when all\ndrained before requests are gone. Also rename the flag to reflect the\nchange and be more clear about it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7f37a240857546a94df6348507edddacab150460.1623772051.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:44:33 -0600 io_uring: switch !DRAIN fast path when possible"
    },
    {
        "commit": "27f6b318dea2d7ccccc9dca416e59431838c2929",
        "message": "fs/io_uring.c: In function 'io_alloc_page_table':\ninclude/linux/minmax.h:20:28: warning: comparison of distinct pointer\n\ttypes lacks a cast\n\nCast everything to size_t using min_t.\n\nReported-by: Stephen Rothwell <sfr@canb.auug.org.au>\nFixes: 9123c8ffce16 (\"io_uring: add helpers for 2 level table alloc\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/50f420a956bca070a43810d4a805293ed54f39d8.1623759527.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:40:17 -0600 io_uring: fix min types mismatch in table alloc"
    },
    {
        "commit": "dd9ae8a0b2985ead64dfcfa2f9a0ce5efa1480aa",
        "message": "The sqe_ptr argument has been gone since 709b302faddf (io_uring:\nsimplify io_get_sqring, 2020-04-08), made the return value of the\nfunction. Update the comment accordingly.\n\nSigned-off-by: Fam Zheng <fam.zheng@bytedance.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20210604164256.12242-1-fam.zheng@bytedance.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:39:16 -0600 io_uring: Fix comment of io_get_sqe"
    },
    {
        "commit": "441b8a7803bfa11af2355beea9a07720d4b5c03a",
        "message": "Replace drain checks with one-way flag set upon seeing the first\nIOSQE_IO_DRAIN request. There are several places where it cuts cycles\nwell:\n\n1) It's much faster than the fast check with two\nconditions in io_drain_req() including pretty complex\nlist_empty_careful().\n\n2) We can mark io_queue_sqe() inline now, that's a huge win.\n\n3) It replaces timeout and drain checks in io_commit_cqring() with a\nsingle flags test. Also great not touching ->defer_list there without a\nreason so limiting cache bouncing.\n\nIt adds a small amount of overhead to drain path, but it's negligible.\nThe main nuisance is that once it meets any DRAIN request in io_uring\ninstance lifetime it will _always_ go through a slower path, so\ndrain-less and offset-mode timeout less applications are preferable.\nThe overhead in that case would be not big, but it's worth to bear in\nmind.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/98d2fff8c4da5144bb0d08499f591d4768128ea3.1623709150.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:38:40 -0600 io_uring: optimise non-drain path"
    },
    {
        "commit": "76cc33d79175a1b224bf02d3ff6c7be53fc684d5",
        "message": "Rename io_req_defer() into io_drain_req() and refactor it uncoupling it\nfrom io_queue_sqe() error handling and preparing for coming\noptimisations. Also, prioritise non IOSQE_ASYNC path.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4f17dd56e7fbe52d1866f8acd8efe3284d2bebcb.1623709150.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:38:40 -0600 io_uring: refactor io_req_defer()"
    },
    {
        "commit": "0499e582aaff4e4072a760d1f31434acb50c7813",
        "message": "->uring_lock is prevalently used for submission, even though it protects\nmany other things like iopoll, registeration, selected bufs, and more.\nAnd it's placed together with ->cq_wait poked on completion and CQ\nwaiting sides. Move them apart, ->uring_lock goes to the submission\ndata, and cq_wait to completion related chunk. The last one requires\nsome reshuffling so everything needed by io_cqring_ev_posted*() is in\none cacheline.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/dea5e845caee4c98aa0922b46d713154d81f7bd8.1623709150.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:38:40 -0600 io_uring: move uring_lock location"
    },
    {
        "commit": "311997b3fcddc2f169fff844bf6b48dbff0bb816",
        "message": "We use several wait_queue_head's for different purposes, but namings are\nconfusing. First rename ctx->cq_wait into ctx->poll_wait, because this\none is used for polling an io_uring instance. Then rename ctx->wait into\nctx->cq_wait, which is responsible for CQE waiting.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/47b97a097780c86c67b20b6ccc4e077523dce682.1623709150.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:38:40 -0600 io_uring: wait heads renaming"
    },
    {
        "commit": "5ed7a37d21b369d03114dea12a1f16ae2e21baa8",
        "message": "There are no users of ->sq_check_overflow, only ->cq_check_overflow is\nused. Combine it and move out of completion related part of struct\nio_ring_ctx.\n\nA not so obvious benefit of it is fitting all completion side fields\ninto a single cacheline. It was taking 2 lines before with 56B padding,\nand io_cqring_ev_posted*() were still touching both of them.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/25927394964df31d113e3c729416af573afff5f5.1623709150.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:38:40 -0600 io_uring: clean up check_overflow flag"
    },
    {
        "commit": "5e159204d7edd5bd329e8cdb419dbd81d25751e0",
        "message": "submit_state.link is used only to assemble a link and not used for\nactual submission, so clear it before io_queue_sqe() in io_submit_sqe(),\nawhile it's hot and in caches and queueing doesn't spoil it. May also\npotentially help compiler with spilling or to do other optimisations.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1579939426f3ad6b55af3005b1389bbbed7d780d.1623709150.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:38:40 -0600 io_uring: small io_submit_sqe() optimisation"
    },
    {
        "commit": "f18ee4cf0a277a0e3d043755046d5817d4ddd618",
        "message": "io_commit_cqring() might be very hot and we definitely don't want to\ntouch ->timeout_list there, because 1) it's shared with the submission\nside so might lead to cache bouncing and 2) may need to load an extra\ncache line, especially for IRQ completions.\n\nWe're interested in it at the completion side only when there are\noffset-mode timeouts, which are not so popular. Replace\nlist_empty(->timeout_list) hot path check with a new one-way flag, which\nis set when we prepare the first offset-mode timeout.\n\nnote: the flag sits in the same line as briefly used after ->rings\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e4892ec68b71a69f92ffbea4a1499be3ec0d463b.1623709150.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:38:40 -0600 io_uring: optimise completion timeout flushing"
    },
    {
        "commit": "15641e427070f05fad2e9d74d191146d6514d30f",
        "message": "Kill ->cached_sq_dropped and wire DRAIN sequence number correction via\n->cq_extra, which is there exactly for that purpose. User visible\ndropped counter will be populated by incrementing it instead of keeping\na copy, similarly as it was done not so long ago with cq_overflow.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/088aceb2707a534d531e2770267c4498e0507cc1.1623709150.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:38:40 -0600 io_uring: don't cache number of dropped SQEs"
    },
    {
        "commit": "17d3aeb33cdae8c87a8ad97c4358a623a630e19a",
        "message": "The line of io_get_sqe() evaluating @head consists of too many\noperations including READ_ONCE(), it's not convenient for probing.\nRefactor it also improving readability.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/866ad6e4ef4851c7c61f6b0e08dbd0a8d1abce84.1623709150.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:38:39 -0600 io_uring: refactor io_get_sqe()"
    },
    {
        "commit": "7f1129d227ea54526380d0f37eb7b33ab9f200c1",
        "message": "Since moving locked_free_* out of struct io_submit_state\nctx->submit_state is accessed on submission side only, so move it into\nthe submission section. Same goes for rsrc table pointers/nodes/etc.,\nthey must be taken and checked during submission because sync'ed by\nuring_lock, so move them there as well.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8a5899a50afc6ccca63249e716f580b246f3dec6.1623709150.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:38:39 -0600 io_uring: shuffle more fields into SQ ctx section"
    },
    {
        "commit": "b52ecf8cb5b5ccb8069adbdb82a68d3fa0f423db",
        "message": "ctx->flags are heavily used by both, completion and submission sides, so\nmove it out from the ctx fields related to submissions. Instead, place\nit together with ctx->refs, because it's already cacheline-aligned and\nso pads lots of space, and both almost never change. Also, in most\noccasions they are accessed together as refs are taken at submission\ntime and put back during completion.\n\nDo same with ctx->rings, where the pointer itself is never modified\napart from ring init/free.\n\nNote: in percpu mode, struct percpu_ref doesn't modify the struct itself\nbut takes indirection with ref->percpu_count_ptr.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4c48c173e63d35591383ba2b87e8b8e8dfdbd23d.1623709150.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:38:39 -0600 io_uring: move ctx->flags from SQ cacheline"
    },
    {
        "commit": "c7af47cf0fab5bad1fb8b250dfab8efc1f991559",
        "message": "sq_array and sq_sqes are always used together, however they are in\ndifferent cachelines, where the borderline is right before\ncq_overflow_list is rather rarely touched. Move the fields together so\nit loads only one cacheline.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3ef2411a94874da06492506a8897eff679244f49.1623709150.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:38:39 -0600 io_uring: keep SQ pointers in a single cacheline"
    },
    {
        "commit": "fdd1dc316e8959b6730d733fba025a39dac7938f",
        "message": "Static analysis is warning that the sizeof being used is should be\nof *data->tags[i] and not data->tags[i]. Although these are the same\nsize on 64 bit systems it is not a portable assumption to assume\nthis is true for all cases.  Fix this by using a temporary pointer\ntag_slot to make the code a clearer.\n\nAddresses-Coverity: (\"Sizeof not portable\")\nFixes: d878c81610e1 (\"io_uring: hide rsrc tag copy into generic helpers\")\nSigned-off-by: Colin Ian King <colin.king@canonical.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20210615130011.57387-1-colin.king@canonical.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-15 15:37:11 -0600 io_uring: Fix incorrect sizeof operator for copy_from_user call"
    },
    {
        "commit": "aeab9506ef50d23b350d1822c324023c9e1cb783",
        "message": "There are only two calls in source code of io_iter_do_read(), the\nfunction is small and pretty hot though is failed to get inlined.\nMakr it as inline.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/25a26dae7660da73fbc2244b361b397ef43d3caf.1623634182.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:13 -0600 io_uring: inline io_iter_do_read()"
    },
    {
        "commit": "78cc687be9c5420d743346f78bb8af9d59a903f9",
        "message": "Merge io_uring_cancel_sqpoll() and __io_uring_cancel() as it's easier to\nhave a conditional ctx traverse inside than keeping them in sync.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/adfe24d6dad4a3883a40eee54352b8b65ac851bb.1623634181.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:13 -0600 io_uring: unify SQPOLL and user task cancellations"
    },
    {
        "commit": "09899b19155a152f3ff4eb5c203232175d630fbc",
        "message": "tctx in submission part is always synchronised because is executed from\nthe task's context, so we can batch allocate tctx/task references and\nstore them across syscall boundaries. It avoids enough of operations,\nincluding an atomic for getting task ref and a percpu_counter_add()\nfunction call, which still fallback to spinlock for large batching\ncases (around >=32). Should be good for SQPOLL submitting in small\nportions and coming at some moment bpf submissions.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/14b327b973410a3eec1f702ecf650e100513aca9.1623634181.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:13 -0600 io_uring: cache task struct refs"
    },
    {
        "commit": "2d091d62b1106e90f195599c67bf385ddedfc915",
        "message": "We don't really need vmalloc for keeping tags, it's not a hot path and\nis there out of convenience, so replace it with two level tables to not\nlitter kernel virtual memory mappings.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/241a3422747113a8909e7e1030eb585d4a349e0d.1623634181.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:13 -0600 io_uring: don't vmalloc rsrc tags"
    },
    {
        "commit": "9123c8ffce1610323ec9c0874fa0262353f41fc3",
        "message": "Some parts like fixed file table use 2 level tables, factor out helpers\nfor allocating/deallocating them as more users are to come.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1709212359cd82eb416d395f86fc78431ccfc0aa.1623634181.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:13 -0600 io_uring: add helpers for 2 level table alloc"
    },
    {
        "commit": "157d257f99c15c43668a98f804e3e3e6eb956464",
        "message": "io_rsrc_put_work() is executed by workqueue in non-irq context, so no\nneed for irqsave/restore variants of spinlocking.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2a7f77220735f4ad404ac885b4d73bdf42d2f836.1623634181.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:13 -0600 io_uring: remove rsrc put work irq save/restore"
    },
    {
        "commit": "d878c81610e187becff1454f36b63c59ec165566",
        "message": "Make io_rsrc_data_alloc() taking care of rsrc tags loading on\nregistration, so we don't need to repeat it for each new rsrc type.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5609680697bd09735de10561b75edb95283459da.1623634181.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:13 -0600 io_uring: hide rsrc tag copy into generic helpers"
    },
    {
        "commit": "eef51daa72f745b6e771d18f6f37c7e5cd4ccdf1",
        "message": "What at some moment was references to struct file used to control\nlifetimes of task/ctx is now just internal tctx structures/nodes,\nso rename outdated *task_file() routines into something more sensible.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e2fbce42932154c2631ce58ffbffaa232afe18d5.1623634181.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:12 -0600 io_uring: rename function *task_file"
    },
    {
        "commit": "cb3d8972c78ab0cdb55a30d6db927a3e0442b3f9",
        "message": "A simple refactoring of io_iopoll_req_issued(), move in_async inside so\nwe don't pass it around and save on double checking it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1513bfde4f0c835be25ac69a82737ab0668d7665.1623634181.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:12 -0600 io_uring: refactor io_iopoll_req_issued"
    },
    {
        "commit": "976517f162a05f4315b2373fd11585c395506259",
        "message": "There is a complaint against sys_io_uring_enter() blocking if it submits\nstdin reads. The problem is in __io_file_supports_async(), which\nsees that it's a cdev and allows it to be processed inline.\n\nPunt char devices using generic rules of io_file_supports_async(),\nincluding checking for presence of *_iter() versions of rw callbacks.\nApparently, it will affect most of cdevs with some exceptions like\nnull and zero devices.\n\nCc: stable@vger.kernel.org\nReported-by: Birk Hirdman <lonjil@gmail.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d60270856b8a4560a639ef5f76e55eb563633599.1623236455.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:05 -0600 io_uring: fix blocking inline submission"
    },
    {
        "commit": "40dad765c045ab6dbd481cc4f00d04953e77510c",
        "message": "Relax buffer registration restictions, which filters out file backed\nmemory, and allow shmem/memfd as they have normal anonymous pages\nunderneath.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:05 -0600 io_uring: enable shmem/memfd memory registration"
    },
    {
        "commit": "d0acdee296d42e700c16271d9f95085a9c897a53",
        "message": "struct io_submit_state contains struct io_comp_state and so\nlocked_free_*, that renders cachelines around ->locked_free* being\ninvalidated on most non-inline completions, that may terrorise caches if\nsubmissions and completions are done by different tasks.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/290cb5412b76892e8631978ee8ab9db0c6290dd5.1621201931.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:05 -0600 io_uring: don't bounce submit_state cachelines"
    },
    {
        "commit": "d068b5068d43353a352b3ec92865f7045fdb213e",
        "message": "Rename io_get_cqring() into io_get_cqe() for consistency with SQ, and\njust because the old name is not as clear.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a46a53e3f781de372f5632c184e61546b86515ce.1621201931.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:05 -0600 io_uring: rename io_get_cqring"
    },
    {
        "commit": "8f6ed49a4443be35a11807695dbae2680f7ca6fc",
        "message": "There are two copies of cq_overflow, shared with userspace and internal\ncached one. It was needed for DRAIN accounting, but now we have yet\nanother knob to tune the accounting, i.e. cq_extra, and we can throw\naway the internal counter and just increment the one in the shared ring.\n\nIf user modifies it as so never gets the right overflow value ever\nagain, it's its problem, even though before we would have restored it\nback by next overflow.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/8427965f5175dd051febc63804909861109ce859.1621201931.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:05 -0600 io_uring: kill cached_cq_overflow"
    },
    {
        "commit": "ea5ab3b579836d784357ae9cb5bf9d7242a645b9",
        "message": "No need to cache cq_mask, it's exactly cq_entries - 1, so just deduce\nit to not carry it around.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d439efad0503c8398451dae075e68a04362fbc8d.1621201931.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:05 -0600 io_uring: deduce cq_mask from cq_entries"
    },
    {
        "commit": "a566c5562d41b99f11c8224b2a3010e60ad93acf",
        "message": "We have numbers of {sq,cq} entries cached in ctx, don't look up them in\nuser-shared rings as 1) it may fetch additional cacheline 2) user may\nchange it and so it's always error prone.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/745d31bc2da41283ddd0489ef784af5c8d6310e9.1621201931.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:05 -0600 io_uring: remove dependency on ring->sq/cq_entries"
    },
    {
        "commit": "b13a8918d395554ff9a8cee17d03ed45d805df24",
        "message": "ring has two types of resource-related fields: used for request\nsubmission, and field needed for update/registration. Reshuffle them\ninto these two groups for better locality and readability. The second\ngroup is not in the hot path, so it's natural to place them somewhere in\nthe end. Also update an outdated comment.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/05b34795bb4440f4ec4510f08abd5a31830f8ca0.1621201931.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:04 -0600 io_uring: better locality for rsrc fields"
    },
    {
        "commit": "b986af7e2df4f0871367c397ba61a542f37c0ab3",
        "message": "There is a bunch of scattered around ctx fields that are almost never\nused, e.g. only on ring exit, plunge them to the end, better locality,\nbetter aesthetically.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/782ff94b00355923eae757d58b1a47821b5b46d4.1621201931.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:04 -0600 io_uring: shuffle rarely used ctx fields"
    },
    {
        "commit": "93d2bcd2cbfed2c714341f7a7ecd511aaedabd83",
        "message": "The main difference is in req_set_fail_links() renamed into\nreq_set_fail(), which now sets REQ_F_FAIL_LINK/REQ_F_FAIL flag\nunconditional on whether it has been a link or not. It only matters in\nio_disarm_next(), which already handles it well, and all calls to it\nhave a fast path checking REQ_F_LINK/HARDLINK.\n\nIt looks cleaner, and sheds binary size\n   text    data     bss     dec     hex filename\n  84235   12390       8   96633   17979 ./fs/io_uring.o\n  84151   12414       8   96573   1793d ./fs/io_uring.o\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e2224154dd6e53b665ac835d29436b177872fa10.1621201931.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:04 -0600 io_uring: make fail flag not link specific"
    },
    {
        "commit": "3dd0c97a9e011b11ce6bd245bacf58c57f6f7875",
        "message": "We don't match against files on cancellation anymore, so no need to drag\naround files_struct anymore, just pass a flag telling whether only\ninflight or all requests should be killed.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7bfc5409a78f8e2d6b27dec3293ec2d248677348.1621201931.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:04 -0600 io_uring: get rid of files in exit cancel"
    },
    {
        "commit": "acfb381d9d714c657ff540099fa5a6fa98e71f07",
        "message": "Going through submission in __io_sq_thread() and still having a full SQ\nis rather unexpected, so remove a check for SQ fullness and just wake up\nwhoever wait on sqo_sq_wait. Also skip if it doesn't do submission in\nthe first place, likely may to happen for SQPOLL sharing and/or IOPOLL.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e2e91751e87b1a39f8d63ef884aaff578123f61e.1621201931.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:04 -0600 io_uring: simplify waking sqo_sq_wait"
    },
    {
        "commit": "21f2fc080f8654ce60b3e9192ba3b596c6a2ead6",
        "message": "As sqpoll cancel via task_work is killed, remove everything related to\npark_task_work as it's not used anymore.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/310d8b76a2fbbf3e139373500e04ad9af7ee3dbb.1621201931.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:04 -0600 io_uring: remove unused park_task_work"
    },
    {
        "commit": "aaa9f0f48172b190a835792abe63f8859372eeec",
        "message": "If SQPOLL task finds a ring requesting it to continue running, no need\nto set wake flag to rest of the rings as it will be cleared in a moment\nanyway, so hide it in a single sqd->ctx_list loop.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1ee5a696d9fd08645994c58ee147d149a8957d94.1621201931.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:04 -0600 io_uring: improve sq_thread waiting check"
    },
    {
        "commit": "e4b6d902a9e38f424ce118106ea4d1665b7951b5",
        "message": "As sqd->state changes rarely, don't check every event one by one but\nlook them all at once. Add a helper function. Also don't go into event\nwaiting sleeping with STOP flag set.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/645025f95c7eeec97f88ff497785f4f1d6f3966f.1621201931.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-06-14 08:23:04 -0600 io_uring: improve sqpoll event/state handling"
    },
    {
        "commit": "b2568eeb961c1bb79ada9c2b90f65f625054adaf",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Just an API change for the registration changes that went into this\n  release. Better to get it sorted out now than before it's too late\"\n\n* tag 'io_uring-5.13-2021-06-12' of git://git.kernel.dk/linux-block:\n  io_uring: add feature flag for rsrc tags\n  io_uring: change registration/upd/rsrc tagging ABI",
        "kernel_version": "v5.13-rc6",
        "release_date": "2021-06-12 11:53:20 -0700 Merge tag 'io_uring-5.13-2021-06-12' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "9690557e22d63f13534fd167d293ac8ed8b104f9",
        "message": "Add IORING_FEAT_RSRC_TAGS indicating that io_uring supports a bunch of\nnew IORING_REGISTER operations, in particular\nIORING_REGISTER_[FILES[,UPDATE]2,BUFFERS[2,UPDATE]] that support rsrc\ntagging, and also indicating implemented dynamic fixed buffer updates.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9b995d4045b6c6b4ab7510ca124fd25ac2203af7.1623339162.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc6",
        "release_date": "2021-06-10 16:33:51 -0600 io_uring: add feature flag for rsrc tags"
    },
    {
        "commit": "992da01aa932b432ef8dc3885fa76415b5dbe43f",
        "message": "There are ABI moments about recently added rsrc registration/update and\ntagging that might become a nuisance in the future. First,\nIORING_REGISTER_RSRC[_UPD] hide different types of resources under it,\nso breaks fine control over them by restrictions. It works for now, but\nonce those are wanted under restrictions it would require a rework.\n\nIt was also inconvenient trying to fit a new resource not supporting\nall the features (e.g. dynamic update) into the interface, so better\nto return to IORING_REGISTER_* top level dispatching.\n\nSecond, register/update were considered to accept a type of resource,\nhowever that's not a good idea because there might be several ways of\nregistration of a single resource type, e.g. we may want to add\nnon-contig buffers or anything more exquisite as dma mapped memory.\nSo, remove IORING_RSRC_[FILE,BUFFER] out of the ABI, and place them\ninternally for now to limit changes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9b554897a7c17ad6e3becc48dfed2f7af9f423d5.1623339162.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc6",
        "release_date": "2021-06-10 16:33:51 -0600 io_uring: change registration/upd/rsrc tagging ABI"
    },
    {
        "commit": "f13ef10059ccf5f4ed201cd050176df62ec25bb8",
        "message": "sock_error() is known to be racy. The code avoids\nan atomic operation is sk_err is zero, and this field\ncould be changed under us, this is fine.\n\nSysbot reported:\n\nBUG: KCSAN: data-race in sock_alloc_send_pskb / unix_release_sock\n\nwrite to 0xffff888131855630 of 4 bytes by task 9365 on cpu 1:\n unix_release_sock+0x2e9/0x6e0 net/unix/af_unix.c:550\n unix_release+0x2f/0x50 net/unix/af_unix.c:859\n __sock_release net/socket.c:599 [inline]\n sock_close+0x6c/0x150 net/socket.c:1258\n __fput+0x25b/0x4e0 fs/file_table.c:280\n ____fput+0x11/0x20 fs/file_table.c:313\n task_work_run+0xae/0x130 kernel/task_work.c:164\n tracehook_notify_resume include/linux/tracehook.h:189 [inline]\n exit_to_user_mode_loop kernel/entry/common.c:174 [inline]\n exit_to_user_mode_prepare+0x156/0x190 kernel/entry/common.c:208\n __syscall_exit_to_user_mode_work kernel/entry/common.c:290 [inline]\n syscall_exit_to_user_mode+0x20/0x40 kernel/entry/common.c:301\n do_syscall_64+0x56/0x90 arch/x86/entry/common.c:57\n entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nread to 0xffff888131855630 of 4 bytes by task 9385 on cpu 0:\n sock_error include/net/sock.h:2269 [inline]\n sock_alloc_send_pskb+0xe4/0x4e0 net/core/sock.c:2336\n unix_dgram_sendmsg+0x478/0x1610 net/unix/af_unix.c:1671\n unix_seqpacket_sendmsg+0xc2/0x100 net/unix/af_unix.c:2055\n sock_sendmsg_nosec net/socket.c:654 [inline]\n sock_sendmsg net/socket.c:674 [inline]\n ____sys_sendmsg+0x360/0x4d0 net/socket.c:2350\n __sys_sendmsg_sock+0x25/0x30 net/socket.c:2416\n io_sendmsg fs/io_uring.c:4367 [inline]\n io_issue_sqe+0x231a/0x6750 fs/io_uring.c:6135\n __io_queue_sqe+0xe9/0x360 fs/io_uring.c:6414\n __io_req_task_submit fs/io_uring.c:2039 [inline]\n io_async_task_func+0x312/0x590 fs/io_uring.c:5074\n __tctx_task_work fs/io_uring.c:1910 [inline]\n tctx_task_work+0x1d4/0x3d0 fs/io_uring.c:1924\n task_work_run+0xae/0x130 kernel/task_work.c:164\n tracehook_notify_signal include/linux/tracehook.h:212 [inline]\n handle_signal_work kernel/entry/common.c:145 [inline]\n exit_to_user_mode_loop kernel/entry/common.c:171 [inline]\n exit_to_user_mode_prepare+0xf8/0x190 kernel/entry/common.c:208\n __syscall_exit_to_user_mode_work kernel/entry/common.c:290 [inline]\n syscall_exit_to_user_mode+0x20/0x40 kernel/entry/common.c:301\n do_syscall_64+0x56/0x90 arch/x86/entry/common.c:57\n entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nvalue changed: 0x00000000 -> 0x00000068\n\nReported by Kernel Concurrency Sanitizer on:\nCPU: 0 PID: 9385 Comm: syz-executor.3 Not tainted 5.13.0-rc4-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\n\nSigned-off-by: Eric Dumazet <edumazet@google.com>\nReported-by: syzbot <syzkaller@googlegroups.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>",
        "kernel_version": "v5.13-rc7",
        "release_date": "2021-06-10 14:12:54 -0700 net: annotate data race in sock_error()"
    },
    {
        "commit": "06af8679449d4ed282df13191fc52d5ba28ec536",
        "message": "Olivier Langlois has been struggling with coredumps being incompletely written in\nprocesses using io_uring.\n\nOlivier Langlois <olivier@trillion01.com> writes:\n> io_uring is a big user of task_work and any event that io_uring made a\n> task waiting for that occurs during the core dump generation will\n> generate a TIF_NOTIFY_SIGNAL.\n>\n> Here are the detailed steps of the problem:\n> 1. io_uring calls vfs_poll() to install a task to a file wait queue\n>    with io_async_wake() as the wakeup function cb from io_arm_poll_handler()\n> 2. wakeup function ends up calling task_work_add() with TWA_SIGNAL\n> 3. task_work_add() sets the TIF_NOTIFY_SIGNAL bit by calling\n>    set_notify_signal()\n\nThe coredump code deliberately supports being interrupted by SIGKILL,\nand depends upon prepare_signal to filter out all other signals.   Now\nthat signal_pending includes wake ups for TIF_NOTIFY_SIGNAL this hack\nin dump_emitted by the coredump code no longer works.\n\nMake the coredump code more robust by explicitly testing for all of\nthe wakeup conditions the coredump code supports.  This prevents\nnew wakeup conditions from breaking the coredump code, as well\nas fixing the current issue.\n\nThe filesystem code that the coredump code uses already limits\nitself to only aborting on fatal_signal_pending.  So it should\nnot develop surprising wake-up reasons either.\n\nv2: Don't remove the now unnecessary code in prepare_signal.\n\nCc: stable@vger.kernel.org\nFixes: 12db8b690010 (\"entry: Add support for TIF_NOTIFY_SIGNAL\")\nReported-by: Olivier Langlois <olivier@trillion01.com>\nSigned-off-by: \"Eric W. Biederman\" <ebiederm@xmission.com>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
        "kernel_version": "v5.13-rc6",
        "release_date": "2021-06-10 14:02:29 -0700 coredump: Limit what can interrupt coredumps"
    },
    {
        "commit": "ec955023967cf9d8669c0bf62fc13aeea002ef9e",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single one-liner fix for an accounting regression in this\n  release\"\n\n* tag 'io_uring-5.13-2021-06-03' of git://git.kernel.dk/linux-block:\n  io_uring: fix misaccounting fix buf pinned pages",
        "kernel_version": "v5.13-rc5",
        "release_date": "2021-06-03 11:41:00 -0700 Merge tag 'io_uring-5.13-2021-06-03' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "216e5835966a709bb87a4d94a7343dd90ab0bd64",
        "message": "As Andres reports \"... io_sqe_buffer_register() doesn't initialize imu.\nio_buffer_account_pin() does imu->acct_pages++, before calling\nio_account_mem(ctx, imu->acct_pages).\", leading to evevntual -ENOMEM.\n\nInitialise the field.\n\nReported-by: Andres Freund <andres@anarazel.de>\nFixes: 41edf1a5ec967 (\"io_uring: keep table of pointers to ubufs\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/438a6f46739ae5e05d9c75a0c8fa235320ff367c.1622285901.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc5",
        "release_date": "2021-05-29 19:27:21 -0600 io_uring: fix misaccounting fix buf pinned pages"
    },
    {
        "commit": "b3dbbae60993365ab4a7ba3f9f6f6eca722b57c1",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A few minor fixes:\n\n   - Fix an issue with hashed wait removal on exit (Zqiang, Pavel)\n\n   - Fix a recent data race introduced in this series (Marco)\"\n\n* tag 'io_uring-5.13-2021-05-28' of git://git.kernel.dk/linux-block:\n  io_uring: fix data race to avoid potential NULL-deref\n  io-wq: Fix UAF when wakeup wqe in hash waitqueue\n  io_uring/io-wq: close io-wq full-stop gap",
        "kernel_version": "v5.13-rc4",
        "release_date": "2021-05-28 14:35:55 -1000 Merge tag 'io_uring-5.13-2021-05-28' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "b16ef427adf31fb4f6522458d37b3fe21d6d03b8",
        "message": "Commit ba5ef6dc8a82 (\"io_uring: fortify tctx/io_wq cleanup\") introduced\nsetting tctx->io_wq to NULL a bit earlier. This has caused KCSAN to\ndetect a data race between accesses to tctx->io_wq:\n\n  write to 0xffff88811d8df330 of 8 bytes by task 3709 on cpu 1:\n   io_uring_clean_tctx                  fs/io_uring.c:9042 [inline]\n   __io_uring_cancel                    fs/io_uring.c:9136\n   io_uring_files_cancel                include/linux/io_uring.h:16 [inline]\n   do_exit                              kernel/exit.c:781\n   do_group_exit                        kernel/exit.c:923\n   get_signal                           kernel/signal.c:2835\n   arch_do_signal_or_restart            arch/x86/kernel/signal.c:789\n   handle_signal_work                   kernel/entry/common.c:147 [inline]\n   exit_to_user_mode_loop               kernel/entry/common.c:171 [inline]\n   ...\n  read to 0xffff88811d8df330 of 8 bytes by task 6412 on cpu 0:\n   io_uring_try_cancel_iowq             fs/io_uring.c:8911 [inline]\n   io_uring_try_cancel_requests         fs/io_uring.c:8933\n   io_ring_exit_work                    fs/io_uring.c:8736\n   process_one_work                     kernel/workqueue.c:2276\n   ...\n\nWith the config used, KCSAN only reports data races with value changes:\nthis implies that in the case here we also know that tctx->io_wq was\nnon-NULL. Therefore, depending on interleaving, we may end up with:\n\n              [CPU 0]                 |        [CPU 1]\n  io_uring_try_cancel_iowq()          | io_uring_clean_tctx()\n    if (!tctx->io_wq) // false        |   ...\n    ...                               |   tctx->io_wq = NULL\n    io_wq_cancel_cb(tctx->io_wq, ...) |   ...\n      -> NULL-deref                   |\n\nNote: It is likely that thus far we've gotten lucky and the compiler\noptimizes the double-read into a single read into a register -- but this\nis never guaranteed, and can easily change with a different config!\n\nFix the data race by restoring the previous behaviour, where both\nsetting io_wq to NULL and put of the wq are _serialized_ after\nconcurrent io_uring_try_cancel_iowq() via acquisition of the uring_lock\nand removal of the node in io_uring_del_task_file().\n\nFixes: ba5ef6dc8a82 (\"io_uring: fortify tctx/io_wq cleanup\")\nSuggested-by: Pavel Begunkov <asml.silence@gmail.com>\nReported-by: syzbot+bf2b3d0435b9b728946c@syzkaller.appspotmail.com\nSigned-off-by: Marco Elver <elver@google.com>\nCc: Jens Axboe <axboe@kernel.dk>\nLink: https://lore.kernel.org/r/20210527092547.2656514-1-elver@google.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc4",
        "release_date": "2021-05-27 07:44:49 -0600 io_uring: fix data race to avoid potential NULL-deref"
    },
    {
        "commit": "3743c1723bfc62e69dbf022417720eed3f431b29",
        "message": "BUG: KASAN: use-after-free in __wake_up_common+0x637/0x650\nRead of size 8 at addr ffff8880304250d8 by task iou-wrk-28796/28802\n\nCall Trace:\n __dump_stack [inline]\n dump_stack+0x141/0x1d7\n print_address_description.constprop.0.cold+0x5b/0x2c6\n __kasan_report [inline]\n kasan_report.cold+0x7c/0xd8\n __wake_up_common+0x637/0x650\n __wake_up_common_lock+0xd0/0x130\n io_worker_handle_work+0x9dd/0x1790\n io_wqe_worker+0xb2a/0xd40\n ret_from_fork+0x1f/0x30\n\nAllocated by task 28798:\n kzalloc_node [inline]\n io_wq_create+0x3c4/0xdd0\n io_init_wq_offload [inline]\n io_uring_alloc_task_context+0x1bf/0x6b0\n __io_uring_add_task_file+0x29a/0x3c0\n io_uring_add_task_file [inline]\n io_uring_install_fd [inline]\n io_uring_create [inline]\n io_uring_setup+0x209a/0x2bd0\n do_syscall_64+0x3a/0xb0\n entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nFreed by task 28798:\n kfree+0x106/0x2c0\n io_wq_destroy+0x182/0x380\n io_wq_put [inline]\n io_wq_put_and_exit+0x7a/0xa0\n io_uring_clean_tctx [inline]\n __io_uring_cancel+0x428/0x530\n io_uring_files_cancel\n do_exit+0x299/0x2a60\n do_group_exit+0x125/0x310\n get_signal+0x47f/0x2150\n arch_do_signal_or_restart+0x2a8/0x1eb0\n handle_signal_work[inline]\n exit_to_user_mode_loop [inline]\n exit_to_user_mode_prepare+0x171/0x280\n __syscall_exit_to_user_mode_work [inline]\n syscall_exit_to_user_mode+0x19/0x60\n do_syscall_64+0x47/0xb0\n entry_SYSCALL_64_after_hwframe\n\nThere are the following scenarios, hash waitqueue is shared by\nio-wq1 and io-wq2. (note: wqe is worker)\n\nio-wq1:worker2     | locks bit1\nio-wq2:worker1     | waits bit1\nio-wq1:worker3     | waits bit1\n\nio-wq1:worker2     | completes all wqe bit1 work items\nio-wq1:worker2     | drop bit1, exit\n\nio-wq2:worker1     | locks bit1\nio-wq1:worker3     | can not locks bit1, waits bit1 and exit\nio-wq1             | exit and free io-wq1\nio-wq2:worker1     | drops bit1\nio-wq1:worker3     | be waked up, even though wqe is freed\n\nAfter all iou-wrk belonging to io-wq1 have exited, remove wqe\nform hash waitqueue, it is guaranteed that there will be no more\nwqe belonging to io-wq1 in the hash waitqueue.\n\nReported-by: syzbot+6cb11ade52aa17095297@syzkaller.appspotmail.com\nSigned-off-by: Zqiang <qiang.zhang@windriver.com>\nLink: https://lore.kernel.org/r/20210526050826.30500-1-qiang.zhang@windriver.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc4",
        "release_date": "2021-05-26 09:03:56 -0600 io-wq: Fix UAF when wakeup wqe in hash waitqueue"
    },
    {
        "commit": "17a91051fe63b40ec651b80097c9fff5b093fdc5",
        "message": "There is an old problem with io-wq cancellation where requests should be\nkilled and are in io-wq but are not discoverable, e.g. in @next_hashed\nor @linked vars of io_worker_handle_work(). It adds some unreliability\nto individual request canellation, but also may potentially get\n__io_uring_cancel() stuck. For instance:\n\n1) An __io_uring_cancel()'s cancellation round have not found any\n   request but there are some as desribed.\n2) __io_uring_cancel() goes to sleep\n3) Then workers wake up and try to execute those hidden requests\n   that happen to be unbound.\n\nAs we already cancel all requests of io-wq there, set IO_WQ_BIT_EXIT\nin advance, so preventing 3) from executing unbound requests. The\nworkers will initially break looping because of getting a signal as they\nare threads of the dying/exec()'ing user task.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/abfcf8c54cb9e8f7bfbad7e9a0cc5433cc70bdc2.1621781238.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc4",
        "release_date": "2021-05-25 19:39:58 -0600 io_uring/io-wq: close io-wq full-stop gap"
    },
    {
        "commit": "b5f3352e0868611b555e1dcb2e1ffb8e346c519c",
        "message": "blkcg has always rejected to attach if any of the member tasks has shared\nio_context. The rationale was that io_contexts can be shared across\ndifferent cgroups making it impossible to define what the appropriate\ncontrol behavior should be. However, this check causes more problems than it\nsolves:\n\n* The check prevents controller enable and migrations but not CLONE_IO\n  itself, which can lead to surprises as the outcome changes depending on\n  the order of operations.\n\n* Sharing within a cgroup is fine but the check can't distinguish that. This\n  leads to unnecessary conflicts with the recent CLONE_IO usage in io_uring.\n\nio_context sharing doesn't make any difference for rq_qos based controllers\nand the way it's used is safe as long as tasks aren't migrated dynamically\nwhich is the vast majority of use cases. While we can try to make the check\nmore precise to avoid false positives, the added complexity doesn't seem\nworthwhile. Let's just drop blkcg_can_attach().\n\nSigned-off-by: Tejun Heo <tj@kernel.org>\nLink: https://lore.kernel.org/r/YJrTvHbrRDbJjw+S@slm.duckdns.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-05-24 06:47:21 -0600 blkcg: drop CLONE_IO check in blkcg_can_attach()"
    },
    {
        "commit": "b9231dfbcbc0034cf333fee33c190853daee48c0",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"One fix for a regression with poll in this merge window, and another\n  just hardens the io-wq exit path a bit\"\n\n* tag 'io_uring-5.13-2021-05-22' of git://git.kernel.dk/linux-block:\n  io_uring: fortify tctx/io_wq cleanup\n  io_uring: don't modify req->poll for rw",
        "kernel_version": "v5.13-rc3",
        "release_date": "2021-05-22 07:36:36 -1000 Merge tag 'io_uring-5.13-2021-05-22' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "ba5ef6dc8a827a904794210a227cdb94828e8ae7",
        "message": "We don't want anyone poking into tctx->io_wq awhile it's being destroyed\nby io_wq_put_and_exit(), and even though it shouldn't even happen, if\nbuggy would be preferable to get a NULL-deref instead of subtle delayed\nfailure or UAF.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/827b021de17926fd807610b3e53a5a5fa8530856.1621513214.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc3",
        "release_date": "2021-05-20 07:29:11 -0600 io_uring: fortify tctx/io_wq cleanup"
    },
    {
        "commit": "0169d8f33ab7a58675a94c18122dba58d8f6a1b8",
        "message": "This reverts commit 942cb357ae7d9249088e3687ee6a00ed2745a0c7.\n\nThe io_uring PF_IO_WORKER threads no longer have PF_KTHREAD set, so no\nneed to special case them for credential checks.\n\nCc: Casey Schaufler <casey@schaufler-ca.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Casey Schaufler <casey@schaufler-ca.com>",
        "kernel_version": "v5.14-rc1",
        "release_date": "2021-05-18 10:36:48 -0700 Revert \"Smack: Handle io_uring kernel thread privileges\""
    },
    {
        "commit": "7a274727702cc07d27cdebd36d1d5132abeea12f",
        "message": "__io_queue_proc() is used by both poll and apoll, so we should not\naccess req->poll directly but selecting right struct io_poll_iocb\ndepending on use case.\n\nReported-and-tested-by: syzbot+a84b8783366ecb1c65d0@syzkaller.appspotmail.com\nFixes: ea6a693d862d (\"io_uring: disable multishot poll for double poll add cases\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/4a6a1de31142d8e0250fe2dfd4c8923d82a5bbfc.1621251795.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc3",
        "release_date": "2021-05-17 07:28:48 -0600 io_uring: don't modify req->poll for rw"
    },
    {
        "commit": "56015910355992f040f6163dcec96642021d2737",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Just a few minor fixes/changes:\n\n   - Fix issue with double free race for linked timeout completions\n\n   - Fix reference issue with timeouts\n\n   - Remove last few places that make SQPOLL special, since it's just an\n     io thread now.\n\n   - Bump maximum allowed registered buffers, as we don't allocate as\n     much anymore\"\n\n* tag 'io_uring-5.13-2021-05-14' of git://git.kernel.dk/linux-block:\n  io_uring: increase max number of reg buffers\n  io_uring: further remove sqpoll limits on opcodes\n  io_uring: fix ltout double free on completion race\n  io_uring: fix link timeout refs",
        "kernel_version": "v5.13-rc2",
        "release_date": "2021-05-15 08:43:44 -0700 Merge tag 'io_uring-5.13-2021-05-14' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "489809e2e22b3dedc0737163d97eb2b574137b42",
        "message": "Since recent changes instead of storing a large array of struct\nio_mapped_ubuf, we store pointers to them, that is 4 times slimmer and\nwe should not to so worry about restricting max number of registererd\nbuffer slots, increase the limit 4 times.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d3dee1da37f46da416aa96a16bf9e5094e10584d.1620990371.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc2",
        "release_date": "2021-05-14 06:06:34 -0600 io_uring: increase max number of reg buffers"
    },
    {
        "commit": "2d74d0421e5afc1e7be7167ffb7eb8b2cf32343a",
        "message": "There are three types of requests that left disabled for sqpoll, namely\nepoll ctx, statx, and resources update. Since SQPOLL task is now closely\nmimics a userspace thread, remove the restrictions.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/909b52d70c45636d8d7897582474ea5aab5eed34.1620990306.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc2",
        "release_date": "2021-05-14 06:06:23 -0600 io_uring: further remove sqpoll limits on opcodes"
    },
    {
        "commit": "447c19f3b5074409c794b350b10306e1da1ef4ba",
        "message": "Always remove linked timeout on io_link_timeout_fn() from the master\nrequest link list, otherwise we may get use-after-free when first\nio_link_timeout_fn() puts linked timeout in the fail path, and then\nwill be found and put on master's free.\n\nCc: stable@vger.kernel.org # 5.10+\nFixes: 90cd7e424969d (\"io_uring: track link timeout's master explicitly\")\nReported-and-tested-by: syzbot+5a864149dd970b546223@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/69c46bf6ce37fec4fdcd98f0882e18eb07ce693a.1620990121.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc2",
        "release_date": "2021-05-14 06:06:15 -0600 io_uring: fix ltout double free on completion race"
    },
    {
        "commit": "a298232ee6b9a1d5d732aa497ff8be0d45b5bd82",
        "message": "WARNING: CPU: 0 PID: 10242 at lib/refcount.c:28 refcount_warn_saturate+0x15b/0x1a0 lib/refcount.c:28\nRIP: 0010:refcount_warn_saturate+0x15b/0x1a0 lib/refcount.c:28\nCall Trace:\n __refcount_sub_and_test include/linux/refcount.h:283 [inline]\n __refcount_dec_and_test include/linux/refcount.h:315 [inline]\n refcount_dec_and_test include/linux/refcount.h:333 [inline]\n io_put_req fs/io_uring.c:2140 [inline]\n io_queue_linked_timeout fs/io_uring.c:6300 [inline]\n __io_queue_sqe+0xbef/0xec0 fs/io_uring.c:6354\n io_submit_sqe fs/io_uring.c:6534 [inline]\n io_submit_sqes+0x2bbd/0x7c50 fs/io_uring.c:6660\n __do_sys_io_uring_enter fs/io_uring.c:9240 [inline]\n __se_sys_io_uring_enter+0x256/0x1d60 fs/io_uring.c:9182\n\nio_link_timeout_fn() should put only one reference of the linked timeout\nrequest, however in case of racing with the master request's completion\nfirst io_req_complete() puts one and then io_put_req_deferred() is\ncalled.\n\nCc: stable@vger.kernel.org # 5.12+\nFixes: 9ae1f8dd372e0 (\"io_uring: fix inconsistent lock state\")\nReported-by: syzbot+a2910119328ce8e7996f@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ff51018ff29de5ffa76f09273ef48cb24c720368.1620417627.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc2",
        "release_date": "2021-05-08 22:11:49 -0600 io_uring: fix link timeout refs"
    },
    {
        "commit": "28b4afeb59db1e78507a747fb872e3ce42cf6d38",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Mostly fixes for merge window merged code. In detail:\n\n   - Error case memory leak fixes (Colin, Zqiang)\n\n   - Add the tools/io_uring/ to the list of maintained files (Lukas)\n\n   - Set of fixes for the modified buffer registration API (Pavel)\n\n   - Sanitize io thread setup on x86 (Stefan)\n\n   - Ensure we truncate transfer count for registered buffers (Thadeu)\"\n\n* tag 'io_uring-5.13-2021-05-07' of git://git.kernel.dk/linux-block:\n  x86/process: setup io_threads more like normal user space threads\n  MAINTAINERS: add io_uring tool to IO_URING\n  io_uring: truncate lengths larger than MAX_RW_COUNT on provide buffers\n  io_uring: Fix memory leak in io_sqe_buffers_register()\n  io_uring: Fix premature return from loop and memory leak\n  io_uring: fix unchecked error in switch_start()\n  io_uring: allow empty slots for reg buffers\n  io_uring: add more build check for uapi\n  io_uring: dont overlap internal and user req flags\n  io_uring: fix drain with rsrc CQEs",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-05-07 11:29:23 -0700 Merge tag 'io_uring-5.13-2021-05-07' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "cf7b39a0cbf6bf57aa07a008d46cf695add05b4c",
        "message": "We get a bug:\n\nBUG: KASAN: slab-out-of-bounds in iov_iter_revert+0x11c/0x404\nlib/iov_iter.c:1139\nRead of size 8 at addr ffff0000d3fb11f8 by task\n\nCPU: 0 PID: 12582 Comm: syz-executor.2 Not tainted\n5.10.0-00843-g352c8610ccd2 #2\nHardware name: linux,dummy-virt (DT)\nCall trace:\n dump_backtrace+0x0/0x2d0 arch/arm64/kernel/stacktrace.c:132\n show_stack+0x28/0x34 arch/arm64/kernel/stacktrace.c:196\n __dump_stack lib/dump_stack.c:77 [inline]\n dump_stack+0x110/0x164 lib/dump_stack.c:118\n print_address_description+0x78/0x5c8 mm/kasan/report.c:385\n __kasan_report mm/kasan/report.c:545 [inline]\n kasan_report+0x148/0x1e4 mm/kasan/report.c:562\n check_memory_region_inline mm/kasan/generic.c:183 [inline]\n __asan_load8+0xb4/0xbc mm/kasan/generic.c:252\n iov_iter_revert+0x11c/0x404 lib/iov_iter.c:1139\n io_read fs/io_uring.c:3421 [inline]\n io_issue_sqe+0x2344/0x2d64 fs/io_uring.c:5943\n __io_queue_sqe+0x19c/0x520 fs/io_uring.c:6260\n io_queue_sqe+0x2a4/0x590 fs/io_uring.c:6326\n io_submit_sqe fs/io_uring.c:6395 [inline]\n io_submit_sqes+0x4c0/0xa04 fs/io_uring.c:6624\n __do_sys_io_uring_enter fs/io_uring.c:9013 [inline]\n __se_sys_io_uring_enter fs/io_uring.c:8960 [inline]\n __arm64_sys_io_uring_enter+0x190/0x708 fs/io_uring.c:8960\n __invoke_syscall arch/arm64/kernel/syscall.c:36 [inline]\n invoke_syscall arch/arm64/kernel/syscall.c:48 [inline]\n el0_svc_common arch/arm64/kernel/syscall.c:158 [inline]\n do_el0_svc+0x120/0x290 arch/arm64/kernel/syscall.c:227\n el0_svc+0x1c/0x28 arch/arm64/kernel/entry-common.c:367\n el0_sync_handler+0x98/0x170 arch/arm64/kernel/entry-common.c:383\n el0_sync+0x140/0x180 arch/arm64/kernel/entry.S:670\n\nAllocated by task 12570:\n stack_trace_save+0x80/0xb8 kernel/stacktrace.c:121\n kasan_save_stack mm/kasan/common.c:48 [inline]\n kasan_set_track mm/kasan/common.c:56 [inline]\n __kasan_kmalloc+0xdc/0x120 mm/kasan/common.c:461\n kasan_kmalloc+0xc/0x14 mm/kasan/common.c:475\n __kmalloc+0x23c/0x334 mm/slub.c:3970\n kmalloc include/linux/slab.h:557 [inline]\n __io_alloc_async_data+0x68/0x9c fs/io_uring.c:3210\n io_setup_async_rw fs/io_uring.c:3229 [inline]\n io_read fs/io_uring.c:3436 [inline]\n io_issue_sqe+0x2954/0x2d64 fs/io_uring.c:5943\n __io_queue_sqe+0x19c/0x520 fs/io_uring.c:6260\n io_queue_sqe+0x2a4/0x590 fs/io_uring.c:6326\n io_submit_sqe fs/io_uring.c:6395 [inline]\n io_submit_sqes+0x4c0/0xa04 fs/io_uring.c:6624\n __do_sys_io_uring_enter fs/io_uring.c:9013 [inline]\n __se_sys_io_uring_enter fs/io_uring.c:8960 [inline]\n __arm64_sys_io_uring_enter+0x190/0x708 fs/io_uring.c:8960\n __invoke_syscall arch/arm64/kernel/syscall.c:36 [inline]\n invoke_syscall arch/arm64/kernel/syscall.c:48 [inline]\n el0_svc_common arch/arm64/kernel/syscall.c:158 [inline]\n do_el0_svc+0x120/0x290 arch/arm64/kernel/syscall.c:227\n el0_svc+0x1c/0x28 arch/arm64/kernel/entry-common.c:367\n el0_sync_handler+0x98/0x170 arch/arm64/kernel/entry-common.c:383\n el0_sync+0x140/0x180 arch/arm64/kernel/entry.S:670\n\nFreed by task 12570:\n stack_trace_save+0x80/0xb8 kernel/stacktrace.c:121\n kasan_save_stack mm/kasan/common.c:48 [inline]\n kasan_set_track+0x38/0x6c mm/kasan/common.c:56\n kasan_set_free_info+0x20/0x40 mm/kasan/generic.c:355\n __kasan_slab_free+0x124/0x150 mm/kasan/common.c:422\n kasan_slab_free+0x10/0x1c mm/kasan/common.c:431\n slab_free_hook mm/slub.c:1544 [inline]\n slab_free_freelist_hook mm/slub.c:1577 [inline]\n slab_free mm/slub.c:3142 [inline]\n kfree+0x104/0x38c mm/slub.c:4124\n io_dismantle_req fs/io_uring.c:1855 [inline]\n __io_free_req+0x70/0x254 fs/io_uring.c:1867\n io_put_req_find_next fs/io_uring.c:2173 [inline]\n __io_queue_sqe+0x1fc/0x520 fs/io_uring.c:6279\n __io_req_task_submit+0x154/0x21c fs/io_uring.c:2051\n io_req_task_submit+0x2c/0x44 fs/io_uring.c:2063\n task_work_run+0xdc/0x128 kernel/task_work.c:151\n get_signal+0x6f8/0x980 kernel/signal.c:2562\n do_signal+0x108/0x3a4 arch/arm64/kernel/signal.c:658\n do_notify_resume+0xbc/0x25c arch/arm64/kernel/signal.c:722\n work_pending+0xc/0x180\n\nblkdev_read_iter can truncate iov_iter's count since the count + pos may\nexceed the size of the blkdev. This will confuse io_read that we have\nconsume the iovec. And once we do the iov_iter_revert in io_read, we\nwill trigger the slab-out-of-bounds. Fix it by reexpand the count with\nsize has been truncated.\n\nblkdev_write_iter can trigger the problem too.\n\nSigned-off-by: yangerkun <yangerkun@huawei.com>\nAcked-by: Pavel Begunkov <asml.silencec@gmail.com>\nLink: https://lore.kernel.org/r/20210401071807.3328235-1-yangerkun@huawei.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-05-06 09:24:03 -0600 block: reexpand iov_iter after read/write"
    },
    {
        "commit": "50b7b6f29de3e18e9d6c09641256a0296361cfee",
        "message": "As io_threads are fully set up USER threads it's clearer to separate the\ncode path from the KTHREAD logic.\n\nThe only remaining difference to user space threads is that io_threads\nnever return to user space again. Instead they loop within the given\nworker function.\n\nThe fact that they never return to user space means they don't have an\nuser space thread stack. In order to indicate that to tools like gdb we\nreset the stack and instruction pointers to 0.\n\nThis allows gdb attach to user space processes using io-uring, which like\nmeans that they have io_threads, without printing worrying message like\nthis:\n\n  warning: Selected architecture i386:x86-64 is not compatible with reported target architecture i386\n\n  warning: Architecture rejected target-supplied description\n\nThe output will be something like this:\n\n  (gdb) info threads\n    Id   Target Id                  Frame\n  * 1    LWP 4863 \"io_uring-cp-for\" syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38\n    2    LWP 4864 \"iou-mgr-4863\"    0x0000000000000000 in ?? ()\n    3    LWP 4865 \"iou-wrk-4863\"    0x0000000000000000 in ?? ()\n  (gdb) thread 3\n  [Switching to thread 3 (LWP 4865)]\n  #0  0x0000000000000000 in ?? ()\n  (gdb) bt\n  #0  0x0000000000000000 in ?? ()\n  Backtrace stopped: Cannot access memory at address 0x0\n\nFixes: 4727dc20e042 (\"arch: setup PF_IO_WORKER threads like PF_KTHREAD\")\nLink: https://lore.kernel.org/io-uring/044d0bad-6888-a211-e1d3-159a4aeed52d@polymtl.ca/T/#m1bbf5727e3d4e839603f6ec7ed79c7eebfba6267\nSigned-off-by: Stefan Metzmacher <metze@samba.org>\ncc: Linus Torvalds <torvalds@linux-foundation.org>\ncc: Jens Axboe <axboe@kernel.dk>\ncc: Andy Lutomirski <luto@kernel.org>\ncc: linux-kernel@vger.kernel.org\ncc: io-uring@vger.kernel.org\ncc: x86@kernel.org\nLink: https://lore.kernel.org/r/20210505110310.237537-1-metze@samba.org\nReviewed-by: Thomas Gleixner <tglx@linutronix.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-05-05 17:47:41 -0600 x86/process: setup io_threads more like normal user space threads"
    },
    {
        "commit": "a5e7da1494e191c561ecce8829a6c19449585e3d",
        "message": "The files in ./tools/io_uring/ are maintained by the IO_URING maintainers.\nReflect that fact in MAINTAINERS.\n\nSigned-off-by: Lukas Bulwahn <lukas.bulwahn@gmail.com>\nLink: https://lore.kernel.org/r/20210505053728.3868-1-lukas.bulwahn@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-05-05 15:22:28 -0600 MAINTAINERS: add io_uring tool to IO_URING"
    },
    {
        "commit": "d1f82808877bb10d3deee7cf3374a4eb3fb582db",
        "message": "Read and write operations are capped to MAX_RW_COUNT. Some read ops rely on\nthat limit, and that is not guaranteed by the IORING_OP_PROVIDE_BUFFERS.\n\nTruncate those lengths when doing io_add_buffers, so buffer addresses still\nuse the uncapped length.\n\nAlso, take the chance and change struct io_buffer len member to __u32, so\nit matches struct io_provide_buffer len member.\n\nThis fixes CVE-2021-3491, also reported as ZDI-CAN-13546.\n\nFixes: ddf0322db79c (\"io_uring: add IORING_OP_PROVIDE_BUFFERS\")\nReported-by: Billy Jheng Bing-Jhong (@st424204)\nSigned-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-05-05 15:17:35 -0600 io_uring: truncate lengths larger than MAX_RW_COUNT on provide buffers"
    },
    {
        "commit": "63135aa3866db99fd923b716c5ff2e468879624a",
        "message": "Patch series \"Improve IOCB_NOWAIT O_DIRECT reads\", v3.\n\nAn internal workload complained because it was using too much CPU, and\nwhen I took a look, we had a lot of io_uring workers going to town.\n\nFor an async buffered read like workload, I am normally expecting _zero_\noffloads to a worker thread, but this one had tons of them.  I'd drop\ncaches and things would look good again, but then a minute later we'd\nregress back to using workers.  Turns out that every minute something\nwas reading parts of the device, which would add page cache for that\ninode.  I put patches like these in for our kernel, and the problem was\nsolved.\n\nDon't -EAGAIN IOCB_NOWAIT dio reads just because we have page cache\nentries for the given range.  This causes unnecessary work from the\ncallers side, when the IO could have been issued totally fine without\nblocking on writeback when there is none.\n\nThis patch (of 3):\n\nFor O_DIRECT reads/writes, we check if we need to issue a call to\nfilemap_write_and_wait_range() to issue and/or wait for writeback for any\npage in the given range.  The existing mechanism just checks for a page in\nthe range, which is suboptimal for IOCB_NOWAIT as we'll fallback to the\nslow path (and needing retry) if there's just a clean page cache page in\nthe range.\n\nProvide filemap_range_needs_writeback() which tries a little harder to\ncheck if we actually need to issue and/or wait for writeback in the range.\n\nLink: https://lkml.kernel.org/r/20210224164455.1096727-1-axboe@kernel.dk\nLink: https://lkml.kernel.org/r/20210224164455.1096727-2-axboe@kernel.dk\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nReviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Jan Kara <jack@suse.cz>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-30 11:20:36 -0700 mm: provide filemap_range_needs_writeback() helper"
    },
    {
        "commit": "bb6659cc0ad3c2afc3801b708b19c4c67e55ddf2",
        "message": "unreferenced object 0xffff8881123bf0a0 (size 32):\ncomm \"syz-executor557\", pid 8384, jiffies 4294946143 (age 12.360s)\nbacktrace:\n[<ffffffff81469b71>] kmalloc_node include/linux/slab.h:579 [inline]\n[<ffffffff81469b71>] kvmalloc_node+0x61/0xf0 mm/util.c:587\n[<ffffffff815f0b3f>] kvmalloc include/linux/mm.h:795 [inline]\n[<ffffffff815f0b3f>] kvmalloc_array include/linux/mm.h:813 [inline]\n[<ffffffff815f0b3f>] kvcalloc include/linux/mm.h:818 [inline]\n[<ffffffff815f0b3f>] io_rsrc_data_alloc+0x4f/0xc0 fs/io_uring.c:7164\n[<ffffffff815f26d8>] io_sqe_buffers_register+0x98/0x3d0 fs/io_uring.c:8383\n[<ffffffff815f84a7>] __io_uring_register+0xf67/0x18c0 fs/io_uring.c:9986\n[<ffffffff81609222>] __do_sys_io_uring_register fs/io_uring.c:10091 [inline]\n[<ffffffff81609222>] __se_sys_io_uring_register fs/io_uring.c:10071 [inline]\n[<ffffffff81609222>] __x64_sys_io_uring_register+0x112/0x230 fs/io_uring.c:10071\n[<ffffffff842f616a>] do_syscall_64+0x3a/0xb0 arch/x86/entry/common.c:47\n[<ffffffff84400068>] entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nFix data->tags memory leak, through io_rsrc_data_free() to release\ndata memory space.\n\nReported-by: syzbot+0f32d05d8b6cd8d7ea3e@syzkaller.appspotmail.com\nSigned-off-by: Zqiang <qiang.zhang@windriver.com>\nLink: https://lore.kernel.org/r/20210430082515.13886-1-qiang.zhang@windriver.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-30 06:44:22 -0600 io_uring: Fix memory leak in io_sqe_buffers_register()"
    },
    {
        "commit": "cf3770e78421f268dee3c1eef5e8a5d284ec3416",
        "message": "Currently the -EINVAL error return path is leaking memory allocated\nto data. Fix this by not returning immediately but instead setting\nthe error return variable to -EINVAL and breaking out of the loop.\n\nKudos to Pavel Begunkov for suggesting a correct fix.\n\nSigned-off-by: Colin Ian King <colin.king@canonical.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/20210429104602.62676-1-colin.king@canonical.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-29 13:26:19 -0600 io_uring: Fix premature return from loop and memory leak"
    },
    {
        "commit": "47b228ce6f66830768eac145efa7746637969101",
        "message": "io_rsrc_node_switch_start() can fail, don't forget to check returned\nerror code.\n\nReported-by: syzbot+a4715dd4b7c866136f79@syzkaller.appspotmail.com\nFixes: eae071c9b4cef (\"io_uring: prepare fixed rw for dynanic buffers\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c4c06e2f3f0c8e43bd8d0a266c79055bcc6b6e60.1619693112.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-29 13:26:19 -0600 io_uring: fix unchecked error in switch_start()"
    },
    {
        "commit": "6224843d56e0c29c0357e86b02b95801897c2caf",
        "message": "Allow empty reg buffer slots any request using which should fail. This\nallows users to not register all buffers in advance, but do it lazily\nand/or on demand via updates. That is achieved by setting iov_base and\niov_len to zero for registration and/or buffer updates. Empty buffer\ncan't have a non-zero tag.\n\nImplementation details: to not add extra overhead to io_import_fixed(),\ncreate a dummy buffer crafted to fail any request using it, and set it\nto all empty buffer slots.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7e95e4d700082baaf010c648c72ac764c9cc8826.1619611868.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-29 13:26:19 -0600 io_uring: allow empty slots for reg buffers"
    },
    {
        "commit": "b0d658ec88a695861c3fd78ef783c1181f81a6e2",
        "message": "Add a couple of BUILD_BUG_ON() checking some rsrc uapi structs and SQE\nflags.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ff960df4d5026b9fb5bfd80994b9d3667d3926da.1619536280.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-29 13:26:18 -0600 io_uring: add more build check for uapi"
    },
    {
        "commit": "dddca22636c9062f284e755e2a49fb8863db8a82",
        "message": "CQE flags take one byte that we store in req->flags together with other\nREQ_F_* internal flags. CQE flags are copied directly into req and then\nverified that requires some handling on failures, e.g. to make sure that\nthat copy doesn't set some of the internal flags.\n\nMove all internal flags to take bits after the first byte, so we don't\nneed extra handling and make it safer overall.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b8b5b02d1ab9d786fcc7db4a3fe86db6b70b8987.1619536280.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-29 13:26:18 -0600 io_uring: dont overlap internal and user req flags"
    },
    {
        "commit": "2840f710f23a3a867426637393acbdfa1f4f1d59",
        "message": "Resource emitted CQEs are not bound to requests, so fix up counters used\nfor DRAIN/defer logic.\n\nFixes: b60c8dce33895 (\"io_uring: preparation for rsrc tagging\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2b32f5f0a40d5928c3466d028f936e167f0654be.1619536280.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-29 13:26:18 -0600 io_uring: fix drain with rsrc CQEs"
    },
    {
        "commit": "625434dafdd97372d15de21972be4b682709e854",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - Support for multi-shot mode for POLL requests\n\n - More efficient reference counting. This is shamelessly stolen from\n   the mm side. Even though referencing is mostly single/dual user, the\n   128 count was retained to keep the code the same. Maybe this\n   should/could be made generic at some point.\n\n - Removal of the need to have a manager thread for each ring. The\n   manager threads only job was checking and creating new io-threads as\n   needed, instead we handle this from the queue path.\n\n - Allow SQPOLL without CAP_SYS_ADMIN or CAP_SYS_NICE. Since 5.12, this\n   thread is \"just\" a regular application thread, so no need to restrict\n   use of it anymore.\n\n - Cleanup of how internal async poll data lifetime is managed.\n\n - Fix for syzbot reported crash on SQPOLL cancelation.\n\n - Make buffer registration more like file registrations, which includes\n   flexibility in avoiding full set unregistration and re-registration.\n\n - Fix for io-wq affinity setting.\n\n - Be a bit more defensive in task->pf_io_worker setup.\n\n - Various SQPOLL fixes.\n\n - Cleanup of SQPOLL creds handling.\n\n - Improvements to in-flight request tracking.\n\n - File registration cleanups.\n\n - Tons of cleanups and little fixes\n\n* tag 'for-5.13/io_uring-2021-04-27' of git://git.kernel.dk/linux-block: (156 commits)\n  io_uring: maintain drain logic for multishot poll requests\n  io_uring: Check current->io_uring in io_uring_cancel_sqpoll\n  io_uring: fix NULL reg-buffer\n  io_uring: simplify SQPOLL cancellations\n  io_uring: fix work_exit sqpoll cancellations\n  io_uring: Fix uninitialized variable up.resv\n  io_uring: fix invalid error check after malloc\n  io_uring: io_sq_thread() no longer needs to reset current->pf_io_worker\n  kernel: always initialize task->pf_io_worker to NULL\n  io_uring: update sq_thread_idle after ctx deleted\n  io_uring: add full-fledged dynamic buffers support\n  io_uring: implement fixed buffers registration similar to fixed files\n  io_uring: prepare fixed rw for dynanic buffers\n  io_uring: keep table of pointers to ubufs\n  io_uring: add generic rsrc update with tags\n  io_uring: add IORING_REGISTER_RSRC\n  io_uring: enumerate dynamic resources\n  io_uring: add generic path for rsrc update\n  io_uring: preparation for rsrc tagging\n  io_uring: decouple CQE filling from requests\n  ...",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-28 14:56:09 -0700 Merge tag 'for-5.13/io_uring-2021-04-27' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "7b289c38335ec7bebe45ed31137d596c808e23ac",
        "message": "Now that we have multishot poll requests, one SQE can emit multiple\nCQEs. given below example:\n    sqe0(multishot poll)-->sqe1-->sqe2(drain req)\nsqe2 is designed to issue after sqe0 and sqe1 completed, but since sqe0\nis a multishot poll request, sqe2 may be issued after sqe0's event\ntriggered twice before sqe1 completed. This isn't what users leverage\ndrain requests for.\nHere the solution is to wait for multishot poll requests fully\ncompleted.\nTo achieve this, we should reconsider the req_need_defer equation, the\noriginal one is:\n\n    all_sqes(excluding dropped ones) == all_cqes(including dropped ones)\n\nThis means we issue a drain request when all the previous submitted\nSQEs have generated their CQEs.\nNow we should consider multishot requests, we deduct all the multishot\nCQEs except the cancellation one, In this way a multishot poll request\nbehave like a normal request, so:\n    all_sqes == all_cqes - multishot_cqes(except cancellations)\n\nHere we introduce cq_extra for it.\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/1618298439-136286-1-git-send-email-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-27 07:38:58 -0600 io_uring: maintain drain logic for multishot poll requests"
    },
    {
        "commit": "6d042ffb598ed83e7d5623cc961d249def5b9829",
        "message": "syzkaller identified KASAN: null-ptr-deref Write in\nio_uring_cancel_sqpoll.\n\nio_uring_cancel_sqpoll is called by io_sq_thread before calling\nio_uring_alloc_task_context. This leads to current->io_uring being NULL.\nio_uring_cancel_sqpoll should not have to deal with threads where\ncurrent->io_uring is NULL.\n\nIn order to cast a wider safety net, perform input sanitisation directly\nin io_uring_cancel_sqpoll and return for NULL value of current->io_uring.\nThis is safe since if current->io_uring isn't set, then there's no way\nfor the task to have submitted any requests.\n\nReported-by: syzbot+be51ca5a4d97f017cd50@syzkaller.appspotmail.com\nCc: stable@vger.kernel.org\nSigned-off-by: Palash Oswal <hello@oswalpalash.com>\nLink: https://lore.kernel.org/r/20210427125148.21816-1-hello@oswalpalash.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-27 07:37:39 -0600 io_uring: Check current->io_uring in io_uring_cancel_sqpoll"
    },
    {
        "commit": "0b8c0e7c9692cfcfa02c9052d4d53ae67901c400",
        "message": "io_import_fixed() doesn't expect a registered buffer slot to be NULL and\nwould fail stumbling on it. We don't allow it, but if during\n__io_sqe_buffers_update() rsrc removal succeeds but following register\nfails, we'll get such a situation.\n\nDo it atomically and don't remove buffers until we sure that a new one\ncan be set.\n\nFixes: 634d00df5e1cf (\"io_uring: add full-fledged dynamic buffers support\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/830020f9c387acddd51962a3123b5566571b8c6d.1619446608.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-26 09:03:57 -0600 io_uring: fix NULL reg-buffer"
    },
    {
        "commit": "9f59a9d88d3bb2708d08e0e1d03899c469c27190",
        "message": "All sqpoll rings (even sharing sqpoll task) are currently dead bound\nto the task that created them, iow when owner task dies it kills all\nits SQPOLL rings and their inflight requests via task_work infra. It's\nneither the nicist way nor the most convenient as adds extra\nlocking/waiting and dependencies.\n\nLeave it alone and rely on SIGKILL being delivered on its thread group\nexit, so there are only two cases left:\n\n1) thread group is dying, so sqpoll task gets a signal and exit itself\n   cancelling all requests.\n\n2) an sqpoll ring is dying. Because refs_kill() is called the sqpoll not\n   going to submit any new request, and that's what we need. And\n   io_ring_exit_work() will do all the cancellation itself before\n   actually killing ctx, so sqpoll doesn't need to worry about it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3cd7f166b9c326a2c932b70e71a655b03257b366.1619389911.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-26 06:59:25 -0600 io_uring: simplify SQPOLL cancellations"
    },
    {
        "commit": "28090c133869b461c5366195a856d73469ab87d9",
        "message": "After closing an SQPOLL ring, io_ring_exit_work() kicks in and starts\ndoing cancellations via io_uring_try_cancel_requests(). It will go\nthrough io_uring_try_cancel_iowq(), which uses ctx->tctx_list, but as\nSQPOLL task don't have a ctx note, its io-wq won't be reachable and so\nis left not cancelled.\n\nIt will eventually cancelled when one of the tasks dies, but if a thread\ngroup survives for long and changes rings, it will spawn lots of\nunreclaimed resources and live locked works.\n\nCancel SQPOLL task's io-wq separately in io_ring_exit_work().\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a71a7fe345135d684025bb529d5cb1d8d6b46e10.1619389911.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-26 06:59:25 -0600 io_uring: fix work_exit sqpoll cancellations"
    },
    {
        "commit": "615cee49b3ca55f54d527f7a6a7d0fd4fd6fef6b",
        "message": "The variable up.resv is not initialized and is being checking for a\nnon-zero value in the call to _io_register_rsrc_update. Fix this by\nexplicitly setting the variable to 0.\n\nAddresses-Coverity: (\"Uninitialized scalar variable)\"\nFixes: c3bdad027183 (\"io_uring: add generic rsrc update with tags\")\nSigned-off-by: Colin Ian King <colin.king@canonical.com>\nLink: https://lore.kernel.org/r/20210426094735.8320-1-colin.king@canonical.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-26 06:51:09 -0600 io_uring: Fix uninitialized variable up.resv"
    },
    {
        "commit": "a2b4198cab7e3edcb78fce77e0e8aca130435403",
        "message": "Now we allocate io_mapped_ubuf instead of bvec, so we clearly have to\ncheck its address after allocation.\n\nFixes: 41edf1a5ec967 (\"io_uring: keep table of pointers to ubufs\")\nReported-by: kernel test robot <lkp@intel.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d28eb1bc4384284f69dbce35b9f70c115ff6176f.1619392565.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-26 06:50:35 -0600 io_uring: fix invalid error check after malloc"
    },
    {
        "commit": "a2a7cc32a5e8cd983912f25a242820107e5613dc",
        "message": "This is done by create_io_thread() now.\n\nSigned-off-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-25 10:29:05 -0600 io_uring: io_sq_thread() no longer needs to reset current->pf_io_worker"
    },
    {
        "commit": "2b4ae19c6d4842dc24d9e0cbec5c98d2766643d5",
        "message": "we shall update sq_thread_idle anytime we do ctx deletion from ctx_list\n\nFixes:734551df6f9b (\"io_uring: fix shared sqpoll cancellation hangs\")\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/1619256380-236460-1-git-send-email-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-25 10:14:25 -0600 io_uring: update sq_thread_idle after ctx deleted"
    },
    {
        "commit": "634d00df5e1cfc4a707b629a814bd607f726bd52",
        "message": "Hook buffers into all rsrc infrastructure, including tagging and\nupdates.\n\nSuggested-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/119ed51d68a491dae87eb55fb467a47870c86aad.1619356238.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-25 10:14:04 -0600 io_uring: add full-fledged dynamic buffers support"
    },
    {
        "commit": "bd54b6fe3316ec1d469513b888ced31eec20032a",
        "message": "Apply fixed_rsrc functionality for fixed buffers support.\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\n[rebase, remove multi-level tables, fix unregister on exit]\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/17035f4f75319dc92962fce4fc04bc0afb5a68dc.1619356238.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-25 10:14:04 -0600 io_uring: implement fixed buffers registration similar to fixed files"
    },
    {
        "commit": "eae071c9b4cefbcc3f985c5abf9a6e32c1608ca9",
        "message": "With dynamic buffer updates, registered buffers in the table may change\nat any moment. First of all we want to prevent future races between\nupdating and importing (i.e. io_import_fixed()), where the latter one\nmay happen without uring_lock held, e.g. from io-wq.\n\nSave the first loaded io_mapped_ubuf buffer and reuse.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/21a2302d07766ae956640b6f753292c45200fe8f.1619356238.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-25 10:14:04 -0600 io_uring: prepare fixed rw for dynanic buffers"
    },
    {
        "commit": "41edf1a5ec967bf4bddedb83c48e02dfea8315b4",
        "message": "Instead of keeping a table of ubufs convert them into pointers to ubuf,\nso we can atomically read one pointer and be sure that the content of\nubuf won't change.\n\nBecause it was already dynamically allocating imu->bvec, throw both\nimu and bvec into a single structure so they can be allocated together.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b96efa4c5febadeccf41d0e849ac099f4c83b0d3.1619356238.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-25 10:14:04 -0600 io_uring: keep table of pointers to ubufs"
    },
    {
        "commit": "c3bdad0271834214be01c1d687c262bf80da6eb0",
        "message": "Add IORING_REGISTER_RSRC_UPDATE, which also supports passing in rsrc\ntags. Implement it for registered files.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d4dc66df204212f64835ffca2c4eb5e8363f2f05.1619356238.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-25 10:14:04 -0600 io_uring: add generic rsrc update with tags"
    },
    {
        "commit": "792e35824be9af9fb4dac956229fb97bda04e25e",
        "message": "Add a new io_uring_register() opcode for rsrc registeration. Instead of\naccepting a pointer to resources, fds or iovecs, it @arg is now pointing\nto a struct io_uring_rsrc_register, and the second argument tells how\nlarge that struct is to make it easily extendible by adding new fields.\n\nAll that is done mainly to be able to pass in a pointer with tags. Pass\nit in and enable CQE posting for file resources. Doesn't support setting\ntags on update yet.\n\nA design choice made here is to not post CQEs on rsrc de-registration,\nbut only when we updated-removed it by rsrc dynamic update.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c498aaec32a4bb277b2406b9069662c02cdda98c.1619356238.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-25 10:14:04 -0600 io_uring: add IORING_REGISTER_RSRC"
    },
    {
        "commit": "fdecb66281e165927059419c3b1de09ffe4f8369",
        "message": "As resources are getting more support and common parts, it'll be more\nconvenient to index resources and use it for indexing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f0be63e9310212d5601d36277c2946ff7a040485.1619356238.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-25 10:14:04 -0600 io_uring: enumerate dynamic resources"
    },
    {
        "commit": "98f0b3b4f1d51911492b9d6eda4add0ec562179b",
        "message": "Extract some common parts for rsrc update, will be used reg buffers\nsupport dynamic (i.e. quiesce-lee) managing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b49c3ff6b9ff0e530295767604fe4de64d349e04.1619356238.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-25 10:14:04 -0600 io_uring: add generic path for rsrc update"
    },
    {
        "commit": "b60c8dce33895f79cbb54700fbeffc7db8aee3f7",
        "message": "We need a way to notify userspace when a lazily removed resource\nactually died out. This will be done by associating a tag, which is u64\nexactly like req->user_data, with each rsrc (e.g. buffer of file). A CQE\nwill be posted once a resource is actually put down.\n\nTag 0 is a special value set by default, for whcih it don't generate an\nCQE, so providing the old behaviour.\n\nDon't expose it to the userspace yet, but prepare internally, allocate\nbuffers, add all posting hooks, etc.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2e6beec5eabe7216bb61fb93cdf5aaf65812a9b0.1619356238.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-25 10:14:04 -0600 io_uring: preparation for rsrc tagging"
    },
    {
        "commit": "d4d19c19d6ae93f99a57c50ccf6d084213e964bd",
        "message": "Make __io_cqring_fill_event() agnostic of struct io_kiocb, pass all the\ndata needed directly into it. Will be used to post rsrc removal\ncompletions, which don't have an associated request.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/c9b8da9e42772db2033547dfebe479dc972a0f2c.1619356238.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-25 10:14:04 -0600 io_uring: decouple CQE filling from requests"
    },
    {
        "commit": "44b31f2fa2c4b6479a578e74e4ed6bf7ad243955",
        "message": "Add io_rsrc_data_free() helper for destroying rsrc_data, easier for\nsearch and the function will get more stuff to destroy shortly.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/562d1d53b5ff184f15b8949a63d76ef19c4ba9ec.1619356238.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-25 10:14:04 -0600 io_uring: return back rsrc data free helper"
    },
    {
        "commit": "fff4db76be297bd4124a503948435a3917d7a702",
        "message": "A preparation patch moving __io_sqe_files_unregister() definition closer\nto other \"files\" functions without any modification.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/95caf17fe837e67bd1f878395f07049062a010d4.1619356238.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-25 10:14:04 -0600 io_uring: move __io_sqe_files_unregister"
    },
    {
        "commit": "724cb4f9ec905173f32c5bd08fec26abaecc6a1d",
        "message": "do this to avoid race below:\n\n         userspace                         kernel\n\n                               |  check sqring and iopoll_list\nsubmit sqe                     |\ncheck IORING_SQ_NEED_WAKEUP    |\n(which is not set)    |        |\n                               |  set IORING_SQ_NEED_WAKEUP\nwait cqe                       |  schedule(never wakeup again)\n\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nLink: https://lore.kernel.org/r/1619018351-75883-1-git-send-email-haoxu@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-23 08:26:41 -0600 io_uring: check sqring and iopoll_list before shedule"
    },
    {
        "commit": "f2a48dd09b8e933f59570692e1382b81d4fddc49",
        "message": "Just a bit of code tossing in io_sq_offload_create(), so it looks a bit\nbetter. No functional changes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/939776f90de8d2cdd0414e1baa29c8ec0926b561.1618916549.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-20 12:55:28 -0600 io_uring: refactor io_sq_offload_create()"
    },
    {
        "commit": "07db298a1c96bdba2102d60ad51fcecb961177c9",
        "message": "Put sq_creds as a part of io_ring_ctx_free(), it's easy to miss doing it\nin io_sq_thread_finish(), especially considering past mistakes related\nto ring creation failures.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3becb1866467a1de82a97345a0a90d7fb8ff875e.1618916549.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-20 12:55:28 -0600 io_uring: safer sq_creds putting"
    },
    {
        "commit": "3a0a690235923b838390500fd46edc23bed092e0",
        "message": "REQ_F_INFLIGHT deaccounting doesn't do any spinlocking or resource\nfreeing anymore, so it's safe to move it into the normal cleanup flow,\ni.e. into io_clean_op(), so making it cleaner.\n\nAlso move io_req_needs_clean() to be first in io_dismantle_req() so it\ndoesn't reload req->flags.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/90653a3a5de4107e3a00536fa4c2ea5f2c38a4ac.1618916549.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-20 12:55:28 -0600 io_uring: move inflight un-tracking into cleanup"
    },
    {
        "commit": "734551df6f9bedfbefcd113ede665945e9de0b99",
        "message": "[  736.982891] INFO: task iou-sqp-4294:4295 blocked for more than 122 seconds.\n[  736.982897] Call Trace:\n[  736.982901]  schedule+0x68/0xe0\n[  736.982903]  io_uring_cancel_sqpoll+0xdb/0x110\n[  736.982908]  io_sqpoll_cancel_cb+0x24/0x30\n[  736.982911]  io_run_task_work_head+0x28/0x50\n[  736.982913]  io_sq_thread+0x4e3/0x720\n\nWe call io_uring_cancel_sqpoll() one by one for each ctx either in\nsq_thread() itself or via task works, and it's intended to cancel all\nrequests of a specified context. However the function uses per-task\ncounters to track the number of inflight requests, so it counts more\nrequests than available via currect io_uring ctx and goes to sleep for\nthem to appear (e.g. from IRQ), that will never happen.\n\nCancel a bit more than before, i.e. all ctxs that share sqpoll\nand continue to use shared counters. Don't forget that we should not\nremove ctx from the list before running that task_work sqpoll-cancel,\notherwise the function wouldn't be able to find the context and will\nhang.\n\nReported-by: Joakim Hassila <joj@mac.com>\nReported-by: Jens Axboe <axboe@kernel.dk>\nFixes: 37d1e2e3642e2 (\"io_uring: move SQPOLL thread io-wq forked worker\")\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1bded7e6c6b32e0bae25fce36be2868e46b116a0.1618752958.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-19 11:34:32 -0600 io_uring: fix shared sqpoll cancellation hangs"
    },
    {
        "commit": "3b763ba1c77da5806e4fdc5684285814fe970c98",
        "message": "SQPOLL task won't submit requests for a context that is currently dying,\nso no need to remove ctx from sqd_list prior the main loop of\nio_ring_exit_work(). Kill it, will be removed by io_sq_thread_finish()\nand only brings confusion and lockups.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f220c2b786ba0f9499bebc9f3cd9714d29efb6a5.1618752958.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-19 11:34:32 -0600 io_uring: remove extra sqpoll submission halting"
    },
    {
        "commit": "75c4021aacbd9b5cc13b173d32b49007fd8ccada",
        "message": "Move restriction checks of __io_uring_register() before quiesce, saves\nfrom waiting for requests in fail case and simplifies the code a bit.\nAlso add array_index_nospec() for safety\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/88d7913c9280ee848fdb7b584eea37a465391cee.1618488258.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-17 19:20:08 -0600 io_uring: check register restriction afore quiesce"
    },
    {
        "commit": "38134ada0ceea3e848fe993263c0ff6207fd46e7",
        "message": "Colin reported before possible overflow and sign extension problems in\nio_provide_buffers_prep(). As Linus pointed out previous attempt did nothing\nuseful, see d81269fecb8ce (\"io_uring: fix provide_buffers sign extension\").\n\nDo that with help of check_<op>_overflow helpers. And fix struct\nio_provide_buf::len type, as it doesn't make much sense to keep it\nsigned.\n\nReported-by: Colin Ian King <colin.king@canonical.com>\nFixes: efe68c1ca8f49 (\"io_uring: validate the full range of provided buffers for access\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/46538827e70fce5f6cdb50897cff4cacc490f380.1618488258.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-17 19:20:07 -0600 io_uring: fix overflows checks in provide buffers"
    },
    {
        "commit": "c82d5bc703825a47af5c600e82e1e0c1db49e036",
        "message": "Don't fail submission attempts if there are CQEs in the overflow\nbacklog, but give away the decision making to the userspace. It\nmight be very inconvenient to the userspace, especially if\nsubmission and completion are done by different threads.\n\nWe can remove it because of recent changes, where requests\nare now not locked by the backlog, backlog entries are allocated\nseparately, so they take less space and cgroup accounted.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-17 19:19:41 -0600 io_uring: don't fail submit with overflow backlog"
    },
    {
        "commit": "9cdbf6467424045617cd6e79dcaad06bb8efa31c",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Fix for a potential hang at exit with SQPOLL from Pavel\"\n\n* tag 'io_uring-5.12-2021-04-16' of git://git.kernel.dk/linux-block:\n  io_uring: fix early sqd_list removal sqpoll hangs",
        "kernel_version": "v5.12-rc8",
        "release_date": "2021-04-16 16:18:53 -0700 Merge tag 'io_uring-5.12-2021-04-16' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "a7be7c23cfdd2cb57609fd2d607923a9cb2a305d",
        "message": "A hand-edit while applying this patch on top of a new base resulted in\na reverted check for re-issue, resulting in spurious -EAGAIN errors.\n\nFixes: 8c130827f417 (\"io_uring: don't alter iopoll reissue fail ret code\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-16 09:47:08 -0600 io_uring: fix merge error for async resubmit"
    },
    {
        "commit": "75652a30ff67539999148859da071ede862090ca",
        "message": "We manage these separately right now, just tie it to the request lifetime\nand make it be part of the usual REQ_F_NEED_CLEANUP logic.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-16 09:47:02 -0600 io_uring: tie req->apoll to request lifetime"
    },
    {
        "commit": "4e3d9ff905cd3e6fc80a1f54b89c3aca67bc72be",
        "message": "We have this in two spots right now, which is a bit fragile. In\npreparation for moving REQ_F_POLLED cleanup into the same spot, move\nthe check into a separate helper so we only have it once.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-16 09:45:47 -0600 io_uring: put flag checking for needing req cleanup in one spot"
    },
    {
        "commit": "ea6a693d862d4f0edd748a1fa3fc6faf2c39afb2",
        "message": "The re-add handling isn't correct for the multi wait case, so let's\njust disable it for now explicitly until we can get that sorted out. This\njust turns it into a one-shot request. Since we pass back whether or not\na poll request terminates in multishot mode on completion, this should\nnot break properly behaving applications that check for IORING_CQE_F_MORE\non completion.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-15 20:17:11 -0600 io_uring: disable multishot poll for double poll add cases"
    },
    {
        "commit": "c7d95613c7d6e003969722a290397b8271bdad17",
        "message": "[  245.463317] INFO: task iou-sqp-1374:1377 blocked for more than 122 seconds.\n[  245.463334] task:iou-sqp-1374    state:D flags:0x00004000\n[  245.463345] Call Trace:\n[  245.463352]  __schedule+0x36b/0x950\n[  245.463376]  schedule+0x68/0xe0\n[  245.463385]  __io_uring_cancel+0xfb/0x1a0\n[  245.463407]  do_exit+0xc0/0xb40\n[  245.463423]  io_sq_thread+0x49b/0x710\n[  245.463445]  ret_from_fork+0x22/0x30\n\nIt happens when sqpoll forgot to run park_task_work and goes to exit,\nthen exiting user may remove ctx from sqd_list, and so corresponding\nio_sq_thread() -> io_uring_cancel_sqpoll() won't be executed. Hopefully\nit just stucks in do_exit() in this case.\n\nFixes: dbe1bdbb39db (\"io_uring: handle signals for IO threads like a normal thread\")\nReported-by: Joakim Hassila <joj@mac.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc8",
        "release_date": "2021-04-14 13:07:27 -0600 io_uring: fix early sqd_list removal sqpoll hangs"
    },
    {
        "commit": "c5de00366e3e675f9e321983d9bd357c1fbea0e9",
        "message": "Having poll update function as a part of IORING_OP_POLL_ADD is not\ngreat, we have to do hack around struct layouts and add some overhead in\nthe way of more popular POLL_ADD. Even more serious drawback is that\nPOLL_ADD requires file and always grabs it, and so poll update, which\ndoesn't need it.\n\nIncorporate poll update into IORING_OP_POLL_REMOVE instead of\nIORING_OP_POLL_ADD. It also more consistent with timeout remove/update.\n\nFixes: b69de288e913 (\"io_uring: allow events and user_data update of running poll requests\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-14 10:43:49 -0600 io_uring: move poll update into remove not add"
    },
    {
        "commit": "9096af3e9c8734a34703bd9fb5ab14292296f911",
        "message": "Isolate poll mask SQE parsing and preparations into a new function,\nwhich will be reused shortly.\n\nFixes: b69de288e913 (\"io_uring: allow events and user_data update of running poll requests\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-14 10:43:47 -0600 io_uring: add helper for parsing poll events"
    },
    {
        "commit": "9ba5fac8cf3b607652397f863dc229bbc8c3cbc1",
        "message": "Don't allow REQ_OP_POLL_REMOVE to kill apoll requests, users should not\nknow about it. Also, remove weird -EACCESS in io_poll_update(), it\nshouldn't know anything about apoll, and have to work even if happened\nto have a poll and an async poll'ed request with same user_data.\n\nFixes: b69de288e913 (\"io_uring: allow events and user_data update of running poll requests\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-14 10:43:42 -0600 io_uring: fix POLL_REMOVE removing apoll"
    },
    {
        "commit": "7f00651aebc9af600be1d9df2a775eeeaee6bebb",
        "message": "Don't reinit io_ring_exit_work()'s exit work/completions on each\niteration, that's wasteful. Also add list_rotate_left(), so if we failed\nto complete the task job, we don't try it again and again but defer it\nuntil others are processed.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-14 10:42:31 -0600 io_uring: refactor io_ring_exit_work()"
    },
    {
        "commit": "f39c8a5b1130fe17db9c66d08aa473d9587543a9",
        "message": "io_iopoll_getevents() is of no use to us anymore, io_iopoll_check()\nhandles all the cases.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7e50b8917390f38bee4f822c6f4a6a98a27be037.1618278933.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-13 09:37:55 -0600 io_uring: inline io_iopoll_getevents()"
    },
    {
        "commit": "e9979b36a467dcdb2073ec8391a2c167971bee46",
        "message": "The only way to get out of io_iopoll_getevents() and continue iterating\nis to have empty iopoll_list, otherwise the main loop would just exit.\nSo, instead of the unlock on 8th time heuristic, do that based on\niopoll_list.\n\nAlso, as no one can add new requests to iopoll_list while\nio_iopoll_check() hold uring_lock, it's useless to spin with the list\nempty, return in that case.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5b8ebe84f5fff7ffa1f708952dfef7fc78b668e2.1618278933.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-13 09:37:55 -0600 io_uring: skip futile iopoll iterations"
    },
    {
        "commit": "cce4b8b0ce1f9fdf67f4f73ed12a2da2a085d5e3",
        "message": "As CQE overflows are now untied from requests and so don't hold any\nref, we don't need to handle exiting/exec'ing cases there anymore.\nMoreover, it's much nicer in regards to userspace to save overflowed\nCQEs whenever possible, so remove failing on in_idle.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d873b7dab75c7f3039ead9628a745bea01f2cfd2.1618278933.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-13 09:37:55 -0600 io_uring: don't fail overflow on in_idle"
    },
    {
        "commit": "e31001a3abb81a2dba976b842b8ab65d123bca2a",
        "message": "Move some parts of io_poll_remove_waitqs() that are opcode independent.\nLooks better and stresses that both do __io_poll_remove_one().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bbc717f82117cc335c89cbe67ec8d72608178732.1618278933.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-13 09:37:55 -0600 io_uring: clean up io_poll_remove_waitqs()"
    },
    {
        "commit": "fd9c7bc542dae7cca3b02c77f7863823d54ddee0",
        "message": "Don't save return values of hrtimer_try_to_cancel() in a variable, but\nuse right away. It's in general safer to not have an intermediate\nvariable, which may be reused and passed out wrongly, but it be\ncontracted out. Also clean io_timeout_extract().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d2566ef7ce632e6882dc13e022a26249b3fd30b5.1618278933.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-13 09:37:55 -0600 io_uring: refactor hrtimer_try_to_cancel uses"
    },
    {
        "commit": "8c855885b8b35af24f45cdd288a9b6ba6274a8ac",
        "message": "Add one more sparse locking annotation for readability in\nio_kill_timeout().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bdbb22026024eac29203c1aa0045c4954a2488d1.1618278933.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-13 09:37:54 -0600 io_uring: add timeout completion_lock annotation"
    },
    {
        "commit": "9d8058926be7008c1dd49a4e5fb33044f17873c1",
        "message": "struct io_poll_iocb became pretty nasty combining also update fields.\nSplit them, so we would have more clarity to it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b2f74d64ffebb57a648f791681af086c7211e3a4.1618278933.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-13 09:37:54 -0600 io_uring: split poll and poll update structures"
    },
    {
        "commit": "66d2d00d0ac44f98499dc7ec61e2289eb8b138e7",
        "message": "Both IORING_POLL_UPDATE_EVENTS and IORING_POLL_UPDATE_USER_DATA need\nold_user_data to find/cancel a poll request, but it's set only for the\nfirst one.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/ab08fd35b7652e977f9a475f01741b04102297f1.1618278933.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-13 09:37:54 -0600 io_uring: fix uninit old data for poll event upd"
    },
    {
        "commit": "084804002e512427bfe52b448cb7cac0d4209b64",
        "message": "If io_sqe_files_unregister() faults on io_rsrc_ref_quiesce(), it will\nfail to do unregister leaving files referenced. And that may well happen\nbecause of a strayed signal or just because it does allocations inside.\n\nIn io_ring_ctx_free() do an unsafe version of unregister, as it's\nguaranteed to not have requests by that point and so quiesce is useless.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e696e9eade571b51997d0dc1d01f144c6d685c05.1618278933.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-13 09:37:54 -0600 io_uring: fix leaking reg files on exit"
    },
    {
        "commit": "f70865db5ff35f5ed0c7e9ef63e7cca3d4947f04",
        "message": "Revert of revert of \"io_uring: wait potential ->release() on resurrect\",\nwhich adds a helper for resurrect not racing completion reinit, as was\nremoved because of a strange bug with no clear root or link to the\npatch.\n\nWas improved, instead of rcu_synchronize(), just wait_for_completion()\nbecause we're at 0 refs and it will happen very shortly. Specifically\nuse non-interruptible version to ignore all pending signals that may\nhave ended prior interruptible wait.\n\nThis reverts commit cb5e1b81304e089ee3ca948db4d29f71902eb575.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7a080c20f686d026efade810b116b72f88abaff9.1618101759.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-12 09:33:10 -0600 io_uring: return back safer resurrect"
    },
    {
        "commit": "e4335ed33eb54ba00c58557753dc84c0ee762ef1",
        "message": "req_set_fail_links() condition checking is bulky. Even though it's\nalways in a slow path, it's inlined and generates lots of extra code,\nsimplify it be moving HARDLINK checking into helpers killing linked\nrequests.\n\n          text    data     bss     dec     hex filename\nbefore:  79318   12330       8   91656   16608 ./fs/io_uring.o\nafter:   79126   12330       8   91464   16548 ./fs/io_uring.o\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/96a9387db658a9d5a44ecbfd57c2a62cb888c9b6.1618101759.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-12 09:33:07 -0600 io_uring: improve hardlink code generation"
    },
    {
        "commit": "88885f66e8c66311923c16caf1ccb6415ebfef72",
        "message": "Set IO_SQ_THREAD_SHOULD_STOP before taking sqd lock, so the sqpoll task\nsees earlier. Not a problem, it will stop eventually. Also check\ninvariant that it's stopped only once.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/653b24ee93843a50ff65a45847d9138f5adb76d7.1618101759.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-12 09:33:04 -0600 io_uring: improve sqo stop"
    },
    {
        "commit": "aeca241b0bdd831ad5706605f5e09b44fe940220",
        "message": "We don't need to store file tables in rsrc nodes, for now it's easier to\nhandle tables not generically, so move file tables into the context. A\nnice side effect is having one less pointer dereference for request with\nfixed file initialisation.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/de9fc4cd3545f24c26c03be4556f58ba3d18b9c3.1618101759.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-12 09:33:01 -0600 io_uring: split file table from rsrc nodes"
    },
    {
        "commit": "87094465d01a248cd888b81da0e6bc10324d4dc0",
        "message": "In preparation for more changes do a little cleanup of\nio_sqe_buffers_register(). Move all args/invariant checking into it from\nio_buffers_map_alloc(), because it's confusing. And add a bit more\ncleaning for the loop.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/93292cb9708c8455e5070cc855861d94e11ca042.1618101759.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-12 09:32:58 -0600 io_uring: cleanup buffer register"
    },
    {
        "commit": "7f61a1e9ef511660d66ea926b5899559fe94b1d0",
        "message": "Add a helper for unmapping registered buffers, better than double\nindexing and will be reused in the future.\n\nSuggested-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/66cbc6ea863be865bac7b7080ed6a3d5c542b71f.1618101759.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-12 09:32:55 -0600 io_uring: add buffer unmap helper"
    },
    {
        "commit": "3e9424989b59fbab5b46d1db29b271cd29643ab4",
        "message": "We don't take many references of struct io_rsrc_data, only one per each\nio_rsrc_node, so using percpu refs is overkill. Use atomic ref instead,\nwhich is much simpler.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1551d90f7c9b183cf2f0d7b5e5b923430acb03fa.1618101759.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-12 09:32:47 -0600 io_uring: simplify io_rsrc_data refcounting"
    },
    {
        "commit": "a1ff1e3f0e1cb8e314220e7af8eb3155da343bf9",
        "message": "Randy reports the following error on CONFIG_BLOCK not being set:\n\n../fs/io_uring.c: In function \u2018kiocb_done\u2019:\n../fs/io_uring.c:2766:7: error: implicit declaration of function \u2018io_resubmit_prep\u2019; did you mean \u2018io_put_req\u2019? [-Werror=implicit-function-declaration]\n   if (io_resubmit_prep(req)) {\n\nProvide a dummy stub for io_resubmit_prep() like we do for\nio_rw_should_reissue(), which also helps remove an ifdef sequence from\nio_complete_rw_iopoll() as well.\n\nFixes: 8c130827f417 (\"io_uring: don't alter iopoll reissue fail ret code\")\nReported-by: Randy Dunlap <rdunlap@infradead.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-12 06:40:02 -0600 io_uring: provide io_resubmit_prep() stub for !CONFIG_BLOCK"
    },
    {
        "commit": "8d13326e56c1a2b4e3af89843e1376b72a2ae6b7",
        "message": "There are three cases where we much care about performance of\nio_cqring_fill_event() -- flushing inline completions, iopoll and\nio_req_complete_post(). Inline a hot part of fill_event() into them.\n\nAll others are not as important and we don't want to bloat binary for\nthem, so add a noinline version of the function for all other use\nuse cases.\n\nnops test(batch=32): 16.932 vs 17.822 KIOPS\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a11d59424bf4417aca33f5ec21008bb3b0ebd11e.1618101759.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:41 -0600 io_uring: optimise fill_event() by inlining"
    },
    {
        "commit": "ff64216423d46396db2ca8b92fc75cc00ee6df4f",
        "message": "A simple preparation patch inlining io_cqring_fill_event(), which only\nrole was to pass cflags=0 into an actual fill event. It helps to keep\nnumber of related helpers sane in following patches.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/704f9c85b7d9843e4ad50a9f057200c58f5adc6e.1618101759.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:41 -0600 io_uring: always pass cflags into fill_event()"
    },
    {
        "commit": "44c769de6ffc3f1ea524fc9b7517c97078796e29",
        "message": "Eventfd is not the canonical way of using io_uring, annotate\nio_should_trigger_evfd() with likely so it improves code generation for\nnon-eventfd branch.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/42fdaa51c68d39479f02cef4fe5bcb24624d60fa.1618101759.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:41 -0600 io_uring: optimise non-eventfd post-event"
    },
    {
        "commit": "4af3417a347d06c8632346a6a9035c28b1dd94b4",
        "message": "Add an entry for user pointer to compat_msghdr into io_connect, so it's\nexplicit that we may use it as this, and removes annoying casts.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/73fd644dea1518f528d3648981cf777ce6e537e9.1618101759.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:41 -0600 io_uring: refactor compat_msghdr import"
    },
    {
        "commit": "0bdf3398b06ef1082b7d796039d34fc61a1285ea",
        "message": "Take advantage of delayed/inline completion flushing and pass right\nissue flags for completion of open, open2, fadvise and poll remove\nopcodes. All others either already use it or always punted and never\nexecuted inline.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0badc7512e82f7350b73bb09abbebbecbdd5dab8.1618101759.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:41 -0600 io_uring: enable inline completion for more cases"
    },
    {
        "commit": "a1fde923e3065a89abccfeef95096c933f6a954c",
        "message": "A small refactoring shrinking it and making easier to read.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/19b24eed7cd491a0243b50366dd2a23b558e2665.1618101759.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:41 -0600 io_uring: refactor io_close"
    },
    {
        "commit": "3f48cf18f886c97a7e775af10696bfed9ddcff31",
        "message": "Now __io_uring_cancel() and __io_uring_files_cancel() are very similar\nand mostly differ by how we count requests, merge them and allow\ntctx_inflight() to handle counting.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1a5986a97df4dc1378f3fe0ca1eb483dbcf42112.1618101759.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:41 -0600 io_uring: unify files and task cancel"
    },
    {
        "commit": "b303fe2e5a3802b0b1fb8d997e5c9caef48f6dd8",
        "message": "Instead of keeping requests in a inflight_list, just track them with a\nper tctx atomic counter. Apart from it being much easier and more\nconsistent with task cancel, it frees ->inflight_entry from being shared\nbetween iopoll and cancel-track, so less headache for us.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3c2ee0863cd7eeefa605f3eaff4c1c461a6f1157.1618101759.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:41 -0600 io_uring: track inflight requests through counter"
    },
    {
        "commit": "368b2080853f4694db780528c942f191f1c1687c",
        "message": "Move tracked inflight number check up the stack into\n__io_uring_files_cancel() so it's similar to task cancel. Will be used\nfor further cleaning.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/dca5a395efebd1e3e0f3bbc6b9640c5e8aa7e468.1618101759.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:40 -0600 io_uring: unify task and files cancel loops"
    },
    {
        "commit": "0ea13b448ee75ef0c68c18d207f6c488f143e725",
        "message": "hash_del() works well with non-hashed nodes, there's no need to check\nif it is hashed first.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:40 -0600 io_uring: simplify apoll hash removal"
    },
    {
        "commit": "e27414bef7b4f25f4569401e42bc68d9fdfc3125",
        "message": "Remove error parameter from io_poll_complete(), 0 is always passed,\nand do a bit of cleaning on top.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:40 -0600 io_uring: refactor io_poll_complete()"
    },
    {
        "commit": "f40b964a66ace54cda811d8ba96eccec210cd7ad",
        "message": "io_poll_complete() always fills an event (even an overflowed one), so we\nalways should do io_cqring_ev_posted() afterwards. And that's what is\ncurrently happening, because second EPOLLONESHOT check is always true,\nit can't return !done for oneshots.\n\nRemove those branching, it's much easier to read.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:40 -0600 io_uring: clean up io_poll_task_func()"
    },
    {
        "commit": "cb3b200e4f66524d03d6410dd51bcf42f265a4d0",
        "message": "We currently allow racy updates to multishot requests, but we can end up\ndouble adding the poll request if both completion and update does it.\nEnsure that we skip re-add on the update side if someone else is\ncompleting it.\n\nFixes: b69de288e913 (\"io_uring: allow events and user_data update of running poll requests\")\nReported-by: Joakim Hassila <joj@mac.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:35 -0600 io_uring: don't attempt re-add of multishot poll request if racing"
    },
    {
        "commit": "53a3126756d6edfe4fd5fa9037cd949df94dfe55",
        "message": "The splice/tee comment in io_prep_async_work() isn't relevant since the\nsection was moved, delete it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/892a549c89c3d422b679677b8e68ffd3fcb736b6.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:35 -0600 io_uring: kill outdated comment about splice punt"
    },
    {
        "commit": "a04b0ac0cb64fc403822de9288d68e6511ce6dc2",
        "message": "Add struct io_fixed_file representing a single registered file, first to\nhide ugly struct file **, which may be misleading, and secondly to\nretype it to unsigned long as conversions to it and back to file * for\nhandling and masking FFS_* flags are getting nasty.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/78669731a605a7614c577c3de552631cfaf0869a.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:35 -0600 io_uring: encapsulate fixed files into struct"
    },
    {
        "commit": "846a4ef22bf6d6ede4547fe8fa500385a90c64ba",
        "message": "Introduce a heler io_free_file_tables() doing all the cleaning, there\nare several places where it's hand coded. Also move all allocations into\nio_sqe_alloc_file_tables() and rename it, so all of it is in one place.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/502a84ebf41ff119b095e59661e678eacb752bf8.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:35 -0600 io_uring: refactor file tables alloc/free"
    },
    {
        "commit": "f4f7d21ce46474128934caeb80dfb1e5396b596e",
        "message": "There is no reason why we would want to fully quiesce ring on\nIORING_REGISTER_FILES, if it's already registered we fail.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/563bb8060bb2d3efbc32fce6101678281c574d2a.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:35 -0600 io_uring: don't quiesce intial files register"
    },
    {
        "commit": "9a321c98490c70653a4f0a10b28c45edbcf7a93d",
        "message": "Set FFS_* flags (e.g. FFS_ASYNC_READ) not only in initial registration\nbut also on registered files update. Not a bug, but may miss getting\nprofit out of the feature.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/df29a841a2d3d3695b509cdffce5070777d9d942.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:35 -0600 io_uring: set proper FFS* flags on reg file update"
    },
    {
        "commit": "044118069a23fdfb31677631cfdfc5e33b488752",
        "message": "Set MSG_NOSIGNAL and REQ_F_NOWAIT in send/recv prep routines and don't\nduplicate it in all four send/recv handlers.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e1133a3ed1c0e192975b7341ea4b0bf91f63b132.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:35 -0600 io_uring: deduplicate NOSIGNAL setting"
    },
    {
        "commit": "df9727affa058f4f18e388b30247650f8ae13cd8",
        "message": "Don't put linked timeout req in io_async_find_and_cancel() but do it in\nio_link_timeout_fn(), so we have only one point for that and won't have\nto do it differently as it's now (put vs put_deferred). Btw, improve a\nbit io_async_find_and_cancel()'s locking.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/d75b70957f245275ab7cba83e0ac9c1b86aae78a.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: put link timeout req consistently"
    },
    {
        "commit": "c4ea060e85eabe40f3572969daff4fc2f242b7b8",
        "message": "Overflowed CQEs doesn't lock requests anymore, so we don't care so much\nabout cancelling them, so kill cq_overflow_flushed and simplify the\ncode.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5799867aeba9e713c32f49aef78e5e1aef9fbc43.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: simplify overflow handling"
    },
    {
        "commit": "e07785b0029165fdb1c72ac12fe42801ba5f9f61",
        "message": "Add timeout and poll ->comletion_lock annotations for Sparse, makes life\neasier while looking at the functions.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2345325643093d41543383ba985a735aeb899eac.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: lock annotate timeouts and poll"
    },
    {
        "commit": "47e90392c8ad982c25f58125e9be3fc4d476b9ed",
        "message": "Kill unused forward declarations for io_ring_file_put() and\nio_queue_next(). Also btw rename the first one.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/64aa27c3f9662e14615cc119189f5eaf12989671.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: kill unused forward decls"
    },
    {
        "commit": "4751f53d74a688137de6a2a0b12ee591288c6dc8",
        "message": "It's a bit more convenient for us to store a registered buffer end\naddress instead of length, see struct io_mapped_ubuf, as it allow to not\nrecompute it every time.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/39164403fe92f1dc437af134adeec2423cdf9395.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: store reg buffer end instead of length"
    },
    {
        "commit": "75769e3f7357171dbe040a5ed55445c2642295d1",
        "message": "Replace a hand-coded overflow check with a specialised function. Even\nthough compilers are smart enough to generate identical binary (i.e.\ncheck carry bit), but it's more foolproof and conveys the intention\nbetter.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e437dcdc929bacbb6f11a4824ecbbf17225cb82a.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: improve import_fixed overflow checks"
    },
    {
        "commit": "0aec38fda2b6e36c0b066a87ff727ace3666cade",
        "message": "Remove extra tctx==NULL checks that are already done by\nio_async_cancel_one().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/70c2a8b958d942e86958a28af0452966ce1095b0.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: refactor io_async_cancel()"
    },
    {
        "commit": "e146a4a3f69e843a2153735875c64990aca244b1",
        "message": "No users of io_uring_ctx::hash_wait left, kill it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e25cb83c233a5f75f15275596b49fbafbea606fa.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: remove unused hash_wait"
    },
    {
        "commit": "7394161cb8bd26be43ebf0075e3b0197a6c3ca01",
        "message": "Instead of io_put_req() to drop not a final ref, use req_ref_put(),\nwhich is slimmer and will also check the invariant.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/85b5774ce13ae55cc2e705abdc8cbafe1212f1bd.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: better ref handling in poll_remove_one"
    },
    {
        "commit": "89b5066ea1d96b321c0743259169c599d3f4f969",
        "message": "io_ring_exit_work() already does uring_lock lock/unlock, no need to\nrepeat it for lock waiting trick in io_ring_ctx_free(). Move the waiting\nwith comments and spinlocking into io_ring_exit_work.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/a8ae0589b0ea64ad4791e2c282e4e9b713dd7024.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: combine lock/unlock sections on exit"
    },
    {
        "commit": "215c39026023dbfb4026b670c318371252be909f",
        "message": "rsrc_data refs should always be valid for potential submitters,\nio_rsrc_ref_quiesce() restores it before unlocking, so\npercpu_ref_is_dying() check in io_sqe_files_unregister() does nothing\nand misleading. Concurrent quiesce is prevented with\nstruct io_rsrc_data::quiesce.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/bf97055e1748ee3a382e66daf384a469eb90b931.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: remove useless is_dying check on quiesce"
    },
    {
        "commit": "28a9fe2521348ee350b65ae89e63c1def87b0cb6",
        "message": "Reuse io_rsrc_node_destroy() in __io_rsrc_put_work(). Also move it to a\nmore appropriate place -- to the other node routines, and remove forward\ndeclaration.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/cccafba41aee1e5bb59988704885b1340aef3a27.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: reuse io_rsrc_node_destroy()"
    },
    {
        "commit": "a7f0ed5acdc9ce251c66b9380e08766e59fa4ee8",
        "message": "If we're going to ever support multiple types of resources we need\nshared rsrc nodes to not bloat requests, that is implemented in this\npatch. It also gives a nicer API and saves one pointer dereference\nin io_req_set_rsrc_node().\n\nWe may say that all requests bound to a resource belong to one and only\none rsrc node, and considering that nodes are removed and recycled\nstrictly in-order, this separates requests into generations, where\ngeneration are changed on each node switch (i.e. io_rsrc_node_switch()).\n\nThe API is simple, io_rsrc_node_switch() switches to a new generation if\nneeded, and also optionally kills a passed in io_rsrc_data. Each call to\nio_rsrc_node_switch() have to be preceded with\nio_rsrc_node_switch_start(). The start function is idempotent and should\nnot necessarily be followed by switch.\n\nOne difference is that once a node was set it will always retain a valid\nrsrc node, even on unregister. It may be a nuisance at the moment, but\nmakes much sense for multiple types of resources. Another thing changed\nis that nodes are bound to/associated with a io_rsrc_data later just\nbefore killing (i.e. switching).\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7e9c693b4b9a2f47aa784b616ce29843021bb65a.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: ctx-wide rsrc nodes"
    },
    {
        "commit": "e7c78371bbf749087ff6b1f37c0d60f0ae82572c",
        "message": "Pass rsrc_node into io_queue_rsrc_removal() explicitly. Just a\nsimple preparation patch, makes following changes nicer.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/002889ce4de7baf287f2b010eef86ffe889174c6.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: refactor io_queue_rsrc_removal()"
    },
    {
        "commit": "40ae0ff70fb1379cb00041ef4061681e5e84e7f9",
        "message": "io_rsrc_node's callback operates only on a single io_rsrc_data and only\nwith its resources, so rsrc_put() callback is actually a property of\nio_rsrc_data. Move it there, it makes code much nicecr.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/9417c2fba3c09e8668f05747006a603d416d34b4.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: move rsrc_put callback into io_rsrc_data"
    },
    {
        "commit": "82fbcfa996e0b0f66ae0187082b0704d0ba50bdd",
        "message": "io_rsrc_node_get() and io_rsrc_node_set() are always used together,\nmerge them into one so most users don't even see io_rsrc_node and don't\nneed to care about it.\n\nIt helped to catch io_sqe_files_register() inferring rsrc data argument\nfor get and set differently, not a problem but a good sign.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0827b080b2e61b3dec795380f7e1a1995595d41f.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: encapsulate rsrc node manipulations"
    },
    {
        "commit": "f3baed39929edc5fa0ce7a897567153c87551776",
        "message": "Keep it consistent with update and use io_rsrc_node_prealloc() +\nio_rsrc_node_get() in io_sqe_files_register() as well, that will be used\nin future patches, not as error prone and allows to deduplicate\nrsrc_node init.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/cf87321e6be5e38f4dc7fe5079d2aa6945b1ace0.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: use rsrc prealloc infra for files reg"
    },
    {
        "commit": "221aa92409f945a19ce28c5cb54b4d9957f90715",
        "message": "Replace queue_delayed_work() with mod_delayed_work() in\nio_rsrc_node_ref_zero() as the later one can schedule a new work, and\ncleanup it further for better readability.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/3b2b23e3a1ea4bbf789cd61815d33e05d9ff945e.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: simplify io_rsrc_node_ref_zero"
    },
    {
        "commit": "b895c9a632e70ad977c1c0e31e640be5c98b56c6",
        "message": "Keep resource related structs' and functions' naming consistent, in\nparticular use \"io_rsrc\" prefix for everything.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/962f5acdf810f3a62831e65da3932cde24f6d9df.1617287883.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:34 -0600 io_uring: name rsrc bits consistently"
    },
    {
        "commit": "b2e720ace221f9be75fefdba7d0ebab9d05fc561",
        "message": "Joakim reports that in some conditions he sees a multishot poll request\nbeing canceled, and that it coincides with getting -EALREADY on\nmodification. As part of the poll update procedure, there's a small window\nwhere the request is marked as canceled, and if this coincides with the\nevent actually triggering, then we can get a spurious -ECANCELED and\ntermination of the multishot request.\n\nDon't mark the poll request as being canceled for update. We also don't\ncare if we race on removal unless it's a one-shot request, we can safely\nupdated for either case.\n\nFixes: b69de288e913 (\"io_uring: allow events and user_data update of running poll requests\")\nReported-by: Joakim Hassila <joj@mac.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 19:30:17 -0600 io_uring: fix race around poll update and poll triggering"
    },
    {
        "commit": "50e96989d736b8e5623059815247be01ca6713c1",
        "message": "We are safe with overflows in io_sqe_buffer_register() because it will\njust yield alloc failure, but it's nicer to check explicitly.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/2b0625551be3d97b80a5fd21c8cd79dc1c91f0b5.1616624589.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:42:00 -0600 io_uring: reg buffer overflow checks hardening"
    },
    {
        "commit": "548d819d1eed7b6bf86d36c8de2fbc54b69db571",
        "message": "Now that we have any worker being attached to the original task as\nthreads, accounting of CPU time is directly attributed to the original\ntask as well. This means that we no longer have to restrict SQPOLL to\nneeding elevated privileges, as it's really no different from just having\nthe task spawn a busy looping thread in userspace.\n\nReported-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:42:00 -0600 io_uring: allow SQPOLL without CAP_SYS_ADMIN or CAP_SYS_NICE"
    },
    {
        "commit": "b69de288e913030082bed3a324ddc58be6c1e983",
        "message": "This adds two new POLL_ADD flags, IORING_POLL_UPDATE_EVENTS and\nIORING_POLL_UPDATE_USER_DATA. As with the other POLL_ADD flag, these are\nmasked into sqe->len. If set, the POLL_ADD will have the following\nbehavior:\n\n- sqe->addr must contain the the user_data of the poll request that\n  needs to be modified. This field is otherwise invalid for a POLL_ADD\n  command.\n\n- If IORING_POLL_UPDATE_EVENTS is set, sqe->poll_events must contain the\n  new mask for the existing poll request. There are no checks for whether\n  these are identical or not, if a matching poll request is found, then it\n  is re-armed with the new mask.\n\n- If IORING_POLL_UPDATE_USER_DATA is set, sqe->off must contain the new\n  user_data for the existing poll request.\n\nA POLL_ADD with any of these flags set may complete with any of the\nfollowing results:\n\n1) 0, which means that we successfully found the existing poll request\n   specified, and performed the re-arm procedure. Any error from that\n   re-arm will be exposed as a completion event for that original poll\n   request, not for the update request.\n2) -ENOENT, if no existing poll request was found with the given\n   user_data.\n3) -EALREADY, if the existing poll request was already in the process of\n   being removed/canceled/completing.\n4) -EACCES, if an attempt was made to modify an internal poll request\n   (eg not one originally issued ass IORING_OP_POLL_ADD).\n\nThe usual -EINVAL cases apply as well, if any invalid fields are set\nin the sqe for this command type.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:42:00 -0600 io_uring: allow events and user_data update of running poll requests"
    },
    {
        "commit": "b2cb805f6dd40938c0398c94787741a08ed5e921",
        "message": "We'll need this helper for another purpose, for now just abstract it\nout and have io_poll_cancel() use it for lookups.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:42:00 -0600 io_uring: abstract out a io_poll_find_helper()"
    },
    {
        "commit": "5082620fb2cab74b623c3bf5da5a222add564871",
        "message": "If we hit overflow and fail to allocate an overflow entry for the\ncompletion, terminate the multishot poll mode.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: terminate multishot poll for CQ ring overflow"
    },
    {
        "commit": "b2c3f7e1715605c045f46fb369d850ada4749388",
        "message": "No functional changes in this patch, just preparation for kill multishot\npoll on CQ overflow.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: abstract out helper for removing poll waitqs/hashes"
    },
    {
        "commit": "88e41cf928a6e1a0eb5a9492e2d091ec6193cce4",
        "message": "The default io_uring poll mode is one-shot, where once the event triggers,\nthe poll command is completed and won't trigger any further events. If\nwe're doing repeated polling on the same file or socket, then it can be\nmore efficient to do multishot, where we keep triggering whenever the\nevent becomes true.\n\nThis deviates from the usual norm of having one CQE per SQE submitted. Add\na CQE flag, IORING_CQE_F_MORE, which tells the application to expect\nfurther completion events from the submitted SQE. Right now the only user\nof this is POLL_ADD in multishot mode.\n\nSince sqe->poll_events is using the space that we normally use for adding\nflags to commands, use sqe->len for the flag space for POLL_ADD. Multishot\nmode is selected by setting IORING_POLL_ADD_MULTI in sqe->len. An\napplication should expect more CQEs for the specificed SQE if the CQE is\nflagged with IORING_CQE_F_MORE. In multishot mode, only cancelation or an\nerror will terminate the poll request, in which case the flag will be\ncleared.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: add multishot mode for IORING_OP_POLL_ADD"
    },
    {
        "commit": "7471e1afabf8a9adcb4659170f4e198c05f5b5a6",
        "message": "We should be including the completion flags for better introspection on\nexactly what completion event was logged.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: include cflags in completion trace event"
    },
    {
        "commit": "6c2450ae55656f6b0370bfd4cb52ec8a4ecd0916",
        "message": "Instead of using a request itself for overflowed CQE stashing, allocate a\nseparate entry. The disadvantage is that the allocation may fail and it\nwill be accounted as lost (see rings->cq_overflow), so we lose reliability\nin case of memory pressure if the application is driving the CQ ring into\noverflow. However, it opens a way for for multiple CQEs per an SQE and\neven generating SQE-less CQEs.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\n[axboe: use GFP_ATOMIC | __GFP_ACCOUNT]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: allocate memory for overflowed CQEs"
    },
    {
        "commit": "464dca612bc6bceceafadfb4bf28f1a27ccc4632",
        "message": "Instead of masking these in as part of regular POLL_ADD prep, do it in\nio_init_poll_iocb(), and include NVAL as that's generally unmaskable,\nand RDHUP alongside the HUP that is already set.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: mask in error/nval/hangup consistently for poll"
    },
    {
        "commit": "9532b99bd9ca3f8f2f17b38500a8901ac1e7baee",
        "message": "Expect read/write to succeed and create a hot path for this case, in\nparticular hide all error handling with resubmission under a single\ncheck with the desired result.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: optimise rw complete error handling"
    },
    {
        "commit": "ab454438aa8dc9eb113df7d00f2cf9ec628a26ce",
        "message": "Move iov_iter_revert() resetting iterator in case of -EIOCBQUEUED into\nio_resubmit_prep(), so we don't do heavy revert in hot path, also saves\na couple of checks.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: hide iter revert in resubmit_prep"
    },
    {
        "commit": "8c130827f417da791edb919df8cac56af30a1489",
        "message": "When reissue_prep failed in io_complete_rw_iopoll(), we change return\ncode to -EIO to prevent io_iopoll_complete() from doing resubmission.\nMark requests with a new flag (i.e. REQ_F_DONT_REISSUE) instead and\nretain the original return value.\n\nIt also removes io_rw_reissue() from io_iopoll_complete() that will be\nused later.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: don't alter iopoll reissue fail ret code"
    },
    {
        "commit": "1c98679db94155a145f8389f9aaee30c99dbbd5a",
        "message": "file_end_write() is only for regular files, so the function do a couple\nof dereferences to get inode and check for it. However, we already have\nREQ_F_ISREG at hand, just use it and inline file_end_write().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: optimise kiocb_end_write for !ISREG"
    },
    {
        "commit": "59d7001345a7b9d849e2e768903458883395b00f",
        "message": "current->files are always valid now even for io-wq threads, so kill not\nused anymore REQ_F_NO_FILE_TABLE.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: kill unused REQ_F_NO_FILE_TABLE"
    },
    {
        "commit": "e1d675df1a36e33e43c614e01d9f714618ac121e",
        "message": "req->work is mostly unused unless it's punted, and io_init_req() is too\nhot for fully initialising it. Fortunately, we can skip init work.next\nas it's controlled by io-wq, and can not touch work.flags by moving\neverything related into io_prep_async_work(). The only field left is\nreq->work.creds, but there is nothing can be done, keep maintaining it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: don't init req->work fully in advance"
    },
    {
        "commit": "05356d86c64271b6f545fc14342526ab33514682",
        "message": "struct io_uring_task::sqpoll is not used anymore, kill it\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: remove tctx->sqpoll"
    },
    {
        "commit": "682076801a2f46867743d9520d228e3c7eca751f",
        "message": "io_match_task() matches all requests with PF_EXITING task, even though\nthose may be valid requests. It was necessary for SQPOLL cancellation,\nbut now it kills all requests before exiting via\nio_uring_cancel_sqpoll(), so it's not needed.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: don't do extra EXITING cancellations"
    },
    {
        "commit": "d4729fbde7665e81f4345e04e2ca86c0b52994d3",
        "message": "REQ_F_LINK_TIMEOUT is a hint that to look for linked timeouts to cancel,\nwe're leaving it even when it's already fired. Hence don't care to clear\nit in io_kill_linked_timeout(), it's safe and is called only once.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: don't clear REQ_F_LINK_TIMEOUT"
    },
    {
        "commit": "c15b79dee51bd73d56fe526a779e8fbc02b09e6c",
        "message": "Inline io_task_work_add() into io_req_task_work_add(). They both work\nwith a request, so keeping them separate doesn't make things much more\nclear, but merging allows optimise it. Apart from small wins like not\nreading req->ctx or not calculating @notify in the hot path, i.e. with\ntctx->task_state set, it avoids doing wake_up_process() for every single\nadd, but only after actually done task_work_add().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: optimise io_req_task_work_add()"
    },
    {
        "commit": "e1d767f078b88423bb8ed179fbfe3369395e10f8",
        "message": "io_put_file() doesn't do a good job at generating a good code. Inline\nit, so we can check REQ_F_FIXED_FILE first, prioritising FIXED_FILE case\nover requests without files, and saving a memory load in that case.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: abolish old io_put_file()"
    },
    {
        "commit": "094bae49e5ed9c30c1a6e50e121be20469486fab",
        "message": "Reshuffle io_dismantle_req() checks to put most of slow path stuff under\na single if.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: optimise io_dismantle_req() fast path"
    },
    {
        "commit": "68fb897966febe814f89f9462aa819abae00725f",
        "message": "Inline io_clean_op(), leaving __io_clean_op() but renaming it. This will\nbe used in following patches.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: inline io_clean_op()'s fast path"
    },
    {
        "commit": "2593553a01c803e01e7c5c2131993885879efbec",
        "message": "Both io_req_complete_failed() and __io_req_task_cancel() do the same\nthing: set failure flag, put both req refs and emit an CQE. The former\none is a bit more advance as it puts req back into a req cache, so make\nit to take over __io_req_task_cancel() and remove the last one.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: remove __io_req_task_cancel()"
    },
    {
        "commit": "dac7a09864938a310eea08f26f5960d369680629",
        "message": "Add a new helper io_flush_cached_locked_reqs() that splices\nlocked_free_list to free_list, and does it right doing all sync and\ninvariant reinit.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: add helper flushing locked_free_list"
    },
    {
        "commit": "a05432fb49b6439d0c5b803053dfdd875940116d",
        "message": "We don't care about ret value in io_free_req_deferred(), make the code a\nbit more concise.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: refactor io_free_req_deferred()"
    },
    {
        "commit": "0d85035a7368a6c6dc91ddeca6da12a50d24164e",
        "message": "One big omission is that io_put_req() haven't been marked inline, and at\nleast gcc 9 doesn't inline it, not to mention that it's really hot and\nextra function call is intolerable, especially when it doesn't put a\nfinal ref.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:59 -0600 io_uring: inline io_put_req and friends"
    },
    {
        "commit": "8dd03afe611d371b8c8a2ebeec2720de662a21dc",
        "message": "There are two problems:\n1) we always allocate refnodes in advance and free them if those\nhaven't been used. It's expensive, takes two allocations, where one of\nthem is percpu. And it may be pretty common not actually using them.\n\n2) Current API with allocating a refnode and setting some of the fields\nis error prone, we don't ever want to have a file node runninng fixed\nbuffer callback...\n\nSolve both with pre-init/get API. Pre-init just leaves the node for\nlater if not used, and for get (i.e. io_rsrc_refnode_get()), you need to\nexplicitly pass all arguments setting callbacks/etc., so it's more\nresilient.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: refactor rsrc refnode allocation"
    },
    {
        "commit": "dd78f49260dd49f21bbf12080cceb8e13ce53db3",
        "message": "Emphasize that return value of io_flush_cached_reqs() depends on number\nof requests in the cache. It looks nicer and might help tools from\nfalse-negative analyses.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: refactor io_flush_cached_reqs()"
    },
    {
        "commit": "1840038e119573fc624a2fc586a1c5ced50b59f2",
        "message": "Move the case of successfully issued request by doing that check first.\nIt's not much of a difference, just generates slightly better code for\nme.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: optimise success case of __io_queue_sqe"
    },
    {
        "commit": "de968c182b4f48a421b0a3862e747c4147a7da22",
        "message": "Inline __io_queue_linked_timeout(), we don't need it\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: inline __io_queue_linked_timeout()"
    },
    {
        "commit": "966706579a7124fa6334f10c48474193fd6780c0",
        "message": "Don't do a function call (io_dismantle_req()) in the middle and place it\nto near other function calls, otherwise may lead to excessive register\nspilling.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: keep io_req_free_batch() call locality"
    },
    {
        "commit": "cf27f3b14961845d816c49abc99aae4863207c77",
        "message": "First of all, w need to set tctx->sqpoll only when we add a new entry\ninto ->xa, so move it from the hot path. Also extract a hot path for\nio_uring_add_task_file() as an inline helper.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: optimise tctx node checks/alloc"
    },
    {
        "commit": "33f993da9829738da3e088fb5d3128880a4137ba",
        "message": "Add unlikely annotations, because my compiler pretty much mispredicts\nevery first check, and apart jumping around in the fast path, it also\ngenerates extra instructions, like in advance setting ret value.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: optimise io_uring_enter()"
    },
    {
        "commit": "493f3b158a1e445e24d567847045baf5a723d206",
        "message": "__tctx_task_work() guarantees that ctx won't be killed while running\ntask_works, so we can remove now unnecessary ctx pinning for internally\narmed polling.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: don't take ctx refs in task_work handler"
    },
    {
        "commit": "45ab03b19e8bf33af3e5f5a24729e5564d54fae9",
        "message": "We can set canceled == true and complete out-of-line, ensure that we catch\nthat and correctly return -ECANCELED if the poll operation got canceled.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: transform ret == 0 for poll cancelation completions"
    },
    {
        "commit": "b9b0e0d39c7b4be7af7976c52bdb8664dfa389f5",
        "message": "The correct function is io_iopoll_complete(), which deals with completions\nof IOPOLL requests, not io_poll_complete().\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: correct comment on poll vs iopoll"
    },
    {
        "commit": "7b29f92da377c358955b522045d0778aa79a540a",
        "message": "We have to dig quite deep to check for particularly whether or not a\nfile supports a fast-path nonblock attempt. For fixed files, we can do\nthis lookup once and cache the state instead.\n\nThis adds two new bits to track whether we support async read/write\nattempt, and lines up the REQ_F_ISREG bit with those two. The file slot\nre-uses the last 3 (or 2, for 32-bit) of the file pointer to cache that\nstate, and then we mask it in when we go and use a fixed file.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: cache async and regular file state for fixed files"
    },
    {
        "commit": "d44f554e105b0c20e5b06b9f821bef228e04d573",
        "message": "We don't allow them at registration time, so limit the check for needing\ninflight tracking in io_file_get() to the non-fixed path.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: don't check for io_uring_fops for fixed files"
    },
    {
        "commit": "c9dca27dc7f9c5dc4ee4ba5b77f7584387f867fe",
        "message": "Use a more comprehensible() max instead of hand coding it with ifs in\nio_sqd_update_thread_idle().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: simplify io_sqd_update_thread_idle()"
    },
    {
        "commit": "abc54d634334f24d9a3253b8207b42eda852f25a",
        "message": "io_uring manipulates references twice for each request, and hence is very\nsensitive to performance of the reference count. This commit borrows a\ntrick from:\n\ncommit f958d7b528b1b40c44cfda5eabe2d82760d868c3\nAuthor: Linus Torvalds <torvalds@linux-foundation.org>\nDate:   Thu Apr 11 10:06:20 2019 -0700\n\n    mm: make page ref count overflow check tighter and more explicit\n\nand switches to atomic_t for references, while still retaining overflow\nand underflow checks.\n\nThis is good for a 2-3% increase in peak IOPS on a single core. Before:\n\nIOPS=2970879, IOS/call=31/31, inflight=128 (128)\nIOPS=2952597, IOS/call=31/31, inflight=128 (128)\nIOPS=2943904, IOS/call=31/31, inflight=128 (128)\nIOPS=2930006, IOS/call=31/31, inflight=96 (96)\n\nand after:\n\nIOPS=3054354, IOS/call=31/31, inflight=128 (128)\nIOPS=3059038, IOS/call=31/31, inflight=128 (128)\nIOPS=3060320, IOS/call=31/31, inflight=128 (128)\nIOPS=3068256, IOS/call=31/31, inflight=96 (96)\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: switch to atomic_t for io_kiocb reference count"
    },
    {
        "commit": "de9b4ccad750f216616730b74ed2be16c80892a4",
        "message": "No functional changes in this patch, just in preparation for handling the\nreferences a bit more efficiently.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: wrap io_kiocb reference count manipulation in helpers"
    },
    {
        "commit": "179ae0d15e8b3a2d9affe680281009f1f10c4a9d",
        "message": "If not for async_data NULL check, io_resubmit_prep() is already an rw\nspecific version of io_req_prep_async(), but slower because 1) it always\ngoes through io_import_iovec() even if following io_setup_async_rw() the\nresult 2) instead of initialising iovec/iter in-place it does it\non-stack and then copies with io_setup_async_rw().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: simplify io_resubmit_prep()"
    },
    {
        "commit": "b7e298d265f20eafc3615be271a3e5d90e4dc3dd",
        "message": "Merge two function and do renaming in favour of the second one, it\nrelays the meaning better.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: merge defer_prep() and prep_async()"
    },
    {
        "commit": "26f0505a9ce571f3b1fcef6e86c5c99c68ca7eca",
        "message": "needs_async_data controls allocation of async_data, and used in two\ncases. 1) when async setup requires it (by io_req_prep_async() or\nhandler themselves), and 2) when op always needs additional space to\noperate, like timeouts do.\n\nOpcode preps already don't bother about the second case and do\nallocation unconditionally, restrict needs_async_data to the first case\nonly and rename it into needs_async_setup.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\n[axboe: update for IOPOLL fix]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: rethink def->needs_async_data"
    },
    {
        "commit": "6cb78689fa94c80784faef76744746aee558c344",
        "message": "All opcode handlers pretty well know whether they need async data or\nnot, and can skip testing for needs_async_data. The exception is rw\nthe generic path, but those test the flag by hand anyway. So, check the\nflag and make io_alloc_async_data() allocating unconditionally.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: untie alloc_async_data and needs_async_data"
    },
    {
        "commit": "2e052d443df15d71277f6b8509badae4310ebd92",
        "message": "IORING_OP_[SEND,RECV] don't need async setup neither will get into\nio_req_prep_async(). Remove them from io_req_prep_async() and remove\nneeds_async_data checks from the related setup functions.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: refactor out send/recv async setup"
    },
    {
        "commit": "8c3f9cd1603d0e4af6c50ebc6d974ab7bdd03cf4",
        "message": "__io_cqring_fill_event() takes cflags as long to squeeze it into u32 in\nan CQE, awhile all users pass int or unsigned. Replace it with unsigned\nint and store it as u32 in struct io_completion to match CQE.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: use better types for cflags"
    },
    {
        "commit": "9fb8cb49c7b634982ac2a4302b5158d7120f0186",
        "message": "Always complete request holding the mutex instead of doing that strange\ndancing with conditional ordering.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: refactor provide/remove buffer locking"
    },
    {
        "commit": "f41db2732d4835799af64159c61e522063786e5c",
        "message": "Add a simple helper doing CQE posting, marking request for link-failure,\nand putting both submission and completion references.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: add a helper failing not issued requests"
    },
    {
        "commit": "dafecf19e25f9b864ce0f3b8bb12de2e3d5f6da6",
        "message": "io_fixed_file_slot() and io_file_from_index() behave pretty similarly,\nDRY and call one from another.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:58 -0600 io_uring: further deduplicate file slot selection"
    },
    {
        "commit": "2c4b8eb6435e615544b92acdcd4b25a85e83f300",
        "message": "Use io_req_task_queue_fail() on the fail path of io_req_task_queue().\nIt's unlikely to happen, so don't care about additional overhead, but\nallows to keep all the req->result invariant in a single function.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:57 -0600 io_uring: reuse io_req_task_queue_fail()"
    },
    {
        "commit": "e83acd7d37d83035f2fe078f656f87418ea2a687",
        "message": "Don't bother to take a ctx->refs for io_req_task_cancel() because it\ntake uring_lock before putting a request, and the context is promised to\nstay alive until unlock happens.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-04-11 17:41:57 -0600 io_uring: avoid taking ctx refs for task-cancel"
    },
    {
        "commit": "3b9784350f990d8fe2ca08978dc25cd5180d5c21",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two minor fixups for the reissue logic, and one for making sure that\n  unbounded work is canceled on io-wq exit\"\n\n* tag 'io_uring-5.12-2021-04-09' of git://git.kernel.dk/linux-block:\n  io-wq: cancel unbounded works on io-wq destroy\n  io_uring: fix rw req completion\n  io_uring: clear F_REISSUE right after getting it",
        "kernel_version": "v5.12-rc7",
        "release_date": "2021-04-09 15:06:52 -0700 Merge tag 'io_uring-5.12-2021-04-09' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "c60eb049f4a19ddddcd3ee97a9c79ab8066a6a03",
        "message": "WARNING: CPU: 5 PID: 227 at fs/io_uring.c:8578 io_ring_exit_work+0xe6/0x470\nRIP: 0010:io_ring_exit_work+0xe6/0x470\nCall Trace:\n process_one_work+0x206/0x400\n worker_thread+0x4a/0x3d0\n kthread+0x129/0x170\n ret_from_fork+0x22/0x30\n\nINFO: task lfs-openat:2359 blocked for more than 245 seconds.\ntask:lfs-openat      state:D stack:    0 pid: 2359 ppid:     1 flags:0x00000004\nCall Trace:\n ...\n wait_for_completion+0x8b/0xf0\n io_wq_destroy_manager+0x24/0x60\n io_wq_put_and_exit+0x18/0x30\n io_uring_clean_tctx+0x76/0xa0\n __io_uring_files_cancel+0x1b9/0x2e0\n do_exit+0xc0/0xb40\n ...\n\nEven after io-wq destroy has been issued io-wq worker threads will\ncontinue executing all left work items as usual, and may hang waiting\nfor I/O that won't ever complete (aka unbounded).\n\n[<0>] pipe_read+0x306/0x450\n[<0>] io_iter_do_read+0x1e/0x40\n[<0>] io_read+0xd5/0x330\n[<0>] io_issue_sqe+0xd21/0x18a0\n[<0>] io_wq_submit_work+0x6c/0x140\n[<0>] io_worker_handle_work+0x17d/0x400\n[<0>] io_wqe_worker+0x2c0/0x330\n[<0>] ret_from_fork+0x22/0x30\n\nCancel all unbounded I/O instead of executing them. This changes the\nuser visible behaviour, but that's inevitable as io-wq is not per task.\n\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/cd4b543154154cba055cf86f351441c2174d7f71.1617842918.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc7",
        "release_date": "2021-04-08 13:33:17 -0600 io-wq: cancel unbounded works on io-wq destroy"
    },
    {
        "commit": "9728463737db027557e8ba315cbbca6b81122c04",
        "message": "WARNING: at fs/io_uring.c:8578 io_ring_exit_work.cold+0x0/0x18\n\nAs reissuing is now passed back by REQ_F_REISSUE and kiocb_done()\ninternally uses __io_complete_rw(), it may stop after setting the flag\nso leaving a dangling request.\n\nThere are tricky edge cases, e.g. reading beyound file, boundary, so\nthe easiest way is to hand code reissue in kiocb_done() as\n__io_complete_rw() was doing for us before.\n\nFixes: 230d50d448ac (\"io_uring: move reissue into regular IO path\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/f602250d292f8a84cca9a01d747744d1e797be26.1617842918.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc7",
        "release_date": "2021-04-08 13:32:59 -0600 io_uring: fix rw req completion"
    },
    {
        "commit": "6ad7f2332e84c46f0c94e73e05b5b7c2bc1a6b74",
        "message": "There are lots of ways r/w request may continue its path after getting\nREQ_F_REISSUE, it's not necessarily io-wq and can be, e.g. apoll,\nand submitted via  io_async_task_func() -> __io_req_task_submit()\n\nClear the flag right after getting it, so the next attempt is well\nprepared regardless how the request will be executed.\n\nFixes: 230d50d448ac (\"io_uring: move reissue into regular IO path\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/11dcead939343f4e27cab0074d34afcab771bfa4.1617842918.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc7",
        "release_date": "2021-04-07 22:10:19 -0600 io_uring: clear F_REISSUE right after getting it"
    },
    {
        "commit": "d83e98f9d8c88cbae1b05fa5751bddfcf0e222b2",
        "message": "POull io_uring fix from Jens Axboe:\n \"Just fixing a silly braino in a previous patch, where we'd end up\n  failing to compile if CONFIG_BLOCK isn't enabled.\n\n  Not that a lot of people do that, but kernel bot spotted it and it's\n  probably prudent to just flush this out now before -rc6.\n\n  Sorry about that, none of my test compile configs have !CONFIG_BLOCK\"\n\n* tag 'io_uring-5.12-2021-04-03' of git://git.kernel.dk/linux-block:\n  io_uring: fix !CONFIG_BLOCK compilation failure",
        "kernel_version": "v5.12-rc6",
        "release_date": "2021-04-03 14:26:47 -0700 Merge tag 'io_uring-5.12-2021-04-03' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "e82ad4853948382d37ac512b27a3e70b6f01c103",
        "message": "kernel test robot correctly pinpoints a compilation failure if\nCONFIG_BLOCK isn't set:\n\nfs/io_uring.c: In function '__io_complete_rw':\n>> fs/io_uring.c:2509:48: error: implicit declaration of function 'io_rw_should_reissue'; did you mean 'io_rw_reissue'? [-Werror=implicit-function-declaration]\n    2509 |  if ((res == -EAGAIN || res == -EOPNOTSUPP) && io_rw_should_reissue(req)) {\n         |                                                ^~~~~~~~~~~~~~~~~~~~\n         |                                                io_rw_reissue\n    cc1: some warnings being treated as errors\n\nEnsure that we have a stub declaration of io_rw_should_reissue() for\n!CONFIG_BLOCK.\n\nFixes: 230d50d448ac (\"io_uring: move reissue into regular IO path\")\nReported-by: kernel test robot <lkp@intel.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc6",
        "release_date": "2021-04-02 19:45:34 -0600 io_uring: fix !CONFIG_BLOCK compilation failure"
    },
    {
        "commit": "1faccb63949988e4cfdfba2e9d2c3476c6d690e5",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Nothing really major in here, and finally nothing really related to\n  signals. A few minor fixups related to the threading changes, and some\n  general fixes, that's it.\n\n  There's the pending gdb-get-confused-about-arch, but that's more of a\n  cosmetic issue, nothing that hinder use of it. And given that other\n  archs will likely be affected by that oddity too, better to postpone\n  any changes there until 5.13 imho\"\n\n* tag 'io_uring-5.12-2021-04-02' of git://git.kernel.dk/linux-block:\n  io_uring: move reissue into regular IO path\n  io_uring: fix EIOCBQUEUED iter revert\n  io_uring/io-wq: protect against sprintf overflow\n  io_uring: don't mark S_ISBLK async work as unbounded\n  io_uring: drop sqd lock before handling signals for SQPOLL\n  io_uring: handle setup-failed ctx in kill_timeouts\n  io_uring: always go for cancellation spin on exec",
        "kernel_version": "v5.12-rc6",
        "release_date": "2021-04-02 16:08:19 -0700 Merge tag 'io_uring-5.12-2021-04-02' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "230d50d448acb6639991440913299e50cacf1daf",
        "message": "It's non-obvious how retry is done for block backed files, when it happens\noff the kiocb done path. It also makes it tricky to deal with the iov_iter\nhandling.\n\nJust mark the req as needing a reissue, and handling it from the\nsubmission path instead. This makes it directly obvious that we're not\nre-importing the iovec from userspace past the submit point, and it means\nthat we can just reuse our usual -EAGAIN retry path from the read/write\nhandling.\n\nAt some point in the future, we'll gain the ability to always reliably\nreturn -EAGAIN through the stack. A previous attempt on the block side\ndidn't pan out and got reverted, hence the need to check for this\ninformation out-of-band right now.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc6",
        "release_date": "2021-04-02 09:24:20 -0600 io_uring: move reissue into regular IO path"
    },
    {
        "commit": "07204f21577a1d882f0259590c3553fe6a476381",
        "message": "iov_iter_revert() is done in completion handlers that happensf before\nread/write returns -EIOCBQUEUED, no need to repeat reverting afterwards.\nMoreover, even though it may appear being just a no-op, it's actually\nraces with 1) user forging a new iovec of a different size 2) reissue,\nthat is done via io-wq continues completely asynchronously.\n\nFixes: 3e6a0d3c7571c (\"io_uring: fix -EAGAIN retry with IOPOLL\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc6",
        "release_date": "2021-04-01 09:31:21 -0600 io_uring: fix EIOCBQUEUED iter revert"
    },
    {
        "commit": "696ee88a7c50f96573f98aa76cc74286033140c1",
        "message": "task_pid may be large enough to not fit into the left space of\nTASK_COMM_LEN-sized buffers and overflow in sprintf. We not so care\nabout uniqueness, so replace it with safer snprintf().\n\nReported-by: Alexey Dobriyan <adobriyan@gmail.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/1702c6145d7e1c46fbc382f28334c02e1a3d3994.1617267273.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc6",
        "release_date": "2021-04-01 09:21:18 -0600 io_uring/io-wq: protect against sprintf overflow"
    },
    {
        "commit": "4b982bd0f383db9132e892c0c5144117359a6289",
        "message": "S_ISBLK is marked as unbounded work for async preparation, because it\ndoesn't match S_ISREG. That is incorrect, as any read/write to a block\ndevice is also a bounded operation. Fix it up and ensure that S_ISBLK\nisn't marked unbounded.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc6",
        "release_date": "2021-04-01 08:56:28 -0600 io_uring: don't mark S_ISBLK async work as unbounded"
    },
    {
        "commit": "82734c5b1b24c020d701cf90ccb075e43a5ccb07",
        "message": "Don't call into get_signal() with the sqd mutex held, it'll fail if we're\nfreezing the task and we'll get complaints on locks still being held:\n\n====================================\nWARNING: iou-sqp-8386/8387 still has locks held!\n5.12.0-rc4-syzkaller #0 Not tainted\n------------------------------------\n1 lock held by iou-sqp-8386/8387:\n #0: ffff88801e1d2470 (&sqd->lock){+.+.}-{3:3}, at: io_sq_thread+0x24c/0x13a0 fs/io_uring.c:6731\n\n stack backtrace:\n CPU: 1 PID: 8387 Comm: iou-sqp-8386 Not tainted 5.12.0-rc4-syzkaller #0\n Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\n Call Trace:\n  __dump_stack lib/dump_stack.c:79 [inline]\n  dump_stack+0x141/0x1d7 lib/dump_stack.c:120\n  try_to_freeze include/linux/freezer.h:66 [inline]\n  get_signal+0x171a/0x2150 kernel/signal.c:2576\n  io_sq_thread+0x8d2/0x13a0 fs/io_uring.c:6748\n\nFold the get_signal() case in with the parking checks, as we need to drop\nthe lock in both cases, and since we need to be checking for parking when\njuggling the lock anyway.\n\nReported-by: syzbot+796d767eb376810256f5@syzkaller.appspotmail.com\nFixes: dbe1bdbb39db (\"io_uring: handle signals for IO threads like a normal thread\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc6",
        "release_date": "2021-03-30 14:36:46 -0600 io_uring: drop sqd lock before handling signals for SQPOLL"
    },
    {
        "commit": "51520426f4bc3e61cbbf7a39ccf4e411b665002d",
        "message": "general protection fault, probably for non-canonical address\n\t0xdffffc0000000018: 0000 [#1] KASAN: null-ptr-deref\n\tin range [0x00000000000000c0-0x00000000000000c7]\nRIP: 0010:io_commit_cqring+0x37f/0xc10 fs/io_uring.c:1318\nCall Trace:\n io_kill_timeouts+0x2b5/0x320 fs/io_uring.c:8606\n io_ring_ctx_wait_and_kill+0x1da/0x400 fs/io_uring.c:8629\n io_uring_create fs/io_uring.c:9572 [inline]\n io_uring_setup+0x10da/0x2ae0 fs/io_uring.c:9599\n do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nIt can get into wait_and_kill() before setting up ctx->rings, and hence\nio_commit_cqring() fails. Mimic poll cancel and do it only when we\ncompleted events, there can't be any requests if it failed before\ninitialising rings.\n\nFixes: 80c4cbdb5ee60 (\"io_uring: do post-completion chore on t-out cancel\")\nReported-by: syzbot+0e905eb8228070c457a0@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/660261a48f0e7abf260c8e43c87edab3c16736fa.1617014345.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc6",
        "release_date": "2021-03-29 06:48:26 -0600 io_uring: handle setup-failed ctx in kill_timeouts"
    },
    {
        "commit": "5a978dcfc0f054e4f6983a0a26355a65e34708cb",
        "message": "Always try to do cancellation in __io_uring_task_cancel() at least once,\nso it actually goes and cleans its sqpoll tasks (i.e. via\nio_sqpoll_cancel_sync()), otherwise sqpoll task may submit new requests\nafter cancellation and it's racy for many reasons.\n\nFixes: 521d6a737a31c (\"io_uring: cancel sqpoll via task_work\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0a21bd6d794bb1629bc906dd57a57b2c2985a8ac.1616839147.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc6",
        "release_date": "2021-03-28 18:11:53 -0600 io_uring: always go for cancellation spin on exec"
    },
    {
        "commit": "b44d1ddcf835b39a8dc14276d770074deaed297c",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Use thread info versions of flag testing, as discussed last week.\n\n - The series enabling PF_IO_WORKER to just take signals, instead of\n   needing to special case that they do not in a bunch of places. Ends\n   up being pretty trivial to do, and then we can revert all the special\n   casing we're currently doing.\n\n - Kill dead pointer assignment\n\n - Fix hashed part of async work queue trace\n\n - Fix sign extension issue for IORING_OP_PROVIDE_BUFFERS\n\n - Fix a link completion ordering regression in this merge window\n\n - Cancellation fixes\n\n* tag 'io_uring-5.12-2021-03-27' of git://git.kernel.dk/linux-block:\n  io_uring: remove unsued assignment to pointer io\n  io_uring: don't cancel extra on files match\n  io_uring: don't cancel-track common timeouts\n  io_uring: do post-completion chore on t-out cancel\n  io_uring: fix timeout cancel return code\n  Revert \"signal: don't allow STOP on PF_IO_WORKER threads\"\n  Revert \"kernel: freezer should treat PF_IO_WORKER like PF_KTHREAD for freezing\"\n  Revert \"kernel: treat PF_IO_WORKER like PF_KTHREAD for ptrace/signals\"\n  Revert \"signal: don't allow sending any signals to PF_IO_WORKER threads\"\n  kernel: stop masking signals in create_io_thread()\n  io_uring: handle signals for IO threads like a normal thread\n  kernel: don't call do_exit() for PF_IO_WORKER threads\n  io_uring: maintain CQE order of a failed link\n  io-wq: fix race around pending work on teardown\n  io_uring: do ctx sqd ejection in a clear context\n  io_uring: fix provide_buffers sign extension\n  io_uring: don't skip file_end_write() on reissue\n  io_uring: correct io_queue_async_work() traces\n  io_uring: don't use {test,clear}_tsk_thread_flag() for current",
        "kernel_version": "v5.12-rc5",
        "release_date": "2021-03-28 11:42:05 -0700 Merge tag 'io_uring-5.12-2021-03-27' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "4e53d1701b574b1ee9d500b4913a1ece2fac8911",
        "message": "Since commit 3bfe6106693b6b4b (\"io-wq: fork worker threads from original\ntask\") stopped using PF_KTHREAD flag for the io_uring PF_IO_WORKER threads,\ntomoyo_kernel_service() no longer needs to check PF_IO_WORKER flag.\n\n(This is a 5.12+ patch. Please don't send to stable kernels.)\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>",
        "kernel_version": "v5.12-rc6",
        "release_date": "2021-03-28 13:11:29 +0900 tomoyo: don't special case PF_IO_WORKER for PF_KTHREAD"
    },
    {
        "commit": "2b8ed1c94182dbbd0163d0eb443a934cbf6b0d85",
        "message": "There is an assignment to io that is never read after the assignment,\nthe assignment is redundant and can be removed.\n\nSigned-off-by: Colin Ian King <colin.king@canonical.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc5",
        "release_date": "2021-03-27 14:09:11 -0600 io_uring: remove unsued assignment to pointer io"
    },
    {
        "commit": "78d9d7c2a331fb7a68a86e53ef7e12966459e0c5",
        "message": "As tasks always wait and kill their io-wq on exec/exit, files are of no\nmore concern to us, so we don't need to specifically cancel them by hand\nin those cases. Moreover we should not, because io_match_task() looks at\nreq->task->files now, which is always true and so leads to extra\ncancellations, that wasn't a case before per-task io-wq.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/0566c1de9b9dd417f5de345c817ca953580e0e2e.1616696997.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc5",
        "release_date": "2021-03-27 14:09:11 -0600 io_uring: don't cancel extra on files match"
    },
    {
        "commit": "2482b58ffbdc80cfaae969ad19cb32803056505b",
        "message": "Don't account usual timeouts (i.e. not linked) as REQ_F_INFLIGHT but\nkeep behaviour prior to dd59a3d595cc1 (\"io_uring: reliably cancel linked\ntimeouts\").\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/104441ef5d97e3932113d44501fda0df88656b83.1616696997.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc5",
        "release_date": "2021-03-27 14:09:11 -0600 io_uring: don't cancel-track common timeouts"
    },
    {
        "commit": "80c4cbdb5ee604712e59fe304d7bf084b562f705",
        "message": "Don't forget about io_commit_cqring() + io_cqring_ev_posted() after\nexit/exec cancelling timeouts. Both functions declared only after\nio_kill_timeouts(), so to avoid tons of forward declarations move\nit down.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/72ace588772c0f14834a6a4185d56c445a366fb4.1616696997.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc5",
        "release_date": "2021-03-27 14:09:11 -0600 io_uring: do post-completion chore on t-out cancel"
    },
    {
        "commit": "1ee4160c73b2102a52bc97a4128a89c34821414f",
        "message": "When we cancel a timeout we should emit a sensible return code, like\n-ECANCELED but not 0, otherwise it may trick users.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7b0ad1065e3bd1994722702bd0ba9e7bc9b0683b.1616696997.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc5",
        "release_date": "2021-03-27 14:09:11 -0600 io_uring: fix timeout cancel return code"
    },
    {
        "commit": "e8b33b8cfafcfcef287ae4c0f23a173bfcf617f3",
        "message": "This reverts commit 6fb8f43cede0e4bd3ead847de78d531424a96be9.\n\nThe IO threads do allow signals now, including SIGSTOP, and we can allow\nptrace attach. Attaching won't reveal anything interesting for the IO\nthreads, but it will allow eg gdb to attach to a task with io_urings\nand IO threads without complaining. And once attached, it will allow\nthe usual introspection into regular threads.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc5",
        "release_date": "2021-03-27 14:09:10 -0600 Revert \"kernel: treat PF_IO_WORKER like PF_KTHREAD for ptrace/signals\""
    },
    {
        "commit": "dbe1bdbb39db7dfe80a903f0d267f62cf3f093d2",
        "message": "We go through various hoops to disallow signals for the IO threads, but\nthere's really no reason why we cannot just allow them. The IO threads\nnever return to userspace like a normal thread, and hence don't go through\nnormal signal processing. Instead, just check for a pending signal as part\nof the work loop, and call get_signal() to handle it for us if anything\nis pending.\n\nWith that, we can support receiving signals, including special ones like\nSIGSTOP.\n\nAcked-by: \"Eric W. Biederman\" <ebiederm@xmission.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc5",
        "release_date": "2021-03-27 14:09:07 -0600 io_uring: handle signals for IO threads like a normal thread"
    },
    {
        "commit": "90b8749022bbdd0c94a13182a78f4903b98fd0d7",
        "message": "Arguably we want CQEs of linked requests be in a strict order of\nsubmission as it always was. Now if init of a request fails its CQE may\nbe posted before all prior linked requests including the head of the\nlink. Fix it by failing it last.\n\nFixes: de59bc104c24f (\"io_uring: fail links more in io_submit_sqe()\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/b7a96b05832e7ab23ad55f84092a2548c4a888b0.1616699075.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc5",
        "release_date": "2021-03-25 13:47:03 -0600 io_uring: maintain CQE order of a failed link"
    },
    {
        "commit": "f5d2d23bf0d948ce0b9307b7bacae7ff0bc03c71",
        "message": "syzbot reports that it's triggering the warning condition on having\npending work on shutdown:\n\nWARNING: CPU: 1 PID: 12346 at fs/io-wq.c:1061 io_wq_destroy fs/io-wq.c:1061 [inline]\nWARNING: CPU: 1 PID: 12346 at fs/io-wq.c:1061 io_wq_put+0x153/0x260 fs/io-wq.c:1072\nModules linked in:\nCPU: 1 PID: 12346 Comm: syz-executor.5 Not tainted 5.12.0-rc2-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nRIP: 0010:io_wq_destroy fs/io-wq.c:1061 [inline]\nRIP: 0010:io_wq_put+0x153/0x260 fs/io-wq.c:1072\nCode: 8d e8 71 90 ea 01 49 89 c4 41 83 fc 40 7d 4f e8 33 4d 97 ff 42 80 7c 2d 00 00 0f 85 77 ff ff ff e9 7a ff ff ff e8 1d 4d 97 ff <0f> 0b eb b9 8d 6b ff 89 ee 09 de bf ff ff ff ff e8 18 51 97 ff 09\nRSP: 0018:ffffc90001ebfb08 EFLAGS: 00010293\nRAX: ffffffff81e16083 RBX: ffff888019038040 RCX: ffff88801e86b780\nRDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000040\nRBP: 1ffff1100b2f8a80 R08: ffffffff81e15fce R09: ffffed100b2f8a82\nR10: ffffed100b2f8a82 R11: 0000000000000000 R12: 0000000000000000\nR13: dffffc0000000000 R14: ffff8880597c5400 R15: ffff888019038000\nFS:  00007f8dcd89c700(0000) GS:ffff8880b9c00000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 000055e9a054e160 CR3: 000000001dfb8000 CR4: 00000000001506f0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\nCall Trace:\n io_uring_clean_tctx+0x1b7/0x210 fs/io_uring.c:8802\n __io_uring_files_cancel+0x13c/0x170 fs/io_uring.c:8820\n io_uring_files_cancel include/linux/io_uring.h:47 [inline]\n do_exit+0x258/0x2340 kernel/exit.c:780\n do_group_exit+0x168/0x2d0 kernel/exit.c:922\n get_signal+0x1734/0x1ef0 kernel/signal.c:2773\n arch_do_signal_or_restart+0x3c/0x610 arch/x86/kernel/signal.c:811\n handle_signal_work kernel/entry/common.c:147 [inline]\n exit_to_user_mode_loop kernel/entry/common.c:171 [inline]\n exit_to_user_mode_prepare+0xac/0x1e0 kernel/entry/common.c:208\n __syscall_exit_to_user_mode_work kernel/entry/common.c:290 [inline]\n syscall_exit_to_user_mode+0x48/0x180 kernel/entry/common.c:301\n entry_SYSCALL_64_after_hwframe+0x44/0xae\nRIP: 0033:0x465f69\n\nwhich shouldn't happen, but seems to be possible due to a race on whether\nor not the io-wq manager sees a fatal signal first, or whether the io-wq\nworkers do. If we race with queueing work and then send a fatal signal to\nthe owning task, and the io-wq worker sees that before the manager sets\nIO_WQ_BIT_EXIT, then it's possible to have the worker exit and leave work\nbehind.\n\nJust turn the WARN_ON_ONCE() into a cancelation condition instead.\n\nReported-by: syzbot+77a738a6bc947bf639ca@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc5",
        "release_date": "2021-03-25 10:16:12 -0600 io-wq: fix race around pending work on teardown"
    },
    {
        "commit": "a185f1db59f13de73aa470559030e90e50b34d93",
        "message": "WARNING: CPU: 1 PID: 27907 at fs/io_uring.c:7147 io_sq_thread_park+0xb5/0xd0 fs/io_uring.c:7147\nCPU: 1 PID: 27907 Comm: iou-sqp-27905 Not tainted 5.12.0-rc4-syzkaller #0\nRIP: 0010:io_sq_thread_park+0xb5/0xd0 fs/io_uring.c:7147\nCall Trace:\n io_ring_ctx_wait_and_kill+0x214/0x700 fs/io_uring.c:8619\n io_uring_release+0x3e/0x50 fs/io_uring.c:8646\n __fput+0x288/0x920 fs/file_table.c:280\n task_work_run+0xdd/0x1a0 kernel/task_work.c:140\n io_run_task_work fs/io_uring.c:2238 [inline]\n io_run_task_work fs/io_uring.c:2228 [inline]\n io_uring_try_cancel_requests+0x8ec/0xc60 fs/io_uring.c:8770\n io_uring_cancel_sqpoll+0x1cf/0x290 fs/io_uring.c:8974\n io_sqpoll_cancel_cb+0x87/0xb0 fs/io_uring.c:8907\n io_run_task_work_head+0x58/0xb0 fs/io_uring.c:1961\n io_sq_thread+0x3e2/0x18d0 fs/io_uring.c:6763\n ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:294\n\nMay happen that last ctx ref is killed in io_uring_cancel_sqpoll(), so\nfput callback (i.e. io_uring_release()) is enqueued through task_work,\nand run by same cancellation. As it's deeply nested we can't do parking\nor taking sqd->lock there, because its state is unclear. So avoid\nctx ejection from sqd list from io_ring_ctx_wait_and_kill() and do it\nin a clear context in io_ring_exit_work().\n\nFixes: f6d54255f423 (\"io_uring: halt SQO submission on ctx exit\")\nReported-by: syzbot+e3a3f84f5cecf61f0583@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/e90df88b8ff2cabb14a7534601d35d62ab4cb8c7.1616496707.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc5",
        "release_date": "2021-03-24 06:55:11 -0600 io_uring: do ctx sqd ejection in a clear context"
    },
    {
        "commit": "d81269fecb8ce16eb07efafc9ff5520b2a31c486",
        "message": "io_provide_buffers_prep()'s \"p->len * p->nbufs\" to sign extension\nproblems. Not a huge problem as it's only used for access_ok() and\nincreases the checked length, but better to keep typing right.\n\nReported-by: Colin Ian King <colin.king@canonical.com>\nFixes: efe68c1ca8f49 (\"io_uring: validate the full range of provided buffers for access\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Colin Ian King <colin.king@canonical.com>\nLink: https://lore.kernel.org/r/562376a39509e260d8532186a06226e56eb1f594.1616149233.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc5",
        "release_date": "2021-03-22 07:41:03 -0600 io_uring: fix provide_buffers sign extension"
    },
    {
        "commit": "b65c128f963df367a8adcfb08f5ecf8721052723",
        "message": "Don't miss to call kiocb_end_write() from __io_complete_rw() on reissue.\nShouldn't be much of a problem as the function actually does some work\nonly for ISREG, and NONBLOCK won't be reissued.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/32af9b77c5b874e1bee1a3c46396094bd969e577.1616366969.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc5",
        "release_date": "2021-03-22 07:40:14 -0600 io_uring: don't skip file_end_write() on reissue"
    },
    {
        "commit": "d07f1e8a42614cc938c9c88866d4474a5a7fee31",
        "message": "Request's io-wq work is hashed in io_prep_async_link(), so\nas trace_io_uring_queue_async_work() looks at it should follow after\nprep has been done.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/709c9f872f4d2e198c7aed9c49019ca7095dd24d.1616366969.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc5",
        "release_date": "2021-03-22 07:40:14 -0600 io_uring: correct io_queue_async_work() traces"
    },
    {
        "commit": "0b8cfa974dfc964e6382c9e25fa6c1bdac6ef499",
        "message": "Linus correctly points out that this is both unnecessary and generates\nmuch worse code on some archs as going from current to thread_info is\nactually backwards - and obviously just wasteful, since the thread_info\nis what we care about.\n\nSince io_uring only operates on current for these operations, just use\ntest_thread_flag() instead. For io-wq, we can further simplify and use\ntracehook_notify_signal() to handle the TIF_NOTIFY_SIGNAL work and clear\nthe flag. The latter isn't an actual bug right now, but it may very well\nbe in the future if we place other work items under TIF_NOTIFY_SIGNAL.\n\nReported-by: Linus Torvalds <torvalds@linux-foundation.org>\nLink: https://lore.kernel.org/io-uring/CAHk-=wgYhNck33YHKZ14mFB5MzTTk8gqXHcfj=RWTAXKwgQJgg@mail.gmail.com/\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc5",
        "release_date": "2021-03-21 14:16:08 -0600 io_uring: don't use {test,clear}_tsk_thread_flag() for current"
    },
    {
        "commit": "2c41fab1c60b02626c8153a1806a7a1e5d62aaf1",
        "message": "Pull io_uring followup fixes from Jens Axboe:\n\n - The SIGSTOP change from Eric, so we properly ignore that for\n   PF_IO_WORKER threads.\n\n - Disallow sending signals to PF_IO_WORKER threads in general, we're\n   not interested in having them funnel back to the io_uring owning\n   task.\n\n - Stable fix from Stefan, ensuring we properly break links for short\n   send/sendmsg recv/recvmsg if MSG_WAITALL is set.\n\n - Catch and loop when needing to run task_work before a PF_IO_WORKER\n   threads goes to sleep.\n\n* tag 'io_uring-5.12-2021-03-21' of git://git.kernel.dk/linux-block:\n  io_uring: call req_set_fail_links() on short send[msg]()/recv[msg]() with MSG_WAITALL\n  io-wq: ensure task is running before processing task_work\n  signal: don't allow STOP on PF_IO_WORKER threads\n  signal: don't allow sending any signals to PF_IO_WORKER threads",
        "kernel_version": "v5.12-rc4",
        "release_date": "2021-03-21 12:25:54 -0700 Merge tag 'io_uring-5.12-2021-03-21' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "0031275d119efe16711cd93519b595e6f9b4b330",
        "message": "Without that it's not safe to use them in a linked combination with\nothers.\n\nNow combinations like IORING_OP_SENDMSG followed by IORING_OP_SPLICE\nshould be possible.\n\nWe already handle short reads and writes for the following opcodes:\n\n- IORING_OP_READV\n- IORING_OP_READ_FIXED\n- IORING_OP_READ\n- IORING_OP_WRITEV\n- IORING_OP_WRITE_FIXED\n- IORING_OP_WRITE\n- IORING_OP_SPLICE\n- IORING_OP_TEE\n\nNow we have it for these as well:\n\n- IORING_OP_SENDMSG\n- IORING_OP_SEND\n- IORING_OP_RECVMSG\n- IORING_OP_RECV\n\nFor IORING_OP_RECVMSG we also check for the MSG_TRUNC and MSG_CTRUNC\nflags in order to call req_set_fail_links().\n\nThere might be applications arround depending on the behavior\nthat even short send[msg]()/recv[msg]() retuns continue an\nIOSQE_IO_LINK chain.\n\nIt's very unlikely that such applications pass in MSG_WAITALL,\nwhich is only defined in 'man 2 recvmsg', but not in 'man 2 sendmsg'.\n\nIt's expected that the low level sock_sendmsg() call just ignores\nMSG_WAITALL, as MSG_ZEROCOPY is also ignored without explicitly set\nSO_ZEROCOPY.\n\nWe also expect the caller to know about the implicit truncation to\nMAX_RW_COUNT, which we don't detect.\n\ncc: netdev@vger.kernel.org\nLink: https://lore.kernel.org/r/c4e1a4cc0d905314f4d5dc567e65a7b09621aab3.1615908477.git.metze@samba.org\nSigned-off-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc4",
        "release_date": "2021-03-21 09:41:14 -0600 io_uring: call req_set_fail_links() on short send[msg]()/recv[msg]() with MSG_WAITALL"
    },
    {
        "commit": "0ada2dad8bf39857f25e6ecbf68bb1664ca1ee5b",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Quieter week this time, which was both expected and desired. About\n  half of the below is fixes for this release, the other half are just\n  fixes in general. In detail:\n\n   - Fix the freezing of IO threads, by making the freezer not send them\n     fake signals. Make them freezable by default.\n\n   - Like we did for personalities, move the buffer IDR to xarray. Kills\n     some code and avoids a use-after-free on teardown.\n\n   - SQPOLL cleanups and fixes (Pavel)\n\n   - Fix linked timeout race (Pavel)\n\n   - Fix potential completion post use-after-free (Pavel)\n\n   - Cleanup and move internal structures outside of general kernel view\n     (Stefan)\n\n   - Use MSG_SIGNAL for send/recv from io_uring (Stefan)\"\n\n* tag 'io_uring-5.12-2021-03-19' of git://git.kernel.dk/linux-block:\n  io_uring: don't leak creds on SQO attach error\n  io_uring: use typesafe pointers in io_uring_task\n  io_uring: remove structures from include/linux/io_uring.h\n  io_uring: imply MSG_NOSIGNAL for send[msg]()/recv[msg]() calls\n  io_uring: fix sqpoll cancellation via task_work\n  io_uring: add generic callback_head helpers\n  io_uring: fix concurrent parking\n  io_uring: halt SQO submission on ctx exit\n  io_uring: replace sqd rw_semaphore with mutex\n  io_uring: fix complete_post use ctx after free\n  io_uring: fix ->flags races by linked timeouts\n  io_uring: convert io_buffer_idr to XArray\n  io_uring: allow IO worker threads to be frozen\n  kernel: freezer should treat PF_IO_WORKER like PF_KTHREAD for freezing",
        "kernel_version": "v5.12-rc4",
        "release_date": "2021-03-19 17:01:09 -0700 Merge tag 'io_uring-5.12-2021-03-19' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "de75a3d3f5a14c9ab3c4883de3471d3c92a8ee78",
        "message": "Attaching to already dead/dying SQPOLL task is disallowed in\nio_sq_offload_create(), but cleanup is hand coded by calling\nio_put_sq_data()/etc., that miss to put ctx->sq_creds.\n\nDefer everything to error-path io_sq_thread_finish(), adding\nctx->sqd_list in the error case as well as finish will handle it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc4",
        "release_date": "2021-03-18 09:44:35 -0600 io_uring: don't leak creds on SQO attach error"
    },
    {
        "commit": "ee53fb2b197b72b126ca0387ae636da75d969428",
        "message": "Signed-off-by: Stefan Metzmacher <metze@samba.org>\nLink: https://lore.kernel.org/r/ce2a598e66e48347bb04afbaf2acc67c0cc7971a.1615809009.git.metze@samba.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc4",
        "release_date": "2021-03-18 09:44:35 -0600 io_uring: use typesafe pointers in io_uring_task"
    },
    {
        "commit": "53e043b2b432ef2294efec04dd8a88d96c024624",
        "message": "Link: https://lore.kernel.org/r/8c1d14f3748105f4caeda01716d47af2fa41d11c.1615809009.git.metze@samba.org\nSigned-off-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc4",
        "release_date": "2021-03-18 09:44:35 -0600 io_uring: remove structures from include/linux/io_uring.h"
    },
    {
        "commit": "76cd979f4f38a27df22efb5773a0d567181a9392",
        "message": "We never want to generate any SIGPIPE, -EPIPE only is much better.\n\nSigned-off-by: Stefan Metzmacher <metze@samba.org>\nLink: https://lore.kernel.org/r/38961085c3ec49fd21550c7788f214d1ff02d2d4.1615908477.git.metze@samba.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc4",
        "release_date": "2021-03-18 09:44:06 -0600 io_uring: imply MSG_NOSIGNAL for send[msg]()/recv[msg]() calls"
    },
    {
        "commit": "b7f5a0bfe2061b2c7b2164de06fa4072d7373a45",
        "message": "Running sqpoll cancellations via task_work_run() is a bad idea because\nit depends on other task works to be run, but those may be locked in\ncurrently running task_work_run() because of how it's (splicing the list\nin batches).\n\nEnqueue and run them through a separate callback head, namely\nstruct io_sq_data::park_task_work. As a nice bonus we now precisely\ncontrol where it's run, that's much safer than guessing where it can\nhappen as it was before.\n\nReported-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc4",
        "release_date": "2021-03-15 09:32:40 -0600 io_uring: fix sqpoll cancellation via task_work"
    },
    {
        "commit": "9b46571142e47503ed4f3ae3be5ed3968d8cb9cc",
        "message": "We already have helpers to run/add callback_head but taking ctx and\nworking with ctx->exit_task_work. Extract generic versions of them\nimplemented in terms of struct callback_head, it will be used later.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc4",
        "release_date": "2021-03-15 09:32:40 -0600 io_uring: add generic callback_head helpers"
    },
    {
        "commit": "9e138a48345427fa42f6076396ea069cebf3c08f",
        "message": "If io_sq_thread_park() of one task got rescheduled right after\nset_bit(), before it gets back to mutex_lock() there can happen\npark()/unpark() by another task with SQPOLL locking again and\ncontinuing running never seeing that first set_bit(SHOULD_PARK),\nso won't even try to put the mutex down for parking.\n\nIt will get parked eventually when SQPOLL drops the lock for reschedule,\nbut may be problematic and will get in the way of further fixes.\n\nAccount number of tasks waiting for parking with a new atomic variable\npark_pending and adjust SHOULD_PARK accordingly. It doesn't entirely\nreplaces SHOULD_PARK bit with this atomic var because it's convenient\nto have it as a bit in the state and will help to do optimisations\nlater.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc4",
        "release_date": "2021-03-15 09:32:40 -0600 io_uring: fix concurrent parking"
    },
    {
        "commit": "f6d54255f4235448d4bbe442362d4caa62da97d5",
        "message": "io_sq_thread_finish() is called in io_ring_ctx_free(), so SQPOLL task is\npotentially running submitting new requests. It's not a disaster because\nof using a \"try\" variant of percpu_ref_get, but is far from nice.\n\nRemove ctx from the sqd ctx list earlier, before cancellation loop, so\nSQPOLL can't find it and so won't submit new requests.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc4",
        "release_date": "2021-03-15 09:32:40 -0600 io_uring: halt SQO submission on ctx exit"
    },
    {
        "commit": "09a6f4efaa6536e760385f949e24078fd78305ad",
        "message": "The only user of read-locking of sqd->rw_lock is sq_thread itself, which\nis by definition alone, so we don't really need rw_semaphore, but mutex\nwill do. Replace it with a mutex, and kill read-to-write upgrading and\nextra task_work handling in io_sq_thread().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc4",
        "release_date": "2021-03-15 09:32:40 -0600 io_uring: replace sqd rw_semaphore with mutex"
    },
    {
        "commit": "180f829fe4026bd192447d261e712b6cb84f6202",
        "message": "If io_req_complete_post() put not a final ref, we can't rely on the\nrequest's ctx ref, and so ctx may potentially be freed while\ncomplete_post() is in io_cqring_ev_posted()/etc.\n\nIn that case get an additional ctx reference, and put it in the end, so\nprotecting following io_cqring_ev_posted(). And also prolong ctx\nlifetime until spin_unlock happens, as we do with mutexes, so added\npercpu_ref_get() doesn't race with ctx free.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc4",
        "release_date": "2021-03-15 09:32:24 -0600 io_uring: fix complete_post use ctx after free"
    },
    {
        "commit": "efe814a471e0e58f28f1efaf430c8784a4f36626",
        "message": "It's racy to modify req->flags from a not owning context, e.g. linked\ntimeout calling req_set_fail_links() for the master request might race\nwith that request setting/clearing flags while being executed\nconcurrently. Just remove req_set_fail_links(prev) from\nio_link_timeout_fn(), io_async_find_and_cancel() and functions down the\nline take care of setting the fail bit.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc4",
        "release_date": "2021-03-15 09:31:19 -0600 io_uring: fix ->flags races by linked timeouts"
    },
    {
        "commit": "9e15c3a0ced5a61f320b989072c24983cb1620c1",
        "message": "Like we did for the personality idr, convert the IO buffer idr to use\nXArray. This avoids a use-after-free on removal of entries, since idr\ndoesn't like doing so from inside an iterator, and it nicely reduces\nthe amount of code we need to support this feature.\n\nFixes: 5a2e745d4d43 (\"io_uring: buffer registration infrastructure\")\nCc: stable@vger.kernel.org\nCc: Matthew Wilcox <willy@infradead.org>\nCc: yangerkun <yangerkun@huawei.com>\nReported-by: Hulk Robot <hulkci@huawei.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc4",
        "release_date": "2021-03-14 09:56:14 -0600 io_uring: convert io_buffer_idr to XArray"
    },
    {
        "commit": "16efa4fce3b7af17bb45d635c3e89992d721e0f3",
        "message": "With the freezer using the proper signaling to notify us of when it's\ntime to freeze a thread, we can re-enable normal freezer usage for the\nIO threads. Ensure that SQPOLL, io-wq, and the io-wq manager call\ntry_to_freeze() appropriately, and remove the default setting of\nPF_NOFREEZE from create_io_thread().\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc4",
        "release_date": "2021-03-12 20:26:13 -0700 io_uring: allow IO worker threads to be frozen"
    },
    {
        "commit": "9278be92f22979a026a68206e226722138c9443d",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Not quite as small this week as I had hoped, but at least this should\n  be the end of it. All the little known issues have been ironed out -\n  most of it little stuff, but cancelations being the bigger part. Only\n  minor tweaks and/or regular fixes expected beyond this point.\n\n   - Fix the creds tracking for async (io-wq and SQPOLL)\n\n   - Various SQPOLL fixes related to parking, sharing, forking, IOPOLL,\n     completions, and life times. Much simpler now.\n\n   - Make IO threads unfreezable by default, on account of a bug report\n     that had them spinning on resume. Honestly not quite sure why\n     thawing leaves us with a perpetual signal pending (causing the\n     spin), but for now make them unfreezable like there were in 5.11\n     and prior.\n\n   - Move personality_idr to xarray, solving a use-after-free related to\n     removing an entry from the iterator callback. Buffer idr needs the\n     same treatment.\n\n   - Re-org around and task vs context tracking, enabling the fixing of\n     cancelations, and then cancelation fixes on top.\n\n   - Various little bits of cleanups and hardening, and removal of now\n     dead parts\"\n\n* tag 'io_uring-5.12-2021-03-12' of git://git.kernel.dk/linux-block: (34 commits)\n  io_uring: fix OP_ASYNC_CANCEL across tasks\n  io_uring: cancel sqpoll via task_work\n  io_uring: prevent racy sqd->thread checks\n  io_uring: remove useless ->startup completion\n  io_uring: cancel deferred requests in try_cancel\n  io_uring: perform IOPOLL reaping if canceler is thread itself\n  io_uring: force creation of separate context for ATTACH_WQ and non-threads\n  io_uring: remove indirect ctx into sqo injection\n  io_uring: fix invalid ctx->sq_thread_idle\n  kernel: make IO threads unfreezable by default\n  io_uring: always wait for sqd exited when stopping SQPOLL thread\n  io_uring: remove unneeded variable 'ret'\n  io_uring: move all io_kiocb init early in io_init_req()\n  io-wq: fix ref leak for req in case of exit cancelations\n  io_uring: fix complete_post races for linked req\n  io_uring: add io_disarm_next() helper\n  io_uring: fix io_sq_offload_create error handling\n  io-wq: remove unused 'user' member of io_wq\n  io_uring: Convert personality_idr to XArray\n  io_uring: clean R_DISABLED startup mess\n  ...",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-12 13:13:57 -0800 Merge tag 'io_uring-5.12-2021-03-12' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "58f99373834151e1ca7edc49bc5578d9d40db099",
        "message": "IORING_OP_ASYNC_CANCEL tries io-wq cancellation only for current task.\nIf it fails go over tctx_list and try it out for every single tctx.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-12 09:42:56 -0700 io_uring: fix OP_ASYNC_CANCEL across tasks"
    },
    {
        "commit": "521d6a737a31c08dbab204a95cd4fb5bee725f0f",
        "message": "1) The first problem is io_uring_cancel_sqpoll() ->\nio_uring_cancel_task_requests() basically doing park(); park(); and so\nhanging.\n\n2) Another one is more subtle, when the master task is doing cancellations,\nbut SQPOLL task submits in-between the end of the cancellation but\nbefore finish() requests taking a ref to the ctx, and so eternally\nlocking it up.\n\n3) Yet another is a dying SQPOLL task doing io_uring_cancel_sqpoll() and\nsame io_uring_cancel_sqpoll() from the owner task, they race for\ntctx->wait events. And there probably more of them.\n\nInstead do SQPOLL cancellations from within SQPOLL task context via\ntask_work, see io_sqpoll_cancel_sync(). With that we don't need temporal\npark()/unpark() during cancellation, which is ugly, subtle and anyway\ndoesn't allow to do io_run_task_work() properly.\n\nio_uring_cancel_sqpoll() is called only from SQPOLL task context and\nunder sqd locking, so all parking is removed from there. And so,\nio_sq_thread_[un]park() and io_sq_thread_stop() are not used now by\nSQPOLL task, and that spare us from some headache.\n\nAlso remove ctx->sqd_list early to avoid 2). And kill tctx->sqpoll,\nwhich is not used anymore.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-12 09:42:55 -0700 io_uring: cancel sqpoll via task_work"
    },
    {
        "commit": "26984fbf3ad9d1c1fb56a0c1e0cdf9fa3b806f0c",
        "message": "SQPOLL thread to which we're trying to attach may be going away, it's\nnot nice but a more serious problem is if io_sq_offload_create() sees\nsqd->thread==NULL, and tries to init it with a new thread. There are\ntons of ways it can be exploited or fail.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-12 09:42:53 -0700 io_uring: prevent racy sqd->thread checks"
    },
    {
        "commit": "0df8ea602b3fe80819a34361027ad40485e78909",
        "message": "We always do complete(&sqd->startup) almost right after sqd->thread\ncreation, either in the success path or in io_sq_thread_finish(). It's\nspecifically created not started for us to be able to set some stuff\nlike sqd->thread and io_uring_alloc_task_context() before following\nright after wake_up_new_task().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-12 07:23:01 -0700 io_uring: remove useless ->startup completion"
    },
    {
        "commit": "e1915f76a8981f0a750cf56515df42582a37c4b0",
        "message": "As io_uring_cancel_files() and others let SQO to run between\nio_uring_try_cancel_requests(), SQO may generate new deferred requests,\nso it's safer to try to cancel them in it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-12 07:23:00 -0700 io_uring: cancel deferred requests in try_cancel"
    },
    {
        "commit": "d052d1d685f5125249ab4ff887562c88ba959638",
        "message": "We bypass IOPOLL completion polling (and reaping) for the SQPOLL thread,\nbut if it's the thread itself invoking cancelations, then we still need\nto perform it or no one will.\n\nFixes: 9936c7c2bc76 (\"io_uring: deduplicate core cancellations sequence\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-11 10:49:20 -0700 io_uring: perform IOPOLL reaping if canceler is thread itself"
    },
    {
        "commit": "5c2469e0a22e035d52f3ba768151cc75e3d4a1cd",
        "message": "Earlier kernels had SQPOLL threads that could share across anything, as\nwe grabbed the context we needed on a per-ring basis. This is no longer\nthe case, so only allow attaching directly if we're in the same thread\ngroup. That is the common use case. For non-group tasks, just setup a\nnew context and thread as we would've done if sharing wasn't set. This\nisn't 100% ideal in terms of CPU utilization for the forked and share\ncase, but hopefully that isn't much of a concern. If it is, there are\nplans in motion for how to improve that. Most importantly, we want to\navoid app side regressions where sharing worked before and now doesn't.\nWith this patch, functionality is equivalent to previous kernels that\nsupported IORING_SETUP_ATTACH_WQ with SQPOLL.\n\nReported-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-11 10:17:56 -0700 io_uring: force creation of separate context for ATTACH_WQ and non-threads"
    },
    {
        "commit": "7d41e8543d809c3c900d1212d6ea887eb284b69a",
        "message": "We use ->ctx_new_list to notify sqo about new ctx pending, then sqo\nshould stop and splice it to its sqd->ctx_list, paired with\n->sq_thread_comp.\n\nThe last one is broken because nobody reinitialises it, and trying to\nfix it would only add more complexity and bugs. And the first isn't\nreally needed as is done under park(), that protects from races well.\nAdd ctx into sqd->ctx_list directly (under park()), it's much simpler\nand allows to kill both, ctx_new_list and sq_thread_comp.\n\nnote: apparently there is no real problem at the moment, because\nsq_thread_comp is used only by io_sq_thread_finish() followed by\nparking, where list_del(&ctx->sqd_list) removes it well regardless\nwhether it's in the new or the active list.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-10 07:30:32 -0700 io_uring: remove indirect ctx into sqo injection"
    },
    {
        "commit": "78d7f6ba82edb7f8763390982be29051c4216772",
        "message": "We have to set ctx->sq_thread_idle before adding a ring to an SQ task,\notherwise sqd races for seeing zero and accounting it as such.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-10 07:29:59 -0700 io_uring: fix invalid ctx->sq_thread_idle"
    },
    {
        "commit": "e22bc9b481a90d7898984ea17621f04a653e2cd1",
        "message": "The io-wq threads were already marked as no-freeze, but the manager was\nnot. On resume, we perpetually have signal_pending() being true, and\nhence the manager will loop and spin 100% of the time.\n\nJust mark the tasks created by create_io_thread() as PF_NOFREEZE by\ndefault, and remove any knowledge of it in io-wq and io_uring.\n\nReported-by: Kevin Locke <kevin@kevinlocke.name>\nTested-by: Kevin Locke <kevin@kevinlocke.name>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-10 07:28:43 -0700 kernel: make IO threads unfreezable by default"
    },
    {
        "commit": "e8f98f24549d62cc54bf608c815904a56d4437bc",
        "message": "We have a tiny race where io_put_sq_data() calls io_sq_thead_stop()\nand finds the thread gone, but the thread has indeed not fully\nexited or called complete() yet. Close it up by always having\nio_sq_thread_stop() wait on completion of the exit event.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-10 07:28:43 -0700 io_uring: always wait for sqd exited when stopping SQPOLL thread"
    },
    {
        "commit": "5199328a0d415b3e372633096b1b92f36b8ac9e5",
        "message": "Fix the following coccicheck warning:\n./fs/io_uring.c:8984:5-8: Unneeded variable: \"ret\". Return \"0\" on line\n8998\n\nReported-by: Abaci Robot <abaci@linux.alibaba.com>\nSigned-off-by: Yang Li <yang.lee@linux.alibaba.com>\nLink: https://lore.kernel.org/r/1615271441-33649-1-git-send-email-yang.lee@linux.alibaba.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-10 07:28:43 -0700 io_uring: remove unneeded variable 'ret'"
    },
    {
        "commit": "93e68e036c2fc1ce18e784418e4e19975a5882b4",
        "message": "If we hit an error path in the function, make sure that the io_kiocb is\nfully initialized at that point so that freeing the request always sees\na valid state.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-10 07:28:43 -0700 io_uring: move all io_kiocb init early in io_init_req()"
    },
    {
        "commit": "7a612350a989866510dc5c874fd8ffe1f37555d2",
        "message": "Calling io_queue_next() after spin_unlock in io_req_complete_post()\nraces with the other side extracting and reusing this request. Hand\ncoded parts of io_req_find_next() considering that io_disarm_next()\nand io_req_task_queue() have (and safe) to be called with\ncompletion_lock held.\n\nIt already does io_commit_cqring() and io_cqring_ev_posted(), so just\nreuse it for post io_disarm_next().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/5672a62f3150ee7c55849f40c0037655c4f2840f.1615250156.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-10 07:28:42 -0700 io_uring: fix complete_post races for linked req"
    },
    {
        "commit": "33cc89a9fc248a486857381584cc6b67d9405fab",
        "message": "A preparation patch placing all preparations before extracting a next\nrequest into a separate helper io_disarm_next().\n\nAlso, don't spuriously do ev_posted in a rare case where REQ_F_FAIL_LINK\nis set but there are no requests linked (i.e. after cancelling a linked\ntimeout or setting IOSQE_IO_LINK on a last request of a submission\nbatch).\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/44ecff68d6b47e1c4e6b891bdde1ddc08cfc3590.1615250156.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-10 07:28:42 -0700 io_uring: add io_disarm_next() helper"
    },
    {
        "commit": "97a73a0f9fbfb2be682fd037814576dbfa0e0da8",
        "message": "Don't set IO_SQ_THREAD_SHOULD_STOP when io_sq_offload_create() has\nfailed on io_uring_alloc_task_context() but leave everything to\nio_sq_thread_finish(), because currently io_sq_thread_finish()\nhangs on trying to park it. That's great it stalls there, because\notherwise the following io_sq_thread_stop() would be skipped on\nIO_SQ_THREAD_SHOULD_STOP check and the sqo would race for sqd with\nfreeing ctx.\n\nA simple error injection gives something like this.\n\n[  245.463955] INFO: task sqpoll-test-hang:523 blocked for more than 122 seconds.\n[  245.463983] Call Trace:\n[  245.463990]  __schedule+0x36b/0x950\n[  245.464005]  schedule+0x68/0xe0\n[  245.464013]  schedule_timeout+0x209/0x2a0\n[  245.464032]  wait_for_completion+0x8b/0xf0\n[  245.464043]  io_sq_thread_finish+0x44/0x1a0\n[  245.464049]  io_uring_setup+0x9ea/0xc80\n[  245.464058]  __x64_sys_io_uring_setup+0x16/0x20\n[  245.464064]  do_syscall_64+0x38/0x50\n[  245.464073]  entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-10 07:28:42 -0700 io_uring: fix io_sq_offload_create error handling"
    },
    {
        "commit": "61cf93700fe6359552848ed5e3becba6cd760efa",
        "message": "You can't call idr_remove() from within a idr_for_each() callback,\nbut you can call xa_erase() from an xa_for_each() loop, so switch the\nentire personality_idr from the IDR to the XArray.  This manifests as a\nuse-after-free as idr_for_each() attempts to walk the rest of the node\nafter removing the last entry from it.\n\nFixes: 071698e13ac6 (\"io_uring: allow registering credentials\")\nCc: stable@vger.kernel.org # 5.6+\nReported-by: yangerkun <yangerkun@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\n[Pavel: rebased (creds load was moved into io_init_req())]\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/7ccff36e1375f2b0ebf73d957f037b43becc0dde.1615212806.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-10 07:28:42 -0700 io_uring: Convert personality_idr to XArray"
    },
    {
        "commit": "0298ef969a110ca03654f0cea9b50e3f3b331acc",
        "message": "There are enough of problems with IORING_SETUP_R_DISABLED, including the\nburden of checking and kicking off the SQO task all over the codebase --\nfor exit/cancel/etc.\n\nRework it, always start the thread but don't do submit unless the flag\nis gone, that's much easier.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-10 07:28:42 -0700 io_uring: clean R_DISABLED startup mess"
    },
    {
        "commit": "f458dd8441e56d122ddf1d8e2af0b6ee62f52af9",
        "message": "io-wq now is per-task, so cancellations now should match against\nrequest's ctx.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-10 07:28:42 -0700 io_uring: fix unrelated ctx reqs cancellation"
    },
    {
        "commit": "05962f95f9ac7af25fea037ef51b37c0eccb5590",
        "message": "We keep running into weird dependency issues between the sqd lock and\nthe parking state. Disentangle the SQPOLL thread from the last bits of\nthe kthread parking inheritance, and just replace the parking state,\nand two associated locks, with a single rw mutex. The SQPOLL thread\nkeeps the mutex for read all the time, except if someone has marked us\nneeding to park. Then we drop/re-acquire and try again.\n\nThis greatly simplifies the parking state machine (by just getting rid\nof it), and makes it a lot more obvious how it works - if you need to\nmodify the ctx list, then you simply park the thread which will grab\nthe lock for writing.\n\nFold in fix from Hillf Danton on not setting STOP on a fatal signal.\n\nFixes: e54945ae947f (\"io_uring: SQPOLL stop error handling fixes\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-10 07:28:22 -0700 io_uring: SQPOLL parking fixes"
    },
    {
        "commit": "041474885e9707a38fad081abe30159eb6d463f9",
        "message": "This brings the behavior back in line with what 5.11 and earlier did,\nand this is no longer needed with the improved handling of creds\nnot needing to do unshare().\n\nSigned-off-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-07 14:12:43 -0700 io_uring: kill io_sq_thread_fork() and return -EOWNERDEAD if the sq_thread is gone"
    },
    {
        "commit": "7c30f36a98ae488741178d69662e4f2baa53e7f6",
        "message": "With IORING_SETUP_ATTACH_WQ we should let __io_sq_thread() use the\ninitial creds from each ctx.\n\nSigned-off-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-07 14:12:43 -0700 io_uring: run __io_sq_thread() with the initial creds from io_uring_setup()"
    },
    {
        "commit": "1b00764f09b6912d25e188d972a7764a457926ba",
        "message": "io_ring_exit_work() have to cancel all requests, including those staying\nin io-wq, however it tries only cancellation of current tctx, which is\nNULL. If we've got task==NULL, use the ctx-to-tctx map to go over all\ntctx/io-wq and try cancellations on them.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-07 14:12:43 -0700 io_uring: cancel reqs of all iowq's on ring exit"
    },
    {
        "commit": "b5bb3a24f69da92e0ec2a301452364333e45be03",
        "message": "We use system_unbound_wq to run io_ring_exit_work(), so it's hard to\nmonitor whether removal hang or not. Add WARN_ONCE to catch hangs.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-07 14:12:43 -0700 io_uring: warn when ring exit takes too long"
    },
    {
        "commit": "baf186c4d345f5a105e63df01100936ad622f369",
        "message": "We don't use task file notes anymore, and no need left in indexing\ntask->io_uring->xa by file, and replace it with ctx. It's better\ndesign-wise, especially since we keep a dangling file, and so have to\nkeep an eye on not dereferencing it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-07 14:12:43 -0700 io_uring: index io_uring->xa by ctx not file"
    },
    {
        "commit": "eebd2e37e662617a6b8041db75205f0a262ce870",
        "message": "With ->flush() gone we're now leaving all uring file notes until the\ntask dies/execs, so the ctx will not be freed until all tasks that have\never submit a request die. It was nicer with flush but not much, we\ncould have locked as described ctx in many cases.\n\nNow we guarantee that ctx outlives all tctx in a sense that\nio_ring_exit_work() waits for all tctxs to drop their corresponding\nenties in ->xa, and ctx won't go away until then. Hence, additional\nio_uring file reference (a.k.a. task file notes) are not needed anymore.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-07 14:12:43 -0700 io_uring: don't take task ring-file notes"
    },
    {
        "commit": "d56d938b4bef3e1421a42023cdcd6e13c1f50831",
        "message": "Another preparation patch. When full quiesce is done on ctx exit, use\ntask_work infra to remove corresponding to the ctx io_uring->xa entries.\nFor that we use the back tctx map. Also use ->in_idle to prevent\nremoving it while we traversing ->xa on cancellation, just ignore it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-07 14:12:43 -0700 io_uring: do ctx initiated file note removal"
    },
    {
        "commit": "13bf43f5f4739739751c0049a1582610c283bdde",
        "message": "For each pair tcxt-ctx create an object and chain it into ctx, so we\nhave a way to traverse all tctx that are using current ctx. Preparation\npatch, will be used later.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-07 14:12:43 -0700 io_uring: introduce ctx to tctx back map"
    },
    {
        "commit": "2941267bd3dad018de1d51fe2cd996b7bc1e5a5d",
        "message": "Rework io_uring_del_task_file(), so it accepts an index to delete, and\nit's not necessarily have to be in the ->xa. Infer file from xa_erase()\nto maintain a single origin of truth.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-07 14:12:43 -0700 io_uring: make del_task_file more forgiving"
    },
    {
        "commit": "886d0137f104a440d9dfa1d16efc1db06c9a2c02",
        "message": "Ran into a use-after-free on the main io-wq struct, wq. It has a worker\nref and completion event, but the manager itself isn't holding a\nreference. This can lead to a race where the manager thinks there are\nno workers and exits, but a worker is being added. That leads to the\nfollowing trace:\n\nBUG: KASAN: use-after-free in io_wqe_worker+0x4c0/0x5e0\nRead of size 8 at addr ffff888108baa8a0 by task iou-wrk-3080422/3080425\n\nCPU: 5 PID: 3080425 Comm: iou-wrk-3080422 Not tainted 5.12.0-rc1+ #110\nHardware name: Micro-Star International Co., Ltd. MS-7C60/TRX40 PRO 10G (MS-7C60), BIOS 1.60 05/13/2020\nCall Trace:\n dump_stack+0x90/0xbe\n print_address_description.constprop.0+0x67/0x28d\n ? io_wqe_worker+0x4c0/0x5e0\n kasan_report.cold+0x7b/0xd4\n ? io_wqe_worker+0x4c0/0x5e0\n __asan_load8+0x6d/0xa0\n io_wqe_worker+0x4c0/0x5e0\n ? io_worker_handle_work+0xc00/0xc00\n ? recalc_sigpending+0xe5/0x120\n ? io_worker_handle_work+0xc00/0xc00\n ? io_worker_handle_work+0xc00/0xc00\n ret_from_fork+0x1f/0x30\n\nAllocated by task 3080422:\n kasan_save_stack+0x23/0x60\n __kasan_kmalloc+0x80/0xa0\n kmem_cache_alloc_node_trace+0xa0/0x480\n io_wq_create+0x3b5/0x600\n io_uring_alloc_task_context+0x13c/0x380\n io_uring_add_task_file+0x109/0x140\n __x64_sys_io_uring_enter+0x45f/0x660\n do_syscall_64+0x32/0x80\n entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nFreed by task 3080422:\n kasan_save_stack+0x23/0x60\n kasan_set_track+0x20/0x40\n kasan_set_free_info+0x24/0x40\n __kasan_slab_free+0xe8/0x120\n kfree+0xa8/0x400\n io_wq_put+0x14a/0x220\n io_wq_put_and_exit+0x9a/0xc0\n io_uring_clean_tctx+0x101/0x140\n __io_uring_files_cancel+0x36e/0x3c0\n do_exit+0x169/0x1340\n __x64_sys_exit+0x34/0x40\n do_syscall_64+0x32/0x80\n entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nHave the manager itself hold a reference, and now both drop points drop\nand complete if we hit zero, and the manager can unconditionally do a\nwait_for_completion() instead of having a race between reading the ref\ncount and waiting if it was non-zero.\n\nFixes: fb3a1f6c745c (\"io-wq: have manager wait for all workers to exit\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc3",
        "release_date": "2021-03-06 10:57:01 -0700 io-wq: fix race in freeing 'wq' and worker access"
    },
    {
        "commit": "f292e8730a349577aaf13635399b39a50b8f5910",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A bit of a mix between fallout from the worker change, cleanups and\n  reductions now possible from that change, and fixes in general. In\n  detail:\n\n   - Fully serialize manager and worker creation, fixing races due to\n     that.\n\n   - Clean up some naming that had gone stale.\n\n   - SQPOLL fixes.\n\n   - Fix race condition around task_work rework that went into this\n     merge window.\n\n   - Implement unshare. Used for when the original task does unshare(2)\n     or setuid/seteuid and friends, drops the original workers and forks\n     new ones.\n\n   - Drop the only remaining piece of state shuffling we had left, which\n     was cred. Move it into issue instead, and we can drop all of that\n     code too.\n\n   - Kill f_op->flush() usage. That was such a nasty hack that we had\n     out of necessity, we no longer need it.\n\n   - Following from ->flush() removal, we can also drop various bits of\n     ctx state related to SQPOLL and cancelations.\n\n   - Fix an issue with IOPOLL retry, which originally was fallout from a\n     filemap change (removing iov_iter_revert()), but uncovered an issue\n     with iovec re-import too late.\n\n   - Fix an issue with system suspend.\n\n   - Use xchg() for fallback work, instead of cmpxchg().\n\n   - Properly destroy io-wq on exec.\n\n   - Add create_io_thread() core helper, and use that in io-wq and\n     io_uring. This allows us to remove various silly completion events\n     related to thread setup.\n\n   - A few error handling fixes.\n\n  This should be the grunt of fixes necessary for the new workers, next\n  week should be quieter. We've got a pending series from Pavel on\n  cancelations, and how tasks and rings are indexed. Outside of that,\n  should just be minor fixes. Even with these fixes, we're still killing\n  a net ~80 lines\"\n\n* tag 'io_uring-5.12-2021-03-05' of git://git.kernel.dk/linux-block: (41 commits)\n  io_uring: don't restrict issue_flags for io_openat\n  io_uring: make SQPOLL thread parking saner\n  io-wq: kill hashed waitqueue before manager exits\n  io_uring: clear IOCB_WAITQ for non -EIOCBQUEUED return\n  io_uring: don't keep looping for more events if we can't flush overflow\n  io_uring: move to using create_io_thread()\n  kernel: provide create_io_thread() helper\n  io_uring: reliably cancel linked timeouts\n  io_uring: cancel-match based on flags\n  io-wq: ensure all pending work is canceled on exit\n  io_uring: ensure that threads freeze on suspend\n  io_uring: remove extra in_idle wake up\n  io_uring: inline __io_queue_async_work()\n  io_uring: inline io_req_clean_work()\n  io_uring: choose right tctx->io_wq for try cancel\n  io_uring: fix -EAGAIN retry with IOPOLL\n  io-wq: fix error path leak of buffered write hash map\n  io_uring: remove sqo_task\n  io_uring: kill sqo_dead and sqo submission halting\n  io_uring: ignore double poll add on the same waitqueue head\n  ...",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-05 12:44:43 -0800 Merge tag 'io_uring-5.12-2021-03-05' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "e45cff58858883290c98f65d409839a7295c95f3",
        "message": "45d189c606292 (\"io_uring: replace force_nonblock with flags\") did\nsomething strange for io_openat() slicing all issue_flags but\nIO_URING_F_NONBLOCK. Not a bug for now, but better to just forward the\nflags.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-05 09:52:29 -0700 io_uring: don't restrict issue_flags for io_openat"
    },
    {
        "commit": "86e0d6766cf909813474857bd22fdc04c97c0b36",
        "message": "We have this weird true/false return from parking, and then some of the\ncallers decide to look at that. It can lead to unbalanced parks and\nsqd locking. Have the callers check the thread status once it's parked.\nWe know we have the lock at that point, so it's either valid or it's NULL.\n\nFix race with parking on thread exit. We need to be careful here with\nordering of the sdq->lock and the IO_SQ_THREAD_SHOULD_PARK bit.\n\nRename sqd->completion to sqd->parked to reflect that this is the only\nthing this completion event doesn.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-05 08:44:39 -0700 io_uring: make SQPOLL thread parking saner"
    },
    {
        "commit": "b5b0ecb736f1ce1e68eb50613c0cfecff10198eb",
        "message": "The callback can only be armed, if we get -EIOCBQUEUED returned. It's\nimportant that we clear the WAITQ bit for other cases, otherwise we can\nqueue for async retry and filemap will assume that we're armed and\nreturn -EAGAIN instead of just blocking for the IO.\n\nCc: stable@vger.kernel.org # 5.9+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-05 08:43:09 -0700 io_uring: clear IOCB_WAITQ for non -EIOCBQUEUED return"
    },
    {
        "commit": "ca0a26511c679a797f86589894a4523db36d833e",
        "message": "It doesn't make sense to wait for more events to come in, if we can't\neven flush the overflow we already have to the ring. Return -EBUSY for\nthat condition, just like we do for attempts to submit with overflow\npending.\n\nCc: stable@vger.kernel.org # 5.11\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-05 08:43:09 -0700 io_uring: don't keep looping for more events if we can't flush overflow"
    },
    {
        "commit": "46fe18b16c4656969347fc0a3d83a034e47d9119",
        "message": "This allows us to do task creation and setup without needing to use\ncompletions to try and synchronize with the starting thread. Get rid of\nthe old io_wq_fork_thread() wrapper, and the 'wq' and 'worker' startup\ncompletion events - we can now do setup before the task is running.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-05 08:43:01 -0700 io_uring: move to using create_io_thread()"
    },
    {
        "commit": "cc440e8738e5c875297ac0e90316745093be7e28",
        "message": "Provide a generic helper for setting up an io_uring worker. Returns a\ntask_struct so that the caller can do whatever setup is needed, then call\nwake_up_new_task() to kick it into gear.\n\nAdd a kernel_clone_args member, io_thread, which tells copy_process() to\nmark the task with PF_IO_WORKER.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 15:45:03 -0700 kernel: provide create_io_thread() helper"
    },
    {
        "commit": "dd59a3d595cc10230ded4c8b727b096e16bceeb5",
        "message": "Linked timeouts are fired asynchronously (i.e. soft-irq), and use\ngeneric cancellation paths to do its stuff, including poking into io-wq.\nThe problem is that it's racy to access tctx->io_wq, as\nio_uring_task_cancel() and others may be happening at this exact moment.\nMark linked timeouts with REQ_F_INLIFGHT for now, making sure there are\nno timeouts before io-wq destraction.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 15:45:01 -0700 io_uring: reliably cancel linked timeouts"
    },
    {
        "commit": "b05a1bcd40184f12f2cd87db79e871aa8c17563f",
        "message": "Instead of going into request internals, like checking req->file->f_op,\ndo match them based on REQ_F_INFLIGHT, it's set only when we want it to\nbe reliably cancelled.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 15:44:50 -0700 io_uring: cancel-match based on flags"
    },
    {
        "commit": "9e4bec5b2a230066a0dc9f79f24b4c1bcb668c5a",
        "message": "Implement mq_poll interface support in megaraid_sas. This feature\nrequires shared host tag support in kernel and driver.\n\nThe driver can work in non-IRQ mode which means there will not be any MSI-x\nvector associated for poll_queues. The MegaRAID hardware has a single\nsubmission queue and multiple reply queues. However, using the shared host\ntagset support will enable the driver to simulate multiple hardware queues.\n\nChange driver to allocate some extra reply queues which will be marked as\npoll_queues. These poll_queues will not have associated MSI-x vectors. All\nI/O completions on these queues will be done through the IOPOLL interface.\n\nmegaraid_sas with 8 poll_queues and using the io_uring hiprio=1 setting can\nreach 3.2M IOPS with zero interrupts generated by the hardware.\n\nThe IOPOLL feature can be enabled using module parameter poll_queues.\n\nLink: https://lore.kernel.org/r/20210215074048.19424-3-kashyap.desai@broadcom.com\nCc: sumit.saxena@broadcom.com\nCc: chandrakanth.patil@broadcom.com\nCc: linux-block@vger.kernel.org\nReviewed-by: Hannes Reinecke <hare@suse.de>\nSigned-off-by: Kashyap Desai <kashyap.desai@broadcom.com>\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>",
        "kernel_version": "v5.13-rc1",
        "release_date": "2021-03-04 17:37:03 -0500 scsi: megaraid_sas: mq_poll support"
    },
    {
        "commit": "e4b4a13f494120c475580927864cc1dd96f595d1",
        "message": "Alex reports that his system fails to suspend using 5.12-rc1, with the\nfollowing dump:\n\n[  240.650300] PM: suspend entry (deep)\n[  240.650748] Filesystems sync: 0.000 seconds\n[  240.725605] Freezing user space processes ...\n[  260.739483] Freezing of tasks failed after 20.013 seconds (3 tasks refusing to freeze, wq_busy=0):\n[  260.739497] task:iou-mgr-446     state:S stack:    0 pid:  516 ppid:   439 flags:0x00004224\n[  260.739504] Call Trace:\n[  260.739507]  ? sysvec_apic_timer_interrupt+0xb/0x81\n[  260.739515]  ? pick_next_task_fair+0x197/0x1cde\n[  260.739519]  ? sysvec_reschedule_ipi+0x2f/0x6a\n[  260.739522]  ? asm_sysvec_reschedule_ipi+0x12/0x20\n[  260.739525]  ? __schedule+0x57/0x6d6\n[  260.739529]  ? del_timer_sync+0xb9/0x115\n[  260.739533]  ? schedule+0x63/0xd5\n[  260.739536]  ? schedule_timeout+0x219/0x356\n[  260.739540]  ? __next_timer_interrupt+0xf1/0xf1\n[  260.739544]  ? io_wq_manager+0x73/0xb1\n[  260.739549]  ? io_wq_create+0x262/0x262\n[  260.739553]  ? ret_from_fork+0x22/0x30\n[  260.739557] task:iou-mgr-517     state:S stack:    0 pid:  522 ppid:   439 flags:0x00004224\n[  260.739561] Call Trace:\n[  260.739563]  ? sysvec_apic_timer_interrupt+0xb/0x81\n[  260.739566]  ? pick_next_task_fair+0x16f/0x1cde\n[  260.739569]  ? sysvec_apic_timer_interrupt+0xb/0x81\n[  260.739571]  ? asm_sysvec_apic_timer_interrupt+0x12/0x20\n[  260.739574]  ? __schedule+0x5b7/0x6d6\n[  260.739578]  ? del_timer_sync+0x70/0x115\n[  260.739581]  ? schedule_timeout+0x211/0x356\n[  260.739585]  ? __next_timer_interrupt+0xf1/0xf1\n[  260.739588]  ? io_wq_check_workers+0x15/0x11f\n[  260.739592]  ? io_wq_manager+0x69/0xb1\n[  260.739596]  ? io_wq_create+0x262/0x262\n[  260.739600]  ? ret_from_fork+0x22/0x30\n[  260.739603] task:iou-wrk-517     state:S stack:    0 pid:  523 ppid:   439 flags:0x00004224\n[  260.739607] Call Trace:\n[  260.739609]  ? __schedule+0x5b7/0x6d6\n[  260.739614]  ? schedule+0x63/0xd5\n[  260.739617]  ? schedule_timeout+0x219/0x356\n[  260.739621]  ? __next_timer_interrupt+0xf1/0xf1\n[  260.739624]  ? task_thread.isra.0+0x148/0x3af\n[  260.739628]  ? task_thread_unbound+0xa/0xa\n[  260.739632]  ? task_thread_bound+0x7/0x7\n[  260.739636]  ? ret_from_fork+0x22/0x30\n[  260.739647] OOM killer enabled.\n[  260.739648] Restarting tasks ... done.\n[  260.740077] PM: suspend exit\n\nPlay nice and ensure that any thread we create will call try_to_freeze()\nat an opportune time so that memory suspend can proceed. For the io-wq\nworker threads, mark them as PF_NOFREEZE. They could potentially be\nblocked for a long time.\n\nReported-by: Alex Xu (Hello71) <alex_y_xu@yahoo.ca>\nTested-by: Alex Xu (Hello71) <alex_y_xu@yahoo.ca>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:38:09 -0700 io_uring: ensure that threads freeze on suspend"
    },
    {
        "commit": "b23fcf477f85164f3b33b2e8c2c99b2ec61ba902",
        "message": "io_dismantle_req() is always followed by io_put_task(), which already do\nproper in_idle wake ups, so we can skip waking the owner task in\nio_dismantle_req(). The rules are simpler now, do io_put_task() shortly\nafter ending a request, and it will be fine.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:38:07 -0700 io_uring: remove extra in_idle wake up"
    },
    {
        "commit": "ebf936670721be805a9cb87781a5ee9271ba4633",
        "message": "__io_queue_async_work() is only called from io_queue_async_work(),\ninline it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:38:05 -0700 io_uring: inline __io_queue_async_work()"
    },
    {
        "commit": "f85c310ac376ce81a954507315ff11be4ddbf214",
        "message": "Inline io_req_clean_work(), less code and easier to analyse\ntctx dependencies and refs usage.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:38:04 -0700 io_uring: inline io_req_clean_work()"
    },
    {
        "commit": "64c7212391e778949aa3055fb3863439417ddba9",
        "message": "When we cancel SQPOLL, @task in io_uring_try_cancel_requests() will\ndiffer from current. Use the right tctx from passed in @task, and don't\nforget that it can be NULL when the io_uring ctx exits.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:38:03 -0700 io_uring: choose right tctx->io_wq for try cancel"
    },
    {
        "commit": "3e6a0d3c7571ce3ed0d25c5c32543a54a7ebcd75",
        "message": "We no longer revert the iovec on -EIOCBQUEUED, see commit ab2125df921d,\nand this started causing issues for IOPOLL on devies that run out of\nrequest slots. Turns out what outside of needing a revert for those, we\nalso had a bug where we didn't properly setup retry inside the submission\npath. That could cause re-import of the iovec, if any, and that could lead\nto spurious results if the application had those allocated on the stack.\n\nCatch -EAGAIN retry and make the iovec stable for IOPOLL, just like we do\nfor !IOPOLL retries.\n\nCc: <stable@vger.kernel.org> # 5.9+\nReported-by: Abaci Robot <abaci@linux.alibaba.com>\nReported-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:38:01 -0700 io_uring: fix -EAGAIN retry with IOPOLL"
    },
    {
        "commit": "16270893d71219816513a255e6c3163bc7224ce4",
        "message": "Now, sqo_task is used only for a warning that is not interesting anymore\nsince sqo_dead is gone, remove all of that including ctx->sqo_task.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:37:57 -0700 io_uring: remove sqo_task"
    },
    {
        "commit": "70aacfe66136809d7f080f89c492c278298719f4",
        "message": "As SQPOLL task doesn't poke into ->sqo_task anymore, there is no need to\nkill the sqo when the master task exits. Before it was necessary to\navoid races accessing sqo_task->files with removing them.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\n[axboe: don't forget to enable SQPOLL before exit, if started disabled]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:37:55 -0700 io_uring: kill sqo_dead and sqo submission halting"
    },
    {
        "commit": "1c3b3e6527e57156bf4082f11c2151957560fe6a",
        "message": "syzbot reports a deadlock, attempting to lock the same spinlock twice:\n\n============================================\nWARNING: possible recursive locking detected\n5.11.0-syzkaller #0 Not tainted\n--------------------------------------------\nswapper/1/0 is trying to acquire lock:\nffff88801b2b1130 (&runtime->sleep){..-.}-{2:2}, at: spin_lock include/linux/spinlock.h:354 [inline]\nffff88801b2b1130 (&runtime->sleep){..-.}-{2:2}, at: io_poll_double_wake+0x25f/0x6a0 fs/io_uring.c:4960\n\nbut task is already holding lock:\nffff88801b2b3130 (&runtime->sleep){..-.}-{2:2}, at: __wake_up_common_lock+0xb4/0x130 kernel/sched/wait.c:137\n\nother info that might help us debug this:\n Possible unsafe locking scenario:\n\n       CPU0\n       ----\n  lock(&runtime->sleep);\n  lock(&runtime->sleep);\n\n *** DEADLOCK ***\n\n May be due to missing lock nesting notation\n\n2 locks held by swapper/1/0:\n #0: ffff888147474908 (&group->lock){..-.}-{2:2}, at: _snd_pcm_stream_lock_irqsave+0x9f/0xd0 sound/core/pcm_native.c:170\n #1: ffff88801b2b3130 (&runtime->sleep){..-.}-{2:2}, at: __wake_up_common_lock+0xb4/0x130 kernel/sched/wait.c:137\n\nstack backtrace:\nCPU: 1 PID: 0 Comm: swapper/1 Not tainted 5.11.0-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nCall Trace:\n <IRQ>\n __dump_stack lib/dump_stack.c:79 [inline]\n dump_stack+0xfa/0x151 lib/dump_stack.c:120\n print_deadlock_bug kernel/locking/lockdep.c:2829 [inline]\n check_deadlock kernel/locking/lockdep.c:2872 [inline]\n validate_chain kernel/locking/lockdep.c:3661 [inline]\n __lock_acquire.cold+0x14c/0x3b4 kernel/locking/lockdep.c:4900\n lock_acquire kernel/locking/lockdep.c:5510 [inline]\n lock_acquire+0x1ab/0x730 kernel/locking/lockdep.c:5475\n __raw_spin_lock include/linux/spinlock_api_smp.h:142 [inline]\n _raw_spin_lock+0x2a/0x40 kernel/locking/spinlock.c:151\n spin_lock include/linux/spinlock.h:354 [inline]\n io_poll_double_wake+0x25f/0x6a0 fs/io_uring.c:4960\n __wake_up_common+0x147/0x650 kernel/sched/wait.c:108\n __wake_up_common_lock+0xd0/0x130 kernel/sched/wait.c:138\n snd_pcm_update_state+0x46a/0x540 sound/core/pcm_lib.c:203\n snd_pcm_update_hw_ptr0+0xa75/0x1a50 sound/core/pcm_lib.c:464\n snd_pcm_period_elapsed+0x160/0x250 sound/core/pcm_lib.c:1805\n dummy_hrtimer_callback+0x94/0x1b0 sound/drivers/dummy.c:378\n __run_hrtimer kernel/time/hrtimer.c:1519 [inline]\n __hrtimer_run_queues+0x609/0xe40 kernel/time/hrtimer.c:1583\n hrtimer_run_softirq+0x17b/0x360 kernel/time/hrtimer.c:1600\n __do_softirq+0x29b/0x9f6 kernel/softirq.c:345\n invoke_softirq kernel/softirq.c:221 [inline]\n __irq_exit_rcu kernel/softirq.c:422 [inline]\n irq_exit_rcu+0x134/0x200 kernel/softirq.c:434\n sysvec_apic_timer_interrupt+0x93/0xc0 arch/x86/kernel/apic/apic.c:1100\n </IRQ>\n asm_sysvec_apic_timer_interrupt+0x12/0x20 arch/x86/include/asm/idtentry.h:632\nRIP: 0010:native_save_fl arch/x86/include/asm/irqflags.h:29 [inline]\nRIP: 0010:arch_local_save_flags arch/x86/include/asm/irqflags.h:70 [inline]\nRIP: 0010:arch_irqs_disabled arch/x86/include/asm/irqflags.h:137 [inline]\nRIP: 0010:acpi_safe_halt drivers/acpi/processor_idle.c:111 [inline]\nRIP: 0010:acpi_idle_do_entry+0x1c9/0x250 drivers/acpi/processor_idle.c:516\nCode: dd 38 6e f8 84 db 75 ac e8 54 32 6e f8 e8 0f 1c 74 f8 e9 0c 00 00 00 e8 45 32 6e f8 0f 00 2d 4e 4a c5 00 e8 39 32 6e f8 fb f4 <9c> 5b 81 e3 00 02 00 00 fa 31 ff 48 89 de e8 14 3a 6e f8 48 85 db\nRSP: 0018:ffffc90000d47d18 EFLAGS: 00000293\nRAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000000000000\nRDX: ffff8880115c3780 RSI: ffffffff89052537 RDI: 0000000000000000\nRBP: ffff888141127064 R08: 0000000000000001 R09: 0000000000000001\nR10: ffffffff81794168 R11: 0000000000000000 R12: 0000000000000001\nR13: ffff888141127000 R14: ffff888141127064 R15: ffff888143331804\n acpi_idle_enter+0x361/0x500 drivers/acpi/processor_idle.c:647\n cpuidle_enter_state+0x1b1/0xc80 drivers/cpuidle/cpuidle.c:237\n cpuidle_enter+0x4a/0xa0 drivers/cpuidle/cpuidle.c:351\n call_cpuidle kernel/sched/idle.c:158 [inline]\n cpuidle_idle_call kernel/sched/idle.c:239 [inline]\n do_idle+0x3e1/0x590 kernel/sched/idle.c:300\n cpu_startup_entry+0x14/0x20 kernel/sched/idle.c:397\n start_secondary+0x274/0x350 arch/x86/kernel/smpboot.c:272\n secondary_startup_64_no_verify+0xb0/0xbb\n\nwhich is due to the driver doing poll_wait() twice on the same\nwait_queue_head. That is perfectly valid, but from checking the rest\nof the kernel tree, it's the only driver that does this.\n\nWe can handle this just fine, we just need to ignore the second addition\nas we'll get woken just fine on the first one.\n\nCc: stable@vger.kernel.org # 5.8+\nFixes: 18bceab101ad (\"io_uring: allow POLL_ADD with double poll_wait() users\")\nReported-by: syzbot+28abd693db9e92c160d8@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:37:14 -0700 io_uring: ignore double poll add on the same waitqueue head"
    },
    {
        "commit": "3ebba796fa251d042be42b929a2d916ee5c34a49",
        "message": "If we create it in a disabled state because IORING_SETUP_R_DISABLED is\nset on ring creation, we need to ensure that we've kicked the thread if\nwe're exiting before it's been explicitly disabled. Otherwise we can run\ninto a deadlock where exit is waiting go park the SQPOLL thread, but the\nSQPOLL thread itself is waiting to get a signal to start.\n\nThat results in the below trace of both tasks hung, waiting on each other:\n\nINFO: task syz-executor458:8401 blocked for more than 143 seconds.\n      Not tainted 5.11.0-next-20210226-syzkaller #0\n\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\ntask:syz-executor458 state:D stack:27536 pid: 8401 ppid:  8400 flags:0x00004004\nCall Trace:\n context_switch kernel/sched/core.c:4324 [inline]\n __schedule+0x90c/0x21a0 kernel/sched/core.c:5075\n schedule+0xcf/0x270 kernel/sched/core.c:5154\n schedule_timeout+0x1db/0x250 kernel/time/timer.c:1868\n do_wait_for_common kernel/sched/completion.c:85 [inline]\n __wait_for_common kernel/sched/completion.c:106 [inline]\n wait_for_common kernel/sched/completion.c:117 [inline]\n wait_for_completion+0x168/0x270 kernel/sched/completion.c:138\n io_sq_thread_park fs/io_uring.c:7115 [inline]\n io_sq_thread_park+0xd5/0x130 fs/io_uring.c:7103\n io_uring_cancel_task_requests+0x24c/0xd90 fs/io_uring.c:8745\n __io_uring_files_cancel+0x110/0x230 fs/io_uring.c:8840\n io_uring_files_cancel include/linux/io_uring.h:47 [inline]\n do_exit+0x299/0x2a60 kernel/exit.c:780\n do_group_exit+0x125/0x310 kernel/exit.c:922\n __do_sys_exit_group kernel/exit.c:933 [inline]\n __se_sys_exit_group kernel/exit.c:931 [inline]\n __x64_sys_exit_group+0x3a/0x50 kernel/exit.c:931\n do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n entry_SYSCALL_64_after_hwframe+0x44/0xae\nRIP: 0033:0x43e899\nRSP: 002b:00007ffe89376d48 EFLAGS: 00000246 ORIG_RAX: 00000000000000e7\nRAX: ffffffffffffffda RBX: 00000000004af2f0 RCX: 000000000043e899\nRDX: 000000000000003c RSI: 00000000000000e7 RDI: 0000000000000000\nRBP: 0000000000000000 R08: ffffffffffffffc0 R09: 0000000010000000\nR10: 0000000000008011 R11: 0000000000000246 R12: 00000000004af2f0\nR13: 0000000000000001 R14: 0000000000000000 R15: 0000000000000001\nINFO: task iou-sqp-8401:8402 can't die for more than 143 seconds.\ntask:iou-sqp-8401    state:D stack:30272 pid: 8402 ppid:  8400 flags:0x00004004\nCall Trace:\n context_switch kernel/sched/core.c:4324 [inline]\n __schedule+0x90c/0x21a0 kernel/sched/core.c:5075\n schedule+0xcf/0x270 kernel/sched/core.c:5154\n schedule_timeout+0x1db/0x250 kernel/time/timer.c:1868\n do_wait_for_common kernel/sched/completion.c:85 [inline]\n __wait_for_common kernel/sched/completion.c:106 [inline]\n wait_for_common kernel/sched/completion.c:117 [inline]\n wait_for_completion+0x168/0x270 kernel/sched/completion.c:138\n io_sq_thread+0x27d/0x1ae0 fs/io_uring.c:6717\n ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:294\nINFO: task iou-sqp-8401:8402 blocked for more than 143 seconds.\n\nReported-by: syzbot+fb5458330b4442f2090d@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:37:14 -0700 io_uring: ensure that SQPOLL thread is started for exit"
    },
    {
        "commit": "28c4721b80a702462fb77373c23428ee698fa5dd",
        "message": "io_run_ctx_fallback() can use xchg() instead of cmpxchg(). It's simpler\nand faster.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:37:14 -0700 io_uring: replace cmpxchg in fallback with xchg"
    },
    {
        "commit": "2c32395d8111037ae2cb8cab883e80bcdbb70713",
        "message": "There is an unlikely but possible race using a freed context. That's\nbecause req->task_work.func() can free a request, but we won't\nnecessarily find a completion in submit_state.comp and so all ctx refs\nmay be put by the time we do mutex_lock(&ctx->uring_ctx);\n\nThere are several reasons why it can miss going through\nsubmit_state.comp: 1) req->task_work.func() didn't complete it itself,\nbut punted to iowq (e.g. reissue) and it got freed later, or a similar\nsituation with it overflowing and getting flushed by someone else, or\nbeing submitted to IRQ completion, 2) As we don't hold the uring_lock,\nsomeone else can do io_submit_flush_completions() and put our ref.\n3) Bugs and code obscurities, e.g. failing to propagate issue_flags\nproperly.\n\nOne example is as follows\n\n  CPU1                                  |  CPU2\n=======================================================================\n@req->task_work.func()                  |\n  -> @req overflwed,                    |\n     so submit_state.comp,nr==0         |\n                                        | flush overflows, and free @req\n                                        | ctx refs == 0, free it\nctx is dead, but we do                  |\n\tlock + flush + unlock           |\n\nSo take a ctx reference for each new ctx we see in __tctx_task_work(),\nand do release it until we do all our flushing.\n\nFixes: 65453d1efbd2 (\"io_uring: enable req cache for task_work items\")\nReported-by: syzbot+a157ac7c03a56397f553@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\n[axboe: fold in my one-liner and fix ref mismatch]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:37:05 -0700 io_uring: fix __tctx_task_work() ctx race"
    },
    {
        "commit": "0d30b3e7eea94cc818fadf2ac0dd189c616028f8",
        "message": "This was always a weird work-around or file referencing, and we don't\nneed it anymore. Get rid of it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:37:03 -0700 io_uring: kill io_uring_flush()"
    },
    {
        "commit": "914390bcfdd6351a4d308da7f43294476ea7d3bf",
        "message": "We already run the fallback task_work in io_uring_try_cancel_requests(),\nno need to duplicate at ring exit explicitly.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:36:28 -0700 io_uring: kill unnecessary io_run_ctx_fallback() in io_ring_exit_work()"
    },
    {
        "commit": "5730b27e84fdb37353c7cc2b11c24a4f9d73626e",
        "message": "If we move it in there, then we no longer have to care about it in io-wq.\nThis means we can drop the cred handling in io-wq, and we can drop the\nREQ_F_WORK_INITIALIZED flag and async init functions as that was the last\nuser of it since we moved to the new workers. Then we can also drop\nio_wq_work->creds, and just hold the personality u16 in there instead.\n\nSuggested-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:36:28 -0700 io_uring: move cred assignment into io_issue_sqe()"
    },
    {
        "commit": "1575f21a09206e914b81dace0add693346d97594",
        "message": "We're no longer checking anything that requires the work item to be\ninitialized, as we're not carrying any file related state there.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:36:26 -0700 io_uring: kill unnecessary REQ_F_WORK_INITIALIZED checks"
    },
    {
        "commit": "4010fec41fd9fc5ca6956b958d14b32e41aded48",
        "message": "We prune the full cache regardless, get rid of the dead argument.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:36:24 -0700 io_uring: remove unused argument 'tsk' from io_req_caches_free()"
    },
    {
        "commit": "8452d4a674b0e59bd53baef0b30b018690dde594",
        "message": "Destroy current's io-wq backend and tctx on __io_uring_task_cancel(),\naka exec(). Looks it's not strictly necessary, because it will be done\nat some point when the task dies and changes of creds/files/etc. are\nhandled, but better to do that earlier to free io-wq and not potentially\nlock previous mm and other resources for the time being.\n\nIt's safe to do because we wait for all requests of the current task to\ncomplete, so no request will use tctx afterwards. Note, that\nio_uring_files_cancel() may leave some requests for later reaping, so it\nleaves tctx intact, that's ok as the task is dying anyway.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:36:22 -0700 io_uring: destroy io-wq on exec"
    },
    {
        "commit": "ef8eaa4e65facb1f51a64dbb4f5500134622c67c",
        "message": "Make sure that we killed an io-wq by the time a task is dead.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:35:00 -0700 io_uring: warn on not destroyed io-wq"
    },
    {
        "commit": "1d5f360dd1a3c04e00a52af74dd84fdb0e1d454b",
        "message": "We clear the bit marking the ctx task_work as active after having run\nthe queued work, but we really should be clearing it before. Otherwise\nwe can hit a tiny race ala:\n\nCPU0\t\t\t\t\tCPU1\nio_task_work_add()\t\t\ttctx_task_work()\n\t\t\t\t\trun_work\n\tadd_to_list\n\ttest_and_set_bit\n\t\t\t\t\tclear_bit\n\t\talready set\n\nand CPU0 will return thinking the task_work is queued, while in reality\nit's already being run. If we hit the condition after __tctx_task_work()\nfound no more work, but before we've cleared the bit, then we'll end up\nthinking it's queued and will be run. In reality it is queued, but we\ndidn't queue the ctx task_work to ensure that it gets run.\n\nFixes: 7cbf1722d5fc (\"io_uring: provide FIFO ordering for task_work\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:35:00 -0700 io_uring: fix race condition in task_work add and clear"
    },
    {
        "commit": "afcc4015d1bf5659b8c722aff679e9b8c41ee156",
        "message": "If we put the io-wq from io_uring, we really want it to exit. Provide\na helper that does that for us. Couple that with not having the manager\nhold a reference to the 'wq' and the normal SQPOLL exit will tear down\nthe io-wq context appropriate.\n\nOn the io-wq side, our wq context is per task, so only the task itself\nis manipulating ->manager and hence it's safe to check and clear without\nany extra locking. We just need to ensure that the manager task stays\naround, in case it exits.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:34:39 -0700 io-wq: provide an io_wq_put_and_exit() helper"
    },
    {
        "commit": "8629397e6e2753bb4cc62ba48a12e1d4d912b6a4",
        "message": "We want to reuse this completion, and a single complete should do just\nfine. Ensure that we park ourselves first if requested, as that is what\nlead to the initial deadlock in this area. If we've got someone attempting\nto park us, then we can't proceed without having them finish first.\n\nFixes: 37d1e2e3642e (\"io_uring: move SQPOLL thread io-wq forked worker\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:34:04 -0700 io_uring: don't use complete_all() on SQPOLL thread exit"
    },
    {
        "commit": "ba50a036f23c44608b1d903c34644a1acd5d21fa",
        "message": "io_uring_try_cancel_requests() matches not only current's requests, but\nalso of other exiting tasks, so we need to actively cancel them and not\njust wait, especially since the function can be called on flush during\ndo_exit() -> exit_files().\nEven if it's not a problem for now, it's much nicer to know that the\nfunction tries to cancel everything it can.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:34:03 -0700 io_uring: run fallback on cancellation"
    },
    {
        "commit": "e54945ae947fb881212a4b97d5599a01bba6ad06",
        "message": "If we fail to fork an SQPOLL worker, we can hit cancel, and hence\nattempted thread stop, with the thread already being stopped. Ensure\nwe check for that.\n\nAlso guard thread stop fully by the sqd mutex, just like we do for\npark.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc2",
        "release_date": "2021-03-04 06:34:01 -0700 io_uring: SQPOLL stop error handling fixes"
    },
    {
        "commit": "5695e51619745d4fe3ec2506a2f0cd982c5e27a4",
        "message": "Pull io_uring thread rewrite from Jens Axboe:\n \"This converts the io-wq workers to be forked off the tasks in question\n  instead of being kernel threads that assume various bits of the\n  original task identity.\n\n  This kills > 400 lines of code from io_uring/io-wq, and it's the worst\n  part of the code. We've had several bugs in this area, and the worry\n  is always that we could be missing some pieces for file types doing\n  unusual things (recent /dev/tty example comes to mind, userfaultfd\n  reads installing file descriptors is another fun one... - both of\n  which need special handling, and I bet it's not the last weird oddity\n  we'll find).\n\n  With these identical workers, we can have full confidence that we're\n  never missing anything. That, in itself, is a huge win. Outside of\n  that, it's also more efficient since we're not wasting space and code\n  on tracking state, or switching between different states.\n\n  I'm sure we're going to find little things to patch up after this\n  series, but testing has been pretty thorough, from the usual\n  regression suite to production. Any issue that may crop up should be\n  manageable.\n\n  There's also a nice series of further reductions we can do on top of\n  this, but I wanted to get the meat of it out sooner rather than later.\n  The general worry here isn't that it's fundamentally broken. Most of\n  the little issues we've found over the last week have been related to\n  just changes in how thread startup/exit is done, since that's the main\n  difference between using kthreads and these kinds of threads. In fact,\n  if all goes according to plan, I want to get this into the 5.10 and\n  5.11 stable branches as well.\n\n  That said, the changes outside of io_uring/io-wq are:\n\n   - arch setup, simple one-liner to each arch copy_thread()\n     implementation.\n\n   - Removal of net and proc restrictions for io_uring, they are no\n     longer needed or useful\"\n\n* tag 'io_uring-worker.v3-2021-02-25' of git://git.kernel.dk/linux-block: (30 commits)\n  io-wq: remove now unused IO_WQ_BIT_ERROR\n  io_uring: fix SQPOLL thread handling over exec\n  io-wq: improve manager/worker handling over exec\n  io_uring: ensure SQPOLL startup is triggered before error shutdown\n  io-wq: make buffered file write hashed work map per-ctx\n  io-wq: fix race around io_worker grabbing\n  io-wq: fix races around manager/worker creation and task exit\n  io_uring: ensure io-wq context is always destroyed for tasks\n  arch: ensure parisc/powerpc handle PF_IO_WORKER in copy_thread()\n  io_uring: cleanup ->user usage\n  io-wq: remove nr_process accounting\n  io_uring: flag new native workers with IORING_FEAT_NATIVE_WORKERS\n  net: remove cmsg restriction from io_uring based send/recvmsg calls\n  Revert \"proc: don't allow async path resolution of /proc/self components\"\n  Revert \"proc: don't allow async path resolution of /proc/thread-self components\"\n  io_uring: move SQPOLL thread io-wq forked worker\n  io-wq: make io_wq_fork_thread() available to other users\n  io-wq: only remove worker from free_list, if it was there\n  io_uring: remove io_identity\n  io_uring: remove any grabbing of context\n  ...",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-27 08:29:02 -0800 Merge tag 'io_uring-worker.v3-2021-02-25' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "efba6d3a7c4bb59f0750609fae0f9644d82304b6",
        "message": "Pull more io_uring updates from Jens Axboe:\n \"A collection of later fixes that we should get into this release:\n\n   - Series of submission cleanups (Pavel)\n\n   - A few fixes for issues from earlier this merge window (Pavel, me)\n\n   - IOPOLL resubmission fix\n\n   - task_work locking fix (Hao)\"\n\n* tag 'for-5.12/io_uring-2021-02-25' of git://git.kernel.dk/linux-block: (25 commits)\n  Revert \"io_uring: wait potential ->release() on resurrect\"\n  io_uring: fix locked_free_list caches_free()\n  io_uring: don't attempt IO reissue from the ring exit path\n  io_uring: clear request count when freeing caches\n  io_uring: run task_work on io_uring_register()\n  io_uring: fix leaving invalid req->flags\n  io_uring: wait potential ->release() on resurrect\n  io_uring: keep generic rsrc infra generic\n  io_uring: zero ref_node after killing it\n  io_uring: make the !CONFIG_NET helpers a bit more robust\n  io_uring: don't hold uring_lock when calling io_run_task_work*\n  io_uring: fail io-wq submission from a task_work\n  io_uring: don't take uring_lock during iowq cancel\n  io_uring: fail links more in io_submit_sqe()\n  io_uring: don't do async setup for links' heads\n  io_uring: do io_*_prep() early in io_submit_sqe()\n  io_uring: split sqe-prep and async setup\n  io_uring: don't submit link on error\n  io_uring: move req link into submit_state\n  io_uring: move io_init_req() into io_submit_sqe()\n  ...",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-26 14:07:12 -0800 Merge tag 'for-5.12/io_uring-2021-02-25' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "5f3f26f98ae484a3e187411f9ea8c88c00a65ffc",
        "message": "Just like the changes for io-wq, ensure that we re-fork the SQPOLL\nthread if the owner execs. Mark the ctx sq thread as sqo_exec if\nit dies, and the ring as needing a wakeup which will force the task\nto enter the kernel. When it does, setup the new thread and proceed\nas usual.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-25 10:17:46 -0700 io_uring: fix SQPOLL thread handling over exec"
    },
    {
        "commit": "eb85890b29e4d7ae1accdcfba35ed8b16ba9fb97",
        "message": "syzbot reports the following hang:\n\nINFO: task syz-executor.0:12538 can't die for more than 143 seconds.\ntask:syz-executor.0  state:D stack:28352 pid:12538 ppid:  8423 flags:0x00004004\nCall Trace:\n context_switch kernel/sched/core.c:4324 [inline]\n __schedule+0x90c/0x21a0 kernel/sched/core.c:5075\n schedule+0xcf/0x270 kernel/sched/core.c:5154\n schedule_timeout+0x1db/0x250 kernel/time/timer.c:1868\n do_wait_for_common kernel/sched/completion.c:85 [inline]\n __wait_for_common kernel/sched/completion.c:106 [inline]\n wait_for_common kernel/sched/completion.c:117 [inline]\n wait_for_completion+0x168/0x270 kernel/sched/completion.c:138\n io_sq_thread_finish+0x96/0x580 fs/io_uring.c:7152\n io_sq_offload_create fs/io_uring.c:7929 [inline]\n io_uring_create fs/io_uring.c:9465 [inline]\n io_uring_setup+0x1fb2/0x2c20 fs/io_uring.c:9550\n do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nwhich is due to exiting after the SQPOLL thread has been created, but\nhasn't been started yet. Ensure that we always complete the startup\nside when waiting for it to exit.\n\nReported-by: syzbot+c927c937cba8ef66dd4a@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-25 10:19:01 -0700 io_uring: ensure SQPOLL startup is triggered before error shutdown"
    },
    {
        "commit": "cb5e1b81304e089ee3ca948db4d29f71902eb575",
        "message": "This reverts commit 88f171ab7798a1ed0b9e39867ee16f307466e870.\n\nI ran into a case where the ref resurrect now spins, so revert\nthis change for now until we can further investigate why it's\nbroken. The bug seems to indicate spinning on the lock itself,\nlikely there's some ABBA deadlock involved:\n\n[<0>] __percpu_ref_switch_mode+0x45/0x180\n[<0>] percpu_ref_resurrect+0x46/0x70\n[<0>] io_refs_resurrect+0x25/0xa0\n[<0>] __io_uring_register+0x135/0x10c0\n[<0>] __x64_sys_io_uring_register+0xc2/0x1a0\n[<0>] do_syscall_64+0x42/0x110\n[<0>] entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-25 07:37:35 -0700 Revert \"io_uring: wait potential ->release() on resurrect\""
    },
    {
        "commit": "8a378fb096a7f02943c72a428bbfd0029260efb6",
        "message": "If the task ends up doing no IO, the context list is empty and we don't\ncall into __io_uring_files_cancel() when the task exits. This can cause\na leak of the io-wq structures.\n\nEnsure we always call __io_uring_files_cancel(), even if the task\ncontext list is empty.\n\nFixes: 5aa75ed5b93f (\"io_uring: tie async worker side to the task context\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-23 20:33:36 -0700 io_uring: ensure io-wq context is always destroyed for tasks"
    },
    {
        "commit": "62e398be275a6c6efefe117b8960ae4e40e047cd",
        "message": "At this point we're only using it for memory accounting, so there's no\nneed to have an extra ->limit_mem - we can just set ->user if we do\nthe accounting, or leave it at NULL if we don't.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-23 20:33:31 -0700 io_uring: cleanup ->user usage"
    },
    {
        "commit": "1c0aa1fae1acb77c5f9917adb0e4cb4500b9f3a6",
        "message": "A few reasons to do this:\n\n- The naming of the manager and worker have changed. That's a user visible\n  change, so makes sense to flag it.\n\n- Opening certain files that use ->signal (like /proc/self or /dev/tty)\n  now works, and the flag tells the application upfront that this is the\n  case.\n\n- Related to the above, using signalfd will now work as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-23 20:32:11 -0700 io_uring: flag new native workers with IORING_FEAT_NATIVE_WORKERS"
    },
    {
        "commit": "e54937963fa249595824439dc839c948188dea83",
        "message": "No need to restrict these anymore, as the worker threads are direct\nclones of the original task. Hence we know for a fact that we can\nsupport anything that the regular task can.\n\nSince the only user of proto_ops->flags was to flag PROTO_CMSG_DATA_ONLY,\nkill the member and the flag definition too.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-23 20:32:11 -0700 net: remove cmsg restriction from io_uring based send/recvmsg calls"
    },
    {
        "commit": "e5547d2c5eb363bfac7632ba789ca834fa829650",
        "message": "Don't forget to zero locked_free_nr, it's not a disaster but makes it\nattempting to flush it with extra locking when there is nothing in the\nlist. Also, don't traverse a potentially long list freeing requests\nunder spinlock, splice the list and do it afterwards.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-23 19:18:54 -0700 io_uring: fix locked_free_list caches_free()"
    },
    {
        "commit": "7c977a58dc83366e488c217fd88b1469d242bee5",
        "message": "If we're exiting the ring, just let the IO fail with -EAGAIN as nobody\nwill care anyway. It's not the right context to reissue from.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-23 19:18:13 -0700 io_uring: don't attempt IO reissue from the ring exit path"
    },
    {
        "commit": "37d1e2e3642e2380750d7f35279180826f29660e",
        "message": "Don't use a kthread for SQPOLL, use a forked worker just like the io-wq\nworkers. With that done, we can drop the various context grabbing we do\nfor SQPOLL, it already has everything it needs.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-23 16:44:42 -0700 io_uring: move SQPOLL thread io-wq forked worker"
    },
    {
        "commit": "6b09b4d33bd964f49d07d3cabfb4204d58cf9811",
        "message": "QUEUE_FLAG_POLL flag will be cleared when turning off 'io_poll', while\nat that moment there may be IOs stuck in hw queue uncompleted. The\nfollowing polling routine won't help reap these IOs, since blk_poll()\nwill return immediately because of cleared QUEUE_FLAG_POLL flag. Thus\nthese IOs will hang until they finnaly time out. The hang out can be\nobserved by 'fio --engine=io_uring iodepth=1', while turning off\n'io_poll' at the same time.\n\nTo fix this, freeze and flush the request queue first when turning off\n'io_poll'.\n\nSigned-off-by: Jeffle Xu <jefflexu@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-22 06:40:02 -0700 block: fix potential IO hang when turning off io_poll"
    },
    {
        "commit": "8e5c66c485a8af3f39a8b0358e9e09f002016d92",
        "message": "BUG: KASAN: double-free or invalid-free in io_req_caches_free.constprop.0+0x3ce/0x530 fs/io_uring.c:8709\n\nWorkqueue: events_unbound io_ring_exit_work\nCall Trace:\n [...]\n __cache_free mm/slab.c:3424 [inline]\n kmem_cache_free_bulk+0x4b/0x1b0 mm/slab.c:3744\n io_req_caches_free.constprop.0+0x3ce/0x530 fs/io_uring.c:8709\n io_ring_ctx_free fs/io_uring.c:8764 [inline]\n io_ring_exit_work+0x518/0x6b0 fs/io_uring.c:8846\n process_one_work+0x98d/0x1600 kernel/workqueue.c:2275\n worker_thread+0x64c/0x1120 kernel/workqueue.c:2421\n kthread+0x3b1/0x4a0 kernel/kthread.c:292\n ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:294\n\nFreed by task 11900:\n [...]\n kmem_cache_free_bulk+0x4b/0x1b0 mm/slab.c:3744\n io_req_caches_free.constprop.0+0x3ce/0x530 fs/io_uring.c:8709\n io_uring_flush+0x483/0x6e0 fs/io_uring.c:9237\n filp_close+0xb4/0x170 fs/open.c:1286\n close_files fs/file.c:403 [inline]\n put_files_struct fs/file.c:418 [inline]\n put_files_struct+0x1d0/0x350 fs/file.c:415\n exit_files+0x7e/0xa0 fs/file.c:435\n do_exit+0xc27/0x2ae0 kernel/exit.c:820\n do_group_exit+0x125/0x310 kernel/exit.c:922\n [...]\n\nio_req_caches_free() doesn't zero submit_state->free_reqs, so io_uring\nconsiders just freed requests to be good and sound and will reuse or\ndouble free them. Zero the counter.\n\nReported-by: syzbot+30b4936dcdb3aafa4fb4@syzkaller.appspotmail.com\nFixes: 41be53e94fb04 (\"io_uring: kill cached requests from exiting task closing the ring\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-22 06:31:31 -0700 io_uring: clear request count when freeing caches"
    },
    {
        "commit": "843bbfd49f02caab7186910480a86378bb84e975",
        "message": "We want to use this in io_uring proper as well, for the SQPOLL thread.\nRename it from fork_thread() to io_wq_fork_thread(), and make it\navailable through the io-wq.h header.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-21 17:25:22 -0700 io-wq: make io_wq_fork_thread() available to other users"
    },
    {
        "commit": "4379bf8bd70b5de6bba7d53015b0c36c57a634ee",
        "message": "We are no longer grabbing state, so no need to maintain an IO identity\nthat we COW if there are changes.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-21 17:25:22 -0700 io_uring: remove io_identity"
    },
    {
        "commit": "44526bedc2ff8fcd58552e3c5bae928524b6f13c",
        "message": "The async workers are siblings of the task itself, so by definition we\nhave all the state that we need. Remove any of the state grabbing that\nwe have, and requests flagging what they need.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-21 17:25:22 -0700 io_uring: remove any grabbing of context"
    },
    {
        "commit": "5aa75ed5b93f086c455a3c67239b0471ff5a1526",
        "message": "Move it outside of the io_ring_ctx, and tie it to the io_uring task\ncontext.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-21 17:25:22 -0700 io_uring: tie async worker side to the task context"
    },
    {
        "commit": "d25e3a3de0d6fb2f660dbc7d643b2c632beb1743",
        "message": "Moving towards making the io_wq per ring per task, so we can't really\nshare it between rings. Which is fine, since we've now dropped some\nof that fat from it.\n\nRetain compatibility with how attaching works, so that any attempt to\nattach to an fd that doesn't exist, or isn't an io_uring fd, will fail\nlike it did before.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-21 17:25:22 -0700 io_uring: disable io-wq attaching"
    },
    {
        "commit": "7c25c0d16ef3c37e49c593ac92f69fa3884d4bb9",
        "message": "We hit this case when the task is exiting, and we need somewhere to\ndo background cleanup of requests. Instead of relying on the io-wq\ntask manager to do this work for us, just stuff it somewhere where\nwe can safely run it ourselves directly.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-21 17:25:22 -0700 io_uring: remove the need for relying on an io-wq fallback worker"
    },
    {
        "commit": "27131549060ee87f1c50c56539b8f6c4c1a4acec",
        "message": "* for-5.12/io_uring: (21 commits)\n  io_uring: run task_work on io_uring_register()\n  io_uring: fix leaving invalid req->flags\n  io_uring: wait potential ->release() on resurrect\n  io_uring: keep generic rsrc infra generic\n  io_uring: zero ref_node after killing it\n  io_uring: make the !CONFIG_NET helpers a bit more robust\n  io_uring: don't hold uring_lock when calling io_run_task_work*\n  io_uring: fail io-wq submission from a task_work\n  io_uring: don't take uring_lock during iowq cancel\n  io_uring: fail links more in io_submit_sqe()\n  io_uring: don't do async setup for links' heads\n  io_uring: do io_*_prep() early in io_submit_sqe()\n  io_uring: split sqe-prep and async setup\n  io_uring: don't submit link on error\n  io_uring: move req link into submit_state\n  io_uring: move io_init_req() into io_submit_sqe()\n  io_uring: move io_init_req()'s definition\n  io_uring: don't duplicate ->file check in sfr\n  io_uring: keep io_*_prep() naming consistent\n  io_uring: kill fictitious submit iteration index\n  ...",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-21 17:22:53 -0700 Merge branch 'for-5.12/io_uring' into io_uring-worker.v3"
    },
    {
        "commit": "b6c23dd5a483174f386e4c2e1711d9532e090c00",
        "message": "Do run task_work before io_uring_register(), that might make a first\nquiesce round much nicer. We generally do that for any syscall invocation\nto avoid spurious -EINTR/-ERESTARTSYS, for task_work that we generate.\nThis patch brings io_uring_register() inline with the two other io_uring\nsyscalls.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-21 17:18:56 -0700 io_uring: run task_work on io_uring_register()"
    },
    {
        "commit": "5bbb336ba75d95611a7b9456355b48705016bdb1",
        "message": "Pull io_uring updates from Jens Axboe:\n \"Highlights from this cycles are things like request recycling and\n  task_work optimizations, which net us anywhere from 10-20% of speedups\n  on workloads that mostly are inline.\n\n  This work was originally done to put io_uring under memcg, which adds\n  considerable overhead. But it's a really nice win as well. Also worth\n  highlighting is the LOOKUP_CACHED work in the VFS, and using it in\n  io_uring. Greatly speeds up the fast path for file opens.\n\n  Summary:\n\n   - Put io_uring under memcg protection. We accounted just the rings\n     themselves under rlimit memlock before, now we account everything.\n\n   - Request cache recycling, persistent across invocations (Pavel, me)\n\n   - First part of a cleanup/improvement to buffer registration (Bijan)\n\n   - SQPOLL fixes (Hao)\n\n   - File registration NULL pointer fixup (Dan)\n\n   - LOOKUP_CACHED support for io_uring\n\n   - Disable /proc/thread-self/ for io_uring, like we do for /proc/self\n\n   - Add Pavel to the io_uring MAINTAINERS entry\n\n   - Tons of code cleanups and optimizations (Pavel)\n\n   - Support for skip entries in file registration (Noah)\"\n\n* tag 'for-5.12/io_uring-2021-02-17' of git://git.kernel.dk/linux-block: (103 commits)\n  io_uring: tctx->task_lock should be IRQ safe\n  proc: don't allow async path resolution of /proc/thread-self components\n  io_uring: kill cached requests from exiting task closing the ring\n  io_uring: add helper to free all request caches\n  io_uring: allow task match to be passed to io_req_cache_free()\n  io-wq: clear out worker ->fs and ->files\n  io_uring: optimise io_init_req() flags setting\n  io_uring: clean io_req_find_next() fast check\n  io_uring: don't check PF_EXITING from syscall\n  io_uring: don't split out consume out of SQE get\n  io_uring: save ctx put/get for task_work submit\n  io_uring: don't duplicate io_req_task_queue()\n  io_uring: optimise SQPOLL mm/files grabbing\n  io_uring: optimise out unlikely link queue\n  io_uring: take compl state from submit state\n  io_uring: inline io_complete_rw_common()\n  io_uring: move res check out of io_rw_reissue()\n  io_uring: simplify iopoll reissuing\n  io_uring: clean up io_req_free_batch_finish()\n  io_uring: move submit side state closer in the ring\n  ...",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-21 11:10:39 -0800 Merge tag 'for-5.12/io_uring-2021-02-17' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "ebf4a5db690a47e71056381ead8a134de7202694",
        "message": "sqe->flags are subset of req flags, so incorrectly copied may span into\nin-kernel flags and wreck havoc, e.g. by setting REQ_F_INFLIGHT.\n\nFixes: 5be9ad1e4287e (\"io_uring: optimise io_init_req() flags setting\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-20 19:02:45 -0700 io_uring: fix leaving invalid req->flags"
    },
    {
        "commit": "88f171ab7798a1ed0b9e39867ee16f307466e870",
        "message": "There is a short window where percpu_refs are already turned zero, but\nwe try to do resurrect(). Play nicer and wait for ->release() to happen\nin this case and proceed as everything is ok. One downside for ctx refs\nis that we can ignore signal_pending() on a rare occasion, but someone\nelse should check for it later if needed.\n\nCc: <stable@vger.kernel.org> # 5.5+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-20 19:02:45 -0700 io_uring: wait potential ->release() on resurrect"
    },
    {
        "commit": "f2303b1f8244d88ffca28d3be6166ce4835cc27a",
        "message": "io_rsrc_ref_quiesce() is a generic resource function, though now it\nwas wired to allocate and initialise ref nodes with file-specific\ncallbacks/etc. Keep it sane by passing in as a parameters everything we\nneed for initialisations, otherwise it will hurt us badly one day.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-20 19:02:45 -0700 io_uring: keep generic rsrc infra generic"
    },
    {
        "commit": "e6cb007c45dedada0a847eaa486c49509d63b1e8",
        "message": "After a rsrc/files reference node's refs are killed, it must never be\nused. And that's how it works, it either assigns a new node or kills the\nwhole data table.\n\nLet's explicitly NULL it, that shouldn't be necessary, but if something\nwould go wrong I'd rather catch a NULL dereference to using a dangling\npointer.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-20 19:02:45 -0700 io_uring: zero ref_node after killing it"
    },
    {
        "commit": "99a10081647168022745859bb2f1c28b2f70dc83",
        "message": "With the prep and prep async split, we now have potentially 3 helpers\nthat need to be defined for !CONFIG_NET. Add some helpers to do just\nthat.\n\nFixes the following compile error on !CONFIG_NET:\n\nfs/io_uring.c:6171:10: error: implicit declaration of function\n'io_sendmsg_prep_async'; did you mean 'io_req_prep_async'?\n[-Werror=implicit-function-declaration]\n   return io_sendmsg_prep_async(req);\n             ^~~~~~~~~~~~~~~~~~~~~\n\t     io_req_prep_async\n\nFixes: 93642ef88434 (\"io_uring: split sqe-prep and async setup\")\nReported-by: Naresh Kamboju <naresh.kamboju@linaro.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-20 19:02:45 -0700 io_uring: make the !CONFIG_NET helpers a bit more robust"
    },
    {
        "commit": "8bad28d8a305b0e5ae444c8c3051e8744f5a4296",
        "message": "Abaci reported the below issue:\n[  141.400455] hrtimer: interrupt took 205853 ns\n[  189.869316] process 'usr/local/ilogtail/ilogtail_0.16.26' started with executable stack\n[  250.188042]\n[  250.188327] ============================================\n[  250.189015] WARNING: possible recursive locking detected\n[  250.189732] 5.11.0-rc4 #1 Not tainted\n[  250.190267] --------------------------------------------\n[  250.190917] a.out/7363 is trying to acquire lock:\n[  250.191506] ffff888114dbcbe8 (&ctx->uring_lock){+.+.}-{3:3}, at: __io_req_task_submit+0x29/0xa0\n[  250.192599]\n[  250.192599] but task is already holding lock:\n[  250.193309] ffff888114dbfbe8 (&ctx->uring_lock){+.+.}-{3:3}, at: __x64_sys_io_uring_register+0xad/0x210\n[  250.194426]\n[  250.194426] other info that might help us debug this:\n[  250.195238]  Possible unsafe locking scenario:\n[  250.195238]\n[  250.196019]        CPU0\n[  250.196411]        ----\n[  250.196803]   lock(&ctx->uring_lock);\n[  250.197420]   lock(&ctx->uring_lock);\n[  250.197966]\n[  250.197966]  *** DEADLOCK ***\n[  250.197966]\n[  250.198837]  May be due to missing lock nesting notation\n[  250.198837]\n[  250.199780] 1 lock held by a.out/7363:\n[  250.200373]  #0: ffff888114dbfbe8 (&ctx->uring_lock){+.+.}-{3:3}, at: __x64_sys_io_uring_register+0xad/0x210\n[  250.201645]\n[  250.201645] stack backtrace:\n[  250.202298] CPU: 0 PID: 7363 Comm: a.out Not tainted 5.11.0-rc4 #1\n[  250.203144] Hardware name: Red Hat KVM, BIOS 0.5.1 01/01/2011\n[  250.203887] Call Trace:\n[  250.204302]  dump_stack+0xac/0xe3\n[  250.204804]  __lock_acquire+0xab6/0x13a0\n[  250.205392]  lock_acquire+0x2c3/0x390\n[  250.205928]  ? __io_req_task_submit+0x29/0xa0\n[  250.206541]  __mutex_lock+0xae/0x9f0\n[  250.207071]  ? __io_req_task_submit+0x29/0xa0\n[  250.207745]  ? 0xffffffffa0006083\n[  250.208248]  ? __io_req_task_submit+0x29/0xa0\n[  250.208845]  ? __io_req_task_submit+0x29/0xa0\n[  250.209452]  ? __io_req_task_submit+0x5/0xa0\n[  250.210083]  __io_req_task_submit+0x29/0xa0\n[  250.210687]  io_async_task_func+0x23d/0x4c0\n[  250.211278]  task_work_run+0x89/0xd0\n[  250.211884]  io_run_task_work_sig+0x50/0xc0\n[  250.212464]  io_sqe_files_unregister+0xb2/0x1f0\n[  250.213109]  __io_uring_register+0x115a/0x1750\n[  250.213718]  ? __x64_sys_io_uring_register+0xad/0x210\n[  250.214395]  ? __fget_files+0x15a/0x260\n[  250.214956]  __x64_sys_io_uring_register+0xbe/0x210\n[  250.215620]  ? trace_hardirqs_on+0x46/0x110\n[  250.216205]  do_syscall_64+0x2d/0x40\n[  250.216731]  entry_SYSCALL_64_after_hwframe+0x44/0xa9\n[  250.217455] RIP: 0033:0x7f0fa17e5239\n[  250.218034] Code: 01 00 48 81 c4 80 00 00 00 e9 f1 fe ff ff 0f 1f 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05  3d 01 f0 ff ff 73 01 c3 48 8b 0d 27 ec 2c 00 f7 d8 64 89 01 48\n[  250.220343] RSP: 002b:00007f0fa1eeac48 EFLAGS: 00000246 ORIG_RAX: 00000000000001ab\n[  250.221360] RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f0fa17e5239\n[  250.222272] RDX: 0000000000000000 RSI: 0000000000000003 RDI: 0000000000000008\n[  250.223185] RBP: 00007f0fa1eeae20 R08: 0000000000000000 R09: 0000000000000000\n[  250.224091] R10: 0000000000000000 R11: 0000000000000246 R12: 0000000000000000\n[  250.224999] R13: 0000000000021000 R14: 0000000000000000 R15: 00007f0fa1eeb700\n\nThis is caused by calling io_run_task_work_sig() to do work under\nuring_lock while the caller io_sqe_files_unregister() already held\nuring_lock.\nTo fix this issue, briefly drop uring_lock when calling\nio_run_task_work_sig(), and there are two things to concern:\n\n- hold uring_lock in io_ring_ctx_free() around io_sqe_files_unregister()\n    this is for consistency of lock/unlock.\n- add new fixed rsrc ref node before dropping uring_lock\n    it's not safe to do io_uring_enter-->percpu_ref_get() with a dying one.\n- check if rsrc_data->refs is dying to avoid parallel io_sqe_files_unregister\n\nReported-by: Abaci <abaci@linux.alibaba.com>\nFixes: 1ffc54220c44 (\"io_uring: fix io_sqe_files_unregister() hangs\")\nSuggested-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\n[axboe: fixes from Pavel folded in]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-20 19:02:12 -0700 io_uring: don't hold uring_lock when calling io_run_task_work*"
    },
    {
        "commit": "a3df769899c0bdc224c94d1d8cc9cbb3f3a72553",
        "message": "In case of failure io_wq_submit_work() needs to post an CQE and so\npotentially take uring_lock. The safest way to deal with it is to do\nthat from under task_work where we can safely take the lock.\n\nAlso, as io_iopoll_check() holds the lock tight and releases it\nreluctantly, it will play nicer in the furuter with notifying an\niopolling task about new such pending failed requests.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-20 19:01:35 -0700 io_uring: fail io-wq submission from a task_work"
    },
    {
        "commit": "792bb6eb862333658bf1bd2260133f0507e2da8d",
        "message": "[   97.866748] a.out/2890 is trying to acquire lock:\n[   97.867829] ffff8881046763e8 (&ctx->uring_lock){+.+.}-{3:3}, at:\nio_wq_submit_work+0x155/0x240\n[   97.869735]\n[   97.869735] but task is already holding lock:\n[   97.871033] ffff88810dfe0be8 (&ctx->uring_lock){+.+.}-{3:3}, at:\n__x64_sys_io_uring_enter+0x3f0/0x5b0\n[   97.873074]\n[   97.873074] other info that might help us debug this:\n[   97.874520]  Possible unsafe locking scenario:\n[   97.874520]\n[   97.875845]        CPU0\n[   97.876440]        ----\n[   97.877048]   lock(&ctx->uring_lock);\n[   97.877961]   lock(&ctx->uring_lock);\n[   97.878881]\n[   97.878881]  *** DEADLOCK ***\n[   97.878881]\n[   97.880341]  May be due to missing lock nesting notation\n[   97.880341]\n[   97.881952] 1 lock held by a.out/2890:\n[   97.882873]  #0: ffff88810dfe0be8 (&ctx->uring_lock){+.+.}-{3:3}, at:\n__x64_sys_io_uring_enter+0x3f0/0x5b0\n[   97.885108]\n[   97.885108] stack backtrace:\n[   97.890457] Call Trace:\n[   97.891121]  dump_stack+0xac/0xe3\n[   97.891972]  __lock_acquire+0xab6/0x13a0\n[   97.892940]  lock_acquire+0x2c3/0x390\n[   97.894894]  __mutex_lock+0xae/0x9f0\n[   97.901101]  io_wq_submit_work+0x155/0x240\n[   97.902112]  io_wq_cancel_cb+0x162/0x490\n[   97.904126]  io_async_find_and_cancel+0x3b/0x140\n[   97.905247]  io_issue_sqe+0x86d/0x13e0\n[   97.909122]  __io_queue_sqe+0x10b/0x550\n[   97.913971]  io_queue_sqe+0x235/0x470\n[   97.914894]  io_submit_sqes+0xcce/0xf10\n[   97.917872]  __x64_sys_io_uring_enter+0x3fb/0x5b0\n[   97.921424]  do_syscall_64+0x2d/0x40\n[   97.922329]  entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nWhile holding uring_lock, e.g. from inline execution, async cancel\nrequest may attempt cancellations through io_wq_submit_work, which may\ntry to grab a lock. Delay it to task_work, so we do it from a clean\ncontext and don't have to worry about locking.\n\nCc: <stable@vger.kernel.org> # 5.5+\nFixes: c07e6719511e (\"io_uring: hold uring_lock while completing failed polled io in io_wq_submit_work()\")\nReported-by: Abaci <abaci@linux.alibaba.com>\nReported-by: Hao Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-18 16:15:31 -0700 io_uring: don't take uring_lock during iowq cancel"
    },
    {
        "commit": "de59bc104c24f2e8637464a9e3ebbd8fd4c0f115",
        "message": "Instead of marking a link with REQ_F_FAIL_LINK on an error and delaying\nits failing to the caller, do it eagerly right when after getting an\nerror in io_submit_sqe(). This renders FAIL_LINK checks in\nio_queue_link_head() useless and we can skip it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-18 13:13:18 -0700 io_uring: fail links more in io_submit_sqe()"
    },
    {
        "commit": "1ee43ba8d267b5e6729c45b8756263f69c2978cc",
        "message": "Now, as we can do async setup without holding an SQE, we can skip doing\nio_req_defer_prep() for link heads, it will be tried to be executed\ninline and follows all the rules of the non-linked requests.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-18 13:13:18 -0700 io_uring: don't do async setup for links' heads"
    },
    {
        "commit": "be7053b7d028dc891857ca3e23b401a901257789",
        "message": "Now as preparations are split from async setup, we can do the first one\npretty early not spilling it across multiple call sites. And after it's\ndone SQE is not needed anymore and we can save on passing it deeply into\nthe submission stack.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-18 13:13:18 -0700 io_uring: do io_*_prep() early in io_submit_sqe()"
    },
    {
        "commit": "93642ef8843445f72a1e6b0c68914746c7aa5b9c",
        "message": "There are two kinds of opcode-specific preparations we do. The first is\njust initialising req with what is always needed for an opcode and\nreading all non-generic SQE fields. And the second is copying some of\nthe stuff like iovec preparing to punt a request to somewhere async,\ne.g. to io-wq or for draining. For requests that have tried an inline\nexecution but still needing to be punted, the second prep type is done\nby the opcode handler itself.\n\nCurrently, we don't explicitly split those preparation steps, but\ncombining both of them into io_*_prep(), altering the behaviour by\nallocating ->async_data. That's pretty messy and hard to follow and also\ngets in the way of some optimisations.\n\nSplit the steps, leave the first type as where it is now, and put the\nsecond into a new io_req_prep_async() helper. It may make us to do opcode\nswitch twice, but it's worth it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-18 13:13:18 -0700 io_uring: split sqe-prep and async setup"
    },
    {
        "commit": "cf109604265156bb22c45e0c2aa62f53a697a3f4",
        "message": "If we get an error in io_init_req() for a request that would have been\nlinked, we break the submission but still issue a partially composed\nlink, that's nasty, fail it instead.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-18 13:13:18 -0700 io_uring: don't submit link on error"
    },
    {
        "commit": "a1ab7b35db8f262cd74edff62b47b4d90f84f997",
        "message": "Move struct io_submit_link into submit_state, which is a part of a\nsubmission state and so belongs to it. It saves us from explicitly\npassing it, and init/deinit is now nicely hidden in\nio_submit_state_[start,end].\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-18 13:13:18 -0700 io_uring: move req link into submit_state"
    },
    {
        "commit": "a6b8cadcea86da0fe92de5c2e6e82824cb6fb57c",
        "message": "Behaves identically, just move io_init_req() call into the beginning of\nio_submit_sqes(). That looks better unloads io_submit_sqes().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-18 13:13:18 -0700 io_uring: move io_init_req() into io_submit_sqe()"
    },
    {
        "commit": "b16fed66bc7dca1a5dfd0af8991e9f58b5ef8d5f",
        "message": "A preparation patch, symbol to symbol move io_init_req() +\nio_check_restriction() a bit up. The submission path is pretty settled\ndown, so don't worry about backports and move the functions instead of\nrelying on forward declarations in the future.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-18 13:13:18 -0700 io_uring: move io_init_req()'s definition"
    },
    {
        "commit": "441960f3b9b8ee6aeea847e3e67093e0840e7059",
        "message": "IORING_OP_SYNC_FILE_RANGE is marked as .needs_file, so the common path\nwill take care of assigning and validating req->file, no need to\nduplicate it in io_sfr_prep().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-18 13:13:18 -0700 io_uring: don't duplicate ->file check in sfr"
    },
    {
        "commit": "1155c76a248364dd182bde90fea6f5682a6a766f",
        "message": "Follow io_*_prep() naming pattern, there are only fsync and sfr that\ndon't do that.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-18 13:13:18 -0700 io_uring: keep io_*_prep() naming consistent"
    },
    {
        "commit": "46c4e16a8625f7afdd8eee1ac8c3b3e592cba974",
        "message": "@i and @submitted are very much coupled together, and there is no need\nto keep them both. Remove @i, it doesn't change generated binary but\nhelps to keep a single source of truth.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-18 13:13:18 -0700 io_uring: kill fictitious submit iteration index"
    },
    {
        "commit": "fe1cdd558619546f76643878e7aa521c32d52131",
        "message": "Don't forget to free iovec read inline completion and bunch of other\ncases that do \"goto done\" before setting up an async context.\n\nFixes: 5ea5dd45844d (\"io_uring: inline io_read()'s iovec freeing\")\nReported-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-17 14:27:51 -0700 io_uring: fix read memory leak"
    },
    {
        "commit": "0b81e80c813f92520667c872d499a2dba8377be6",
        "message": "We add task_work from any context, hence we need to ensure that we can\ntolerate it being from IRQ context as well.\n\nFixes: 7cbf1722d5fc (\"io_uring: provide FIFO ordering for task_work\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-16 11:11:20 -0700 io_uring: tctx->task_lock should be IRQ safe"
    },
    {
        "commit": "0d4370cfe36b7f1719123b621a4ec4d9c7a25f89",
        "message": "If this is attempted by an io-wq kthread, then return -EOPNOTSUPP as we\ndon't currently support that. Once we can get task_pid_ptr() doing the\nright thing, then this can go away again.\n\nUse PF_IO_WORKER for this to speciically target the io_uring workers.\nModify the /proc/self/ check to use PF_IO_WORKER as well.\n\nCc: stable@vger.kernel.org\nFixes: 8d4c3e76e3be (\"proc: don't allow async path resolution of /proc/self components\")\nReported-by: Eric W. Biederman <ebiederm@xmission.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-15 11:02:16 -0700 proc: don't allow async path resolution of /proc/thread-self components"
    },
    {
        "commit": "41be53e94fb04cc69fdf2f524c2a05d8069e047b",
        "message": "Be nice and prune these upfront, in case the ring is being shared and\none of the tasks is going away. This is a bit more important now that\nwe account the allocations.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-13 09:11:04 -0700 io_uring: kill cached requests from exiting task closing the ring"
    },
    {
        "commit": "9a4fdbd8ee0d8aca0cb5692446e5ca583b230cd7",
        "message": "We have three different ones, put it in a helper for easy calling. This\nis in preparation for doing it outside of ring freeing as well. With\nthat in mind, also ensure that we do the proper locking for safe calling\nfrom a context where the ring it still live.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-13 09:09:44 -0700 io_uring: add helper to free all request caches"
    },
    {
        "commit": "68e68ee6e359318c40891f614612616d219066d0",
        "message": "No changes in this patch, just allows a caller to pass in a targeted\ntask that we must match for freeing requests in the cache.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-13 09:00:02 -0700 io_uring: allow task match to be passed to io_req_cache_free()"
    },
    {
        "commit": "c6d8570e4d642a0c0bfbe7362ffa1b1433c72db1",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Revert of a patch from this release that caused a regression\"\n\n* tag 'io_uring-5.11-2021-02-12' of git://git.kernel.dk/linux-block:\n  Revert \"io_uring: don't take fs for recvmsg/sendmsg\"",
        "kernel_version": "v5.11",
        "release_date": "2021-02-12 11:48:02 -0800 Merge tag 'io_uring-5.11-2021-02-12' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "5be9ad1e4287e1742fd8d253267c86446441bdaf",
        "message": "Invalid req->flags are tolerated by free/put well, avoid this dancing\nneedlessly presetting it to zero, and then not even resetting but\nmodifying it, i.e. \"|=\".\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-12 11:49:50 -0700 io_uring: optimise io_init_req() flags setting"
    },
    {
        "commit": "cdbff98223330cdb6c57ead1533ce066dddd61b7",
        "message": "Indirectly io_req_find_next() is called for every request, optimise the\ncheck by testing flags as it was long before -- __io_req_find_next()\ntolerates false-positives well (i.e. link==NULL), and those should be\nreally rare.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-12 11:49:49 -0700 io_uring: clean io_req_find_next() fast check"
    },
    {
        "commit": "dc0eced5d92052a84d58df03a3bc6382f64fecfa",
        "message": "io_sq_thread_acquire_mm_files() can find a PF_EXITING task only when\nit's called from task_work context. Don't check it in all other cases,\nthat are when we're in io_uring_enter().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-12 11:49:48 -0700 io_uring: don't check PF_EXITING from syscall"
    },
    {
        "commit": "4fccfcbb733794634d4e873e7973c1847beca5bf",
        "message": "Remove io_consume_sqe() and inline it back into io_get_sqe(). It\nrequires req dealloc on error, but in exchange we get cleaner\nio_submit_sqes() and better locality for cached_sq_head.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-12 05:30:36 -0700 io_uring: don't split out consume out of SQE get"
    },
    {
        "commit": "04fc6c802dfacba800f5a5d00bea0ebfcc60f840",
        "message": "Do a little trick in io_ring_ctx_free() briefly taking uring_lock, that\nwill wait for everyone currently holding it, so we can skip pinning ctx\nwith ctx->refs for __io_req_task_submit(), which is executed and loses\nits refs/reqs while holding the lock.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-12 05:30:25 -0700 io_uring: save ctx put/get for task_work submit"
    },
    {
        "commit": "921b9054e0c4c443c479c21800f6c4c8b43fa1b0",
        "message": "Don't hand code io_req_task_queue() inside of io_async_buf_func(), just\ncall it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-12 05:30:25 -0700 io_uring: don't duplicate io_req_task_queue()"
    },
    {
        "commit": "4e32635834a30b8aa9583d3899a8ecc6416023fb",
        "message": "There are two reasons for this. First is to optimise\nio_sq_thread_acquire_mm_files() for non-SQPOLL case, which currently do\ntoo many checks and function calls in the hot path, e.g. in\nio_init_req().\n\nThe second is to not grab mm/files when there are not needed. As\n__io_queue_sqe() issues only one request now, we can reuse\nio_sq_thread_acquire_mm_files() instead of unconditional acquire\nmm/files.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-12 05:30:25 -0700 io_uring: optimise SQPOLL mm/files grabbing"
    },
    {
        "commit": "d3d7298d05cb026305b0f5033acc9c9c4f281e14",
        "message": "__io_queue_sqe() tries to issue as much requests of a link as it can,\nand uses io_put_req_find_next() to extract a next one, targeting inline\ncompleted requests. As now __io_queue_sqe() is always used together with\nstruct io_comp_state, it leaves next propagation only a small window and\nonly for async reqs, that doesn't justify its existence.\n\nRemove it, make __io_queue_sqe() to issue only a head request. It\nsimplifies the code and will allow other optimisations.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-12 05:30:25 -0700 io_uring: optimise out unlikely link queue"
    },
    {
        "commit": "bd75904590de1c2bbdff55180cef209b13bd50fa",
        "message": "Completion and submission states are now coupled together, it's weird to\nget one from argument and another from ctx, do it consistently for\nio_req_free_batch(). It's also faster as we already have @state cached\nin registers.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-12 05:30:25 -0700 io_uring: take compl state from submit state"
    },
    {
        "commit": "dcc0b49040c70ad827a7f3d58a21b01fdb14e749",
        "message": "Pull powerpc fix from Michael Ellerman:\n \"One fix for a regression seen in io_uring, introduced by our support\n  for KUAP (Kernel User Access Prevention) with the Hash MMU.\n\n  Thanks to Aneesh Kumar K.V, and Zorro Lang\"\n\n* tag 'powerpc-5.11-8' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux:\n  powerpc/kuap: Allow kernel thread to access userspace after kthread_use_mm",
        "kernel_version": "v5.11",
        "release_date": "2021-02-11 15:41:07 -0800 Merge tag 'powerpc-5.11-8' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux"
    },
    {
        "commit": "2f8e45f16c57360dd4d8b1310c2952a29a8fa890",
        "message": "__io_complete_rw() casts request to kiocb for it to be immediately\ncontainer_of()'ed by io_complete_rw_common(). And the last function's name\ndoesn't do a great job of illuminating its purposes, so just inline it in\nits only user.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-11 11:42:19 -0700 io_uring: inline io_complete_rw_common()"
    },
    {
        "commit": "23faba36ce287e4af9018dea51893a1067701508",
        "message": "We pass return code into io_rw_reissue() only to be able to check if it's\n-EAGAIN. That's not the cleanest approach and may prevent inlining of the\nnon-EAGAIN fast path, so do it at call sites.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-11 11:41:49 -0700 io_uring: move res check out of io_rw_reissue()"
    },
    {
        "commit": "f161340d9e85b9038031b497b32383e50ff00ca1",
        "message": "Don't stash -EAGAIN'ed iopoll requests into a list to reissue it later,\ndo it eagerly. It removes overhead on keeping and checking that list,\nand allows in case of failure for these requests to be completed through\nnormal iopoll completion path.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-11 11:40:42 -0700 io_uring: simplify iopoll reissuing"
    },
    {
        "commit": "6e833d538b3123767393c987d11c40b7728b3f79",
        "message": "io_req_free_batch_finish() is final and does not permit struct req_batch\nto be reused without re-init. To be more consistent don't clear ->task\nthere.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-11 11:40:40 -0700 io_uring: clean up io_req_free_batch_finish()"
    },
    {
        "commit": "3c1a2ead915c1bcb7b1f9e902469ea0ee1f7857f",
        "message": "We recently added the submit side req cache, but it was placed at the\nend of the struct. Move it near the other submission state for better\nmemory placement, and reshuffle a few other members at the same time.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-11 10:48:03 -0700 io_uring: move submit side state closer in the ring"
    },
    {
        "commit": "e68a3ff8c342b655f01f74a577c15605eec9aa12",
        "message": "We use the assigned slot in io_sqe_file_register(), and a previous\npatch moved the assignment to after we have called it. This isn't\nsuper pretty, and will get cleaned up in the future. For now, fix\nthe regression by restoring the previous assignment/clear of the\nfile_slot.\n\nFixes: ea64ec02b31d (\"io_uring: deduplicate file table slot calculation\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-11 07:45:08 -0700 io_uring: assign file_slot prior to calling io_sqe_file_register()"
    },
    {
        "commit": "4a245479c2312e6b51862c21af134d4191ab9cf7",
        "message": "The variable ret is being initialized with a value that is never read\nand it is being updated later with a new value.  The initialization is\nredundant and can be removed.\n\nAddresses-Coverity: (\"Unused value\")\nFixes: b63534c41e20 (\"io_uring: re-issue block requests that failed because of resources\")\nSigned-off-by: Colin Ian King <colin.king@canonical.com>\nReviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 13:28:41 -0700 io_uring: remove redundant initialization of variable ret"
    },
    {
        "commit": "34343786ecc5ff493ca4d1f873b4386759ba52ee",
        "message": "We park SQPOLL task before going into io_uring_cancel_files(), so the\ntask won't run task_works including those that might be important for\nthe cancellation passes. In this case it's io_poll_remove_one(), which\nfrees requests via io_put_req_deferred().\n\nUnpark it for while waiting, it's ok as we disable submissions\nbeforehand, so no new requests will be generated.\n\nINFO: task syz-executor893:8493 blocked for more than 143 seconds.\nCall Trace:\n context_switch kernel/sched/core.c:4327 [inline]\n __schedule+0x90c/0x21a0 kernel/sched/core.c:5078\n schedule+0xcf/0x270 kernel/sched/core.c:5157\n io_uring_cancel_files fs/io_uring.c:8912 [inline]\n io_uring_cancel_task_requests+0xe70/0x11a0 fs/io_uring.c:8979\n __io_uring_files_cancel+0x110/0x1b0 fs/io_uring.c:9067\n io_uring_files_cancel include/linux/io_uring.h:51 [inline]\n do_exit+0x2fe/0x2ae0 kernel/exit.c:780\n do_group_exit+0x125/0x310 kernel/exit.c:922\n __do_sys_exit_group kernel/exit.c:933 [inline]\n __se_sys_exit_group kernel/exit.c:931 [inline]\n __x64_sys_exit_group+0x3a/0x50 kernel/exit.c:931\n do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nCc: stable@vger.kernel.org # 5.5+\nReported-by: syzbot+695b03d82fa8e4901b06@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 13:28:41 -0700 io_uring: unpark SQPOLL thread for cancelation"
    },
    {
        "commit": "92c75f7594d5060a4cb240f0e987a802f8486b11",
        "message": "This reverts commit 10cad2c40dcb04bb46b2bf399e00ca5ea93d36b0.\n\nPetr reports that with this commit in place, io_uring fails the chroot\ntest (CVE-202-29373). We do need to retain ->fs for send/recvmsg, so\nrevert this commit.\n\nReported-by: Petr Vorel <pvorel@suse.cz>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11",
        "release_date": "2021-02-10 12:37:58 -0700 Revert \"io_uring: don't take fs for recvmsg/sendmsg\""
    },
    {
        "commit": "26bfa89e25f42d2b26fe951bbcf04bb13937fbba",
        "message": "Instead of imposing rlimit memlock limits for the rings themselves,\nensure that we account them properly under memcg with __GFP_ACCOUNT.\nWe retain rlimit memlock for registered buffers, this is just for the\nring arrays themselves.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:33:15 -0700 io_uring: place ring SQ/CQ arrays under memcg memory limits"
    },
    {
        "commit": "91f245d5d5de0802428a478802ec051f7de2f5d6",
        "message": "This puts io_uring under the memory cgroups accounting and limits for\nrequests.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:33:15 -0700 io_uring: enable kmemcg account for io_uring requests"
    },
    {
        "commit": "c7dae4ba46c9d7d56430b800907b708711995414",
        "message": "This is the last class of requests that cannot utilize the req alloc\ncache. Add a per-ctx req cache that is protected by the completion_lock,\nand refill our submit side cache when it gets over our batch count.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:33:12 -0700 io_uring: enable req cache for IRQ driven IO"
    },
    {
        "commit": "ed670c3f90a67d9e16ab6d8893be6f072d79cd4c",
        "message": "Abaci reported follow issue:\n\n[   30.615891] ======================================================\n[   30.616648] WARNING: possible circular locking dependency detected\n[   30.617423] 5.11.0-rc3-next-20210115 #1 Not tainted\n[   30.618035] ------------------------------------------------------\n[   30.618914] a.out/1128 is trying to acquire lock:\n[   30.619520] ffff88810b063868 (&ep->mtx){+.+.}-{3:3}, at: __ep_eventpoll_poll+0x9f/0x220\n[   30.620505]\n[   30.620505] but task is already holding lock:\n[   30.621218] ffff88810e952be8 (&ctx->uring_lock){+.+.}-{3:3}, at: __x64_sys_io_uring_enter+0x3f0/0x5b0\n[   30.622349]\n[   30.622349] which lock already depends on the new lock.\n[   30.622349]\n[   30.623289]\n[   30.623289] the existing dependency chain (in reverse order) is:\n[   30.624243]\n[   30.624243] -> #1 (&ctx->uring_lock){+.+.}-{3:3}:\n[   30.625263]        lock_acquire+0x2c7/0x390\n[   30.625868]        __mutex_lock+0xae/0x9f0\n[   30.626451]        io_cqring_overflow_flush.part.95+0x6d/0x70\n[   30.627278]        io_uring_poll+0xcb/0xd0\n[   30.627890]        ep_item_poll.isra.14+0x4e/0x90\n[   30.628531]        do_epoll_ctl+0xb7e/0x1120\n[   30.629122]        __x64_sys_epoll_ctl+0x70/0xb0\n[   30.629770]        do_syscall_64+0x2d/0x40\n[   30.630332]        entry_SYSCALL_64_after_hwframe+0x44/0xa9\n[   30.631187]\n[   30.631187] -> #0 (&ep->mtx){+.+.}-{3:3}:\n[   30.631985]        check_prevs_add+0x226/0xb00\n[   30.632584]        __lock_acquire+0x1237/0x13a0\n[   30.633207]        lock_acquire+0x2c7/0x390\n[   30.633740]        __mutex_lock+0xae/0x9f0\n[   30.634258]        __ep_eventpoll_poll+0x9f/0x220\n[   30.634879]        __io_arm_poll_handler+0xbf/0x220\n[   30.635462]        io_issue_sqe+0xa6b/0x13e0\n[   30.635982]        __io_queue_sqe+0x10b/0x550\n[   30.636648]        io_queue_sqe+0x235/0x470\n[   30.637281]        io_submit_sqes+0xcce/0xf10\n[   30.637839]        __x64_sys_io_uring_enter+0x3fb/0x5b0\n[   30.638465]        do_syscall_64+0x2d/0x40\n[   30.638999]        entry_SYSCALL_64_after_hwframe+0x44/0xa9\n[   30.639643]\n[   30.639643] other info that might help us debug this:\n[   30.639643]\n[   30.640618]  Possible unsafe locking scenario:\n[   30.640618]\n[   30.641402]        CPU0                    CPU1\n[   30.641938]        ----                    ----\n[   30.642664]   lock(&ctx->uring_lock);\n[   30.643425]                                lock(&ep->mtx);\n[   30.644498]                                lock(&ctx->uring_lock);\n[   30.645668]   lock(&ep->mtx);\n[   30.646321]\n[   30.646321]  *** DEADLOCK ***\n[   30.646321]\n[   30.647642] 1 lock held by a.out/1128:\n[   30.648424]  #0: ffff88810e952be8 (&ctx->uring_lock){+.+.}-{3:3}, at: __x64_sys_io_uring_enter+0x3f0/0x5b0\n[   30.649954]\n[   30.649954] stack backtrace:\n[   30.650592] CPU: 1 PID: 1128 Comm: a.out Not tainted 5.11.0-rc3-next-20210115 #1\n[   30.651554] Hardware name: Red Hat KVM, BIOS 0.5.1 01/01/2011\n[   30.652290] Call Trace:\n[   30.652688]  dump_stack+0xac/0xe3\n[   30.653164]  check_noncircular+0x11e/0x130\n[   30.653747]  ? check_prevs_add+0x226/0xb00\n[   30.654303]  check_prevs_add+0x226/0xb00\n[   30.654845]  ? add_lock_to_list.constprop.49+0xac/0x1d0\n[   30.655564]  __lock_acquire+0x1237/0x13a0\n[   30.656262]  lock_acquire+0x2c7/0x390\n[   30.656788]  ? __ep_eventpoll_poll+0x9f/0x220\n[   30.657379]  ? __io_queue_proc.isra.88+0x180/0x180\n[   30.658014]  __mutex_lock+0xae/0x9f0\n[   30.658524]  ? __ep_eventpoll_poll+0x9f/0x220\n[   30.659112]  ? mark_held_locks+0x5a/0x80\n[   30.659648]  ? __ep_eventpoll_poll+0x9f/0x220\n[   30.660229]  ? _raw_spin_unlock_irqrestore+0x2d/0x40\n[   30.660885]  ? trace_hardirqs_on+0x46/0x110\n[   30.661471]  ? __io_queue_proc.isra.88+0x180/0x180\n[   30.662102]  ? __ep_eventpoll_poll+0x9f/0x220\n[   30.662696]  __ep_eventpoll_poll+0x9f/0x220\n[   30.663273]  ? __ep_eventpoll_poll+0x220/0x220\n[   30.663875]  __io_arm_poll_handler+0xbf/0x220\n[   30.664463]  io_issue_sqe+0xa6b/0x13e0\n[   30.664984]  ? __lock_acquire+0x782/0x13a0\n[   30.665544]  ? __io_queue_proc.isra.88+0x180/0x180\n[   30.666170]  ? __io_queue_sqe+0x10b/0x550\n[   30.666725]  __io_queue_sqe+0x10b/0x550\n[   30.667252]  ? __fget_files+0x131/0x260\n[   30.667791]  ? io_req_prep+0xd8/0x1090\n[   30.668316]  ? io_queue_sqe+0x235/0x470\n[   30.668868]  io_queue_sqe+0x235/0x470\n[   30.669398]  io_submit_sqes+0xcce/0xf10\n[   30.669931]  ? xa_load+0xe4/0x1c0\n[   30.670425]  __x64_sys_io_uring_enter+0x3fb/0x5b0\n[   30.671051]  ? lockdep_hardirqs_on_prepare+0xde/0x180\n[   30.671719]  ? syscall_enter_from_user_mode+0x2b/0x80\n[   30.672380]  do_syscall_64+0x2d/0x40\n[   30.672901]  entry_SYSCALL_64_after_hwframe+0x44/0xa9\n[   30.673503] RIP: 0033:0x7fd89c813239\n[   30.673962] Code: 01 00 48 81 c4 80 00 00 00 e9 f1 fe ff ff 0f 1f 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05  3d 01 f0 ff ff 73 01 c3 48 8b 0d 27 ec 2c 00 f7 d8 64 89 01 48\n[   30.675920] RSP: 002b:00007ffc65a7c628 EFLAGS: 00000217 ORIG_RAX: 00000000000001aa\n[   30.676791] RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007fd89c813239\n[   30.677594] RDX: 0000000000000000 RSI: 0000000000000014 RDI: 0000000000000003\n[   30.678678] RBP: 00007ffc65a7c720 R08: 0000000000000000 R09: 0000000003000000\n[   30.679492] R10: 0000000000000000 R11: 0000000000000217 R12: 0000000000400ff0\n[   30.680282] R13: 00007ffc65a7c840 R14: 0000000000000000 R15: 0000000000000000\n\nThis might happen if we do epoll_wait on a uring fd while reading/writing\nthe former epoll fd in a sqe in the former uring instance.\nSo let's don't flush cqring overflow list, just do a simple check.\n\nReported-by: Abaci <abaci@linux.alibaba.com>\nFixes: 6c503150ae33 (\"io_uring: patch up IOPOLL overflow_flush sync\")\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:28:44 -0700 io_uring: fix possible deadlock in io_uring_poll"
    },
    {
        "commit": "e5d1bc0a91f16959aa279aa3ee9fdc246d4bb382",
        "message": "Awhile there are requests in the allocation cache -- use them, only if\nthose ended go for the stashed memory in comp.free_list. As list\nmanipulation are generally heavy and are not good for caches, flush them\nall or as much as can in one go.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\n[axboe: return success/failure from io_flush_cached_reqs()]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:28:43 -0700 io_uring: defer flushing cached reqs"
    },
    {
        "commit": "c5eef2b9449ba267f53bfa7cf63d2bc93acbee32",
        "message": "__io_queue_sqe() is always called with a non-NULL comp_state, which is\ntaken directly from context. Don't pass it around but infer from ctx.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:28:43 -0700 io_uring: take comp_state from ctx"
    },
    {
        "commit": "65453d1efbd20f3825beba2a9c93ffb2ec729ece",
        "message": "task_work is run without utilizing the req alloc cache, so any deferred\nitems don't get to take advantage of either the alloc or free side of it.\nWith task_work now being wrapped by io_uring, we can use the ctx\ncompletion state to both use the req cache and the completion flush\nbatching.\n\nWith this, the only request type that cannot take advantage of the req\ncache is IRQ driven IO for regular files / block devices. Anything else,\nincluding IOPOLL polled IO to those same tyes, will take advantage of it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:28:43 -0700 io_uring: enable req cache for task_work items"
    },
    {
        "commit": "7cbf1722d5fc5779946ee8f338e9e38b5de15856",
        "message": "task_work is a LIFO list, due to how it's implemented as a lockless\nlist. For long chains of task_work, this can be problematic as the\nfirst entry added is the last one processed. Similarly, we'd waste\na lot of CPU cycles reversing this list.\n\nWrap the task_work so we have a single task_work entry per task per\nctx, and use that to run it in the right order.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:28:43 -0700 io_uring: provide FIFO ordering for task_work"
    },
    {
        "commit": "1b4c351f6eb7467c77fc19e0cd7e5f0083ecd847",
        "message": "Now that we have the submit_state in the ring itself, we can have io_kiocb\nallocations that are persistent across invocations. This reduces the time\nspent doing slab allocations and frees.\n\n[sil: rebased]\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:28:43 -0700 io_uring: use persistent request cache"
    },
    {
        "commit": "6ff119a6e4c3fe900e75e6667930dc086f185f2b",
        "message": "Make io_req_free_batch(), which is used for inline executed requests and\nIOPOLL, to return requests back into the allocation cache, so avoid\nmost of kmalloc()/kfree() for those cases.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:28:43 -0700 io_uring: feed reqs back into alloc cache"
    },
    {
        "commit": "bf019da7fcbe7e42372582cc339fd1fb8e1e4fa5",
        "message": "Don't free batch-allocated requests across syscalls.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:28:43 -0700 io_uring: persistent req cache"
    },
    {
        "commit": "9ae7246321d2b735867f6767e0fab96dd248c555",
        "message": "Currently batch free handles request memory freeing and ctx ref putting\ntogether. Separate them and use different counters, that will be needed\nfor reusing reqs memory.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:28:43 -0700 io_uring: count ctx refs separately from reqs"
    },
    {
        "commit": "3893f39f2245eec04b8052cd441c2cb8a9ea3447",
        "message": "Remove fallback_req for now, it gets in the way of other changes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:28:43 -0700 io_uring: remove fallback_req"
    },
    {
        "commit": "905c172f32c56f0740630b639ca5c10ba3689da0",
        "message": "io_submit_flush_completions() does completion batching, but may also use\nfree batching as iopoll does. The main beneficiaries should be buffered\nreads/writes and send/recv.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:28:43 -0700 io_uring: submit-completion free batching"
    },
    {
        "commit": "6dd0be1e2481b32c39870e187840ade6c2a11a72",
        "message": "Reincarnation of an old patch that replaces a list in struct\nio_compl_batch with an array. It's needed to avoid hooking requests via\ntheir compl.list, because it won't be always available in the future.\n\nIt's also nice to split io_submit_flush_completions() to avoid free\nunder locks and remove unlock/lock with a long comment describing when\nit can be done.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:28:43 -0700 io_uring: replace list with array for compl batch"
    },
    {
        "commit": "5087275dba02943179720bd95d1d6c7047007550",
        "message": "As now submit_state is retained across syscalls, we can save ourself\nfrom initialising it from ground up for each io_submit_sqes(). Set some\nfields during ctx allocation, and just keep them always consistent.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\n[axboe: remove unnecessary zeroing of ctx members]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:28:43 -0700 io_uring: don't reinit submit state every time"
    },
    {
        "commit": "ba88ff112bdfde8103a8143f867bcdc46bc0e50f",
        "message": "completion state is closely bound to ctx, we don't need to store ctx\ninside as we always have it around to pass to flush.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:28:43 -0700 io_uring: remove ctx from comp_state"
    },
    {
        "commit": "258b29a93bfe74a57c01e1b10b698d5b62e173fe",
        "message": "struct io_submit_state is quite big (168 bytes) and going to grow. It's\nbetter to not keep it on stack as it is now. Move it to context, it's\nalways protected by uring_lock, so it's fine to have only one instance\nof it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:28:42 -0700 io_uring: don't keep submit_state on stack"
    },
    {
        "commit": "889fca73287b0ae21c9d8712379c9ae5a3b27d08",
        "message": "There is no reason to drag io_comp_state into opcode handlers, we just\nneed a flag and the actual work will be done in __io_queue_sqe().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-10 07:28:38 -0700 io_uring: don't propagate io_comp_state"
    },
    {
        "commit": "61e98203047983fd959cfef889b328a57315847c",
        "message": "Make opcode handler interfaces a bit more consistent by always passing\nin issue flags. Bulky but pretty easy and mechanical change.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-09 19:15:14 -0700 io_uring: make op handlers always take issue flags"
    },
    {
        "commit": "45d189c6062922ffe272e98013ba464b355dede7",
        "message": "Replace bool force_nonblock with flags. It has a long standing goal of\ndifferentiating context from which we execute. Currently we have some\nsubtle places where some invariants, like holding of uring_lock, are\nsubtly inferred.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-09 19:15:13 -0700 io_uring: replace force_nonblock with flags"
    },
    {
        "commit": "0e9ddb39b7d964d716cddd6e6bd1aab3f800066e",
        "message": "For SQPOLL rings tctx_inflight() always returns zero, so it might skip\ndoing full cancelation. It's fine because we jam all sqpoll submissions\nin any case and do go through files cancel for them, but not nice.\n\nDo the intended full cancellation, by mimicking __io_uring_task_cancel()\nwaiting but impersonating SQPOLL task.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-08 08:27:25 -0700 io_uring: cleanup up cancel SQPOLL reqs across exec"
    },
    {
        "commit": "860b45dae969966a52b4dd0470d8fca8479e4e4b",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two small fixes that should go into 5.11:\n\n   - task_work resource drop fix (Pavel)\n\n   - identity COW fix (Xiaoguang)\"\n\n* tag 'io_uring-5.11-2021-02-05' of git://git.kernel.dk/linux-block:\n  io_uring: drop mm/files between task_work_submit\n  io_uring: don't modify identity's files uncess identity is cowed",
        "kernel_version": "v5.11-rc7",
        "release_date": "2021-02-06 14:37:24 -0800 Merge tag 'io_uring-5.11-2021-02-05' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "257e84a5377fbbc336ff563833a8712619acce56",
        "message": "Current iov handling with recvmsg/sendmsg may be confusing. First make a\nrule for msg->iov: either it points to an allocated iov that have to be\nkfree()'d later, or it's NULL and we use fast_iov. That's much better\nthan current 3-state (also can point to fast_iov). And rename it into\nfree_iov for uniformity with read/write.\n\nAlso, instead of after struct io_async_msghdr copy fixing up of\nmsg.msg_iter.iov has been happening in io_recvmsg()/io_sendmsg(). Move\nit into io_setup_async_msg(), that's the right place.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\n[axboe: add comment on NULL check before kfree()]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-05 07:46:22 -0700 io_uring: refactor sendmsg/recvmsg iov managing"
    },
    {
        "commit": "5476dfed29ad9b19d4e187685ab71bb9c496f965",
        "message": "Don't pretend we don't know that REQ_F_BUFFER_SELECT for recvmsg always\nuses fast_iov -- clean up confusing intermixing kmsg->iov and\nkmsg->fast_iov for buffer select.\n\nAlso don't init iter with garbage in __io_recvmsg_copy_hdr() only for it\nto be set shortly after in io_recvmsg().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-05 07:45:41 -0700 io_uring: clean iov usage for recvmsg buf select"
    },
    {
        "commit": "2a7808024b195a342779fb5d7b7df1c4af45cc71",
        "message": "io_setup_async_msg() should fully prepare io_async_msghdr, let it also\nhandle assigning msg_name and don't hand code it in [send,recv]msg().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-05 07:45:41 -0700 io_uring: set msg_name on msg fixup"
    },
    {
        "commit": "aec18a57edad562d620f7d19016de1fc0cc2208c",
        "message": "Since SQPOLL task can be shared and so task_work entries can be a mix of\nthem, we need to drop mm and files before trying to issue next request.\n\nCc: stable@vger.kernel.org # 5.10+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc7",
        "release_date": "2021-02-04 12:42:58 -0700 io_uring: drop mm/files between task_work_submit"
    },
    {
        "commit": "5280f7e530f71ba85baf90169393196976ad0e52",
        "message": "Saving one lock/unlock for io-wq is not super important, but adds some\nugliness in the code. More important, atomic decs not turning it to zero\nfor some archs won't give the right ordering/barriers so the\nio_steal_work() may pretty easily get subtly and completely broken.\n\nReturn back 2-step io-wq work exchange and clean it up.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-04 08:05:46 -0700 io_uring/io-wq: return 2-step work swap scheme"
    },
    {
        "commit": "ea64ec02b31d5b05ae94ac4d57e38f8a02117c76",
        "message": "Extract a helper io_fixed_file_slot() returning a place in our fixed\nfiles table, so we don't hand-code it three times in the code.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-04 08:05:46 -0700 io_uring: deduplicate file table slot calculation"
    },
    {
        "commit": "847595de1732a6e928f241929d24dde2e9ffaf15",
        "message": "io_import_iovec() doesn't return IO size anymore, only error code. Make\nit more apparent by returning int instead of ssize and clean up\nleftovers.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-04 08:05:46 -0700 io_uring: io_import_iovec return type cleanup"
    },
    {
        "commit": "75c668cdd6ca05dd9c7138a5a080c0088d72cf51",
        "message": "Make decision making of whether we need to retry read/write similar for\nO_NONBLOCK and RWF_NOWAIT. Set REQ_F_NOWAIT when either is specified and\nuse it for all relevant checks. Also fix resubmitting NOWAIT requests\nvia io_rw_reissue().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-04 08:05:46 -0700 io_uring: treat NONBLOCK and RWF_NOWAIT similarly"
    },
    {
        "commit": "b23df91bff954ebd8aee39eb22e5028f41cd9e56",
        "message": "We already have implicit do-while for read-retries but with goto in the\nend. Convert it to an actual do-while, it highlights it so making a\nbit more understandable and is cleaner in general.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-04 08:05:46 -0700 io_uring: highlight read-retry loop"
    },
    {
        "commit": "5ea5dd45844d1b727ab2a76f47d6e9aa65d1e921",
        "message": "io_read() has not the simpliest control flow with a lot of jumps and\nit's hard to read. One of those is a out_free: label, which frees iovec.\nHowever, from the middle of io_read() iovec is NULL'ed and so\nkfree(iovec) is no-op, it leaves us with two place where we can inline\nit and further clean up the code.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-04 08:05:46 -0700 io_uring: inline io_read()'s iovec freeing"
    },
    {
        "commit": "7335e3bf9d0a92be09bb4f38d06ab22c40f0fead",
        "message": "We have invariant in io_read() of how much we're trying to read spilled\ninto an iter and io_size variable. The last one controls decision making\nabout whether to do read-retries. However, io_size is modified only\nafter the first read attempt, so if we happen to go for a third retry in\na single call to io_read(), we will get io_size greater than in the\niterator, so may lead to various side effects up to live-locking.\n\nModify io_size each time.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-04 08:05:46 -0700 io_uring: don't forget to adjust io_size"
    },
    {
        "commit": "6bf985dc50dd882a95fffa9c7eef0d1416f512e6",
        "message": "Now we give out ownership of iovec into io_setup_async_rw(), so it\neither sets request's context right or frees the iovec on error itself.\nMakes our life a bit easier at call sites.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-04 08:05:46 -0700 io_uring: let io_setup_async_rw take care of iovec"
    },
    {
        "commit": "1a2cc0ce8d18c9e5592733cb6381e9ff5c23d916",
        "message": "First, instead of checking iov_iter_count(iter) for 0 to find out that\nall needed bytes were read, just compare returned code against io_size.\nIt's more reliable and arguably cleaner.\n\nAlso, place the half-read case into an else branch and delete an extra\nlabel.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-04 08:05:46 -0700 io_uring: further simplify do_read error parsing"
    },
    {
        "commit": "6713e7a6145a4b5a61e33a37f0b4d06ca6d2c6d8",
        "message": "!io_file_supports_async() case of io_read() is hard to read, it jumps\nsomewhere in the middle of the function just to do async setup and fail\non a similar check. Call io_setup_async_rw() directly for this case,\nit's much easier to follow.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-04 08:05:46 -0700 io_uring: refactor io_read for unsupported nowait"
    },
    {
        "commit": "eeb60b9ab4000d20261973642dfc9fb0e4b5d073",
        "message": "It's easy to make a mistake in io_cqring_wait() because for all\nbreak/continue clauses we need to watch for prepare/finish_wait to be\nused correctly. Extract all those into a new helper\nio_cqring_wait_schedule(), and transforming the loop into simple series\nof func calls: prepare(); check_and_schedule(); finish();\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-04 08:05:46 -0700 io_uring: refactor io_cqring_wait"
    },
    {
        "commit": "c1d5a224683b333ddbe278e455d639ccd4f5ca2b",
        "message": "schedule_timeout() with timeout=MAX_SCHEDULE_TIMEOUT is guaranteed to\nwork just as schedule(), so instead of hand-coding it based on arguments\nalways use the timeout version and simplify code.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-04 08:05:46 -0700 io_uring: refactor scheduling in io_cqring_wait"
    },
    {
        "commit": "9936c7c2bc76a0b2276f6d19de6d1d92f03deeab",
        "message": "Files and task cancellations go over same steps trying to cancel\nrequests in io-wq, poll, etc. Deduplicate it with a helper.\n\nnote: new io_uring_try_cancel_requests() is former\n__io_uring_cancel_task_requests() with files passed as an agrument and\nflushing overflowed requests.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-04 08:05:46 -0700 io_uring: deduplicate core cancellations sequence"
    },
    {
        "commit": "d7e10d47691d1702db1cd1edcc689d3031eefc67",
        "message": "Abaci Robot reported following panic:\nBUG: kernel NULL pointer dereference, address: 0000000000000000\nPGD 800000010ef3f067 P4D 800000010ef3f067 PUD 10d9df067 PMD 0\nOops: 0002 [#1] SMP PTI\nCPU: 0 PID: 1869 Comm: io_wqe_worker-0 Not tainted 5.11.0-rc3+ #1\nHardware name: Red Hat KVM, BIOS 0.5.1 01/01/2011\nRIP: 0010:put_files_struct+0x1b/0x120\nCode: 24 18 c7 00 f4 ff ff ff e9 4d fd ff ff 66 90 0f 1f 44 00 00 41 57 41 56 49 89 fe 41 55 41 54 55 53 48 83 ec 08 e8 b5 6b db ff  41 ff 0e 74 13 48 83 c4 08 5b 5d 41 5c 41 5d 41 5e 41 5f e9 9c\nRSP: 0000:ffffc90002147d48 EFLAGS: 00010293\nRAX: 0000000000000000 RBX: ffff88810d9a5300 RCX: 0000000000000000\nRDX: ffff88810d87c280 RSI: ffffffff8144ba6b RDI: 0000000000000000\nRBP: 0000000000000080 R08: 0000000000000001 R09: ffffffff81431500\nR10: ffff8881001be000 R11: 0000000000000000 R12: ffff88810ac2f800\nR13: ffff88810af38a00 R14: 0000000000000000 R15: ffff8881057130c0\nFS:  0000000000000000(0000) GS:ffff88813bc00000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 0000000000000000 CR3: 000000010dbaa002 CR4: 00000000003706f0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\nCall Trace:\n __io_clean_op+0x10c/0x2a0\n io_dismantle_req+0x3c7/0x600\n __io_free_req+0x34/0x280\n io_put_req+0x63/0xb0\n io_worker_handle_work+0x60e/0x830\n ? io_wqe_worker+0x135/0x520\n io_wqe_worker+0x158/0x520\n ? __kthread_parkme+0x96/0xc0\n ? io_worker_handle_work+0x830/0x830\n kthread+0x134/0x180\n ? kthread_create_worker_on_cpu+0x90/0x90\n ret_from_fork+0x1f/0x30\nModules linked in:\nCR2: 0000000000000000\n---[ end trace c358ca86af95b1e7 ]---\n\nI guess case below can trigger above panic: there're two threads which\noperates different io_uring ctxs and share same sqthread identity, and\nlater one thread exits, io_uring_cancel_task_requests() will clear\ntask->io_uring->identity->files to be NULL in sqpoll mode, then another\nctx that uses same identity will panic.\n\nIndeed we don't need to clear task->io_uring->identity->files here,\nio_grab_identity() should handle identity->files changes well, if\ntask->io_uring->identity->files is not equal to current->files,\nio_cow_identity() should handle this changes well.\n\nCc: stable@vger.kernel.org # 5.5+\nReported-by: Abaci Robot <abaci@linux.alibaba.com>\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc7",
        "release_date": "2021-02-04 07:43:21 -0700 io_uring: don't modify identity's files uncess identity is cowed"
    },
    {
        "commit": "57cd657b8272a66277c139e7bbdc8b86057cb415",
        "message": "do_read() returning 0 bytes read (not -EAGAIN/etc.) is not an important\nenough of a case to prioritise it. Fold it into ret < 0 check, so we get\nrid of an extra if and make it a bit more readable.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 13:09:21 -0700 io_uring: simplify do_read return parsing"
    },
    {
        "commit": "ce3d5aae331fa0eb1e88199e0380f517ed0c58f6",
        "message": "We don't know for how long REQ_F_INFLIGHT is going to stay, cleaner to\nextract a helper for marking requests as so.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 13:09:21 -0700 io_uring: deduplicate adding to REQ_F_INFLIGHT"
    },
    {
        "commit": "e86d004729ae9ce7d16ff3fad3708e1601eec0d2",
        "message": "Shouldn't be a problem now, but it's better to clean\nREQ_F_WORK_INITIALIZED and work->flags only after relevant resources are\nkilled, so cancellation see them.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 13:09:21 -0700 io_uring: remove work flags after cleanup"
    },
    {
        "commit": "34e08fed2c1cc67df88d85fedde1d05fec62e5ca",
        "message": "req->files now have same lifetime as all other iowq-work resources,\ninline io_req_drop_files() for consistency. Moreover, since\nREQ_F_INFLIGHT is no more files specific, the function name became\nvery confusing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 13:09:21 -0700 io_uring: inline io_req_drop_files()"
    },
    {
        "commit": "ba13e23f37c795bdd993523a6749d7afbf5ff7fb",
        "message": "We have no request types left using needs_file_no_error, remove it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 13:09:21 -0700 io_uring: kill not used needs_file_no_error"
    },
    {
        "commit": "9ae1f8dd372e0e4c020b345cf9e09f519265e981",
        "message": "WARNING: inconsistent lock state\n\ninconsistent {HARDIRQ-ON-W} -> {IN-HARDIRQ-W} usage.\nsyz-executor217/8450 [HC1[1]:SC0[0]:HE0:SE1] takes:\nffff888023d6e620 (&fs->lock){?.+.}-{2:2}, at: spin_lock include/linux/spinlock.h:354 [inline]\nffff888023d6e620 (&fs->lock){?.+.}-{2:2}, at: io_req_clean_work fs/io_uring.c:1398 [inline]\nffff888023d6e620 (&fs->lock){?.+.}-{2:2}, at: io_dismantle_req+0x66f/0xf60 fs/io_uring.c:2029\n\nother info that might help us debug this:\n Possible unsafe locking scenario:\n\n       CPU0\n       ----\n  lock(&fs->lock);\n  <Interrupt>\n    lock(&fs->lock);\n\n *** DEADLOCK ***\n\n1 lock held by syz-executor217/8450:\n #0: ffff88802417c3e8 (&ctx->uring_lock){+.+.}-{3:3}, at: __do_sys_io_uring_enter+0x1071/0x1f30 fs/io_uring.c:9442\n\nstack backtrace:\nCPU: 1 PID: 8450 Comm: syz-executor217 Not tainted 5.11.0-rc5-next-20210129-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nCall Trace:\n <IRQ>\n[...]\n _raw_spin_lock+0x2a/0x40 kernel/locking/spinlock.c:151\n spin_lock include/linux/spinlock.h:354 [inline]\n io_req_clean_work fs/io_uring.c:1398 [inline]\n io_dismantle_req+0x66f/0xf60 fs/io_uring.c:2029\n __io_free_req+0x3d/0x2e0 fs/io_uring.c:2046\n io_free_req fs/io_uring.c:2269 [inline]\n io_double_put_req fs/io_uring.c:2392 [inline]\n io_put_req+0xf9/0x570 fs/io_uring.c:2388\n io_link_timeout_fn+0x30c/0x480 fs/io_uring.c:6497\n __run_hrtimer kernel/time/hrtimer.c:1519 [inline]\n __hrtimer_run_queues+0x609/0xe40 kernel/time/hrtimer.c:1583\n hrtimer_interrupt+0x334/0x940 kernel/time/hrtimer.c:1645\n local_apic_timer_interrupt arch/x86/kernel/apic/apic.c:1085 [inline]\n __sysvec_apic_timer_interrupt+0x146/0x540 arch/x86/kernel/apic/apic.c:1102\n asm_call_irq_on_stack+0xf/0x20\n </IRQ>\n __run_sysvec_on_irqstack arch/x86/include/asm/irq_stack.h:37 [inline]\n run_sysvec_on_irqstack_cond arch/x86/include/asm/irq_stack.h:89 [inline]\n sysvec_apic_timer_interrupt+0xbd/0x100 arch/x86/kernel/apic/apic.c:1096\n asm_sysvec_apic_timer_interrupt+0x12/0x20 arch/x86/include/asm/idtentry.h:629\nRIP: 0010:__raw_spin_unlock_irq include/linux/spinlock_api_smp.h:169 [inline]\nRIP: 0010:_raw_spin_unlock_irq+0x25/0x40 kernel/locking/spinlock.c:199\n spin_unlock_irq include/linux/spinlock.h:404 [inline]\n io_queue_linked_timeout+0x194/0x1f0 fs/io_uring.c:6525\n __io_queue_sqe+0x328/0x1290 fs/io_uring.c:6594\n io_queue_sqe+0x631/0x10d0 fs/io_uring.c:6639\n io_queue_link_head fs/io_uring.c:6650 [inline]\n io_submit_sqe fs/io_uring.c:6697 [inline]\n io_submit_sqes+0x19b5/0x2720 fs/io_uring.c:6960\n __do_sys_io_uring_enter+0x107d/0x1f30 fs/io_uring.c:9443\n do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nDon't free requests from under hrtimer context (softirq) as it may sleep\nor take spinlocks improperly (e.g. non-irq versions).\n\nCc: stable@vger.kernel.org # 5.6+\nReported-by: syzbot+81d17233a2b02eafba33@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 13:09:21 -0700 io_uring: fix inconsistent lock state"
    },
    {
        "commit": "13770a71ed35512cc73c6b350297a797f0b27880",
        "message": "If we hit a \"goto out_free;\" before the \"ctx->file_data\" pointer has\nbeen assigned then it leads to a NULL derefence when we call:\n\n\tfree_fixed_rsrc_data(ctx->file_data);\n\nWe can fix this by moving the assignment earlier.\n\nFixes: 1ad555c6ae6e (\"io_uring: create common fixed_rsrc_data allocation routines\")\nSigned-off-by: Dan Carpenter <dan.carpenter@oracle.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 11:56:41 -0700 io_uring: Fix NULL dereference in error in io_sqe_files_register()"
    },
    {
        "commit": "8b28fdf21193d35d6ec5a8430f0241f5f977c6ac",
        "message": "Abaci reported this issue:\n\n#[  605.170872] INFO: task kworker/u4:1:53 blocked for more than 143 seconds.\n[  605.172123]       Not tainted 5.10.0+ #1\n[  605.172811] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\n[  605.173915] task:kworker/u4:1    state:D stack:    0 pid:   53 ppid:     2 flags:0x00004000\n[  605.175130] Workqueue: events_unbound io_ring_exit_work\n[  605.175931] Call Trace:\n[  605.176334]  __schedule+0xe0e/0x25a0\n[  605.176971]  ? firmware_map_remove+0x1a1/0x1a1\n[  605.177631]  ? write_comp_data+0x2a/0x80\n[  605.178272]  schedule+0xd0/0x270\n[  605.178811]  schedule_timeout+0x6b6/0x940\n[  605.179415]  ? mark_lock.part.0+0xca/0x1420\n[  605.180062]  ? usleep_range+0x170/0x170\n[  605.180684]  ? wait_for_completion+0x16d/0x280\n[  605.181392]  ? mark_held_locks+0x9e/0xe0\n[  605.182079]  ? rwlock_bug.part.0+0x90/0x90\n[  605.182853]  ? lockdep_hardirqs_on_prepare+0x286/0x400\n[  605.183817]  wait_for_completion+0x175/0x280\n[  605.184713]  ? wait_for_completion_interruptible+0x340/0x340\n[  605.185611]  ? _raw_spin_unlock_irq+0x24/0x30\n[  605.186307]  ? migrate_swap_stop+0x9c0/0x9c0\n[  605.187046]  kthread_park+0x127/0x1c0\n[  605.187738]  io_sq_thread_stop+0xd5/0x530\n[  605.188459]  io_ring_exit_work+0xb1/0x970\n[  605.189207]  process_one_work+0x92c/0x1510\n[  605.189947]  ? pwq_dec_nr_in_flight+0x360/0x360\n[  605.190682]  ? rwlock_bug.part.0+0x90/0x90\n[  605.191430]  ? write_comp_data+0x2a/0x80\n[  605.192207]  worker_thread+0x9b/0xe20\n[  605.192900]  ? process_one_work+0x1510/0x1510\n[  605.193599]  kthread+0x353/0x460\n[  605.194154]  ? _raw_spin_unlock_irq+0x24/0x30\n[  605.194910]  ? kthread_create_on_node+0x100/0x100\n[  605.195821]  ret_from_fork+0x1f/0x30\n[  605.196605]\n[  605.196605] Showing all locks held in the system:\n[  605.197598] 1 lock held by khungtaskd/25:\n[  605.198301]  #0: ffffffff8b5f76a0 (rcu_read_lock){....}-{1:2}, at: rcu_lock_acquire.constprop.0+0x0/0x30\n[  605.199914] 3 locks held by kworker/u4:1/53:\n[  605.200609]  #0: ffff888100109938 ((wq_completion)events_unbound){+.+.}-{0:0}, at: process_one_work+0x82a/0x1510\n[  605.202108]  #1: ffff888100e47dc0 ((work_completion)(&ctx->exit_work)){+.+.}-{0:0}, at: process_one_work+0x85e/0x1510\n[  605.203681]  #2: ffff888116931870 (&sqd->lock){+.+.}-{3:3}, at: io_sq_thread_park.part.0+0x19/0x50\n[  605.205183] 3 locks held by systemd-journal/161:\n[  605.206037] 1 lock held by syslog-ng/254:\n[  605.206674] 2 locks held by agetty/311:\n[  605.207292]  #0: ffff888101097098 (&tty->ldisc_sem){++++}-{0:0}, at: tty_ldisc_ref_wait+0x27/0x80\n[  605.208715]  #1: ffffc900000332e8 (&ldata->atomic_read_lock){+.+.}-{3:3}, at: n_tty_read+0x222/0x1bb0\n[  605.210131] 2 locks held by bash/677:\n[  605.210723]  #0: ffff88810419a098 (&tty->ldisc_sem){++++}-{0:0}, at: tty_ldisc_ref_wait+0x27/0x80\n[  605.212105]  #1: ffffc900000512e8 (&ldata->atomic_read_lock){+.+.}-{3:3}, at: n_tty_read+0x222/0x1bb0\n[  605.213777]\n[  605.214151] =============================================\n\nI believe this is caused by the follow race:\n\n(ctx_list is empty now)\n=> io_put_sq_data               |\n==> kthread_park(sqd->thread);  |\n====> set KTHREAD_SHOULD_PARK\t|\n====> wake_up_process(k)        | sq thread is running\n\t\t\t\t|\n\t\t\t\t|\n\t\t\t\t| needs_sched is true since no ctx,\n\t\t\t\t| so TASK_INTERRUPTIBLE set and schedule\n\t\t\t\t| out then never wake up again\n\t\t\t\t|\n====> wait_for_completion\t|\n\t(stuck here)\n\nSo check if sqthread gets park flag right before schedule().\nsince ctx_list is always empty when this problem happens, here I put\nkthread_should_park() before setting the wakeup flag(ctx_list is empty\nso this for loop is fast), where is close enough to schedule(). The\nproblem doesn't show again in my repro testing after this fix.\n\nReported-by: Abaci <abaci@linux.alibaba.com>\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:43 -0700 io_uring: check kthread parked flag before sqthread goes to sleep"
    },
    {
        "commit": "090da7d52fe2aeabb73bf300154278e411cd069e",
        "message": "Add the missing kernel io_uring header, add Pavel as a reviewer, and\nexclude io_uring from the FILESYSTEMS section to avoid keep spamming Al\n(mainly) with bug reports, patches, etc.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:43 -0700 MAINTAINERS: update io_uring section"
    },
    {
        "commit": "4e0377a1c5c633852f443a562ec55f7dfea65350",
        "message": "This patch adds support for skipping a file descriptor when using\nIORING_REGISTER_FILES_UPDATE.  __io_sqe_files_update will skip fds set\nto IORING_REGISTER_FILES_SKIP. IORING_REGISTER_FILES_SKIP is inturn\nadded as a #define in io_uring.h\n\nSigned-off-by: noah <goldstein.w.n@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:43 -0700 io_uring: Add skip option for __io_sqe_files_update"
    },
    {
        "commit": "67973b933e347c38478b591d6c9dc076bea7c9dc",
        "message": "Replace a while with a simple for loop, that looks way more natural, and\nenables us to use \"continue\" as indexes are no more updated by hand in\nthe end of the loop.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:43 -0700 io_uring: cleanup files_update looping"
    },
    {
        "commit": "7c6607313f032b73638a6f752cb4adf50ba947cf",
        "message": "We grab a task for each request and while putting it it also have to do\nextra work like inflight accounting and waking up that task. This\nsequence is duplicated several time, it's good time to add a helper.\nMore to that, the helper generates better code due to better locality\nand so not failing alias analysis.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:43 -0700 io_uring: consolidate putting reqs task"
    },
    {
        "commit": "ecfc8492820732be652146280912554ced62c32b",
        "message": "For SQPOLL io_uring we want to have only one file note held by\nsqo_task. Add a warning to make sure it holds. It's deep in\nio_uring_add_task_file() out of hot path, so shouldn't hurt.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:43 -0700 io_uring: ensure only sqo_task has file notes"
    },
    {
        "commit": "0bead8cd39b9c9c7c4e902018ccf129107ac50ef",
        "message": "The function io_remove_personalities() is very similar to\nio_unregister_personality(),so implement io_remove_personalities()\ncalling io_unregister_personality().\n\nSigned-off-by: Yejune Deng <yejune.deng@gmail.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:43 -0700 io_uring: simplify io_remove_personalities()"
    },
    {
        "commit": "4014d943cb62db892eb023d385a966a3fce5ee4c",
        "message": "It's no longer used as IORING_OP_CLOSE got rid for the need of flagging\nit as uncancelable, kill it of.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:43 -0700 io_uring/io-wq: kill off now unused IO_WQ_WORK_NO_CANCEL"
    },
    {
        "commit": "9eac1904d3364254d622bf2c771c4f85cd435fc2",
        "message": "We currently split the close into two, in case we have a ->flush op\nthat we can't safely handle from non-blocking context. This requires\nus to flag the op as uncancelable if we do need to punt it async, and\nthat means special handling for just this op type.\n\nUse __close_fd_get_file() and grab the files lock so we can get the file\nand check if we need to go async in one atomic operation. That gets rid\nof the need for splitting this into two steps, and hence the need for\nIO_WQ_WORK_NO_CANCEL.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:43 -0700 io_uring: get rid of intermediate IORING_OP_CLOSE stage"
    },
    {
        "commit": "e342c807f556dbcee1370ab78af1d8faf497d771",
        "message": "When a request is completed with comp_state, its completion reference\nput is deferred to io_submit_flush_completions(), but the submission\nis put not far from there, so do it together to save one atomic dec per\nrequest. That targets requests that complete inline, e.g. buffered rw,\nsend/recv.\n\nProper benchmarking haven't been conducted but for nops(batch=32) it was\naround 7901 vs 8117 KIOPS (~2.7%), or ~4% per perf profiling.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: save atomic dec for inline executed reqs"
    },
    {
        "commit": "9affd664f0e0512d8997dbdddb1448a4faf9bc82",
        "message": "io_submit_flush_completions() is called down the stack in the _state\nversion of io_req_complete(), that's ok because is only called by\nio_uring opcode handler functions directly. Move it up to\n__io_queue_sqe() as preparation.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: don't flush CQEs deep down the stack"
    },
    {
        "commit": "a38d68db6742c19a74141c0f56785ef67f51c504",
        "message": "__io_req_complete() inlining is a bit weird, some compilers don't\noptimise out the non-NULL branch of it even when called as\nio_req_complete(). Help it a bit by extracting state and stateless\nhelpers out of __io_req_complete().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: help inlining of io_req_complete()"
    },
    {
        "commit": "8662daec09edcdba2659799040aee1ba575c4799",
        "message": "Deduplicates translation of timeout flags into hrtimer_mode.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: add a helper timeout mode calculation"
    },
    {
        "commit": "eab30c4d20dc761d463445e5130421863ff81505",
        "message": "When io_req_task_work_add() fails, the request will be cancelled by\nenqueueing via task_works of io-wq. Extract a function for that.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: deduplicate failing task_work_add"
    },
    {
        "commit": "02b23a9af5ba4db0a85ebb81c8b376b2fe860d0f",
        "message": "The check in io_state_file_put() is optimised pretty well when called\nfrom __io_file_get(). Don't pollute the code with all these variants.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: remove __io_state_file_put"
    },
    {
        "commit": "85bcb6c67ea145b8032089db891218e3339cbdb8",
        "message": "Get rid of a label in io_alloc_req(), it's cleaner to do return\ndirectly.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: simplify io_alloc_req()"
    },
    {
        "commit": "888aae2eeddfe1d6c9731cf4af1a1b2605af6470",
        "message": "Apparently, there is one more place hand coded calculation of number of\nCQ events in the ring. Use __io_cqring_events() helper in\nio_get_cqring() as well. Naturally, assembly stays identical.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: further deduplicate #CQ events calc"
    },
    {
        "commit": "ec30e04ba4a5c265f52482092a5f5f5232947c48",
        "message": "Inline it in its only user, that's cleaner\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: inline __io_commit_cqring()"
    },
    {
        "commit": "2d7e935809b7f740442ce79fc6f53e94a1f0b874",
        "message": "The name is confusing and it's used only in one place.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: inline io_async_submit()"
    },
    {
        "commit": "5c766a908d06e96d30e0ec2511a24fa311553d2c",
        "message": "personality_idr is usually synchronised by uring_lock, the exception\nwould be removing personalities in io_ring_ctx_wait_and_kill(), which\nis legit as refs are killed by that point but still would be more\nresilient to do it under the lock.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: cleanup personalities under uring_lock"
    },
    {
        "commit": "dc2a6e9aa9c349d76c318d22bbe26006fda1ce97",
        "message": "It's awkward to pass return a value into a function for it to return it\nback. Check it at the caller site and clean up io_resubmit_prep() a bit.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: refactor io_resubmit_prep()"
    },
    {
        "commit": "bf6182b6d46e28c3e59b9c0d6097b379cae56b94",
        "message": "The hot path is IO completing on the first try. Reshuffle io_rw_reissue() so\nit's checked first.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: optimise io_rw_reissue()"
    },
    {
        "commit": "00835dce1406e746fe5ab8c522cceb9594c78acb",
        "message": "Make the percpu ref release function names consistent between rsrc data\nand nodes.\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: make percpu_ref_release names consistent"
    },
    {
        "commit": "1ad555c6ae6e28ec7b1acaa2af72a9904e6ba96a",
        "message": "Create common alloc/free fixed_rsrc_data routines for both files and\nbuffers.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\n[remove buffer part]\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: create common fixed_rsrc_data allocation routines"
    },
    {
        "commit": "d7954b2ba94639b7f5b08760d36e54c28544730f",
        "message": "Create common routines to be used for both files/buffers registration.\n\n[remove io_sqe_rsrc_set_node substitution]\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\n[merge, quiesce only for files]\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: create common fixed_rsrc_ref_node handling routines"
    },
    {
        "commit": "bc9744cd162b2f6c38d75dc49c310677dc13afa8",
        "message": "A simple prep patch allowing to set refnode callbacks after it was\nallocated. This needed to 1) keep ourself off of hi-level functions\nwhere it's not pretty and they are not necessary 2) amortise ref_node\nallocation in the future, e.g. for updates.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: split ref_node alloc and init"
    },
    {
        "commit": "6802535df7bf807c94de32a9d0bf0401d3109671",
        "message": "Split alloc_fixed_file_ref_node into resource generic/specific parts,\nto be leveraged for fixed buffers.\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: split alloc_fixed_file_ref_node"
    },
    {
        "commit": "2a63b2d9c30b2029892c368d11ede1434de6c565",
        "message": "Encapsulate resource reference locking into separate routines.\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: add rsrc_ref locking routines"
    },
    {
        "commit": "d67d2263fb2350a68074f2cb4dd78549aeebbfae",
        "message": "Uplevel ref_list and make it common to all resources.  This is to\nallow one common ref_list to be used for both files, and buffers\nin upcoming patches.\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:42 -0700 io_uring: separate ref_list from fixed_rsrc_data"
    },
    {
        "commit": "5023853183699dd1e3e47622c03d7ae11343837a",
        "message": "Generalize io_queue_rsrc_removal to handle both files and buffers.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\n[remove io_mapped_ubuf from rsrc tables/etc. for now]\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:41 -0700 io_uring: generalize io_queue_rsrc_removal"
    },
    {
        "commit": "269bbe5fd4d2fdd3b0d3a82a3c3c1dd1209aa8b8",
        "message": "This is a prep rename patch for subsequent patches to generalize file\nregistration.\n\n[io_uring_rsrc_update:: rename fds -> data]\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\n[leave io_uring_files_update as struct]\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:41 -0700 io_uring: rename file related variables to rsrc"
    },
    {
        "commit": "2b358604aa6e8c12d7efa14777fcc66c377682b0",
        "message": "Move allocation of buffer management structures, and validation of\nbuffers into separate routines.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:41 -0700 io_uring: modularize io_sqe_buffers_register"
    },
    {
        "commit": "0a96bbe49994a46c1fea34619a501ead46aa7584",
        "message": "Split io_sqe_buffer_register into two routines:\n\n- io_sqe_buffer_register() registers a single buffer\n- io_sqe_buffers_register iterates over all user specified buffers\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:41 -0700 io_uring: modularize io_sqe_buffer_register"
    },
    {
        "commit": "3a81fd02045c329f25e5900fa61f613c9b317644",
        "message": "Instead of being pessimistic and assume that path lookup will block, use\nLOOKUP_CACHED to attempt just a cached lookup. This ensures that the\nfast path is always done inline, and we only punt to async context if\nIO is needed to satisfy the lookup.\n\nFor forced nonblock open attempts, mark the file O_NONBLOCK over the\nactual ->open() call as well. We can safely clear this again before\ndoing fd_install(), so it'll never be user visible that we fiddled with\nit.\n\nThis greatly improves the performance of file open where the dentry is\nalready cached:\n\nached\t\t5.10-git\t5.10-git+LOOKUP_CACHED\tSpeedup\n---------------------------------------------------------------\n33%\t\t1,014,975\t900,474\t\t\t1.1x\n89%\t\t 545,466\t292,937\t\t\t1.9x\n100%\t\t 435,636\t151,475\t\t\t2.9x\n\nThe more cache hot we are, the faster the inline LOOKUP_CACHED\noptimization helps. This is unsurprising and expected, as a thread\noffload becomes a more dominant part of the total overhead. If we look\nat io_uring tracing, doing an IORING_OP_OPENAT on a file that isn't in\nthe dentry cache will yield:\n\n275.550481: io_uring_create: ring 00000000ddda6278, fd 3 sq size 8, cq size 16, flags 0\n275.550491: io_uring_submit_sqe: ring 00000000ddda6278, op 18, data 0x0, non block 1, sq_thread 0\n275.550498: io_uring_queue_async_work: ring 00000000ddda6278, request 00000000c0267d17, flags 69760, normal queue, work 000000003d683991\n275.550502: io_uring_cqring_wait: ring 00000000ddda6278, min_events 1\n275.550556: io_uring_complete: ring 00000000ddda6278, user_data 0x0, result 4\n\nwhich shows a failed nonblock lookup, then punt to worker, and then we\ncomplete with fd == 4. This takes 65 usec in total. Re-running the same\ntest case again:\n\n281.253956: io_uring_create: ring 0000000008207252, fd 3 sq size 8, cq size 16, flags 0\n281.253967: io_uring_submit_sqe: ring 0000000008207252, op 18, data 0x0, non block 1, sq_thread 0\n281.253973: io_uring_complete: ring 0000000008207252, user_data 0x0, result 4\n\nshows the same request completing inline, also returning fd == 4. This\ntakes 6 usec.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:41 -0700 io_uring: enable LOOKUP_CACHED path resolution for filename lookups"
    },
    {
        "commit": "b2d86c7cec35f7f4cc00c41e387bdbc5bde2cf0f",
        "message": "Merge RESOLVE_CACHED bits from Al, as the io_uring changes will build on\ntop of that.\n\n* 'work.namei' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:\n  fs: expose LOOKUP_CACHED through openat2() RESOLVE_CACHED\n  fs: add support for LOOKUP_CACHED\n  saner calling conventions for unlazy_child()\n  fs: make unlazy_walk() error handling consistent\n  fs/namei.c: Remove unlikely of status being -ECHILD in lookup_fast()\n  do_tmpfile(): don't mess with finish_open()",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:02:28 -0700 Merge branch 'work.namei' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs into for-5.12/io_uring"
    },
    {
        "commit": "0b9112a58836ad6a7e84eebec06a2de9778b7573",
        "message": "Linux 5.11-rc6\n\n* tag 'v5.11-rc6': (1466 commits)\n  Linux 5.11-rc6\n  leds: rt8515: Add Richtek RT8515 LED driver\n  dt-bindings: leds: Add DT binding for Richtek RT8515\n  leds: trigger: fix potential deadlock with libata\n  leds: leds-ariel: convert comma to semicolon\n  leds: leds-lm3533: convert comma to semicolon\n  dt-bindings: Cleanup standard unit properties\n  soc: litex: Properly depend on HAS_IOMEM\n  tty: avoid using vfs_iocb_iter_write() for redirected console writes\n  null_blk: cleanup zoned mode initialization\n  cifs: fix dfs domain referrals\n  drm/nouveau/kms/gk104-gp1xx: Fix > 64x64 cursors\n  drm/nouveau/kms/nv50-: Report max cursor size to userspace\n  drivers/nouveau/kms/nv50-: Reject format modifiers for cursor planes\n  drm/nouveau/svm: fail NOUVEAU_SVM_INIT ioctl on unsupported devices\n  drm/nouveau/dispnv50: Restore pushing of all data.\n  io_uring: reinforce cancel on flush during exit\n  cifs: returning mount parm processing errors correctly\n  rxrpc: Fix memory leak in rxrpc_lookup_local\n  mlxsw: spectrum_span: Do not overwrite policer configuration\n  ...",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 10:03:45 +0100 Merge tag 'v5.11-rc6' into patchwork"
    },
    {
        "commit": "9c83465f3245c2faa82ffeb7016f40f02bfaa0ad",
        "message": "Commit db68ce10c4f0a27c (\"new helper: uaccess_kernel()\") replaced\nsegment_eq(get_fs(), KERNEL_DS) with uaccess_kernel(). But the correct\nmethod for tomoyo to check whether current is a kernel thread in order\nto assume that kernel threads are privileged for socket operations was\n(current->flags & PF_KTHREAD). Now that uaccess_kernel() became 0 on x86,\ntomoyo has to fix this problem. Do like commit 942cb357ae7d9249 (\"Smack:\nHandle io_uring kernel thread privileges\") does.\n\nSigned-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-02-01 11:53:05 +0900 tomoyo: recognize kernel threads correctly"
    },
    {
        "commit": "c0ec4ffc40939e9a5a5844ce455f2b5b66a005fd",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"We got the cancelation story sorted now, so for all intents and\n  purposes, this should be it for 5.11 outside of any potential little\n  fixes that may come in. This contains:\n\n   - task_work task state fixes (Hao, Pavel)\n\n   - Cancelation fixes (me, Pavel)\n\n   - Fix for an inflight req patch in this release (Pavel)\n\n   - Fix for a lock deadlock issue (Pavel)\"\n\n* tag 'io_uring-5.11-2021-01-29' of git://git.kernel.dk/linux-block:\n  io_uring: reinforce cancel on flush during exit\n  io_uring: fix sqo ownership false positive warning\n  io_uring: fix list corruption for splice file_get\n  io_uring: fix flush cqring overflow list while TASK_INTERRUPTIBLE\n  io_uring: fix wqe->lock/completion_lock deadlock\n  io_uring: fix cancellation taking mutex while TASK_UNINTERRUPTIBLE\n  io_uring: fix __io_uring_files_cancel() with TASK_UNINTERRUPTIBLE\n  io_uring: only call io_cqring_ev_posted() if events were posted\n  io_uring: if we see flush on exit, cancel related tasks",
        "kernel_version": "v5.11-rc6",
        "release_date": "2021-01-29 13:47:47 -0800 Merge tag 'io_uring-5.11-2021-01-29' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "3a7efd1ad269ccaf9c1423364d97c9661ba6dafa",
        "message": "What 84965ff8a84f0 (\"io_uring: if we see flush on exit, cancel related tasks\")\nreally wants is to cancel all relevant REQ_F_INFLIGHT requests reliably.\nThat can be achieved by io_uring_cancel_files(), but we'll miss it\ncalling io_uring_cancel_task_requests(files=NULL) from io_uring_flush(),\nbecause it will go through __io_uring_cancel_task_requests().\n\nJust always call io_uring_cancel_files() during cancel, it's good enough\nfor now.\n\nCc: stable@vger.kernel.org # 5.9+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc6",
        "release_date": "2021-01-28 17:04:24 -0700 io_uring: reinforce cancel on flush during exit"
    },
    {
        "commit": "70b2c60d3797bffe182dddb9bb55975b9be5889a",
        "message": "WARNING: CPU: 0 PID: 21359 at fs/io_uring.c:9042\n    io_uring_cancel_task_requests+0xe55/0x10c0 fs/io_uring.c:9042\nCall Trace:\n io_uring_flush+0x47b/0x6e0 fs/io_uring.c:9227\n filp_close+0xb4/0x170 fs/open.c:1295\n close_files fs/file.c:403 [inline]\n put_files_struct fs/file.c:418 [inline]\n put_files_struct+0x1cc/0x350 fs/file.c:415\n exit_files+0x7e/0xa0 fs/file.c:435\n do_exit+0xc22/0x2ae0 kernel/exit.c:820\n do_group_exit+0x125/0x310 kernel/exit.c:922\n get_signal+0x427/0x20f0 kernel/signal.c:2773\n arch_do_signal_or_restart+0x2a8/0x1eb0 arch/x86/kernel/signal.c:811\n handle_signal_work kernel/entry/common.c:147 [inline]\n exit_to_user_mode_loop kernel/entry/common.c:171 [inline]\n exit_to_user_mode_prepare+0x148/0x250 kernel/entry/common.c:201\n __syscall_exit_to_user_mode_work kernel/entry/common.c:291 [inline]\n syscall_exit_to_user_mode+0x19/0x50 kernel/entry/common.c:302\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nNow io_uring_cancel_task_requests() can be called not through file\nnotes but directly, remove a WARN_ONCE() there that give us false\npositives. That check is not very important and we catch it in other\nplaces.\n\nFixes: 84965ff8a84f0 (\"io_uring: if we see flush on exit, cancel related tasks\")\nCc: stable@vger.kernel.org # 5.9+\nReported-by: syzbot+3e3d9bd0c6ce9efbc3ef@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc6",
        "release_date": "2021-01-28 11:44:13 -0700 io_uring: fix sqo ownership false positive warning"
    },
    {
        "commit": "f609cbb8911e40e15f9055e8f945f926ac906924",
        "message": "kernel BUG at lib/list_debug.c:29!\nCall Trace:\n __list_add include/linux/list.h:67 [inline]\n list_add include/linux/list.h:86 [inline]\n io_file_get+0x8cc/0xdb0 fs/io_uring.c:6466\n __io_splice_prep+0x1bc/0x530 fs/io_uring.c:3866\n io_splice_prep fs/io_uring.c:3920 [inline]\n io_req_prep+0x3546/0x4e80 fs/io_uring.c:6081\n io_queue_sqe+0x609/0x10d0 fs/io_uring.c:6628\n io_submit_sqe fs/io_uring.c:6705 [inline]\n io_submit_sqes+0x1495/0x2720 fs/io_uring.c:6953\n __do_sys_io_uring_enter+0x107d/0x1f30 fs/io_uring.c:9353\n do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nio_file_get() may be called from splice, and so REQ_F_INFLIGHT may\nalready be set.\n\nFixes: 02a13674fa0e8 (\"io_uring: account io_uring internal files as REQ_F_INFLIGHT\")\nCc: stable@vger.kernel.org # 5.9+\nReported-by: syzbot+6879187cf57845801267@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc6",
        "release_date": "2021-01-28 11:44:12 -0700 io_uring: fix list corruption for splice file_get"
    },
    {
        "commit": "1875dc5b8ff4690547c446ef222083e28e2d9463",
        "message": "The comment says:\n  /* task_struct member predeclarations (sorted alphabetically): */\n\nSo move io_uring_task where it belongs (alphabetically).\n\nSigned-off-by: Peter Oskolkov <posk@google.com>\nSigned-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>\nLink: https://lkml.kernel.org/r/20210126193449.487547-1-posk@google.com",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-01-27 17:26:43 +0100 sched: Correctly sort struct predeclarations"
    },
    {
        "commit": "6195ba09822c87cad09189bbf550d0fbe714687a",
        "message": "Abaci reported the follow warning:\n\n[   27.073425] do not call blocking ops when !TASK_RUNNING; state=1 set at [] prepare_to_wait_exclusive+0x3a/0xc0\n[   27.075805] WARNING: CPU: 0 PID: 951 at kernel/sched/core.c:7853 __might_sleep+0x80/0xa0\n[   27.077604] Modules linked in:\n[   27.078379] CPU: 0 PID: 951 Comm: a.out Not tainted 5.11.0-rc3+ #1\n[   27.079637] Hardware name: Red Hat KVM, BIOS 0.5.1 01/01/2011\n[   27.080852] RIP: 0010:__might_sleep+0x80/0xa0\n[   27.081835] Code: 65 48 8b 04 25 80 71 01 00 48 8b 90 c0 15 00 00 48 8b 70 18 48 c7 c7 08 39 95 82 c6 05 f9 5f de 08 01 48 89 d1 e8 00 c6 fa ff  0b eb bf 41 0f b6 f5 48 c7 c7 40 23 c9 82 e8 f3 48 ec 00 eb a7\n[   27.084521] RSP: 0018:ffffc90000fe3ce8 EFLAGS: 00010286\n[   27.085350] RAX: 0000000000000000 RBX: ffffffff82956083 RCX: 0000000000000000\n[   27.086348] RDX: ffff8881057a0000 RSI: ffffffff8118cc9e RDI: ffff88813bc28570\n[   27.087598] RBP: 00000000000003a7 R08: 0000000000000001 R09: 0000000000000001\n[   27.088819] R10: ffffc90000fe3e00 R11: 00000000fffef9f0 R12: 0000000000000000\n[   27.089819] R13: 0000000000000000 R14: ffff88810576eb80 R15: ffff88810576e800\n[   27.091058] FS:  00007f7b144cf740(0000) GS:ffff88813bc00000(0000) knlGS:0000000000000000\n[   27.092775] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n[   27.093796] CR2: 00000000022da7b8 CR3: 000000010b928002 CR4: 00000000003706f0\n[   27.094778] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n[   27.095780] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n[   27.097011] Call Trace:\n[   27.097685]  __mutex_lock+0x5d/0xa30\n[   27.098565]  ? prepare_to_wait_exclusive+0x71/0xc0\n[   27.099412]  ? io_cqring_overflow_flush.part.101+0x6d/0x70\n[   27.100441]  ? lockdep_hardirqs_on_prepare+0xe9/0x1c0\n[   27.101537]  ? _raw_spin_unlock_irqrestore+0x2d/0x40\n[   27.102656]  ? trace_hardirqs_on+0x46/0x110\n[   27.103459]  ? io_cqring_overflow_flush.part.101+0x6d/0x70\n[   27.104317]  io_cqring_overflow_flush.part.101+0x6d/0x70\n[   27.105113]  io_cqring_wait+0x36e/0x4d0\n[   27.105770]  ? find_held_lock+0x28/0xb0\n[   27.106370]  ? io_uring_remove_task_files+0xa0/0xa0\n[   27.107076]  __x64_sys_io_uring_enter+0x4fb/0x640\n[   27.107801]  ? rcu_read_lock_sched_held+0x59/0xa0\n[   27.108562]  ? lockdep_hardirqs_on_prepare+0xe9/0x1c0\n[   27.109684]  ? syscall_enter_from_user_mode+0x26/0x70\n[   27.110731]  do_syscall_64+0x2d/0x40\n[   27.111296]  entry_SYSCALL_64_after_hwframe+0x44/0xa9\n[   27.112056] RIP: 0033:0x7f7b13dc8239\n[   27.112663] Code: 01 00 48 81 c4 80 00 00 00 e9 f1 fe ff ff 0f 1f 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05  3d 01 f0 ff ff 73 01 c3 48 8b 0d 27 ec 2c 00 f7 d8 64 89 01 48\n[   27.115113] RSP: 002b:00007ffd6d7f5c88 EFLAGS: 00000286 ORIG_RAX: 00000000000001aa\n[   27.116562] RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f7b13dc8239\n[   27.117961] RDX: 000000000000478e RSI: 0000000000000000 RDI: 0000000000000003\n[   27.118925] RBP: 00007ffd6d7f5cb0 R08: 0000000020000040 R09: 0000000000000008\n[   27.119773] R10: 0000000000000001 R11: 0000000000000286 R12: 0000000000400480\n[   27.120614] R13: 00007ffd6d7f5d90 R14: 0000000000000000 R15: 0000000000000000\n[   27.121490] irq event stamp: 5635\n[   27.121946] hardirqs last  enabled at (5643): [] console_unlock+0x5c4/0x740\n[   27.123476] hardirqs last disabled at (5652): [] console_unlock+0x4e7/0x740\n[   27.125192] softirqs last  enabled at (5272): [] __do_softirq+0x3c5/0x5aa\n[   27.126430] softirqs last disabled at (5267): [] asm_call_irq_on_stack+0xf/0x20\n[   27.127634] ---[ end trace 289d7e28fa60f928 ]---\n\nThis is caused by calling io_cqring_overflow_flush() which may sleep\nafter calling prepare_to_wait_exclusive() which set task state to\nTASK_INTERRUPTIBLE\n\nReported-by: Abaci <abaci@linux.alibaba.com>\nFixes: 6c503150ae33 (\"io_uring: patch up IOPOLL overflow_flush sync\")\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc6",
        "release_date": "2021-01-27 09:18:28 -0700 io_uring: fix flush cqring overflow list while TASK_INTERRUPTIBLE"
    },
    {
        "commit": "907d1df30a51cc1a1d25414a00cde0494b83df7b",
        "message": "Joseph reports following deadlock:\n\nCPU0:\n...\nio_kill_linked_timeout  // &ctx->completion_lock\nio_commit_cqring\n__io_queue_deferred\n__io_queue_async_work\nio_wq_enqueue\nio_wqe_enqueue  // &wqe->lock\n\nCPU1:\n...\n__io_uring_files_cancel\nio_wq_cancel_cb\nio_wqe_cancel_pending_work  // &wqe->lock\nio_cancel_task_cb  // &ctx->completion_lock\n\nOnly __io_queue_deferred() calls queue_async_work() while holding\nctx->completion_lock, enqueue drained requests via io_req_task_queue()\ninstead.\n\nCc: stable@vger.kernel.org # 5.9+\nReported-by: Joseph Qi <joseph.qi@linux.alibaba.com>\nTested-by: Joseph Qi <joseph.qi@linux.alibaba.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc6",
        "release_date": "2021-01-26 19:36:55 -0700 io_uring: fix wqe->lock/completion_lock deadlock"
    },
    {
        "commit": "ca70f00bed6cb255b7a9b91aa18a2717c9217f70",
        "message": "do not call blocking ops when !TASK_RUNNING; state=2 set at\n\t[<00000000ced9dbfc>] prepare_to_wait+0x1f4/0x3b0\n\tkernel/sched/wait.c:262\nWARNING: CPU: 1 PID: 19888 at kernel/sched/core.c:7853\n\t__might_sleep+0xed/0x100 kernel/sched/core.c:7848\nRIP: 0010:__might_sleep+0xed/0x100 kernel/sched/core.c:7848\nCall Trace:\n __mutex_lock_common+0xc4/0x2ef0 kernel/locking/mutex.c:935\n __mutex_lock kernel/locking/mutex.c:1103 [inline]\n mutex_lock_nested+0x1a/0x20 kernel/locking/mutex.c:1118\n io_wq_submit_work+0x39a/0x720 fs/io_uring.c:6411\n io_run_cancel fs/io-wq.c:856 [inline]\n io_wqe_cancel_pending_work fs/io-wq.c:990 [inline]\n io_wq_cancel_cb+0x614/0xcb0 fs/io-wq.c:1027\n io_uring_cancel_files fs/io_uring.c:8874 [inline]\n io_uring_cancel_task_requests fs/io_uring.c:8952 [inline]\n __io_uring_files_cancel+0x115d/0x19e0 fs/io_uring.c:9038\n io_uring_files_cancel include/linux/io_uring.h:51 [inline]\n do_exit+0x2e6/0x2490 kernel/exit.c:780\n do_group_exit+0x168/0x2d0 kernel/exit.c:922\n get_signal+0x16b5/0x2030 kernel/signal.c:2770\n arch_do_signal_or_restart+0x8e/0x6a0 arch/x86/kernel/signal.c:811\n handle_signal_work kernel/entry/common.c:147 [inline]\n exit_to_user_mode_loop kernel/entry/common.c:171 [inline]\n exit_to_user_mode_prepare+0xac/0x1e0 kernel/entry/common.c:201\n __syscall_exit_to_user_mode_work kernel/entry/common.c:291 [inline]\n syscall_exit_to_user_mode+0x48/0x190 kernel/entry/common.c:302\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nRewrite io_uring_cancel_files() to mimic __io_uring_task_cancel()'s\ncounting scheme, so it does all the heavy work before setting\nTASK_UNINTERRUPTIBLE.\n\nCc: stable@vger.kernel.org # 5.9+\nReported-by: syzbot+f655445043a26a7cfab8@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\n[axboe: fix inverted task check]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc6",
        "release_date": "2021-01-26 09:01:49 -0700 io_uring: fix cancellation taking mutex while TASK_UNINTERRUPTIBLE"
    },
    {
        "commit": "a1bb3cd58913338e1b627ea6b8c03c2ae82d293f",
        "message": "If the tctx inflight number haven't changed because of cancellation,\n__io_uring_task_cancel() will continue leaving the task in\nTASK_UNINTERRUPTIBLE state, that's not expected by\n__io_uring_files_cancel(). Ensure we always call finish_wait() before\nretrying.\n\nCc: stable@vger.kernel.org # 5.9+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc6",
        "release_date": "2021-01-26 08:51:08 -0700 io_uring: fix __io_uring_files_cancel() with TASK_UNINTERRUPTIBLE"
    },
    {
        "commit": "b18032bb0a883cd7edd22a7fe6c57e1059b81ed0",
        "message": "This normally doesn't cause any extra harm, but it does mean that we'll\nincrement the eventfd notification count, if one has been registered\nwith the ring. This can confuse applications, when they see more\nnotifications on the eventfd side than are available in the ring.\n\nDo the nice thing and only increment this count, if we actually posted\n(or even overflowed) events.\n\nReported-and-tested-by: Dan Melnic <dmm@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc6",
        "release_date": "2021-01-24 18:13:56 -0700 io_uring: only call io_cqring_ev_posted() if events were posted"
    },
    {
        "commit": "84965ff8a84f0368b154c9b367b62e59c1193f30",
        "message": "Ensure we match tasks that belong to a dead or dying task as well, as we\nneed to reap those in addition to those belonging to the exiting task.\n\nCc: stable@vger.kernel.org # 5.9+\nReported-by: Josef Grieb <josef.grieb@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc6",
        "release_date": "2021-01-24 18:13:56 -0700 io_uring: if we see flush on exit, cancel related tasks"
    },
    {
        "commit": "ef7b1a0ea857af076ea64d131e95b59166ab6163",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Still need a final cancelation fix that isn't quite done done,\n  expected in the next day or two. That said, this contains:\n\n   - Wakeup fix for IOPOLL requests\n\n   - SQPOLL split close op handling fix\n\n   - Ensure that any use of io_uring fd itself is marked as inflight\n\n   - Short non-regular file read fix (Pavel)\n\n   - Fix up bad false positive warning (Pavel)\n\n   - SQPOLL fixes (Pavel)\n\n   - In-flight removal fix (Pavel)\"\n\n* tag 'io_uring-5.11-2021-01-24' of git://git.kernel.dk/linux-block:\n  io_uring: account io_uring internal files as REQ_F_INFLIGHT\n  io_uring: fix sleeping under spin in __io_clean_op\n  io_uring: fix short read retries for non-reg files\n  io_uring: fix SQPOLL IORING_OP_CLOSE cancelation state\n  io_uring: fix skipping disabling sqo on exec\n  io_uring: fix uring_flush in exit_files() warning\n  io_uring: fix false positive sqo warning on flush\n  io_uring: iopoll requests should also wake task ->in_idle state",
        "kernel_version": "v5.11-rc5",
        "release_date": "2021-01-24 12:30:14 -0800 Merge tag 'io_uring-5.11-2021-01-24' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "02a13674fa0e8dd326de8b9f4514b41b03d99003",
        "message": "We need to actively cancel anything that introduces a potential circular\nloop, where io_uring holds a reference to itself. If the file in question\nis an io_uring file, then add the request to the inflight list.\n\nCc: stable@vger.kernel.org # 5.9+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc5",
        "release_date": "2021-01-24 10:15:33 -0700 io_uring: account io_uring internal files as REQ_F_INFLIGHT"
    },
    {
        "commit": "9d5c8190683a462dbc787658467a0da17011ea5f",
        "message": "[   27.629441] BUG: sleeping function called from invalid context\n\tat fs/file.c:402\n[   27.631317] in_atomic(): 1, irqs_disabled(): 1, non_block: 0,\n\tpid: 1012, name: io_wqe_worker-0\n[   27.633220] 1 lock held by io_wqe_worker-0/1012:\n[   27.634286]  #0: ffff888105e26c98 (&ctx->completion_lock)\n\t{....}-{2:2}, at: __io_req_complete.part.102+0x30/0x70\n[   27.649249] Call Trace:\n[   27.649874]  dump_stack+0xac/0xe3\n[   27.650666]  ___might_sleep+0x284/0x2c0\n[   27.651566]  put_files_struct+0xb8/0x120\n[   27.652481]  __io_clean_op+0x10c/0x2a0\n[   27.653362]  __io_cqring_fill_event+0x2c1/0x350\n[   27.654399]  __io_req_complete.part.102+0x41/0x70\n[   27.655464]  io_openat2+0x151/0x300\n[   27.656297]  io_issue_sqe+0x6c/0x14e0\n[   27.660991]  io_wq_submit_work+0x7f/0x240\n[   27.662890]  io_worker_handle_work+0x501/0x8a0\n[   27.664836]  io_wqe_worker+0x158/0x520\n[   27.667726]  kthread+0x134/0x180\n[   27.669641]  ret_from_fork+0x1f/0x30\n\nInstead of cleaning files on overflow, return back overflow cancellation\ninto io_uring_cancel_files(). Previously it was racy to clean\nREQ_F_OVERFLOW flag, but we got rid of it, and can do it through\nrepetitive attempts targeting all matching requests.\n\nReported-by: Abaci <abaci@linux.alibaba.com>\nReported-by: Joseph Qi <joseph.qi@linux.alibaba.com>\nCc: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc5",
        "release_date": "2021-01-24 10:15:33 -0700 io_uring: fix sleeping under spin in __io_clean_op"
    },
    {
        "commit": "ae29e4220fd3047b5442e7e8db8027d7745093f5",
        "message": "If the inode is not pinned by the time fsync is called we don't need the\nilock to protect against concurrent clearing of ili_fsync_fields as the\ninode won't need a log flush or clearing of these fields.  Not taking\nthe iolock allows for full concurrency of fsync and thus O_DSYNC\ncompletions with io_uring/aio write submissions.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Darrick J. Wong <djwong@kernel.org>\nSigned-off-by: Darrick J. Wong <djwong@kernel.org>\nReviewed-by: Dave Chinner <dchinner@redhat.com>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-01-22 16:54:52 -0800 xfs: reduce ilock acquisitions in xfs_file_fsync"
    },
    {
        "commit": "9a173346bd9e16ab19c7addb8862d95a5cea9feb",
        "message": "Sockets and other non-regular files may actually expect short reads to\nhappen, don't retry reads for them. Because non-reg files don't set\nFMODE_BUF_RASYNC and so it won't do second/retry do_read, we can filter\nout those cases after first do_read() attempt with ret>0.\n\nCc: stable@vger.kernel.org # 5.9+\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc5",
        "release_date": "2021-01-22 12:42:54 -0700 io_uring: fix short read retries for non-reg files"
    },
    {
        "commit": "607ec89ed18f49ca59689572659b9c0076f1991f",
        "message": "IORING_OP_CLOSE is special in terms of cancelation, since it has an\nintermediate state where we've removed the file descriptor but hasn't\nclosed the file yet. For that reason, it's currently marked with\nIO_WQ_WORK_NO_CANCEL to prevent cancelation. This ensures that the op\nis always run even if canceled, to prevent leaving us with a live file\nbut an fd that is gone. However, with SQPOLL, since a cancel request\ndoesn't carry any resources on behalf of the request being canceled, if\nwe cancel before any of the close op has been run, we can end up with\nio-wq not having the ->files assigned. This can result in the following\noops reported by Joseph:\n\nBUG: kernel NULL pointer dereference, address: 00000000000000d8\nPGD 800000010b76f067 P4D 800000010b76f067 PUD 10b462067 PMD 0\nOops: 0000 [#1] SMP PTI\nCPU: 1 PID: 1788 Comm: io_uring-sq Not tainted 5.11.0-rc4 #1\nHardware name: Red Hat KVM, BIOS 0.5.1 01/01/2011\nRIP: 0010:__lock_acquire+0x19d/0x18c0\nCode: 00 00 8b 1d fd 56 dd 08 85 db 0f 85 43 05 00 00 48 c7 c6 98 7b 95 82 48 c7 c7 57 96 93 82 e8 9a bc f5 ff 0f 0b e9 2b 05 00 00 <48> 81 3f c0 ca 67 8a b8 00 00 00 00 41 0f 45 c0 89 04 24 e9 81 fe\nRSP: 0018:ffffc90001933828 EFLAGS: 00010002\nRAX: 0000000000000001 RBX: 0000000000000001 RCX: 0000000000000000\nRDX: 0000000000000000 RSI: 0000000000000000 RDI: 00000000000000d8\nRBP: 0000000000000246 R08: 0000000000000001 R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000000\nR13: 0000000000000000 R14: ffff888106e8a140 R15: 00000000000000d8\nFS:  0000000000000000(0000) GS:ffff88813bd00000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00000000000000d8 CR3: 0000000106efa004 CR4: 00000000003706e0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\nCall Trace:\n lock_acquire+0x31a/0x440\n ? close_fd_get_file+0x39/0x160\n ? __lock_acquire+0x647/0x18c0\n _raw_spin_lock+0x2c/0x40\n ? close_fd_get_file+0x39/0x160\n close_fd_get_file+0x39/0x160\n io_issue_sqe+0x1334/0x14e0\n ? lock_acquire+0x31a/0x440\n ? __io_free_req+0xcf/0x2e0\n ? __io_free_req+0x175/0x2e0\n ? find_held_lock+0x28/0xb0\n ? io_wq_submit_work+0x7f/0x240\n io_wq_submit_work+0x7f/0x240\n io_wq_cancel_cb+0x161/0x580\n ? io_wqe_wake_worker+0x114/0x360\n ? io_uring_get_socket+0x40/0x40\n io_async_find_and_cancel+0x3b/0x140\n io_issue_sqe+0xbe1/0x14e0\n ? __lock_acquire+0x647/0x18c0\n ? __io_queue_sqe+0x10b/0x5f0\n __io_queue_sqe+0x10b/0x5f0\n ? io_req_prep+0xdb/0x1150\n ? mark_held_locks+0x6d/0xb0\n ? mark_held_locks+0x6d/0xb0\n ? io_queue_sqe+0x235/0x4b0\n io_queue_sqe+0x235/0x4b0\n io_submit_sqes+0xd7e/0x12a0\n ? _raw_spin_unlock_irq+0x24/0x30\n ? io_sq_thread+0x3ae/0x940\n io_sq_thread+0x207/0x940\n ? do_wait_intr_irq+0xc0/0xc0\n ? __ia32_sys_io_uring_enter+0x650/0x650\n kthread+0x134/0x180\n ? kthread_create_worker_on_cpu+0x90/0x90\n ret_from_fork+0x1f/0x30\n\nFix this by moving the IO_WQ_WORK_NO_CANCEL until _after_ we've modified\nthe fdtable. Canceling before this point is totally fine, and running\nit in the io-wq context _after_ that point is also fine.\n\nFor 5.12, we'll handle this internally and get rid of the no-cancel\nflag, as IORING_OP_CLOSE is the only user of it.\n\nCc: stable@vger.kernel.org\nFixes: b5dba59e0cf7 (\"io_uring: add support for IORING_OP_CLOSE\")\nReported-by: \"Abaci <abaci@linux.alibaba.com>\"\nReviewed-and-tested-by: Joseph Qi <joseph.qi@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc5",
        "release_date": "2021-01-22 12:42:54 -0700 io_uring: fix SQPOLL IORING_OP_CLOSE cancelation state"
    },
    {
        "commit": "ac687e6e8c26181a33270efd1a2e2241377924b0",
        "message": "There is a need to distinguish geniune per-cpu kthreads from kthreads\nthat happen to have a single CPU affinity.\n\nGeniune per-cpu kthreads are kthreads that are CPU affine for\ncorrectness, these will obviously have PF_KTHREAD set, but must also\nhave PF_NO_SETAFFINITY set, lest userspace modify their affinity and\nruins things.\n\nHowever, these two things are not sufficient, PF_NO_SETAFFINITY is\nalso set on other tasks that have their affinities controlled through\nother means, like for instance workqueues.\n\nTherefore another bit is needed; it turns out kthread_create_per_cpu()\nalready has such a bit: KTHREAD_IS_PER_CPU, which is used to make\nkthread_park()/kthread_unpark() work correctly.\n\nExpose this flag and remove the implicit setting of it from\nkthread_create_on_cpu(); the io_uring usage of it seems dubious at\nbest.\n\nSigned-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>\nReviewed-by: Valentin Schneider <valentin.schneider@arm.com>\nTested-by: Valentin Schneider <valentin.schneider@arm.com>\nLink: https://lkml.kernel.org/r/20210121103506.557620262@infradead.org",
        "kernel_version": "v5.11-rc5",
        "release_date": "2021-01-22 15:09:42 +0100 kthread: Extract KTHREAD_IS_PER_CPU"
    },
    {
        "commit": "0b5cd6c32b14413bf87e10ee62be3162588dcbe6",
        "message": "If there are no requests at the time __io_uring_task_cancel() is called,\ntctx_inflight() returns zero and and it terminates not getting a chance\nto go through __io_uring_files_cancel() and do\nio_disable_sqo_submit(). And we absolutely want them disabled by the\ntime cancellation ends.\n\nReported-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nFixes: d9d05217cb69 (\"io_uring: stop SQPOLL submit on creator's death\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc5",
        "release_date": "2021-01-16 21:02:49 -0700 io_uring: fix skipping disabling sqo on exec"
    },
    {
        "commit": "4325cb498cb743dacaa3edbec398c5255f476ef6",
        "message": "WARNING: CPU: 1 PID: 11100 at fs/io_uring.c:9096\n\tio_uring_flush+0x326/0x3a0 fs/io_uring.c:9096\nRIP: 0010:io_uring_flush+0x326/0x3a0 fs/io_uring.c:9096\nCall Trace:\n filp_close+0xb4/0x170 fs/open.c:1280\n close_files fs/file.c:401 [inline]\n put_files_struct fs/file.c:416 [inline]\n put_files_struct+0x1cc/0x350 fs/file.c:413\n exit_files+0x7e/0xa0 fs/file.c:433\n do_exit+0xc22/0x2ae0 kernel/exit.c:820\n do_group_exit+0x125/0x310 kernel/exit.c:922\n get_signal+0x3e9/0x20a0 kernel/signal.c:2770\n arch_do_signal_or_restart+0x2a8/0x1eb0 arch/x86/kernel/signal.c:811\n handle_signal_work kernel/entry/common.c:147 [inline]\n exit_to_user_mode_loop kernel/entry/common.c:171 [inline]\n exit_to_user_mode_prepare+0x148/0x250 kernel/entry/common.c:201\n __syscall_exit_to_user_mode_work kernel/entry/common.c:291 [inline]\n syscall_exit_to_user_mode+0x19/0x50 kernel/entry/common.c:302\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nAn SQPOLL ring creator task may have gotten rid of its file note during\nexit and called io_disable_sqo_submit(), but the io_uring is still left\nreferenced through fdtable, which will be put during close_files() and\ncause a false positive warning.\n\nFirst split the warning into two for more clarity when is hit, and the\nadd sqo_dead check to handle the described case.\n\nReported-by: syzbot+a32b546d58dde07875a1@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc5",
        "release_date": "2021-01-16 12:14:02 -0700 io_uring: fix uring_flush in exit_files() warning"
    },
    {
        "commit": "6b393a1ff1746a1c91bd95cbb2d79b104d8f15ac",
        "message": "WARNING: CPU: 1 PID: 9094 at fs/io_uring.c:8884\n\tio_disable_sqo_submit+0x106/0x130 fs/io_uring.c:8884\nCall Trace:\n io_uring_flush+0x28b/0x3a0 fs/io_uring.c:9099\n filp_close+0xb4/0x170 fs/open.c:1280\n close_fd+0x5c/0x80 fs/file.c:626\n __do_sys_close fs/open.c:1299 [inline]\n __se_sys_close fs/open.c:1297 [inline]\n __x64_sys_close+0x2f/0xa0 fs/open.c:1297\n do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nio_uring's final close() may be triggered by any task not only the\ncreator. It's well handled by io_uring_flush() including SQPOLL case,\nthough a warning in io_disable_sqo_submit() will fallaciously fire by\nmoving this warning out to the only call site that matters.\n\nReported-by: syzbot+2f5d1785dc624932da78@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc5",
        "release_date": "2021-01-16 12:14:02 -0700 io_uring: fix false positive sqo warning on flush"
    },
    {
        "commit": "c93cc9e16d88e0f5ea95d2d65d58a8a4dab258bc",
        "message": "If we're freeing/finishing iopoll requests, ensure we check if the task\nis in idling in terms of cancelation. Otherwise we could end up waiting\nforever in __io_uring_task_cancel() if the task has active iopoll\nrequests that need cancelation.\n\nCc: stable@vger.kernel.org # 5.9+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc5",
        "release_date": "2021-01-16 12:13:59 -0700 io_uring: iopoll requests should also wake task ->in_idle state"
    },
    {
        "commit": "11c0239ae26450709d37e0d7f658aa0875047229",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"We still have a pending fix for a cancelation issue, but it's still\n  being investigated. In the meantime:\n\n   - Dead mm handling fix (Pavel)\n\n   - SQPOLL setup error handling (Pavel)\n\n   - Flush timeout sequence fix (Marcelo)\n\n   - Missing finish_wait() for one exit case\"\n\n* tag 'io_uring-5.11-2021-01-16' of git://git.kernel.dk/linux-block:\n  io_uring: ensure finish_wait() is always called in __io_uring_task_cancel()\n  io_uring: flush timeouts that should already have expired\n  io_uring: do sqo disable on install_fd error\n  io_uring: fix null-deref in io_disable_sqo_submit\n  io_uring: don't take files/mm for a dead task\n  io_uring: drop mm and files after task_work_run",
        "kernel_version": "v5.11-rc4",
        "release_date": "2021-01-16 11:12:02 -0800 Merge tag 'io_uring-5.11-2021-01-16' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "a8d13dbccb137c46fead2ec1a4f1fbc8cfc9ea91",
        "message": "If we enter with requests pending and performm cancelations, we'll have\na different inflight count before and after calling prepare_to_wait().\nThis causes the loop to restart. If we actually ended up canceling\neverything, or everything completed in-between, then we'll break out\nof the loop without calling finish_wait() on the waitqueue. This can\ntrigger a warning on exit_signals(), as we leave the task state in\nTASK_UNINTERRUPTIBLE.\n\nPut a finish_wait() after the loop to catch that case.\n\nCc: stable@vger.kernel.org # 5.9+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc4",
        "release_date": "2021-01-15 16:04:23 -0700 io_uring: ensure finish_wait() is always called in __io_uring_task_cancel()"
    },
    {
        "commit": "f010505b78a4fa8d5b6480752566e7313fb5ca6e",
        "message": "Right now io_flush_timeouts() checks if the current number of events\nis equal to ->timeout.target_seq, but this will miss some timeouts if\nthere have been more than 1 event added since the last time they were\nflushed (possible in io_submit_flush_completions(), for example). Fix\nit by recording the last sequence at which timeouts were flushed so\nthat the number of events seen can be compared to the number of events\nneeded without overflow.\n\nSigned-off-by: Marcelo Diop-Gonzalez <marcelo827@gmail.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc4",
        "release_date": "2021-01-15 10:02:28 -0700 io_uring: flush timeouts that should already have expired"
    },
    {
        "commit": "06585c497b55045ec21aa8128e340f6a6587351c",
        "message": "WARNING: CPU: 0 PID: 8494 at fs/io_uring.c:8717\n\tio_ring_ctx_wait_and_kill+0x4f2/0x600 fs/io_uring.c:8717\nCall Trace:\n io_uring_release+0x3e/0x50 fs/io_uring.c:8759\n __fput+0x283/0x920 fs/file_table.c:280\n task_work_run+0xdd/0x190 kernel/task_work.c:140\n tracehook_notify_resume include/linux/tracehook.h:189 [inline]\n exit_to_user_mode_loop kernel/entry/common.c:174 [inline]\n exit_to_user_mode_prepare+0x249/0x250 kernel/entry/common.c:201\n __syscall_exit_to_user_mode_work kernel/entry/common.c:291 [inline]\n syscall_exit_to_user_mode+0x19/0x50 kernel/entry/common.c:302\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nfailed io_uring_install_fd() is a special case, we don't do\nio_ring_ctx_wait_and_kill() directly but defer it to fput, though still\nneed to io_disable_sqo_submit() before.\n\nnote: it doesn't fix any real problem, just a warning. That's because\nsqring won't be available to the userspace in this case and so SQPOLL\nwon't submit anything.\n\nReported-by: syzbot+9c9c35374c0ecac06516@syzkaller.appspotmail.com\nFixes: d9d05217cb69 (\"io_uring: stop SQPOLL submit on creator's death\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc4",
        "release_date": "2021-01-13 08:29:17 -0700 io_uring: do sqo disable on install_fd error"
    },
    {
        "commit": "b4411616c26f26c4017b8fa4d3538b1a02028733",
        "message": "general protection fault, probably for non-canonical address\n\t0xdffffc0000000022: 0000 [#1] KASAN: null-ptr-deref\n\tin range [0x0000000000000110-0x0000000000000117]\nRIP: 0010:io_ring_set_wakeup_flag fs/io_uring.c:6929 [inline]\nRIP: 0010:io_disable_sqo_submit+0xdb/0x130 fs/io_uring.c:8891\nCall Trace:\n io_uring_create fs/io_uring.c:9711 [inline]\n io_uring_setup+0x12b1/0x38e0 fs/io_uring.c:9739\n do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nio_disable_sqo_submit() might be called before user rings were\nallocated, don't do io_ring_set_wakeup_flag() in those cases.\n\nReported-by: syzbot+ab412638aeb652ded540@syzkaller.appspotmail.com\nFixes: d9d05217cb69 (\"io_uring: stop SQPOLL submit on creator's death\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc4",
        "release_date": "2021-01-13 08:29:17 -0700 io_uring: fix null-deref in io_disable_sqo_submit"
    },
    {
        "commit": "621fadc22365f3cf307bcd9048e3372e9ee9cdcc",
        "message": "In rare cases a task may be exiting while io_ring_exit_work() trying to\ncancel/wait its requests. It's ok for __io_sq_thread_acquire_mm()\nbecause of SQPOLL check, but is not for __io_sq_thread_acquire_files().\nPlay safe and fail for both of them.\n\nCc: stable@vger.kernel.org # 5.5+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc4",
        "release_date": "2021-01-11 07:39:54 -0700 io_uring: don't take files/mm for a dead task"
    },
    {
        "commit": "d434ab6db524ab1efd0afad4ffa1ee65ca6ac097",
        "message": "__io_req_task_submit() run by task_work can set mm and files, but\nio_sq_thread() in some cases, and because __io_sq_thread_acquire_mm()\nand __io_sq_thread_acquire_files() do a simple current->mm/files check\nit may end up submitting IO with mm/files of another task.\n\nWe also need to drop it after in the end to drop potentially grabbed\nreferences to them.\n\nCc: stable@vger.kernel.org # 5.9+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc4",
        "release_date": "2021-01-11 07:39:54 -0700 io_uring: drop mm and files after task_work_run"
    },
    {
        "commit": "d430adfea8d2c5baa186cabb130235f72fecbd5b",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A bit larger than I had hoped at this point, but it's all changes that\n  will be directed towards stable anyway. In detail:\n\n   - Fix a merge window regression on error return (Matthew)\n\n   - Remove useless variable declaration/assignment (Ye Bin)\n\n   - IOPOLL fixes (Pavel)\n\n   - Exit and cancelation fixes (Pavel)\n\n   - fasync lockdep complaint fix (Pavel)\n\n   - Ensure SQPOLL is synchronized with creator life time (Pavel)\"\n\n* tag 'io_uring-5.11-2021-01-10' of git://git.kernel.dk/linux-block:\n  io_uring: stop SQPOLL submit on creator's death\n  io_uring: add warn_once for io_uring_flush()\n  io_uring: inline io_uring_attempt_task_drop()\n  io_uring: io_rw_reissue lockdep annotations\n  io_uring: synchronise ev_posted() with waitqueues\n  io_uring: dont kill fasync under completion_lock\n  io_uring: trigger eventfd for IOPOLL\n  io_uring: Fix return value from alloc_fixed_file_ref_node\n  io_uring: Delete useless variable \u2018id\u2019 in io_prep_async_work\n  io_uring: cancel more aggressively in exit_work\n  io_uring: drop file refs after task cancel\n  io_uring: patch up IOPOLL overflow_flush sync\n  io_uring: synchronise IOPOLL on task_submit fail",
        "kernel_version": "v5.11-rc3",
        "release_date": "2021-01-10 12:39:38 -0800 Merge tag 'io_uring-5.11-2021-01-10' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "d9d05217cb6990b9a56e13b56e7a1b71e2551f6c",
        "message": "When the creator of SQPOLL io_uring dies (i.e. sqo_task), we don't want\nits internals like ->files and ->mm to be poked by the SQPOLL task, it\nhave never been nice and recently got racy. That can happen when the\nowner undergoes destruction and SQPOLL tasks tries to submit new\nrequests in parallel, and so calls io_sq_thread_acquire*().\n\nThat patch halts SQPOLL submissions when sqo_task dies by introducing\nsqo_dead flag. Once set, the SQPOLL task must not do any submission,\nwhich is synchronised by uring_lock as well as the new flag.\n\nThe tricky part is to make sure that disabling always happens, that\nmeans either the ring is discovered by creator's do_exit() -> cancel,\nor if the final close() happens before it's done by the creator. The\nlast is guaranteed by the fact that for SQPOLL the creator task and only\nit holds exactly one file note, so either it pins up to do_exit() or\nremoved by the creator on the final put in flush. (see comments in\nuring_flush() around file->f_count == 2).\n\nOne more place that can trigger io_sq_thread_acquire_*() is\n__io_req_task_submit(). Shoot off requests on sqo_dead there, even\nthough actually we don't need to. That's because cancellation of\nsqo_task should wait for the request before going any further.\n\nnote 1: io_disable_sqo_submit() does io_ring_set_wakeup_flag() so the\ncaller would enter the ring to get an error, but it still doesn't\nguarantee that the flag won't be cleared.\n\nnote 2: if final __userspace__ close happens not from the creator\ntask, the file note will pin the ring until the task dies.\n\nFixed: b1b6b5a30dce8 (\"kernel/io_uring: cancel io_uring before task works\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc3",
        "release_date": "2021-01-09 09:21:43 -0700 io_uring: stop SQPOLL submit on creator's death"
    },
    {
        "commit": "6b5733eb638b7068ab7cb34e663b55a1d1892d85",
        "message": "files_cancel() should cancel all relevant requests and drop file notes,\nso we should never have file notes after that, including on-exit fput\nand flush. Add a WARN_ONCE to be sure.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc3",
        "release_date": "2021-01-09 09:21:43 -0700 io_uring: add warn_once for io_uring_flush()"
    },
    {
        "commit": "4f793dc40bc605b97624fd36baf085b3c35e8bfd",
        "message": "A simple preparation change inlining io_uring_attempt_task_drop() into\nio_uring_flush().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc3",
        "release_date": "2021-01-09 09:21:43 -0700 io_uring: inline io_uring_attempt_task_drop()"
    },
    {
        "commit": "55e6ac1e1f31c7f678d9f3c8d54c6f102e5f1550",
        "message": "We expect io_rw_reissue() to take place only during submission with\nuring_lock held. Add a lockdep annotation to check that invariant.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc3",
        "release_date": "2021-01-09 09:21:43 -0700 io_uring: io_rw_reissue lockdep annotations"
    },
    {
        "commit": "b1445e59cc9a10fdb8f83810ae1f4feb941ab36b",
        "message": "waitqueue_active() needs smp_mb() to be in sync with waitqueues\nmodification, but we miss it in io_cqring_ev_posted*() apart from\ncq_wait() case.\n\nTake an smb_mb() out of wq_has_sleeper() making it waitqueue_active(),\nand place it a few lines before, so it can synchronise other\nwaitqueue_active() as well.\n\nThe patch doesn't add any additional overhead, so even if there are\nno problems currently, it's just safer to have it this way.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc3",
        "release_date": "2021-01-07 07:48:09 -0700 io_uring: synchronise ev_posted() with waitqueues"
    },
    {
        "commit": "4aa84f2ffa81f71e15e5cffc2cc6090dbee78f8e",
        "message": "CPU0                    CPU1\n       ----                    ----\n  lock(&new->fa_lock);\n                               local_irq_disable();\n                               lock(&ctx->completion_lock);\n                               lock(&new->fa_lock);\n  <Interrupt>\n    lock(&ctx->completion_lock);\n\n *** DEADLOCK ***\n\nMove kill_fasync() out of io_commit_cqring() to io_cqring_ev_posted(),\nso it doesn't hold completion_lock while doing it. That saves from the\nreported deadlock, and it's just nice to shorten the locking time and\nuntangle nested locks (compl_lock -> wq_head::lock).\n\nReported-by: syzbot+91ca3f25bd7f795f019c@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc3",
        "release_date": "2021-01-07 07:48:09 -0700 io_uring: dont kill fasync under completion_lock"
    },
    {
        "commit": "80c18e4ac20c9cde420cb3ffab48c936147cf07d",
        "message": "Make sure io_iopoll_complete() tries to wake up eventfd, which currently\nis skipped together with io_cqring_ev_posted() for non-SQPOLL IOPOLL.\n\nAdd an iopoll version of io_cqring_ev_posted(), duplicates a bit of\ncode, but they actually use different sets of wait queues may be for\nbetter.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc3",
        "release_date": "2021-01-07 07:48:09 -0700 io_uring: trigger eventfd for IOPOLL"
    },
    {
        "commit": "3e2224c5867fead6c0b94b84727cc676ac6353a3",
        "message": "alloc_fixed_file_ref_node() currently returns an ERR_PTR on failure.\nio_sqe_files_unregister() expects it to return NULL and since it can only\nreturn -ENOMEM, it makes more sense to change alloc_fixed_file_ref_node()\nto behave that way.\n\nFixes: 1ffc54220c44 (\"io_uring: fix io_sqe_files_unregister() hangs\")\nReported-by: Dan Carpenter <dan.carpenter@oracle.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc3",
        "release_date": "2021-01-06 09:19:49 -0700 io_uring: Fix return value from alloc_fixed_file_ref_node"
    },
    {
        "commit": "170b3bbda08852277b97f4f0516df0785c939764",
        "message": "Fix follow warning:\nfs/io_uring.c:1523:22: warning: variable \u2018id\u2019 set but not used\n[-Wunused-but-set-variable]\n  struct io_identity *id;\n                        ^~\nReported-by: Hulk Robot <hulkci@huawei.com>\nSigned-off-by: Ye Bin <yebin10@huawei.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc3",
        "release_date": "2021-01-05 11:34:23 -0700 io_uring: Delete useless variable \u2018id\u2019 in io_prep_async_work"
    },
    {
        "commit": "90df08538c07b7135703358a0c8c08d97889a704",
        "message": "While io_ring_exit_work() is running new requests of all sorts may be\nissued, so it should do a bit more to cancel them, otherwise they may\njust get stuck. e.g. in io-wq, in poll lists, etc.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc3",
        "release_date": "2021-01-04 15:22:51 -0700 io_uring: cancel more aggressively in exit_work"
    },
    {
        "commit": "de7f1d9e99d8b99e4e494ad8fcd91f0c4c5c9357",
        "message": "io_uring fds marked O_CLOEXEC and we explicitly cancel all requests\nbefore going through exec, so we don't want to leave task's file\nreferences to not our anymore io_uring instances.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc3",
        "release_date": "2021-01-04 15:22:50 -0700 io_uring: drop file refs after task cancel"
    },
    {
        "commit": "6c503150ae33ee19036255cfda0998463613352c",
        "message": "IOPOLL skips completion locking but keeps it under uring_lock, thus\nio_cqring_overflow_flush() and so io_cqring_events() need additional\nlocking with uring_lock in some cases for IOPOLL.\n\nRemove __io_cqring_overflow_flush() from io_cqring_events(), introduce a\nwrapper around flush doing needed synchronisation and call it by hand.\n\nCc: stable@vger.kernel.org # 5.5+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc3",
        "release_date": "2021-01-04 15:22:29 -0700 io_uring: patch up IOPOLL overflow_flush sync"
    },
    {
        "commit": "81b6d05ccad4f3d8a9dfb091fb46ad6978ee40e4",
        "message": "io_req_task_submit() might be called for IOPOLL, do the fail path under\nuring_lock to comply with IOPOLL synchronisation based solely on it.\n\nCc: stable@vger.kernel.org # 5.5+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc3",
        "release_date": "2021-01-04 15:22:27 -0700 io_uring: synchronise IOPOLL on task_submit fail"
    },
    {
        "commit": "6c6ec2b0a3e0381d886d531bd1471dfdb1509237",
        "message": "io_uring always punts opens to async context, since there's no control\nover whether the lookup blocks or not. Add LOOKUP_CACHED to support\njust doing the fast RCU based lookups, which we know will not block. If\nwe can do a cached path resolution of the filename, then we don't have\nto always punt lookups for a worker.\n\nDuring path resolution, we always do LOOKUP_RCU first. If that fails and\nwe terminate LOOKUP_RCU, then fail a LOOKUP_CACHED attempt as well.\n\nCc: Al Viro <viro@zeniv.linux.org.uk>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Al Viro <viro@zeniv.linux.org.uk>",
        "kernel_version": "v5.12-rc1",
        "release_date": "2021-01-04 11:42:21 -0500 fs: add support for LOOKUP_CACHED"
    },
    {
        "commit": "dc3e24b214c50a2ac2dd3d2cc7fb88c9a1e842d4",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A few fixes that should go into 5.11, all marked for stable as well:\n\n   - Fix issue around identity COW'ing and users that share a ring\n     across processes\n\n   - Fix a hang associated with unregistering fixed files (Pavel)\n\n   - Move the 'process is exiting' cancelation a bit earlier, so\n     task_works aren't affected by it (Pavel)\"\n\n* tag 'io_uring-5.11-2021-01-01' of git://git.kernel.dk/linux-block:\n  kernel/io_uring: cancel io_uring before task works\n  io_uring: fix io_sqe_files_unregister() hangs\n  io_uring: add a helper for setting a ref node\n  io_uring: don't assume mm is constant across submits",
        "kernel_version": "v5.11-rc2",
        "release_date": "2021-01-01 12:29:49 -0800 Merge tag 'io_uring-5.11-2021-01-01' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "b1b6b5a30dce872f500dc43f067cba8e7f86fc7d",
        "message": "For cancelling io_uring requests it needs either to be able to run\ncurrently enqueued task_works or having it shut down by that moment.\nOtherwise io_uring_cancel_files() may be waiting for requests that won't\never complete.\n\nGo with the first way and do cancellations before setting PF_EXITING and\nso before putting the task_work infrastructure into a transition state\nwhere task_work_run() would better not be called.\n\nCc: stable@vger.kernel.org # 5.5+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc2",
        "release_date": "2020-12-30 19:36:54 -0700 kernel/io_uring: cancel io_uring before task works"
    },
    {
        "commit": "1ffc54220c444774b7f09e6d2121e732f8e19b94",
        "message": "io_sqe_files_unregister() uninterruptibly waits for enqueued ref nodes,\nhowever requests keeping them may never complete, e.g. because of some\nuserspace dependency. Make sure it's interruptible otherwise it would\nhang forever.\n\nCc: stable@vger.kernel.org # 5.6+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc2",
        "release_date": "2020-12-30 19:35:53 -0700 io_uring: fix io_sqe_files_unregister() hangs"
    },
    {
        "commit": "1642b4450d20e31439c80c28256c8eee08684698",
        "message": "Setting a new reference node to a file data is not trivial, don't repeat\nit, add and use a helper.\n\nCc: stable@vger.kernel.org # 5.6+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc2",
        "release_date": "2020-12-30 19:35:53 -0700 io_uring: add a helper for setting a ref node"
    },
    {
        "commit": "77788775c7132a8d93c6930ab1bd84fc743c7cb7",
        "message": "If we COW the identity, we assume that ->mm never changes. But this\nisn't true of multiple processes end up sharing the ring. Hence treat\nid->mm like like any other process compontent when it comes to the\nidentity mapping. This is pretty trivial, just moving the existing grab\ninto io_grab_identity(), and including a check for the match.\n\nCc: stable@vger.kernel.org # 5.10\nFixes: 1e6fa5216a0e (\"io_uring: COW io_identity on mismatch\")\nReported-by: Christian Brauner <christian.brauner@ubuntu.com>:\nTested-by: Christian Brauner <christian.brauner@ubuntu.com>:\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc2",
        "release_date": "2020-12-29 11:00:36 -0700 io_uring: don't assume mm is constant across submits"
    },
    {
        "commit": "2f2fce3d535779cb1b0d77ce839029d5d875d4f4",
        "message": "Pull smack fix from Casey Schaufler:\n \"Provide a fix for the incorrect handling of privilege in the face of\n  io_uring's use of kernel threads. That invalidated an long standing\n  assumption regarding the privilege of kernel threads.\n\n  The fix is simple and safe. It was provided by Jens Axboe and has been\n  tested\"\n\n* tag 'Smack-for-5.11-io_uring-fix' of git://github.com/cschaufler/smack-next:\n  Smack: Handle io_uring kernel thread privileges",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-24 14:08:43 -0800 Merge tag 'Smack-for-5.11-io_uring-fix' of git://github.com/cschaufler/smack-next"
    },
    {
        "commit": "60e8edd2513abffdb6d4a9b5affca7f9dd5ec73d",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"All straight fixes, or a prep patch for a fix, either bound for stable\n  or fixing issues from this merge window. In particular:\n\n   - Fix new shutdown op not breaking links on failure\n\n   - Hold mm->mmap_sem for mm->locked_vm manipulation\n\n   - Various cancelation fixes (me, Pavel)\n\n   - Fix error path potential double ctx free (Pavel)\n\n   - IOPOLL fixes (Xiaoguang)\"\n\n* tag 'io_uring-5.11-2020-12-23' of git://git.kernel.dk/linux-block:\n  io_uring: hold uring_lock while completing failed polled io in io_wq_submit_work()\n  io_uring: fix double io_uring free\n  io_uring: fix ignoring xa_store errors\n  io_uring: end waiting before task cancel attempts\n  io_uring: always progress task_work on task cancel\n  io-wq: kill now unused io_wq_cancel_all()\n  io_uring: make ctx cancel on exit targeted to actual ctx\n  io_uring: fix 0-iov read buffer select\n  io_uring: close a small race gap for files cancel\n  io_uring: fix io_wqe->work_list corruption\n  io_uring: limit {io|sq}poll submit locking scope\n  io_uring: inline io_cqring_mark_overflow()\n  io_uring: consolidate CQ nr events calculation\n  io_uring: remove racy overflow list fast checks\n  io_uring: cancel reqs shouldn't kill overflow list\n  io_uring: hold mmap_sem for mm->locked_vm manipulation\n  io_uring: break links on shutdown failure",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-24 12:35:00 -0800 Merge tag 'io_uring-5.11-2020-12-23' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "c07e6719511e77c4b289f62bfe96423eb6ea061d",
        "message": "io_iopoll_complete() does not hold completion_lock to complete polled io,\nso in io_wq_submit_work(), we can not call io_req_complete() directly, to\ncomplete polled io, otherwise there maybe concurrent access to cqring,\ndefer_list, etc, which is not safe. Commit dad1b1242fd5 (\"io_uring: always\nlet io_iopoll_complete() complete polled io\") has fixed this issue, but\nPavel reported that IOPOLL apart from rw can do buf reg/unreg requests(\nIORING_OP_PROVIDE_BUFFERS or IORING_OP_REMOVE_BUFFERS), so the fix is not\ngood.\n\nGiven that io_iopoll_complete() is always called under uring_lock, so here\nfor polled io, we can also get uring_lock to fix this issue.\n\nFixes: dad1b1242fd5 (\"io_uring: always let io_iopoll_complete() complete polled io\")\nCc: <stable@vger.kernel.org> # 5.5+\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\n[axboe: don't deref 'req' after completing it']\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-22 17:14:53 -0700 io_uring: hold uring_lock while completing failed polled io in io_wq_submit_work()"
    },
    {
        "commit": "9faadcc8abe4b83d0263216dc3a6321d5bbd616b",
        "message": "Once we created a file for current context during setup, we should not\ncall io_ring_ctx_wait_and_kill() directly as it'll be done by fput(file)\n\nCc: stable@vger.kernel.org # 5.10\nReported-by: syzbot+c9937dfb2303a5f18640@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\n[axboe: fix unused 'ret' for !CONFIG_UNIX]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-22 17:14:50 -0700 io_uring: fix double io_uring free"
    },
    {
        "commit": "942cb357ae7d9249088e3687ee6a00ed2745a0c7",
        "message": "Smack assumes that kernel threads are privileged for smackfs\noperations. This was necessary because the credential of the\nkernel thread was not related to a user operation. With io_uring\nthe credential does reflect a user's rights and can be used.\n\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nAcked-by: Jens Axboe <axboe@kernel.dk>\nAcked-by: Eric W. Biederman <ebiederm@xmission.com>\nSigned-off-by: Casey Schaufler <casey@schaufler-ca.com>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-22 15:34:24 -0800 Smack: Handle io_uring kernel thread privileges"
    },
    {
        "commit": "a528b04ea40690ff40501f50d618a62a02b19620",
        "message": "xa_store() may fail, check the result.\n\nCc: stable@vger.kernel.org # 5.10\nFixes: 0f2122045b946 (\"io_uring: don't rely on weak ->files references\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-21 13:01:10 -0700 io_uring: fix ignoring xa_store errors"
    },
    {
        "commit": "f57555eda979ca085d2524db81e14b8a6089e15e",
        "message": "Get rid of TASK_UNINTERRUPTIBLE and waiting with finish_wait before\ngoing for next iteration in __io_uring_task_cancel(), because\n__io_uring_files_cancel() doesn't expect that sheduling is disallowed.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-20 11:05:39 -0700 io_uring: end waiting before task cancel attempts"
    },
    {
        "commit": "55583d72e2303638d30dd4a7aabef59ffa0a017a",
        "message": "Might happen that __io_uring_cancel_task_requests() cancels nothing but\nthere are task_works pending. We need to always run them.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-20 11:05:39 -0700 io_uring: always progress task_work on task cancel"
    },
    {
        "commit": "446bc1c207331080d8c711a4456799b7d0b9df26",
        "message": "io_uring no longer issues full cancelations on the io-wq, so remove any\nremnants of this code and the IO_WQ_BIT_CANCEL flag.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-20 10:47:42 -0700 io-wq: kill now unused io_wq_cancel_all()"
    },
    {
        "commit": "00c18640c2430c4bafaaeede1f9dd6f7ec0e4b25",
        "message": "Before IORING_SETUP_ATTACH_WQ, we could just cancel everything on the\nio-wq when exiting. But that's not the case if they are shared, so\ncancel for the specific ctx instead.\n\nCc: stable@vger.kernel.org\nFixes: 24369c2e3bb0 (\"io_uring: add io-wq workqueue sharing\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-20 10:45:02 -0700 io_uring: make ctx cancel on exit targeted to actual ctx"
    },
    {
        "commit": "dd20166236953c8cd14f4c668bf972af32f0c6be",
        "message": "Doing vectored buf-select read with 0 iovec passed is meaningless and\nutterly broken, forbid it.\n\nCc: <stable@vger.kernel.org> # 5.7+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-19 06:26:56 -0700 io_uring: fix 0-iov read buffer select"
    },
    {
        "commit": "dfea9fce29fda6f2f91161677e0e0d9b671bc099",
        "message": "The purpose of io_uring_cancel_files() is to wait for all requests\nmatching ->files to go/be cancelled. We should first drop files of a\nrequest in io_req_drop_files() and only then make it undiscoverable for\nio_uring_cancel_files.\n\nFirst drop, then delete from list. It's ok to leave req->id->files\ndangling, because it's not dereferenced by cancellation code, only\ncompared against. It would potentially go to sleep and be awaken by\nfollowing in io_req_drop_files() wake_up().\n\nFixes: 0f2122045b946 (\"io_uring: don't rely on weak ->files references\")\nCc: <stable@vger.kernel.org> # 5.5+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-18 08:16:02 -0700 io_uring: close a small race gap for files cancel"
    },
    {
        "commit": "0020ef04e48571a88d4f482ad08f71052c5c5a08",
        "message": "For the first time a req punted to io-wq, we'll initialize io_wq_work's\nlist to be NULL, then insert req to io_wqe->work_list. If this req is not\ninserted into tail of io_wqe->work_list, this req's io_wq_work list will\npoint to another req's io_wq_work. For splitted bio case, this req maybe\ninserted to io_wqe->work_list repeatedly, once we insert it to tail of\nio_wqe->work_list for the second time, now io_wq_work->list->next will be\ninvalid pointer, which then result in many strang error, panic, kernel\nsoft-lockup, rcu stall, etc.\n\nIn my vm, kernel doest not have commit cc29e1bf0d63f7 (\"block: disable\niopoll for split bio\"), below fio job can reproduce this bug steadily:\n[global]\nname=iouring-sqpoll-iopoll-1\nioengine=io_uring\niodepth=128\nnumjobs=1\nthread\nrw=randread\ndirect=1\nregisterfiles=1\nhipri=1\nbs=4m\nsize=100M\nruntime=120\ntime_based\ngroup_reporting\nrandrepeat=0\n\n[device]\ndirectory=/home/feiman.wxg/mntpoint/  # an ext4 mount point\n\nIf we have commit cc29e1bf0d63f7 (\"block: disable iopoll for split bio\"),\nthere will no splitted bio case for polled io, but I think we still to need\nto fix this list corruption, it also should maybe go to stable branchs.\n\nTo fix this corruption, if a req is inserted into tail of io_wqe->work_list,\ninitialize req->io_wq_work->list->next to bu NULL.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-18 08:15:10 -0700 io_uring: fix io_wqe->work_list corruption"
    },
    {
        "commit": "89448c47b8452b67c146dc6cad6f737e004c5caf",
        "message": "We don't need to take uring_lock for SQPOLL|IOPOLL to do\nio_cqring_overflow_flush() when cq_overflow_list is empty, remove it\nfrom the hot path.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-17 08:40:52 -0700 io_uring: limit {io|sq}poll submit locking scope"
    },
    {
        "commit": "09e88404f46cc32237f596c66f48a826294e08f2",
        "message": "There is only one user of it and the name is misleading, get rid of it\nby inlining. By the way make overflow_flush's return value deduction\nsimpler.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-17 08:40:52 -0700 io_uring: inline io_cqring_mark_overflow()"
    },
    {
        "commit": "e23de15fdbd3070446b2d212373c0ae556f63d93",
        "message": "Add a helper which calculates number of events in CQ. Handcoded version\nof it in io_cqring_overflow_flush() is not the clearest thing, so it\nmakes it slightly more readable.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-17 08:40:52 -0700 io_uring: consolidate CQ nr events calculation"
    },
    {
        "commit": "9cd2be519d05ee78876d55e8e902b7125f78b74f",
        "message": "list_empty_careful() is not racy only if some conditions are met, i.e.\nno re-adds after del_init. io_cqring_overflow_flush() does list_move(),\nso it's actually racy.\n\nRemove those checks, we have ->cq_check_overflow for the fast path.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-17 08:40:52 -0700 io_uring: remove racy overflow list fast checks"
    },
    {
        "commit": "cda286f0715c82f8117e166afd42cca068876dde",
        "message": "io_uring_cancel_task_requests() doesn't imply that the ring is going\naway, it may continue to work well after that. The problem is that it\nsets ->cq_overflow_flushed effectively disabling the CQ overflow feature\n\nSplit setting cq_overflow_flushed from flush, and do the first one only\non exit. It's ok in terms of cancellations because there is a\nio_uring->in_idle check in __io_cqring_fill_event().\n\nIt also fixes a race with setting ->cq_overflow_flushed in\nio_uring_cancel_task_requests, whuch's is not atomic and a part of a\nbitmask with other flags. Though, the only other flag that's not set\nduring init is drain_next, so it's not as bad for sane architectures.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nFixes: 0f2122045b946 (\"io_uring: don't rely on weak ->files references\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-17 08:40:45 -0700 io_uring: cancel reqs shouldn't kill overflow list"
    },
    {
        "commit": "4bc4a912534a72f1c96f483448f0be16e5a48063",
        "message": "The kernel doesn't seem to have clear rules around this, but various\nspots are using the mmap_sem to serialize access to modifying the\nlocked_vm count. Play it safe and lock the mm for write when accounting\nor unaccounting locked memory.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-17 07:53:33 -0700 io_uring: hold mmap_sem for mm->locked_vm manipulation"
    },
    {
        "commit": "a146468d76e0462393a3e15b77b8b3ede60e2d06",
        "message": "Ensure that the return value of __sys_shutdown_sock() is used to\npotentially break links to the request, if we fail.\n\nFixes: 36f4fa6886a8 (\"io_uring: add support for shutdown(2)\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-16 14:56:36 -0700 io_uring: break links on shutdown failure"
    },
    {
        "commit": "48aba79bcf6ea05148dc82ad9c40713960b00396",
        "message": "Pull io_uring updates from Jens Axboe:\n \"Fairly light set of changes this time around, and mostly some bits\n  that were pushed out to 5.11 instead of 5.10, fixes/cleanups, and a\n  few features. In particular:\n\n   - Cleanups around iovec import (David Laight, Pavel)\n\n   - Add timeout support for io_uring_enter(2), which enables us to\n     clean up liburing and avoid a timeout sqe submission in the\n     completion path.\n\n     The big win here is that it allows setups that split SQ and CQ\n     handling into separate threads to avoid locking, as the CQ side\n     will no longer submit when timeouts are needed when waiting for\n     events (Hao Xu)\n\n   - Add support for socket shutdown, and renameat/unlinkat.\n\n   - SQPOLL cleanups and improvements (Xiaoguang Wang)\n\n   - Allow SQPOLL setups for CAP_SYS_NICE, and enable regular\n     (non-fixed) files to be used.\n\n   - Cancelation improvements (Pavel)\n\n   - Fixed file reference improvements (Pavel)\n\n   - IOPOLL related race fixes (Pavel)\n\n   - Lots of other little fixes and cleanups (mostly Pavel)\"\n\n* tag 'for-5.11/io_uring-2020-12-14' of git://git.kernel.dk/linux-block: (43 commits)\n  io_uring: fix io_cqring_events()'s noflush\n  io_uring: fix racy IOPOLL flush overflow\n  io_uring: fix racy IOPOLL completions\n  io_uring: always let io_iopoll_complete() complete polled io\n  io_uring: add timeout update\n  io_uring: restructure io_timeout_cancel()\n  io_uring: fix files cancellation\n  io_uring: use bottom half safe lock for fixed file data\n  io_uring: fix miscounting ios_left\n  io_uring: change submit file state invariant\n  io_uring: check kthread stopped flag when sq thread is unparked\n  io_uring: share fixed_file_refs b/w multiple rsrcs\n  io_uring: replace inflight_wait with tctx->wait\n  io_uring: don't take fs for recvmsg/sendmsg\n  io_uring: only wake up sq thread while current task is in io worker context\n  io_uring: don't acquire uring_lock twice\n  io_uring: initialize 'timeout' properly in io_sq_thread()\n  io_uring: refactor io_sq_thread() handling\n  io_uring: always batch cancel in *cancel_files()\n  io_uring: pass files into kill timeouts/poll\n  ...",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-16 12:44:05 -0800 Merge tag 'for-5.11/io_uring-2020-12-14' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "005b2a9dc819a1265a8c765595f8f6d88d6173d9",
        "message": "Pull TIF_NOTIFY_SIGNAL updates from Jens Axboe:\n \"This sits on top of of the core entry/exit and x86 entry branch from\n  the tip tree, which contains the generic and x86 parts of this work.\n\n  Here we convert the rest of the archs to support TIF_NOTIFY_SIGNAL.\n\n  With that done, we can get rid of JOBCTL_TASK_WORK from task_work and\n  signal.c, and also remove a deadlock work-around in io_uring around\n  knowing that signal based task_work waking is invoked with the sighand\n  wait queue head lock.\n\n  The motivation for this work is to decouple signal notify based\n  task_work, of which io_uring is a heavy user of, from sighand. The\n  sighand lock becomes a huge contention point, particularly for\n  threaded workloads where it's shared between threads. Even outside of\n  threaded applications it's slower than it needs to be.\n\n  Roman Gershman <romger@amazon.com> reported that his networked\n  workload dropped from 1.6M QPS at 80% CPU to 1.0M QPS at 100% CPU\n  after io_uring was changed to use TIF_NOTIFY_SIGNAL. The time was all\n  spent hammering on the sighand lock, showing 57% of the CPU time there\n  [1].\n\n  There are further cleanups possible on top of this. One example is\n  TIF_PATCH_PENDING, where a patch already exists to use\n  TIF_NOTIFY_SIGNAL instead. Hopefully this will also lead to more\n  consolidation, but the work stands on its own as well\"\n\n[1] https://github.com/axboe/liburing/issues/215\n\n* tag 'tif-task_work.arch-2020-12-14' of git://git.kernel.dk/linux-block: (28 commits)\n  io_uring: remove 'twa_signal_ok' deadlock work-around\n  kernel: remove checking for TIF_NOTIFY_SIGNAL\n  signal: kill JOBCTL_TASK_WORK\n  io_uring: JOBCTL_TASK_WORK is no longer used by task_work\n  task_work: remove legacy TWA_SIGNAL path\n  sparc: add support for TIF_NOTIFY_SIGNAL\n  riscv: add support for TIF_NOTIFY_SIGNAL\n  nds32: add support for TIF_NOTIFY_SIGNAL\n  ia64: add support for TIF_NOTIFY_SIGNAL\n  h8300: add support for TIF_NOTIFY_SIGNAL\n  c6x: add support for TIF_NOTIFY_SIGNAL\n  alpha: add support for TIF_NOTIFY_SIGNAL\n  xtensa: add support for TIF_NOTIFY_SIGNAL\n  arm: add support for TIF_NOTIFY_SIGNAL\n  microblaze: add support for TIF_NOTIFY_SIGNAL\n  hexagon: add support for TIF_NOTIFY_SIGNAL\n  csky: add support for TIF_NOTIFY_SIGNAL\n  openrisc: add support for TIF_NOTIFY_SIGNAL\n  sh: add support for TIF_NOTIFY_SIGNAL\n  um: add support for TIF_NOTIFY_SIGNAL\n  ...",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-16 12:33:35 -0800 Merge tag 'tif-task_work.arch-2020-12-14' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "faf145d6f3f3d6f2c066f65602ba9d0a03106915",
        "message": "Pull execve updates from Eric Biederman:\n \"This set of changes ultimately fixes the interaction of posix file\n  lock and exec. Fundamentally most of the change is just moving where\n  unshare_files is called during exec, and tweaking the users of\n  files_struct so that the count of files_struct is not unnecessarily\n  played with.\n\n  Along the way fcheck and related helpers were renamed to more\n  accurately reflect what they do.\n\n  There were also many other small changes that fell out, as this is the\n  first time in a long time much of this code has been touched.\n\n  Benchmarks haven't turned up any practical issues but Al Viro has\n  observed a possibility for a lot of pounding on task_lock. So I have\n  some changes in progress to convert put_files_struct to always rcu\n  free files_struct. That wasn't ready for the merge window so that will\n  have to wait until next time\"\n\n* 'exec-for-v5.11' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (27 commits)\n  exec: Move io_uring_task_cancel after the point of no return\n  coredump: Document coredump code exclusively used by cell spufs\n  file: Remove get_files_struct\n  file: Rename __close_fd_get_file close_fd_get_file\n  file: Replace ksys_close with close_fd\n  file: Rename __close_fd to close_fd and remove the files parameter\n  file: Merge __alloc_fd into alloc_fd\n  file: In f_dupfd read RLIMIT_NOFILE once.\n  file: Merge __fd_install into fd_install\n  proc/fd: In fdinfo seq_show don't use get_files_struct\n  bpf/task_iter: In task_file_seq_get_next use task_lookup_next_fd_rcu\n  proc/fd: In proc_readfd_common use task_lookup_next_fd_rcu\n  file: Implement task_lookup_next_fd_rcu\n  kcmp: In get_file_raw_ptr use task_lookup_fd_rcu\n  proc/fd: In tid_fd_mode use task_lookup_fd_rcu\n  file: Implement task_lookup_fd_rcu\n  file: Rename fcheck lookup_fd_rcu\n  file: Replace fcheck_files with files_lookup_fd_rcu\n  file: Factor files_lookup_fd_locked out of fcheck_files\n  file: Rename __fcheck_files to files_lookup_fd_raw\n  ...",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-15 19:29:43 -0800 Merge branch 'exec-for-v5.11' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace"
    },
    {
        "commit": "1ac0884d5474fea8dc6ceabbd0e870d1bf4b7b42",
        "message": "Pull core entry/exit updates from Thomas Gleixner:\n \"A set of updates for entry/exit handling:\n\n   - More generalization of entry/exit functionality\n\n   - The consolidation work to reclaim TIF flags on x86 and also for\n     non-x86 specific TIF flags which are solely relevant for syscall\n     related work and have been moved into their own storage space. The\n     x86 specific part had to be merged in to avoid a major conflict.\n\n   - The TIF_NOTIFY_SIGNAL work which replaces the inefficient signal\n     delivery mode of task work and results in an impressive performance\n     improvement for io_uring. The non-x86 consolidation of this is\n     going to come seperate via Jens.\n\n   - The selective syscall redirection facility which provides a clean\n     and efficient way to support the non-Linux syscalls of WINE by\n     catching them at syscall entry and redirecting them to the user\n     space emulation. This can be utilized for other purposes as well\n     and has been designed carefully to avoid overhead for the regular\n     fastpath. This includes the core changes and the x86 support code.\n\n   - Simplification of the context tracking entry/exit handling for the\n     users of the generic entry code which guarantee the proper ordering\n     and protection.\n\n   - Preparatory changes to make the generic entry code accomodate S390\n     specific requirements which are mostly related to their syscall\n     restart mechanism\"\n\n* tag 'core-entry-2020-12-14' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (36 commits)\n  entry: Add syscall_exit_to_user_mode_work()\n  entry: Add exit_to_user_mode() wrapper\n  entry_Add_enter_from_user_mode_wrapper\n  entry: Rename exit_to_user_mode()\n  entry: Rename enter_from_user_mode()\n  docs: Document Syscall User Dispatch\n  selftests: Add benchmark for syscall user dispatch\n  selftests: Add kselftest for syscall user dispatch\n  entry: Support Syscall User Dispatch on common syscall entry\n  kernel: Implement selective syscall userspace redirection\n  signal: Expose SYS_USER_DISPATCH si_code type\n  x86: vdso: Expose sigreturn address on vdso to the kernel\n  MAINTAINERS: Add entry for common entry code\n  entry: Fix boot for !CONFIG_GENERIC_ENTRY\n  x86: Support HAVE_CONTEXT_TRACKING_OFFSTACK\n  context_tracking: Only define schedule_user() on !HAVE_CONTEXT_TRACKING_OFFSTACK archs\n  sched: Detect call to schedule from critical entry code\n  context_tracking: Don't implement exception_enter/exit() on CONFIG_HAVE_CONTEXT_TRACKING_OFFSTACK\n  context_tracking: Introduce HAVE_CONTEXT_TRACKING_OFFSTACK\n  x86: Reclaim unused x86 TI flags\n  ...",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-14 17:13:53 -0800 Merge tag 'core-entry-2020-12-14' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip"
    },
    {
        "commit": "31d00f6eb1f2b498a1d7af62cffeba3fbea8cf75",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two fixes in here, fixing issues introduced in this merge window\"\n\n* tag 'io_uring-5.10-2020-12-11' of git://git.kernel.dk/linux-block:\n  io_uring: fix file leak on error path of io ctx creation\n  io_uring: fix mis-seting personality's creds",
        "kernel_version": "v5.10",
        "release_date": "2020-12-12 09:45:01 -0800 Merge tag 'io_uring-5.10-2020-12-11' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "355fb9e2b78e78b38ec00f5cd9b05c6aceb98335",
        "message": "The TIF_NOTIFY_SIGNAL based implementation of TWA_SIGNAL is always safe\nto use, regardless of context, as we won't be recursing into the signal\nlock. So now that all archs are using that, we can drop this deadlock\nwork-around as it's always safe to use TWA_SIGNAL.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-12 09:17:38 -0700 io_uring: remove 'twa_signal_ok' deadlock work-around"
    },
    {
        "commit": "792ee0f6db5b942ee68ee7c9aea9d34dde4c4ff2",
        "message": "Remove the dead code, TWA_SIGNAL will never set JOBCTL_TASK_WORK at\nthis point.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-12 09:17:38 -0700 io_uring: JOBCTL_TASK_WORK is no longer used by task_work"
    },
    {
        "commit": "9ee1206dcfb9d56503c0de9f8320f7b29c795867",
        "message": "Now that unshare_files happens in begin_new_exec after the point of no\nreturn, io_uring_task_cancel can also happen later.\n\nEffectively this means io_uring activities for a task are only canceled\nwhen exec succeeds.\n\nLink: https://lkml.kernel.org/r/878saih2op.fsf@x220.int.ebiederm.org\nSigned-off-by: Eric W. Biederman <ebiederm@xmission.com>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-10 12:57:13 -0600 exec: Move io_uring_task_cancel after the point of no return"
    },
    {
        "commit": "125c00af3b2c498875b275c4ad932b4db2c6bae2",
        "message": "A while ago it was reported that posix file locking goes wrong when a\nmulti-threaded process calls exec.  I looked into the history and this\nis definitely a regression, that should be fixed if we can.\n\nThis set of changes cleanups of the code in exec so hopefully this code\nwill not regress again.  Then it adds helpers and fixes the users of\nfiles_struct so the reference count is only incremented if COPY_FILES is\npassed to clone (or if io_uring takes a reference).  Then it removes\nhelpers (get_files_struct, __install_fd, __alloc_fd, __close_fd) that\nare no longer needed and if used would encourage code that increments\nthe count of files_struct somewhere besides in clone when COPY_FILES is\npassed.\n\nIn addition to fixing the bug in exec and simplifing the code this set\nof changes by virtue of getting files_struct.count correct it optimizes\nfdget.  With proc and other places not temporarily increasing the count\non files_struct __fget_light should succeed more often in being able to\nreturn a struct file without touching it's reference count.\n\nFixing the count in files_struct was suggested by Oleg[1].\n\nFor those that are interested in the history of this issue I have\nincluded as much of it as I could find in the first change.\n\nSince v1:\n\n- Renamed the functions\n  __fcheck_files      -> files_lookup_fd_raw\n  fcheck_files        -> files_lookup_fd_locked\n  fcheck_files        -> files_lookup_fd_rcu\n  fcheck_files        -> lookup_fd_rcu\n  fcheck_task         -> task_lookup_fd_rcu\n  fnext_task          -> task_lookup_next_fd_rcu\n  __close_fd_get_file -> close_fd_get_file\n\n- Simplified get_file_raw_ptr\n\n- Removed ksys_close\n\n- Examined the penalty for taking task_lock.  The helper\n  task_lookup_next_fd_rcu takes task_lock each iteration.  Concern was\n  expressed that this might be a problem.  The function tid_fd_mode\n  isn called from tid_fd_revalidate which is called when ever a file\n  descriptor file is stat'ed, opened, or otherwise accessed.  The\n  function tid_fd_mode histrocally called get_files_struct which took\n  and dropped task_lock.  So the volume of task_lock calls is already\n  proportional to the number of file descriptors.  A micro benchmark\n  did not see the move to task_lookup_next_fd_rcu making a difference\n  in performance.  Which suggests that the change to taking the task\n  lock for every file descriptor found in task_lookup_next_fd will not\n  be a problem.\n\n- Reviewed the code for conflicts with io_uring (especially the\n  removal of get_files_struct).  To my surprise no conflicts were\n  found as io_uring does not use standard helpers but instead rolls\n  it's own version of get_files_struct by hand.\n\n Documentation/filesystems/files.rst          |   8 +-\n arch/powerpc/platforms/cell/spufs/coredump.c |   2 +-\n drivers/android/binder.c                     |   2 +-\n fs/autofs/dev-ioctl.c                        |   5 +-\n fs/coredump.c                                |   5 +-\n fs/exec.c                                    |  29 +++----\n fs/file.c                                    | 124 +++++++++++++--------------\n fs/io_uring.c                                |   2 +-\n fs/locks.c                                   |  14 +--\n fs/notify/dnotify/dnotify.c                  |   2 +-\n fs/open.c                                    |   2 +-\n fs/proc/fd.c                                 |  48 ++++-------\n include/linux/fdtable.h                      |  40 +++++----\n include/linux/syscalls.h                     |  12 ---\n kernel/bpf/syscall.c                         |  20 +----\n kernel/bpf/task_iter.c                       |  44 +++-------\n kernel/fork.c                                |  12 +--\n kernel/kcmp.c                                |  29 ++-----\n 18 files changed, 153 insertions(+), 247 deletions(-)\n\nEric W. Biederman (25):\n      exec: Don't open code get_close_on_exec\n      exec: Move unshare_files to fix posix file locking during exec\n      exec: Simplify unshare_files\n      exec: Remove reset_files_struct\n      kcmp: In kcmp_epoll_target use fget_task\n      bpf: In bpf_task_fd_query use fget_task\n      proc/fd: In proc_fd_link use fget_task\n      file: Rename __fcheck_files to files_lookup_fd_raw\n      file: Factor files_lookup_fd_locked out of fcheck_files\n      file: Replace fcheck_files with files_lookup_fd_rcu\n      file: Rename fcheck lookup_fd_rcu\n      file: Implement task_lookup_fd_rcu\n      proc/fd: In tid_fd_mode use task_lookup_fd_rcu\n      kcmp: In get_file_raw_ptr use task_lookup_fd_rcu\n      file: Implement task_lookup_next_fd_rcu\n      proc/fd: In proc_readfd_common use task_lookup_next_fd_rcu\n      bpf/task_iter: In task_file_seq_get_next use task_lookup_next_fd_rcu\n      proc/fd: In fdinfo seq_show don't use get_files_struct\n      file: Merge __fd_install into fd_install\n      file: In f_dupfd read RLIMIT_NOFILE once.\n      file: Merge __alloc_fd into alloc_fd\n      file: Rename __close_fd to close_fd and remove the files parameter\n      file: Replace ksys_close with close_fd\n      file: Rename __close_fd_get_file close_fd_get_file\n      file: Remove get_files_struct\n\n[1] https://lkml.kernel.org/r/20180915160423.GA31461@redhat.com\nv1: https://lkml.kernel.org/r/87ft8l6ic3.fsf@x220.int.ebiederm.org\nReported-by: Jeff Layton <jlayton@redhat.com>\nReported-by: Daniel P. Berrang\u00e9 <berrange@redhat.com>\nSuggested-by: Oleg Nesterov <oleg@redhat.com>\nAcked-by: Linus Torvalds <torvalds@linux-foundation.org>\nLink: https://lkml.kernel.org/r/87r1on1v62.fsf@x220.int.ebiederm.org\nLink: https://lists.openvz.org/pipermail/criu/2020-November/045123.html\nLink: https://marc.info/?l=openvz-criu&m=160591423214257\nSigned-off-by: \"Eric W. Biederman\" <ebiederm@xmission.com>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-10 12:52:05 -0600 exec: Move unshare_files and guarantee files_struct.count is correct"
    },
    {
        "commit": "59850d226e4907a6f37c1d2fe5ba97546a8691a4",
        "message": "Checking !list_empty(&ctx->cq_overflow_list) around noflush in\nio_cqring_events() is racy, because if it fails but a request overflowed\njust after that, io_cqring_overflow_flush() still will be called.\n\nRemove the second check, it shouldn't be a problem for performance,\nbecause there is cq_check_overflow bit check just above.\n\nCc: <stable@vger.kernel.org> # 5.5+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:02 -0700 io_uring: fix io_cqring_events()'s noflush"
    },
    {
        "commit": "634578f800652035debba3098d8ab0d21af7c7a5",
        "message": "It's not safe to call io_cqring_overflow_flush() for IOPOLL mode without\nhodling uring_lock, because it does synchronisation differently. Make\nsure we have it.\n\nAs for io_ring_exit_work(), we don't even need it there because\nio_ring_ctx_wait_and_kill() already set force flag making all overflowed\nrequests to be dropped.\n\nCc: <stable@vger.kernel.org> # 5.5+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:01 -0700 io_uring: fix racy IOPOLL flush overflow"
    },
    {
        "commit": "31bff9a51b264df6d144931a6a5f1d6cc815ed4b",
        "message": "IOPOLL allows buffer remove/provide requests, but they doesn't\nsynchronise by rules of IOPOLL, namely it have to hold uring_lock.\n\nCc: <stable@vger.kernel.org> # 5.7+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:01 -0700 io_uring: fix racy IOPOLL completions"
    },
    {
        "commit": "dad1b1242fd5717af18ae4ac9d12b9f65849e13a",
        "message": "Abaci Fuzz reported a double-free or invalid-free BUG in io_commit_cqring():\n[   95.504842] BUG: KASAN: double-free or invalid-free in io_commit_cqring+0x3ec/0x8e0\n[   95.505921]\n[   95.506225] CPU: 0 PID: 4037 Comm: io_wqe_worker-0 Tainted: G    B\nW         5.10.0-rc5+ #1\n[   95.507434] Hardware name: Red Hat KVM, BIOS 0.5.1 01/01/2011\n[   95.508248] Call Trace:\n[   95.508683]  dump_stack+0x107/0x163\n[   95.509323]  ? io_commit_cqring+0x3ec/0x8e0\n[   95.509982]  print_address_description.constprop.0+0x3e/0x60\n[   95.510814]  ? vprintk_func+0x98/0x140\n[   95.511399]  ? io_commit_cqring+0x3ec/0x8e0\n[   95.512036]  ? io_commit_cqring+0x3ec/0x8e0\n[   95.512733]  kasan_report_invalid_free+0x51/0x80\n[   95.513431]  ? io_commit_cqring+0x3ec/0x8e0\n[   95.514047]  __kasan_slab_free+0x141/0x160\n[   95.514699]  kfree+0xd1/0x390\n[   95.515182]  io_commit_cqring+0x3ec/0x8e0\n[   95.515799]  __io_req_complete.part.0+0x64/0x90\n[   95.516483]  io_wq_submit_work+0x1fa/0x260\n[   95.517117]  io_worker_handle_work+0xeac/0x1c00\n[   95.517828]  io_wqe_worker+0xc94/0x11a0\n[   95.518438]  ? io_worker_handle_work+0x1c00/0x1c00\n[   95.519151]  ? __kthread_parkme+0x11d/0x1d0\n[   95.519806]  ? io_worker_handle_work+0x1c00/0x1c00\n[   95.520512]  ? io_worker_handle_work+0x1c00/0x1c00\n[   95.521211]  kthread+0x396/0x470\n[   95.521727]  ? _raw_spin_unlock_irq+0x24/0x30\n[   95.522380]  ? kthread_mod_delayed_work+0x180/0x180\n[   95.523108]  ret_from_fork+0x22/0x30\n[   95.523684]\n[   95.523985] Allocated by task 4035:\n[   95.524543]  kasan_save_stack+0x1b/0x40\n[   95.525136]  __kasan_kmalloc.constprop.0+0xc2/0xd0\n[   95.525882]  kmem_cache_alloc_trace+0x17b/0x310\n[   95.533930]  io_queue_sqe+0x225/0xcb0\n[   95.534505]  io_submit_sqes+0x1768/0x25f0\n[   95.535164]  __x64_sys_io_uring_enter+0x89e/0xd10\n[   95.535900]  do_syscall_64+0x33/0x40\n[   95.536465]  entry_SYSCALL_64_after_hwframe+0x44/0xa9\n[   95.537199]\n[   95.537505] Freed by task 4035:\n[   95.538003]  kasan_save_stack+0x1b/0x40\n[   95.538599]  kasan_set_track+0x1c/0x30\n[   95.539177]  kasan_set_free_info+0x1b/0x30\n[   95.539798]  __kasan_slab_free+0x112/0x160\n[   95.540427]  kfree+0xd1/0x390\n[   95.540910]  io_commit_cqring+0x3ec/0x8e0\n[   95.541516]  io_iopoll_complete+0x914/0x1390\n[   95.542150]  io_do_iopoll+0x580/0x700\n[   95.542724]  io_iopoll_try_reap_events.part.0+0x108/0x200\n[   95.543512]  io_ring_ctx_wait_and_kill+0x118/0x340\n[   95.544206]  io_uring_release+0x43/0x50\n[   95.544791]  __fput+0x28d/0x940\n[   95.545291]  task_work_run+0xea/0x1b0\n[   95.545873]  do_exit+0xb6a/0x2c60\n[   95.546400]  do_group_exit+0x12a/0x320\n[   95.546967]  __x64_sys_exit_group+0x3f/0x50\n[   95.547605]  do_syscall_64+0x33/0x40\n[   95.548155]  entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nThe reason is that once we got a non EAGAIN error in io_wq_submit_work(),\nwe'll complete req by calling io_req_complete(), which will hold completion_lock\nto call io_commit_cqring(), but for polled io, io_iopoll_complete() won't\nhold completion_lock to call io_commit_cqring(), then there maybe concurrent\naccess to ctx->defer_list, double free may happen.\n\nTo fix this bug, we always let io_iopoll_complete() complete polled io.\n\nCc: <stable@vger.kernel.org> # 5.5+\nReported-by: Abaci Fuzz <abaci@linux.alibaba.com>\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Joseph Qi <joseph.qi@linux.alibaba.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:01 -0700 io_uring: always let io_iopoll_complete() complete polled io"
    },
    {
        "commit": "9c8e11b36c9b640a85a4a33a9e9dff418993cc34",
        "message": "Support timeout updates through IORING_OP_TIMEOUT_REMOVE with passed in\nIORING_TIMEOUT_UPDATE. Updates doesn't support offset timeout mode.\nOirignal timeout.off will be ignored as well.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\n[axboe: remove now unused 'ret' variable]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:01 -0700 io_uring: add timeout update"
    },
    {
        "commit": "fbd15848f3c13506253b6c5de0077a603947cb67",
        "message": "Add io_timeout_extract() helper, which searches and disarms timeouts,\nbut doesn't complete them. No functional changes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:01 -0700 io_uring: restructure io_timeout_cancel()"
    },
    {
        "commit": "bee749b187ac57d1faf00b2ab356ff322230fce8",
        "message": "io_uring_cancel_files()'s task check condition mistakenly got flipped.\n\n1. There can't be a request in the inflight list without\nIO_WQ_WORK_FILES, kill this check to keep the whole condition simpler.\n2. Also, don't call the function for files==NULL to not do such a check,\nall that staff is already handled well by its counter part,\n__io_uring_cancel_task_requests().\n\nWith that just flip the task check.\n\nAlso, it iowq-cancels all request of current task there, don't forget to\nset right ->files into struct io_task_cancel.\n\nFixes: c1973b38bf639 (\"io_uring: cancel only requests of current task\")\nReported-by: syzbot+c0d52d0b3c0c3ffb9525@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:01 -0700 io_uring: fix files cancellation"
    },
    {
        "commit": "ac0648a56c1ff66c1cbf735075ad33a26cbc50de",
        "message": "io_file_data_ref_zero() can be invoked from soft-irq from the RCU core,\nhence we need to ensure that the file_data lock is bottom half safe. Use\nthe _bh() variants when grabbing this lock.\n\nReported-by: syzbot+1f4ba1e5520762c523c6@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:01 -0700 io_uring: use bottom half safe lock for fixed file data"
    },
    {
        "commit": "bd5bbda72f7fa013ddea0ff7c4d91daedb821869",
        "message": "io_req_init() doesn't decrement state->ios_left if a request doesn't\nneed ->file, it just returns before that on if(!needs_file). That's\nnot really a problem but may cause overhead for an additional fput().\nAlso inline and kill io_req_set_file() as it's of no use anymore.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:01 -0700 io_uring: fix miscounting ios_left"
    },
    {
        "commit": "6e1271e60c1d5e822fd1a32a56d52d9ae1823e62",
        "message": "Keep submit state invariant of whether there are file refs left based on\nstate->nr_refs instead of (state->file==NULL), and always check against\nthe first one. It's easier to track and allows to remove 1 if. It also\nautomatically leaves struct submit_state in a consistent state after\nio_submit_state_end(), that's not used yet but nice.\n\nbtw rename has_refs to file_refs for more clarity.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:01 -0700 io_uring: change submit file state invariant"
    },
    {
        "commit": "65b2b213484acd89a3c20dbb524e52a2f3793b78",
        "message": "syzbot reports following issue:\nINFO: task syz-executor.2:12399 can't die for more than 143 seconds.\ntask:syz-executor.2  state:D stack:28744 pid:12399 ppid:  8504 flags:0x00004004\nCall Trace:\n context_switch kernel/sched/core.c:3773 [inline]\n __schedule+0x893/0x2170 kernel/sched/core.c:4522\n schedule+0xcf/0x270 kernel/sched/core.c:4600\n schedule_timeout+0x1d8/0x250 kernel/time/timer.c:1847\n do_wait_for_common kernel/sched/completion.c:85 [inline]\n __wait_for_common kernel/sched/completion.c:106 [inline]\n wait_for_common kernel/sched/completion.c:117 [inline]\n wait_for_completion+0x163/0x260 kernel/sched/completion.c:138\n kthread_stop+0x17a/0x720 kernel/kthread.c:596\n io_put_sq_data fs/io_uring.c:7193 [inline]\n io_sq_thread_stop+0x452/0x570 fs/io_uring.c:7290\n io_finish_async fs/io_uring.c:7297 [inline]\n io_sq_offload_create fs/io_uring.c:8015 [inline]\n io_uring_create fs/io_uring.c:9433 [inline]\n io_uring_setup+0x19b7/0x3730 fs/io_uring.c:9507\n do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\nRIP: 0033:0x45deb9\nCode: Unable to access opcode bytes at RIP 0x45de8f.\nRSP: 002b:00007f174e51ac78 EFLAGS: 00000246 ORIG_RAX: 00000000000001a9\nRAX: ffffffffffffffda RBX: 0000000000008640 RCX: 000000000045deb9\nRDX: 0000000000000000 RSI: 0000000020000140 RDI: 00000000000050e5\nRBP: 000000000118bf58 R08: 0000000000000000 R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000246 R12: 000000000118bf2c\nR13: 00007ffed9ca723f R14: 00007f174e51b9c0 R15: 000000000118bf2c\nINFO: task syz-executor.2:12399 blocked for more than 143 seconds.\n      Not tainted 5.10.0-rc3-next-20201110-syzkaller #0\n\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\n\nCurrently we don't have a reproducer yet, but seems that there is a\nrace in current codes:\n=> io_put_sq_data\n      ctx_list is empty now.       |\n==> kthread_park(sqd->thread);     |\n                                   | T1: sq thread is parked now.\n==> kthread_stop(sqd->thread);     |\n    KTHREAD_SHOULD_STOP is set now.|\n===> kthread_unpark(k);            |\n                                   | T2: sq thread is now unparkd, run again.\n                                   |\n                                   | T3: sq thread is now preempted out.\n                                   |\n===> wake_up_process(k);           |\n                                   |\n                                   | T4: Since sqd ctx_list is empty, needs_sched will be true,\n                                   | then sq thread sets task state to TASK_INTERRUPTIBLE,\n                                   | and schedule, now sq thread will never be waken up.\n===> wait_for_completion           |\n\nI have artificially used mdelay() to simulate above race, will get same\nstack like this syzbot report, but to be honest, I'm not sure this code\nrace triggers syzbot report.\n\nTo fix this possible code race, when sq thread is unparked, need to check\nwhether sq thread has been stopped.\n\nReported-by: syzbot+03beeb595f074db9cfd1@syzkaller.appspotmail.com\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:01 -0700 io_uring: check kthread stopped flag when sq thread is unparked"
    },
    {
        "commit": "36f72fe2792c4304f1203a44a6a7178e49b447f7",
        "message": "Double fixed files for splice/tee are done in a nasty way, it takes 2\nref_node refs, and during the second time it blindly overrides\nreq->fixed_file_refs hoping that it haven't changed. That works because\nall that is done under iouring_lock in a single go but is error-prone.\n\nBind everything explicitly to a single ref_node and take only one ref,\nwith current ref_node ordering it's guaranteed to keep all files valid\nawhile the request is inflight.\n\nThat's mainly a cleanup + preparation for generic resource handling,\nbut also saves pcpu_ref get/put for splice/tee with 2 fixed files.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:01 -0700 io_uring: share fixed_file_refs b/w multiple rsrcs"
    },
    {
        "commit": "c98de08c990e190fc7cc3aaf8079b4a0674c6425",
        "message": "As tasks now cancel only theirs requests, and inflight_wait is awaited\nonly in io_uring_cancel_files(), which should be called with ->in_idle\nset, instead of keeping a separate inflight_wait use tctx->wait.\n\nThat will add some spurious wakeups but actually is safer from point of\nnot hanging the task.\n\ne.g.\ntask1                   | IRQ\n                        | *start* io_complete_rw_common(link)\n                        |        link: req1 -> req2 -> req3(with files)\n*cancel_files()         |\nio_wq_cancel(), etc.    |\n                        | put_req(link), adds to io-wq req2\nschedule()              |\n\nSo, task1 will never try to cancel req2 or req3. If req2 is\nlong-standing (e.g. read(empty_pipe)), this may hang.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:01 -0700 io_uring: replace inflight_wait with tctx->wait"
    },
    {
        "commit": "10cad2c40dcb04bb46b2bf399e00ca5ea93d36b0",
        "message": "We don't even allow not plain data msg_control, which is disallowed in\n__sys_{send,revb}msg_sock(). So no need in fs for IORING_OP_SENDMSG and\nIORING_OP_RECVMSG. fs->lock is less contanged not as much as before, but\nthere are cases that can be, e.g. IOSQE_ASYNC.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:01 -0700 io_uring: don't take fs for recvmsg/sendmsg"
    },
    {
        "commit": "2e9dbe902d1020ef70f968e8675c8d2457c4ffaa",
        "message": "If IORING_SETUP_SQPOLL is enabled, sqes are either handled in sq thread\ntask context or in io worker task context. If current task context is sq\nthread, we don't need to check whether should wake up sq thread.\n\nio_iopoll_req_issued() calls wq_has_sleeper(), which has smp_mb() memory\nbarrier, before this patch, perf shows obvious overhead:\n  Samples: 481K of event 'cycles', Event count (approx.): 299807382878\n  Overhead  Comma  Shared Object     Symbol\n     3.69%  :9630  [kernel.vmlinux]  [k] io_issue_sqe\n\nWith this patch, perf shows:\n  Samples: 482K of event 'cycles', Event count (approx.): 299929547283\n  Overhead  Comma  Shared Object     Symbol\n     0.70%  :4015  [kernel.vmlinux]  [k] io_issue_sqe\n\nIt shows some obvious improvements.\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:00 -0700 io_uring: only wake up sq thread while current task is in io worker context"
    },
    {
        "commit": "906a3c6f9ca072e917c701f7421647e169740954",
        "message": "Both IOPOLL and sqes handling need to acquire uring_lock, combine\nthem together, then we just need to acquire uring_lock once.\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:00 -0700 io_uring: don't acquire uring_lock twice"
    },
    {
        "commit": "a0d9205f7d36bf72279f34a93850fd14789fdc7e",
        "message": "Some static checker reports below warning:\n    fs/io_uring.c:6939 io_sq_thread()\n    error: uninitialized symbol 'timeout'.\n\nThis is a false positive, but let's just initialize 'timeout' to make\nsure we don't trip over this.\n\nReported-by: Dan Carpenter <dan.carpenter@oracle.com>\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nReviewed-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:00 -0700 io_uring: initialize 'timeout' properly in io_sq_thread()"
    },
    {
        "commit": "08369246344077a9cf8109c1cf92a640733314f2",
        "message": "There are some issues about current io_sq_thread() implementation:\n  1. The prepare_to_wait() usage in __io_sq_thread() is weird. If\nmultiple ctxs share one same poll thread, one ctx will put poll thread\nin TASK_INTERRUPTIBLE, but if other ctxs have work to do, we don't\nneed to change task's stat at all. I think only if all ctxs don't have\nwork to do, we can do it.\n  2. We use round-robin strategy to make multiple ctxs share one same\npoll thread, but there are various condition in __io_sq_thread(), which\nseems complicated and may affect round-robin strategy.\n\nTo improve above issues, I take below actions:\n  1. If multiple ctxs share one same poll thread, only if all all ctxs\ndon't have work to do, we can call prepare_to_wait() and schedule() to\nmake poll thread enter sleep state.\n  2. To make round-robin strategy more straight, I simplify\n__io_sq_thread() a bit, it just does io poll and sqes submit work once,\ndoes not check various condition.\n  3. For multiple ctxs share one same poll thread, we choose the biggest\nsq_thread_idle among these ctxs as timeout condition, and will update\nit when ctx is in or out.\n  4. Not need to check EBUSY especially, if io_submit_sqes() returns\nEBUSY, IORING_SQ_CQ_OVERFLOW should be set, helper in liburing should\nbe aware of cq overflow and enters kernel to flush work.\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:00 -0700 io_uring: refactor io_sq_thread() handling"
    },
    {
        "commit": "f6edbabb8359798c541b0776616c5eab3a840d3d",
        "message": "Instead of iterating over each request and cancelling it individually in\nio_uring_cancel_files(), try to cancel all matching requests and use\n->inflight_list only to check if there anything left.\n\nIn many cases it should be faster, and we can reuse a lot of code from\ntask cancellation.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:00 -0700 io_uring: always batch cancel in *cancel_files()"
    },
    {
        "commit": "6b81928d4ca8668513251f9c04cdcb9d38ef51c7",
        "message": "Make io_poll_remove_all() and io_kill_timeouts() to match against files\nas well. A preparation patch, effectively not used by now.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:00 -0700 io_uring: pass files into kill timeouts/poll"
    },
    {
        "commit": "b52fda00dd9df8b4a6de5784df94f9617f6133a1",
        "message": "io_uring_cancel_files() guarantees to cancel all matching requests,\nthat's not necessary to do that in a loop. Move it up in the callchain\ninto io_uring_cancel_task_requests().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:00 -0700 io_uring: don't iterate io_uring_cancel_files()"
    },
    {
        "commit": "df9923f96717d0aebb0a73adbcf6285fa79e38cb",
        "message": "io_uring_cancel_files() cancels all request that match files regardless\nof task. There is no real need in that, cancel only requests of the\nspecified task. That also handles SQPOLL case as it already changes task\nto it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:00 -0700 io_uring: cancel only requests of current task"
    },
    {
        "commit": "08d23634643c239ddae706758f54d3a8e0c24962",
        "message": "Add io_match_task() that matches both task and files.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:00 -0700 io_uring: add a {task,files} pair matching helper"
    },
    {
        "commit": "06de5f5973c641c7ae033f133ecfaaf64fe633a6",
        "message": "If IORING_SETUP_SQPOLL is set all requests belong to the corresponding\nSQPOLL task, so skip task checking in that case and always match.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:00 -0700 io_uring: simplify io_task_match()"
    },
    {
        "commit": "2846c481c9dd1f1fb504b4885bcb815c311df532",
        "message": "Inline io_import_iovec() and leave only its former __io_import_iovec()\nrenamed to the original name. That makes it more obious what is reused in\nio_read/write().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:00 -0700 io_uring: inline io_import_iovec()"
    },
    {
        "commit": "632546c4b5a4dad8e3ac456406c65c0db9a0b570",
        "message": "io_size and iov_count in io_read() and io_write() hold the same value,\nkill the last one.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:00 -0700 io_uring: remove duplicated io_size from rw"
    },
    {
        "commit": "10fc72e43352753a08f9cf83aa5c40baec00d212",
        "message": "This is the only code that relies on import_iovec() returning\niter.count on success.\nThis allows a better interface to import_iovec().\n\nSigned-off-by: David Laight <david.laight@aculab.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:04:00 -0700 fs/io_uring Don't use the return value from import_iovec()."
    },
    {
        "commit": "1a38ffc9cbca361cc274d6e234f5ef8922f0b6d9",
        "message": "SQPOLL task may find sqo_task->files == NULL and\n__io_sq_thread_acquire_files() would leave it unset, so following\nfget_many() and others try to dereference NULL and fault. Propagate\nan error files are missing.\n\n[  118.962785] BUG: kernel NULL pointer dereference, address:\n\t0000000000000020\n[  118.963812] #PF: supervisor read access in kernel mode\n[  118.964534] #PF: error_code(0x0000) - not-present page\n[  118.969029] RIP: 0010:__fget_files+0xb/0x80\n[  119.005409] Call Trace:\n[  119.005651]  fget_many+0x2b/0x30\n[  119.005964]  io_file_get+0xcf/0x180\n[  119.006315]  io_submit_sqes+0x3a4/0x950\n[  119.007481]  io_sq_thread+0x1de/0x6a0\n[  119.007828]  kthread+0x114/0x150\n[  119.008963]  ret_from_fork+0x22/0x30\n\nReported-by: Josef Grieb <josef.grieb@gmail.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:03:59 -0700 io_uring: NULL files dereference by SQPOLL"
    },
    {
        "commit": "c73ebb685fb6dfb513d394cbea64fb81ba3d994f",
        "message": "Now users who want to get woken when waiting for events should submit a\ntimeout command first. It is not safe for applications that split SQ and\nCQ handling between two threads, such as mysql. Users should synchronize\nthe two threads explicitly to protect SQ and that will impact the\nperformance.\n\nThis patch adds support for timeout to existing io_uring_enter(). To\navoid overloading arguments, it introduces a new parameter structure\nwhich contains sigmask and timeout.\n\nI have tested the workloads with one thread submiting nop requests\nwhile the other reaping the cqe with timeout. It shows 1.8~2x faster\nwhen the iodepth is 16.\n\nSigned-off-by: Jiufei Xue <jiufei.xue@linux.alibaba.com>\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\n[axboe: various cleanups/fixes, and name change to SIG_IS_DATA]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:03:59 -0700 io_uring: add timeout support for io_uring_enter()"
    },
    {
        "commit": "27926b683db03be307c6905b44ecfc1f081d9d6f",
        "message": "We unconditionally call blk_start_plug() when starting the IO\nsubmission, but we only really should do that if we have more than 1\nrequest to submit AND we're potentially dealing with block based storage\nunderneath. For any other type of request, it's just a waste of time to\ndo so.\n\nAdd a ->plug bit to io_op_def and set it for read/write requests. We\ncould make this more precise and check the file itself as well, but it\ndoesn't matter that much and would quickly become more expensive.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:03:59 -0700 io_uring: only plug when appropriate"
    },
    {
        "commit": "0415767e7f0542b3cd1ab270c2e61e90e87aafa2",
        "message": "We've got extra 8 bytes in the 2nd cacheline, put ->fixed_file_refs\nthere, so inline execution path mostly doesn't touch the 3rd cacheline\nfor fixed_file requests as well.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:03:59 -0700 io_uring: rearrange io_kiocb fields for better caching"
    },
    {
        "commit": "f2f87370bb6664e5babb6705e886cfb340f163e1",
        "message": "Singly linked list for keeping linked requests is enough, because we\nalmost always operate on the head and traverse forward with the\nexception of linked timeouts going 1 hop backwards.\n\nReplace ->link_list with a handmade singly linked list. Also kill\nREQ_F_LINK_HEAD in favour of checking a newly added ->list for NULL\ndirectly.\n\nThat saves 8B in io_kiocb, is not as heavy as list fixup, makes better\nuse of cache by not touching a previous request (i.e. last request of\nthe link) each time on list modification and optimises cache use further\nin the following patch, and actually makes travesal easier removing in\nthe end some lines. Also, keeping invariant in ->list instead of having\nREQ_F_LINK_HEAD is less error-prone.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:03:59 -0700 io_uring: link requests with singly linked list"
    },
    {
        "commit": "90cd7e424969d29aff653333b4dcb4e2e199d791",
        "message": "In preparation for converting singly linked lists for chaining requests,\nmake linked timeouts save requests that they're responsible for and not\ncount on doubly linked list for back referencing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:03:59 -0700 io_uring: track link timeout's master explicitly"
    },
    {
        "commit": "863e05604a6fb45f0f56b3e9eca5cd533001253b",
        "message": "Explicitly save not only a link's head in io_submit_sqe[s]() but the\ntail as well. That's in preparation for keeping linked requests in a\nsingly linked list.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:03:59 -0700 io_uring: track link's head and tail during submit"
    },
    {
        "commit": "018043be1f1bc43ad6956bfd39b7beea12fb4ca6",
        "message": "Don't use a single struct for polls and poll remove requests, they have\ntotally different layouts.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:03:59 -0700 io_uring: split poll and poll_remove structs"
    },
    {
        "commit": "14a1143b68ee2e4ec4e8d54f71cddb9724f9ec70",
        "message": "IORING_OP_UNLINKAT behaves like unlinkat(2) and takes the same flags\nand arguments.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:03:59 -0700 io_uring: add support for IORING_OP_UNLINKAT"
    },
    {
        "commit": "80a261fd00327898e272ddc84ccc9510c036453c",
        "message": "IORING_OP_RENAMEAT behaves like renameat2(), and takes the same flags\netc.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:03:59 -0700 io_uring: add support for IORING_OP_RENAMEAT"
    },
    {
        "commit": "14587a46646d30d2b4a6b69865682cfe6bbdcd1f",
        "message": "Now that SQPOLL supports non-registered files and grabs the file table,\nwe can relax the restriction on open/close/accept/connect and allow\nthem on a ring that is setup with IORING_SETUP_SQPOLL.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:03:59 -0700 io_uring: enable file table usage for SQPOLL rings"
    },
    {
        "commit": "28cea78af44918b920306df150afbd116bd94301",
        "message": "The restriction of needing fixed files for SQPOLL is problematic, and\nprevents/inhibits several valid uses cases. With the referenced\nfiles_struct that we have now, it's trivially supportable.\n\nTreat ->files like we do the mm for the SQPOLL thread - grab a reference\nto it (and assign it), and drop it when we're done.\n\nThis feature is exposed as IORING_FEAT_SQPOLL_NONFIXED.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-12-09 12:03:54 -0700 io_uring: allow non-fixed files with SQPOLL"
    },
    {
        "commit": "f26c08b444df833b19c00838a530d93963ce9cd0",
        "message": "Put file as part of error handling when setting up io ctx to fix\nmemory leaks like the following one.\n\n   BUG: memory leak\n   unreferenced object 0xffff888101ea2200 (size 256):\n     comm \"syz-executor355\", pid 8470, jiffies 4294953658 (age 32.400s)\n     hex dump (first 32 bytes):\n       00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................\n       20 59 03 01 81 88 ff ff 80 87 a8 10 81 88 ff ff   Y..............\n     backtrace:\n       [<000000002e0a7c5f>] kmem_cache_zalloc include/linux/slab.h:654 [inline]\n       [<000000002e0a7c5f>] __alloc_file+0x1f/0x130 fs/file_table.c:101\n       [<000000001a55b73a>] alloc_empty_file+0x69/0x120 fs/file_table.c:151\n       [<00000000fb22349e>] alloc_file+0x33/0x1b0 fs/file_table.c:193\n       [<000000006e1465bb>] alloc_file_pseudo+0xb2/0x140 fs/file_table.c:233\n       [<000000007118092a>] anon_inode_getfile fs/anon_inodes.c:91 [inline]\n       [<000000007118092a>] anon_inode_getfile+0xaa/0x120 fs/anon_inodes.c:74\n       [<000000002ae99012>] io_uring_get_fd fs/io_uring.c:9198 [inline]\n       [<000000002ae99012>] io_uring_create fs/io_uring.c:9377 [inline]\n       [<000000002ae99012>] io_uring_setup+0x1125/0x1630 fs/io_uring.c:9411\n       [<000000008280baad>] do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n       [<00000000685d8cf0>] entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nReported-by: syzbot+71c4697e27c99fddcf17@syzkaller.appspotmail.com\nFixes: 0f2122045b94 (\"io_uring: don't rely on weak ->files references\")\nCc: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Hillf Danton <hdanton@sina.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10",
        "release_date": "2020-12-08 08:54:26 -0700 io_uring: fix file leak on error path of io ctx creation"
    },
    {
        "commit": "e8c954df234145c5765870382c2bc630a48beec9",
        "message": "After io_identity_cow() copies an work.identity it wants to copy creds\nto the new just allocated id, not the old one. Otherwise it's\nakin to req->work.identity->creds = req->work.identity->creds.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10",
        "release_date": "2020-12-07 08:43:44 -0700 io_uring: fix mis-seting personality's creds"
    },
    {
        "commit": "619ca2664cc6ebf6ecaff347d15ee8093b634e0c",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a small fix this time, for an issue with 32-bit compat apps and\n  buffer selection with recvmsg\"\n\n* tag 'io_uring-5.10-2020-12-05' of git://git.kernel.dk/linux-block:\n  io_uring: fix recvmsg setup with compat buf-select",
        "kernel_version": "v5.10-rc7",
        "release_date": "2020-12-05 14:39:59 -0800 Merge tag 'io_uring-5.10-2020-12-05' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "2d280bc8930ba9ed1705cfd548c6c8924949eaf1",
        "message": "__io_compat_recvmsg_copy_hdr() with REQ_F_BUFFER_SELECT reads out iov\nlen but never assigns it to iov/fast_iov, leaving sr->len with garbage.\nHopefully, following io_buffer_select() truncates it to the selected\nbuffer size, but the value is still may be under what was specified.\n\nCc: <stable@vger.kernel.org> # 5.7\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc7",
        "release_date": "2020-11-30 11:12:03 -0700 io_uring: fix recvmsg setup with compat buf-select"
    },
    {
        "commit": "9223e74f9960778bd3edd39e15edd5532708b7fb",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Out of bounds fix for the cq size cap from earlier this release (Joseph)\n\n - iov_iter type check fix (Pavel)\n\n - Files grab + cancelation fix (Pavel)\n\n* tag 'io_uring-5.10-2020-11-27' of git://git.kernel.dk/linux-block:\n  io_uring: fix files grab/cancel race\n  io_uring: fix ITER_BVEC check\n  io_uring: fix shift-out-of-bounds when round up cq size",
        "kernel_version": "v5.10-rc6",
        "release_date": "2020-11-27 12:56:04 -0800 Merge tag 'io_uring-5.10-2020-11-27' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "af60470347de6ac2b9f0cc3703975a543a3de075",
        "message": "When one task is in io_uring_cancel_files() and another is doing\nio_prep_async_work() a race may happen. That's because after accounting\na request inflight in first call to io_grab_identity() it still may fail\nand go to io_identity_cow(), which migh briefly keep dangling\nwork.identity and not only.\n\nGrab files last, so io_prep_async_work() won't fail if it did get into\n->inflight_list.\n\nnote: the bug shouldn't exist after making io_uring_cancel_files() not\npoking into other tasks' requests.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc6",
        "release_date": "2020-11-26 08:50:21 -0700 io_uring: fix files grab/cancel race"
    },
    {
        "commit": "9c3a205c5ffa36e96903c2e37eb5f41c0f03c43e",
        "message": "iov_iter::type is a bitmask that also keeps direction etc., so it\nshouldn't be directly compared against ITER_*. Use proper helper.\n\nFixes: ff6165b2d7f6 (\"io_uring: retain iov_iter state over io_read/io_write calls\")\nReported-by: David Howells <dhowells@redhat.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nCc: <stable@vger.kernel.org> # 5.9\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc6",
        "release_date": "2020-11-24 07:54:30 -0700 io_uring: fix ITER_BVEC check"
    },
    {
        "commit": "eb2667b343361863da7b79be26de641e22844ba0",
        "message": "Abaci Fuzz reported a shift-out-of-bounds BUG in io_uring_create():\n\n[ 59.598207] UBSAN: shift-out-of-bounds in ./include/linux/log2.h:57:13\n[ 59.599665] shift exponent 64 is too large for 64-bit type 'long unsigned int'\n[ 59.601230] CPU: 0 PID: 963 Comm: a.out Not tainted 5.10.0-rc4+ #3\n[ 59.602502] Hardware name: Red Hat KVM, BIOS 0.5.1 01/01/2011\n[ 59.603673] Call Trace:\n[ 59.604286] dump_stack+0x107/0x163\n[ 59.605237] ubsan_epilogue+0xb/0x5a\n[ 59.606094] __ubsan_handle_shift_out_of_bounds.cold+0xb2/0x20e\n[ 59.607335] ? lock_downgrade+0x6c0/0x6c0\n[ 59.608182] ? rcu_read_lock_sched_held+0xaf/0xe0\n[ 59.609166] io_uring_create.cold+0x99/0x149\n[ 59.610114] io_uring_setup+0xd6/0x140\n[ 59.610975] ? io_uring_create+0x2510/0x2510\n[ 59.611945] ? lockdep_hardirqs_on_prepare+0x286/0x400\n[ 59.613007] ? syscall_enter_from_user_mode+0x27/0x80\n[ 59.614038] ? trace_hardirqs_on+0x5b/0x180\n[ 59.615056] do_syscall_64+0x2d/0x40\n[ 59.615940] entry_SYSCALL_64_after_hwframe+0x44/0xa9\n[ 59.617007] RIP: 0033:0x7f2bb8a0b239\n\nThis is caused by roundup_pow_of_two() if the input entries larger\nenough, e.g. 2^32-1. For sq_entries, it will check first and we allow\nat most IORING_MAX_ENTRIES, so it is okay. But for cq_entries, we do\nround up first, that may overflow and truncate it to 0, which is not\nthe expected behavior. So check the cq size first and then do round up.\n\nFixes: 88ec3211e463 (\"io_uring: round-up cq size before comparing with rounded sq size\")\nReported-by: Abaci Fuzz <abaci@linux.alibaba.com>\nSigned-off-by: Joseph Qi <joseph.qi@linux.alibaba.com>\nReviewed-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc6",
        "release_date": "2020-11-24 07:54:30 -0700 io_uring: fix shift-out-of-bounds when round up cq size"
    },
    {
        "commit": "36f4fa6886a81266d7c82b1c90a65205e73a7c85",
        "message": "This adds support for the shutdown(2) system call, which is useful for\ndealing with sockets.\n\nshutdown(2) may block, so we have to punt it to async context.\n\nSuggested-by: Norman Maurer <norman.maurer@googlemail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-11-23 09:15:15 -0700 io_uring: add support for shutdown(2)"
    },
    {
        "commit": "b713c195d59332277a31a59c91f755e53b5b302b",
        "message": "No functional changes in this patch, needed to provide io_uring support\nfor shutdown(2).\n\nCc: netdev@vger.kernel.org\nCc: David S. Miller <davem@davemloft.net>\nAcked-by: Jakub Kicinski <kuba@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-11-23 09:15:15 -0700 net: provide __sys_shutdown_sock() that takes a socket"
    },
    {
        "commit": "ce59fc69b1c2da555706f6b0e77fc099f80e9d0e",
        "message": "CAP_SYS_ADMIN is too restrictive for a lot of uses cases, allow\nCAP_SYS_NICE based on the premise that such users are already allowed\nto raise the priority of tasks.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-11-23 09:15:15 -0700 io_uring: allow SQPOLL with CAP_SYS_NICE privileges"
    },
    {
        "commit": "fa5fca78bb2fe7a58ae7297407dcda1914ea8353",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Mostly regression or stable fodder:\n\n   - Disallow async path resolution of /proc/self\n\n   - Tighten constraints for segmented async buffered reads\n\n   - Fix double completion for a retry error case\n\n   - Fix for fixed file life times (Pavel)\"\n\n* tag 'io_uring-5.10-2020-11-20' of git://git.kernel.dk/linux-block:\n  io_uring: order refnode recycling\n  io_uring: get an active ref_node from files_data\n  io_uring: don't double complete failed reissue request\n  mm: never attempt async page lock if we've transferred data already\n  io_uring: handle -EOPNOTSUPP on path resolution\n  proc: don't allow async path resolution of /proc/self components",
        "kernel_version": "v5.10-rc5",
        "release_date": "2020-11-20 11:47:22 -0800 Merge tag 'io_uring-5.10-2020-11-20' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "e297822b20e7fe683e107aea46e6402adcf99c70",
        "message": "Don't recycle a refnode until we're done with all requests of nodes\nejected before.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nCc: stable@vger.kernel.org # v5.7+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc5",
        "release_date": "2020-11-18 08:02:10 -0700 io_uring: order refnode recycling"
    },
    {
        "commit": "1e5d770bb8a23dd01e28e92f4fb0b1093c8bdbe6",
        "message": "An active ref_node always can be found in ctx->files_data, it's much\nsafer to get it this way instead of poking into files_data->ref_list.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nCc: stable@vger.kernel.org # v5.7+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc5",
        "release_date": "2020-11-18 08:02:10 -0700 io_uring: get an active ref_node from files_data"
    },
    {
        "commit": "c993df5a688975bf9ce899706ca13d2bc8d6be25",
        "message": "Zorro reports that an xfstest test case is failing, and it turns out that\nfor the reissue path we can potentially issue a double completion on the\nrequest for the failure path. There's an issue around the retry as well,\nbut for now, at least just make sure that we handle the error path\ncorrectly.\n\nCc: stable@vger.kernel.org\nFixes: b63534c41e20 (\"io_uring: re-issue block requests that failed because of resources\")\nReported-by: Zorro Lang <zlang@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc5",
        "release_date": "2020-11-17 15:17:29 -0700 io_uring: don't double complete failed reissue request"
    },
    {
        "commit": "944d1444d53f5a213457e5096db370cfd06923d4",
        "message": "Any attempt to do path resolution on /proc/self from an async worker will\nyield -EOPNOTSUPP. We can safely do that resolution from the task itself,\nand without blocking, so retry it from there.\n\nIdeally io_uring would know this upfront and not have to go through the\nworker thread to find out, but that doesn't currently seem feasible.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc5",
        "release_date": "2020-11-14 10:22:30 -0700 io_uring: handle -EOPNOTSUPP on path resolution"
    },
    {
        "commit": "1b1e9262ca644b5b7f1d12b2f8c2edfff420c5f3",
        "message": "Pull io_uring fix from Jens Axboe:\n \"A single fix in here, for a missed rounding case at setup time, which\n  caused an otherwise legitimate setup case to return -EINVAL if used\n  with unaligned ring size values\"\n\n* tag 'io_uring-5.10-2020-11-13' of git://git.kernel.dk/linux-block:\n  io_uring: round-up cq size before comparing with rounded sq size",
        "kernel_version": "v5.10-rc4",
        "release_date": "2020-11-13 15:05:19 -0800 Merge tag 'io_uring-5.10-2020-11-13' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "88ec3211e46344a7d10cf6cb5045f839f7785f8e",
        "message": "If an application specifies IORING_SETUP_CQSIZE to set the CQ ring size\nto a specific size, we ensure that the CQ size is at least that of the\nSQ ring size. But in doing so, we compare the already rounded up to power\nof two SQ size to the as-of yet unrounded CQ size. This means that if an\napplication passes in non power of two sizes, we can return -EINVAL when\nthe final value would've been fine. As an example, an application passing\nin 100/100 for sq/cq size should end up with 128 for both. But since we\nround the SQ size first, we compare the CQ size of 100 to 128, and return\n-EINVAL as that is too small.\n\nCc: stable@vger.kernel.org\nFixes: 33a107f0a1b8 (\"io_uring: allow application controlled CQ ring size\")\nReported-by: Dan Melnic <dmm@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc4",
        "release_date": "2020-11-11 10:42:41 -0700 io_uring: round-up cq size before comparing with rounded sq size"
    },
    {
        "commit": "e9c02d68cc26b28a9a12ebd1aeaed673ad0e73e2",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A set of fixes for io_uring:\n\n   - SQPOLL cancelation fixes\n\n   - Two fixes for the io_identity COW\n\n   - Cancelation overflow fix (Pavel)\n\n   - Drain request cancelation fix (Pavel)\n\n   - Link timeout race fix (Pavel)\"\n\n* tag 'io_uring-5.10-2020-11-07' of git://git.kernel.dk/linux-block:\n  io_uring: fix link lookup racing with link timeout\n  io_uring: use correct pointer for io_uring_show_cred()\n  io_uring: don't forget to task-cancel drained reqs\n  io_uring: fix overflowed cancel w/ linked ->files\n  io_uring: drop req/tctx io_identity separately\n  io_uring: ensure consistent view of original task ->mm from SQPOLL\n  io_uring: properly handle SQPOLL request cancelations\n  io-wq: cancel request if it's asking for files and we don't have them",
        "kernel_version": "v5.10-rc3",
        "release_date": "2020-11-07 13:49:24 -0800 Merge tag 'io_uring-5.10-2020-11-07' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "9a472ef7a3690ac0b77ebfb04c88fa795de2adea",
        "message": "We can't just go over linked requests because it may race with linked\ntimeouts. Take ctx->completion_lock in that case.\n\nCc: stable@vger.kernel.org # v5.7+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc3",
        "release_date": "2020-11-05 15:36:40 -0700 io_uring: fix link lookup racing with link timeout"
    },
    {
        "commit": "6b47ab81c9a9b56a94882815e9949d40e4207c92",
        "message": "Previous commit changed how we index the registered credentials, but\nneglected to update one spot that is used when the personalities are\niterated through ->show_fdinfo(). Ensure we use the right struct type\nfor the iteration.\n\nReported-by: syzbot+a6d494688cdb797bdfce@syzkaller.appspotmail.com\nFixes: 1e6fa5216a0e (\"io_uring: COW io_identity on mismatch\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc3",
        "release_date": "2020-11-05 09:50:16 -0700 io_uring: use correct pointer for io_uring_show_cred()"
    },
    {
        "commit": "ef9865a442286e2737f37f56eb54c12ef8465905",
        "message": "If there is a long-standing request of one task locking up execution of\ndeferred requests, and the defer list contains requests of another task\n(all files-less), then a potential execution of __io_uring_task_cancel()\nby that another task will sleep until that first long-standing request\ncompletion, and that may take long.\n\nE.g.\ntsk1: req1/read(empty_pipe) -> tsk2: req(DRAIN)\nThen __io_uring_task_cancel(tsk2) waits for req1 completion.\n\nIt seems we even can manufacture a complicated case with many tasks\nsharing many rings that can lock them forever.\n\nCancel deferred requests for __io_uring_task_cancel() as well.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc3",
        "release_date": "2020-11-05 09:15:24 -0700 io_uring: don't forget to task-cancel drained reqs"
    },
    {
        "commit": "99b328084f6a98bcee9fcd423c82ccfd52115da5",
        "message": "Current io_match_files() check in io_cqring_overflow_flush() is useless\nbecause requests drop ->files before going to the overflow list, however\nlinked to it request do not, and we don't check them.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc3",
        "release_date": "2020-11-04 10:22:57 -0700 io_uring: fix overflowed cancel w/ linked ->files"
    },
    {
        "commit": "cb8a8ae310741d743fd02982307797f6a126f614",
        "message": "We can't bundle this into one operation, as the identity may not have\noriginated from the tctx to begin with. Drop one ref for each of them\nseparately, if they don't match the static assignment. If we don't, then\nif the identity is a lookup from registered credentials, we could be\nfreeing that identity as we're dropping a reference assuming it came from\nthe tctx. syzbot reports this as a use-after-free, as the identity is\nstill referencable from idr lookup:\n\n==================================================================\nBUG: KASAN: use-after-free in instrument_atomic_read_write include/linux/instrumented.h:101 [inline]\nBUG: KASAN: use-after-free in atomic_fetch_add_relaxed include/asm-generic/atomic-instrumented.h:142 [inline]\nBUG: KASAN: use-after-free in __refcount_add include/linux/refcount.h:193 [inline]\nBUG: KASAN: use-after-free in __refcount_inc include/linux/refcount.h:250 [inline]\nBUG: KASAN: use-after-free in refcount_inc include/linux/refcount.h:267 [inline]\nBUG: KASAN: use-after-free in io_init_req fs/io_uring.c:6700 [inline]\nBUG: KASAN: use-after-free in io_submit_sqes+0x15a9/0x25f0 fs/io_uring.c:6774\nWrite of size 4 at addr ffff888011e08e48 by task syz-executor165/8487\n\nCPU: 1 PID: 8487 Comm: syz-executor165 Not tainted 5.10.0-rc1-next-20201102-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nCall Trace:\n __dump_stack lib/dump_stack.c:77 [inline]\n dump_stack+0x107/0x163 lib/dump_stack.c:118\n print_address_description.constprop.0.cold+0xae/0x4c8 mm/kasan/report.c:385\n __kasan_report mm/kasan/report.c:545 [inline]\n kasan_report.cold+0x1f/0x37 mm/kasan/report.c:562\n check_memory_region_inline mm/kasan/generic.c:186 [inline]\n check_memory_region+0x13d/0x180 mm/kasan/generic.c:192\n instrument_atomic_read_write include/linux/instrumented.h:101 [inline]\n atomic_fetch_add_relaxed include/asm-generic/atomic-instrumented.h:142 [inline]\n __refcount_add include/linux/refcount.h:193 [inline]\n __refcount_inc include/linux/refcount.h:250 [inline]\n refcount_inc include/linux/refcount.h:267 [inline]\n io_init_req fs/io_uring.c:6700 [inline]\n io_submit_sqes+0x15a9/0x25f0 fs/io_uring.c:6774\n __do_sys_io_uring_enter+0xc8e/0x1b50 fs/io_uring.c:9159\n do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\nRIP: 0033:0x440e19\nCode: 18 89 d0 c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 eb 0f fc ff c3 66 2e 0f 1f 84 00 00 00 00\nRSP: 002b:00007fff644ff178 EFLAGS: 00000246 ORIG_RAX: 00000000000001aa\nRAX: ffffffffffffffda RBX: 0000000000000005 RCX: 0000000000440e19\nRDX: 0000000000000000 RSI: 000000000000450c RDI: 0000000000000003\nRBP: 0000000000000004 R08: 0000000000000000 R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000246 R12: 00000000022b4850\nR13: 0000000000000010 R14: 0000000000000000 R15: 0000000000000000\n\nAllocated by task 8487:\n kasan_save_stack+0x1b/0x40 mm/kasan/common.c:48\n kasan_set_track mm/kasan/common.c:56 [inline]\n __kasan_kmalloc.constprop.0+0xc2/0xd0 mm/kasan/common.c:461\n kmalloc include/linux/slab.h:552 [inline]\n io_register_personality fs/io_uring.c:9638 [inline]\n __io_uring_register fs/io_uring.c:9874 [inline]\n __do_sys_io_uring_register+0x10f0/0x40a0 fs/io_uring.c:9924\n do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nFreed by task 8487:\n kasan_save_stack+0x1b/0x40 mm/kasan/common.c:48\n kasan_set_track+0x1c/0x30 mm/kasan/common.c:56\n kasan_set_free_info+0x1b/0x30 mm/kasan/generic.c:355\n __kasan_slab_free+0x102/0x140 mm/kasan/common.c:422\n slab_free_hook mm/slub.c:1544 [inline]\n slab_free_freelist_hook+0x5d/0x150 mm/slub.c:1577\n slab_free mm/slub.c:3140 [inline]\n kfree+0xdb/0x360 mm/slub.c:4122\n io_identity_cow fs/io_uring.c:1380 [inline]\n io_prep_async_work+0x903/0xbc0 fs/io_uring.c:1492\n io_prep_async_link fs/io_uring.c:1505 [inline]\n io_req_defer fs/io_uring.c:5999 [inline]\n io_queue_sqe+0x212/0xed0 fs/io_uring.c:6448\n io_submit_sqe fs/io_uring.c:6542 [inline]\n io_submit_sqes+0x14f6/0x25f0 fs/io_uring.c:6784\n __do_sys_io_uring_enter+0xc8e/0x1b50 fs/io_uring.c:9159\n do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nThe buggy address belongs to the object at ffff888011e08e00\n which belongs to the cache kmalloc-96 of size 96\nThe buggy address is located 72 bytes inside of\n 96-byte region [ffff888011e08e00, ffff888011e08e60)\nThe buggy address belongs to the page:\npage:00000000a7104751 refcount:1 mapcount:0 mapping:0000000000000000 index:0x0 pfn:0x11e08\nflags: 0xfff00000000200(slab)\nraw: 00fff00000000200 ffffea00004f8540 0000001f00000002 ffff888010041780\nraw: 0000000000000000 0000000080200020 00000001ffffffff 0000000000000000\npage dumped because: kasan: bad access detected\n\nMemory state around the buggy address:\n ffff888011e08d00: 00 00 00 00 00 00 00 00 00 00 00 00 fc fc fc fc\n ffff888011e08d80: 00 00 00 00 00 00 00 00 00 00 00 00 fc fc fc fc\n> ffff888011e08e00: fa fb fb fb fb fb fb fb fb fb fb fb fc fc fc fc\n                                              ^\n ffff888011e08e80: 00 00 00 00 00 00 00 00 00 00 00 00 fc fc fc fc\n ffff888011e08f00: 00 00 00 00 00 00 00 00 00 00 00 00 fc fc fc fc\n==================================================================\n\nReported-by: syzbot+625ce3bb7835b63f7f3d@syzkaller.appspotmail.com\nFixes: 1e6fa5216a0e (\"io_uring: COW io_identity on mismatch\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc3",
        "release_date": "2020-11-04 10:22:57 -0700 io_uring: drop req/tctx io_identity separately"
    },
    {
        "commit": "4b70cf9dea4cd239b425f3282fa56ce19e234c8a",
        "message": "Ensure we get a valid view of the task mm, by using task_lock() when\nattempting to grab the original task mm.\n\nReported-by: syzbot+b57abf7ee60829090495@syzkaller.appspotmail.com\nFixes: 2aede0e417db (\"io_uring: stash ctx task reference for SQPOLL\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc3",
        "release_date": "2020-11-04 10:22:57 -0700 io_uring: ensure consistent view of original task ->mm from SQPOLL"
    },
    {
        "commit": "fdaf083cdfb556a45c422c8998268baf1ab26829",
        "message": "Track if a given task io_uring context contains SQPOLL instances, so we\ncan iterate those for cancelation (and request counts). This ensures that\nwe properly wait on SQPOLL contexts, and find everything that needs\ncanceling.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc3",
        "release_date": "2020-11-04 10:22:56 -0700 io_uring: properly handle SQPOLL request cancelations"
    },
    {
        "commit": "cf9446cc8e6d85355642209538dde619f53770dc",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fixes for linked timeouts (Pavel)\n\n - Set IO_WQ_WORK_CONCURRENT early for async offload (Pavel)\n\n - Two minor simplifications that make the code easier to read and\n   follow (Pavel)\n\n* tag 'io_uring-5.10-2020-10-30' of git://git.kernel.dk/linux-block:\n  io_uring: use type appropriate io_kiocb handler for double poll\n  io_uring: simplify __io_queue_sqe()\n  io_uring: simplify nxt propagation in io_queue_sqe\n  io_uring: don't miss setting IO_WQ_WORK_CONCURRENT\n  io_uring: don't defer put of cancelled ltimeout\n  io_uring: always clear LINK_TIMEOUT after cancel\n  io_uring: don't adjust LINK_HEAD in cancel ltimeout\n  io_uring: remove opcode check on ltimeout kill",
        "kernel_version": "v5.10-rc2",
        "release_date": "2020-10-30 14:55:36 -0700 Merge tag 'io_uring-5.10-2020-10-30' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "114518eb6430b832d2f9f5a008043b913ccf0e24",
        "message": "If the arch supports TIF_NOTIFY_SIGNAL, then use that for TWA_SIGNAL as\nit's more efficient than using the signal delivery method. This is\nespecially true on threaded applications, where ->sighand is shared across\nthreads, but it's also lighter weight on non-shared cases.\n\nio_uring is a heavy consumer of TWA_SIGNAL based task_work. A test with\nthreads shows a nice improvement running an io_uring based echo server.\n\nstock kernel:\n0.01% <= 0.1 milliseconds\n95.86% <= 0.2 milliseconds\n98.27% <= 0.3 milliseconds\n99.71% <= 0.4 milliseconds\n100.00% <= 0.5 milliseconds\n100.00% <= 0.6 milliseconds\n100.00% <= 0.7 milliseconds\n100.00% <= 0.8 milliseconds\n100.00% <= 0.9 milliseconds\n100.00% <= 1.0 milliseconds\n100.00% <= 1.1 milliseconds\n100.00% <= 2 milliseconds\n100.00% <= 3 milliseconds\n100.00% <= 3 milliseconds\n1378930.00 requests per second\n~1600% CPU\n\n1.38M requests/second, and all 16 CPUs are maxed out.\n\npatched kernel:\n0.01% <= 0.1 milliseconds\n98.24% <= 0.2 milliseconds\n99.47% <= 0.3 milliseconds\n99.99% <= 0.4 milliseconds\n100.00% <= 0.5 milliseconds\n100.00% <= 0.6 milliseconds\n100.00% <= 0.7 milliseconds\n100.00% <= 0.8 milliseconds\n100.00% <= 0.9 milliseconds\n100.00% <= 1.2 milliseconds\n1666111.38 requests per second\n~1450% CPU\n\n1.67M requests/second, and we're no longer just hammering on the sighand\nlock. The original reporter states:\n\n\"For 5.7.15 my benchmark achieves 1.6M qps and system cpu is at ~80%.\n for 5.7.16 or later it achieves only 1M qps and the system cpu is is\n at ~100%\"\n\nwith the only difference there being that TWA_SIGNAL is used\nunconditionally in 5.7.16, since it's required to be able to handle the\ninability to run task_work if the application is waiting in the kernel\nalready on an event that needs task_work run to be satisfied. Also see\ncommit 0ba9c9edcd15.\n\nReported-by: Roman Gershman <romger@amazon.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Thomas Gleixner <tglx@linutronix.de>\nReviewed-by: Oleg Nesterov <oleg@redhat.com>\nLink: https://lore.kernel.org/r/20201026203230.386348-5-axboe@kernel.dk",
        "kernel_version": "v5.11-rc1",
        "release_date": "2020-10-29 09:37:37 +0100 task_work: Use TIF_NOTIFY_SIGNAL if available"
    },
    {
        "commit": "c8b5e2600a2cfa1cdfbecf151afd67aee227381d",
        "message": "io_poll_double_wake() is called for both request types - both pure poll\nrequests, and internal polls. This means that we should be using the\nright handler based on the request type. Use the one that the original\ncaller already assigned for the waitqueue handling, that will always\nmatch the correct type.\n\nCc: stable@vger.kernel.org # v5.8+\nReported-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc2",
        "release_date": "2020-10-25 13:53:26 -0600 io_uring: use type appropriate io_kiocb handler for double poll"
    },
    {
        "commit": "af0041875ce7f5a05362b884e90cf82c27876096",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - fsize was missed in previous unification of work flags\n\n - Few fixes cleaning up the flags unification creds cases (Pavel)\n\n - Fix NUMA affinities for completely unplugged/replugged node for io-wq\n\n - Two fallout fixes from the set_fs changes. One local to io_uring, one\n   for the splice entry point that io_uring uses.\n\n - Linked timeout fixes (Pavel)\n\n - Removal of ->flush() ->files work-around that we don't need anymore\n   with referenced files (Pavel)\n\n - Various cleanups (Pavel)\n\n* tag 'io_uring-5.10-2020-10-24' of git://git.kernel.dk/linux-block:\n  splice: change exported internal do_splice() helper to take kernel offset\n  io_uring: make loop_rw_iter() use original user supplied pointers\n  io_uring: remove req cancel in ->flush()\n  io-wq: re-set NUMA node affinities if CPUs come online\n  io_uring: don't reuse linked_timeout\n  io_uring: unify fsize with def->work_flags\n  io_uring: fix racy REQ_F_LINK_TIMEOUT clearing\n  io_uring: do poll's hash_node init in common code\n  io_uring: inline io_poll_task_handler()\n  io_uring: remove extra ->file check in poll prep\n  io_uring: make cached_cq_overflow non atomic_t\n  io_uring: inline io_fail_links()\n  io_uring: kill ref get/drop in personality init\n  io_uring: flags-based creds init in queue",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-24 12:40:18 -0700 Merge tag 'io_uring-5.10-2020-10-24' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "0d63c148d6d9ac57c124b618f66269bb4558553b",
        "message": "Restructure __io_queue_sqe() so it follows simple if/else if/else\ncontrol flow. It's more readable and removes extra goto/labels.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc2",
        "release_date": "2020-10-23 13:07:12 -0600 io_uring: simplify __io_queue_sqe()"
    },
    {
        "commit": "9aaf354352f1142831457492790d6bfa9c883021",
        "message": "Don't overuse goto's, complex control flow doesn't make compilers happy\nand makes code harder to read.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc2",
        "release_date": "2020-10-23 13:07:12 -0600 io_uring: simplify nxt propagation in io_queue_sqe"
    },
    {
        "commit": "feaadc4fc2ebdbd53ffed1735077725855a2af53",
        "message": "Set IO_WQ_WORK_CONCURRENT for all REQ_F_FORCE_ASYNC requests, do that in\nthat is also looks better.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc2",
        "release_date": "2020-10-23 13:07:11 -0600 io_uring: don't miss setting IO_WQ_WORK_CONCURRENT"
    },
    {
        "commit": "c9abd7ad832b9eef06d887f4971894af5de617fd",
        "message": "Inline io_link_cancel_timeout() and __io_kill_linked_timeout() into\nio_kill_linked_timeout(). That allows to easily move a put of a cancelled\nlinked timeout out of completion_lock and to not deferring it. It is also\nmuch more readable when not scattered across three different functions.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc2",
        "release_date": "2020-10-23 13:07:11 -0600 io_uring: don't defer put of cancelled ltimeout"
    },
    {
        "commit": "cdfcc3ee04599ce51e5c84432c177163637dd0e0",
        "message": "Move REQ_F_LINK_TIMEOUT clearing out of __io_kill_linked_timeout()\nbecause it might return early and leave the flag set. It's not a\nproblem, but may be confusing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc2",
        "release_date": "2020-10-23 13:07:11 -0600 io_uring: always clear LINK_TIMEOUT after cancel"
    },
    {
        "commit": "ac877d2edd094e161801d72b49cfb56c5fc860fb",
        "message": "An armed linked timeout can never be a head of a link, so we don't need\nto clear REQ_F_LINK_HEAD for it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc2",
        "release_date": "2020-10-23 13:07:11 -0600 io_uring: don't adjust LINK_HEAD in cancel ltimeout"
    },
    {
        "commit": "e08102d507f34e6591de521a4c2587c6f02c7996",
        "message": "__io_kill_linked_timeout() already checks for REQ_F_LTIMEOUT_ACTIVE and\nit's set only for linked timeouts. No need to verify next request's\nopcode.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc2",
        "release_date": "2020-10-23 13:07:11 -0600 io_uring: remove opcode check on ltimeout kill"
    },
    {
        "commit": "ee6e00c868221f5f7d0b6eb4e8379a148e26bc20",
        "message": "With the set_fs change, we can no longer rely on copy_{to,from}_user()\naccepting a kernel pointer, and it was bad form to do so anyway. Clean\nthis up and change the internal helper that io_uring uses to deal with\nkernel pointers instead. This puts the offset copy in/out in __do_splice()\ninstead, which just calls the same helper.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-22 14:15:51 -0600 splice: change exported internal do_splice() helper to take kernel offset"
    },
    {
        "commit": "4017eb91a9e79bbb5d14868c207436f4a6a0af50",
        "message": "We jump through a hoop for fixed buffers, where we first map these to\na bvec(), then kmap() the bvec to obtain the pointer we copy to/from.\nThis was always a bit ugly, and with the set_fs changes, it ends up\nbeing practically problematic as well.\n\nThere's no need to jump through these hoops, just use the original user\npointers and length for the non iter based read/write.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-22 14:14:12 -0600 io_uring: make loop_rw_iter() use original user supplied pointers"
    },
    {
        "commit": "c8fb20b5b4206e9206ea8f129aa4592ad15918bd",
        "message": "Every close(io_uring) causes cancellation of all inflight requests\ncarrying ->files. That's not nice but was neccessary up until recently.\nNow task->files removal is handled in the core code, so that part of\nflush can be removed.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-22 09:54:19 -0600 io_uring: remove req cancel in ->flush()"
    },
    {
        "commit": "ff5771613cd7b3a76cd16cb54aa81d30d3c11d48",
        "message": "Clear linked_timeout for next requests in __io_queue_sqe() so we won't\nqueue it up unnecessary when it's going to be punted.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nCc: stable@vger.kernel.org # v5.9\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-21 16:37:56 -0600 io_uring: don't reuse linked_timeout"
    },
    {
        "commit": "69228338c9c3f0519f0daeca362a730130211c83",
        "message": "This one was missed in the earlier conversion, should be included like\nany of the other IO identity flags. Make sure we restore to RLIM_INIFITY\nwhen dropping the personality again.\n\nFixes: 98447d65b4a7 (\"io_uring: move io identity items into separate struct\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-20 16:03:13 -0600 io_uring: unify fsize with def->work_flags"
    },
    {
        "commit": "4962a85696f9439970bfd84f7ce23b2721f13549",
        "message": "Pull io_uring updates from Jens Axboe:\n \"A mix of fixes and a few stragglers. In detail:\n\n   - Revert the bogus __read_mostly that we discussed for the initial\n     pull request.\n\n   - Fix a merge window regression with fixed file registration error\n     path handling.\n\n   - Fix io-wq numa node affinities.\n\n   - Series abstracting out an io_identity struct, making it both easier\n     to see what the personality items are, and also easier to to adopt\n     more. Use this to cover audit logging.\n\n   - Fix for read-ahead disabled block condition in async buffered\n     reads, and using single page read-ahead to unify what\n     generic_file_buffer_read() path is used.\n\n   - Series for REQ_F_COMP_LOCKED fix and removal of it (Pavel)\n\n   - Poll fix (Pavel)\"\n\n* tag 'io_uring-5.10-2020-10-20' of git://git.kernel.dk/linux-block: (21 commits)\n  io_uring: use blk_queue_nowait() to check if NOWAIT supported\n  mm: use limited read-ahead to satisfy read\n  mm: mark async iocb read as NOWAIT once some data has been copied\n  io_uring: fix double poll mask init\n  io-wq: inherit audit loginuid and sessionid\n  io_uring: use percpu counters to track inflight requests\n  io_uring: assign new io_identity for task if members have changed\n  io_uring: store io_identity in io_uring_task\n  io_uring: COW io_identity on mismatch\n  io_uring: move io identity items into separate struct\n  io_uring: rely solely on work flags to determine personality.\n  io_uring: pass required context in as flags\n  io-wq: assign NUMA node locality if appropriate\n  io_uring: fix error path cleanup in io_sqe_files_register()\n  Revert \"io_uring: mark io_uring_fops/io_op_defs as __read_mostly\"\n  io_uring: fix REQ_F_COMP_LOCKED by killing it\n  io_uring: dig out COMP_LOCK from deep call chain\n  io_uring: don't put a poll req under spinlock\n  io_uring: don't unnecessarily clear F_LINK_TIMEOUT\n  io_uring: don't set COMP_LOCKED if won't put\n  ...",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-20 13:19:30 -0700 Merge tag 'io_uring-5.10-2020-10-20' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "900fad45dc75c8af6015bc514cc11aa3d265426a",
        "message": "io_link_timeout_fn() removes REQ_F_LINK_TIMEOUT from the link head's\nflags, it's not atomic and may race with what the head is doing.\n\nIf io_link_timeout_fn() doesn't clear the flag, as forced by this patch,\nthen it may happen that for \"req -> link_timeout1 -> link_timeout2\",\n__io_kill_linked_timeout() would find link_timeout2 and try to cancel\nit, so miscounting references. Teach it to ignore such double timeouts\nby marking the active one with a new flag in io_prep_linked_timeout().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-19 13:29:51 -0600 io_uring: fix racy REQ_F_LINK_TIMEOUT clearing"
    },
    {
        "commit": "4d52f338992bfc9f494e5a7eb55f4249ab11cd66",
        "message": "Move INIT_HLIST_NODE(&req->hash_node) into __io_arm_poll_handler(), so\nthat it doesn't duplicated and common poll code would be responsible for\nit.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-19 13:29:29 -0600 io_uring: do poll's hash_node init in common code"
    },
    {
        "commit": "dd221f46f68ad9df2c1daf7a7626c75fa9bd0bf0",
        "message": "io_poll_task_handler() doesn't add clarity, inline it in its only user.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-19 13:29:29 -0600 io_uring: inline io_poll_task_handler()"
    },
    {
        "commit": "069b89384d77c8972a8aa12588e74507714159d4",
        "message": "io_poll_add_prep() doesn't need to verify ->file because it's already\ndone in io_init_req().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-19 13:29:29 -0600 io_uring: remove extra ->file check in poll prep"
    },
    {
        "commit": "2c3bac6dd6c7af4cb38d1a67cafd29e96ba1fea7",
        "message": "ctx->cached_cq_overflow is changed only under completion_lock. Convert\nit from atomic_t to just int, and mark all places when it's read without\nlock with READ_ONCE, which guarantees atomicity (relaxed ordering).\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-19 13:29:29 -0600 io_uring: make cached_cq_overflow non atomic_t"
    },
    {
        "commit": "d148ca4b07d05656f3b25b662f2283c4d98e7299",
        "message": "Inline io_fail_links() and kill extra io_cqring_ev_posted().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-19 13:29:29 -0600 io_uring: inline io_fail_links()"
    },
    {
        "commit": "ec99ca6c4747bc7b024b35e0326694fe4f8ad140",
        "message": "Don't take an identity on personality/creds init only to drop it a few\nlines after. Extract a function which prepares req->work but leaves it\nwithout identity.\n\nNote: it's safe to not check REQ_F_WORK_INITIALIZED there because it's\nnobody had a chance to init it before io_init_req().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-19 13:29:29 -0600 io_uring: kill ref get/drop in personality init"
    },
    {
        "commit": "2e5aa6cb4d159f1785eee50e4bcc7e9ffd4e4690",
        "message": "Use IO_WQ_WORK_CREDS to figure out if req has creds to be used.\nSince recently it should rely only on flags, but not value of\nwork.creds.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-19 13:29:29 -0600 io_uring: flags-based creds init in queue"
    },
    {
        "commit": "9ba0d0c81284f4ec0b24529bdba2fc68b9d6a09a",
        "message": "commit 021a24460dc2 (\"block: add QUEUE_FLAG_NOWAIT\") adds a new helper\nfunction blk_queue_nowait() to check if the bdev supports handling of\nREQ_NOWAIT or not. Since then bio-based dm device can also support\nREQ_NOWAIT, and currently only dm-linear supports that since\ncommit 6abc49468eea (\"dm: add support for REQ_NOWAIT and enable it for\nlinear target\").\n\nSigned-off-by: Jeffle Xu <jefflexu@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-19 07:32:36 -0600 io_uring: use blk_queue_nowait() to check if NOWAIT supported"
    },
    {
        "commit": "0726b01e70455f9900ab524117c7b520d197dc8c",
        "message": "Patch series \"introduce memory hinting API for external process\", v9.\n\nNow, we have MADV_PAGEOUT and MADV_COLD as madvise hinting API.  With\nthat, application could give hints to kernel what memory range are\npreferred to be reclaimed.  However, in some platform(e.g., Android), the\ninformation required to make the hinting decision is not known to the app.\nInstead, it is known to a centralized userspace daemon(e.g.,\nActivityManagerService), and that daemon must be able to initiate reclaim\non its own without any app involvement.\n\nTo solve the concern, this patch introduces new syscall -\nprocess_madvise(2).  Bascially, it's same with madvise(2) syscall but it\nhas some differences.\n\n1. It needs pidfd of target process to provide the hint\n\n2. It supports only MADV_{COLD|PAGEOUT|MERGEABLE|UNMEREABLE} at this\n   moment.  Other hints in madvise will be opened when there are explicit\n   requests from community to prevent unexpected bugs we couldn't support.\n\n3. Only privileged processes can do something for other process's\n   address space.\n\nFor more detail of the new API, please see \"mm: introduce external memory\nhinting API\" description in this patchset.\n\nThis patch (of 3):\n\nIn upcoming patches, do_madvise will be called from external process\ncontext so we shouldn't asssume \"current\" is always hinted process's\ntask_struct.\n\nFurthermore, we must not access mm_struct via task->mm, but obtain it via\naccess_mm() once (in the following patch) and only use that pointer [1],\nso pass it to do_madvise() as well.  Note the vma->vm_mm pointers are\nsafe, so we can use them further down the call stack.\n\nAnd let's pass current->mm as arguments of do_madvise so it shouldn't\nchange existing behavior but prepare next patch to make review easy.\n\n[vbabka@suse.cz: changelog tweak]\n[minchan@kernel.org: use current->mm for io_uring]\n  Link: http://lkml.kernel.org/r/20200423145215.72666-1-minchan@kernel.org\n[akpm@linux-foundation.org: fix it for upstream changes]\n[akpm@linux-foundation.org: whoops]\n[rdunlap@infradead.org: add missing includes]\n\nSigned-off-by: Minchan Kim <minchan@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nReviewed-by: Suren Baghdasaryan <surenb@google.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: David Rientjes <rientjes@google.com>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Jann Horn <jannh@google.com>\nCc: Tim Murray <timmurray@google.com>\nCc: Daniel Colascione <dancol@google.com>\nCc: Sandeep Patil <sspatil@google.com>\nCc: Sonny Rao <sonnyrao@google.com>\nCc: Brian Geffon <bgeffon@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Shakeel Butt <shakeelb@google.com>\nCc: John Dias <joaodias@google.com>\nCc: Joel Fernandes <joel@joelfernandes.org>\nCc: Alexander Duyck <alexander.h.duyck@linux.intel.com>\nCc: SeongJae Park <sj38.park@gmail.com>\nCc: Christian Brauner <christian@brauner.io>\nCc: Kirill Tkhai <ktkhai@virtuozzo.com>\nCc: Oleksandr Natalenko <oleksandr@redhat.com>\nCc: SeongJae Park <sjpark@amazon.de>\nCc: Christian Brauner <christian.brauner@ubuntu.com>\nCc: Florian Weimer <fw@deneb.enyo.de>\nCc: <linux-man@vger.kernel.org>\nLink: https://lkml.kernel.org/r/20200901000633.1920247-1-minchan@kernel.org\nLink: http://lkml.kernel.org/r/20200622192900.22757-1-minchan@kernel.org\nLink: http://lkml.kernel.org/r/20200302193630.68771-2-minchan@kernel.org\nLink: http://lkml.kernel.org/r/20200622192900.22757-2-minchan@kernel.org\nLink: https://lkml.kernel.org/r/20200901000633.1920247-2-minchan@kernel.org\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-18 09:27:09 -0700 mm/madvise: pass mm to do_madvise"
    },
    {
        "commit": "58852d4d673760cf7c88b9360b3c24a041bec298",
        "message": "__io_queue_proc() is used by both, poll reqs and apoll. Don't use\nreq->poll.events to copy poll mask because for apoll it aliases with\nprivate data of the request.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-17 09:25:47 -0600 io_uring: fix double poll mask init"
    },
    {
        "commit": "d8a6df10aac9f2e4d5f30aff3129d552d2984ce7",
        "message": "Even though we place the req_issued and req_complete in separate\ncachelines, there's considerable overhead in doing the atomics\nparticularly on the completion side.\n\nGet rid of having the two counters, and just use a percpu_counter for\nthis. That's what it was made for, after all. This considerably\nreduces the overhead in __io_free_req().\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-17 09:25:47 -0600 io_uring: use percpu counters to track inflight requests"
    },
    {
        "commit": "500a373d731ac506612db12631ec21295c1ff360",
        "message": "This avoids doing a copy for each new async IO, if some parts of the\nio_identity has changed. We avoid reference counting for the normal\nfast path of nothing ever changing.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-17 09:25:46 -0600 io_uring: assign new io_identity for task if members have changed"
    },
    {
        "commit": "5c3462cfd123b341c9d3c947c1a2bab373f1697f",
        "message": "This is, by definition, a per-task structure. So store it in the\ntask context, instead of doing carrying it in each io_kiocb. We're being\na bit inefficient if members have changed, as that requires an alloc and\ncopy of a new io_identity struct. The next patch will fix that up.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-17 09:25:46 -0600 io_uring: store io_identity in io_uring_task"
    },
    {
        "commit": "1e6fa5216a0e59ef02e8b6b40d553238a3b81d49",
        "message": "If the io_identity doesn't completely match the task, then create a\ncopy of it and use that. The existing copy remains valid until the last\nuser of it has gone away.\n\nThis also changes the personality lookup to be indexed by io_identity,\ninstead of creds directly.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-17 09:25:46 -0600 io_uring: COW io_identity on mismatch"
    },
    {
        "commit": "98447d65b4a7a59f8ea37dc6e5d743247d9a7b01",
        "message": "io-wq contains a pointer to the identity, which we just hold in io_kiocb\nfor now. This is in preparation for putting this outside io_kiocb. The\nonly exception is struct files_struct, which we'll need different rules\nfor to avoid a circular dependency.\n\nNo functional changes in this patch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-17 09:25:45 -0600 io_uring: move io identity items into separate struct"
    },
    {
        "commit": "dfead8a8e2c494b947480bac90a6f9792f08bc12",
        "message": "We solely rely on work->work_flags now, so use that for proper checking\nand clearing/dropping of various identity items.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-17 09:25:45 -0600 io_uring: rely solely on work flags to determine personality."
    },
    {
        "commit": "0f203765880c4416675726be558b65da4a7604e2",
        "message": "We have a number of bits that decide what context to inherit. Set up\nio-wq flags for these instead. This is in preparation for always having\nthe various members set, but not always needing them for all requests.\n\nNo intended functional changes in this patch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-17 09:25:45 -0600 io_uring: pass required context in as flags"
    },
    {
        "commit": "55cbc2564ab2fd555ec0fc39311a9cfb811d7da5",
        "message": "syzbot reports the following crash:\n\ngeneral protection fault, probably for non-canonical address 0xdffffc0000000000: 0000 [#1] PREEMPT SMP KASAN\nKASAN: null-ptr-deref in range [0x0000000000000000-0x0000000000000007]\nCPU: 1 PID: 8927 Comm: syz-executor.3 Not tainted 5.9.0-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nRIP: 0010:io_file_from_index fs/io_uring.c:5963 [inline]\nRIP: 0010:io_sqe_files_register fs/io_uring.c:7369 [inline]\nRIP: 0010:__io_uring_register fs/io_uring.c:9463 [inline]\nRIP: 0010:__do_sys_io_uring_register+0x2fd2/0x3ee0 fs/io_uring.c:9553\nCode: ec 03 49 c1 ee 03 49 01 ec 49 01 ee e8 57 61 9c ff 41 80 3c 24 00 0f 85 9b 09 00 00 4d 8b af b8 01 00 00 4c 89 e8 48 c1 e8 03 <80> 3c 28 00 0f 85 76 09 00 00 49 8b 55 00 89 d8 c1 f8 09 48 98 4c\nRSP: 0018:ffffc90009137d68 EFLAGS: 00010246\nRAX: 0000000000000000 RBX: 0000000000000000 RCX: ffffc9000ef2a000\nRDX: 0000000000040000 RSI: ffffffff81d81dd9 RDI: 0000000000000005\nRBP: dffffc0000000000 R08: 0000000000000001 R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000000 R12: ffffed1012882a37\nR13: 0000000000000000 R14: ffffed1012882a38 R15: ffff888094415000\nFS:  00007f4266f3c700(0000) GS:ffff8880ae500000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 000000000118c000 CR3: 000000008e57d000 CR4: 00000000001506e0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\nCall Trace:\n do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\nRIP: 0033:0x45de59\nCode: 0d b4 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 db b3 fb ff c3 66 2e 0f 1f 84 00 00 00 00\nRSP: 002b:00007f4266f3bc78 EFLAGS: 00000246 ORIG_RAX: 00000000000001ab\nRAX: ffffffffffffffda RBX: 00000000000083c0 RCX: 000000000045de59\nRDX: 0000000020000280 RSI: 0000000000000002 RDI: 0000000000000005\nRBP: 000000000118bf68 R08: 0000000000000000 R09: 0000000000000000\nR10: 40000000000000a1 R11: 0000000000000246 R12: 000000000118bf2c\nR13: 00007fff2fa4f12f R14: 00007f4266f3c9c0 R15: 000000000118bf2c\nModules linked in:\n---[ end trace 2a40a195e2d5e6e6 ]---\nRIP: 0010:io_file_from_index fs/io_uring.c:5963 [inline]\nRIP: 0010:io_sqe_files_register fs/io_uring.c:7369 [inline]\nRIP: 0010:__io_uring_register fs/io_uring.c:9463 [inline]\nRIP: 0010:__do_sys_io_uring_register+0x2fd2/0x3ee0 fs/io_uring.c:9553\nCode: ec 03 49 c1 ee 03 49 01 ec 49 01 ee e8 57 61 9c ff 41 80 3c 24 00 0f 85 9b 09 00 00 4d 8b af b8 01 00 00 4c 89 e8 48 c1 e8 03 <80> 3c 28 00 0f 85 76 09 00 00 49 8b 55 00 89 d8 c1 f8 09 48 98 4c\nRSP: 0018:ffffc90009137d68 EFLAGS: 00010246\nRAX: 0000000000000000 RBX: 0000000000000000 RCX: ffffc9000ef2a000\nRDX: 0000000000040000 RSI: ffffffff81d81dd9 RDI: 0000000000000005\nRBP: dffffc0000000000 R08: 0000000000000001 R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000000 R12: ffffed1012882a37\nR13: 0000000000000000 R14: ffffed1012882a38 R15: ffff888094415000\nFS:  00007f4266f3c700(0000) GS:ffff8880ae400000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 000000000074a918 CR3: 000000008e57d000 CR4: 00000000001506f0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n\nwhich is a copy of fget failure condition jumping to cleanup, but the\ncleanup requires ctx->file_data to be assigned. Assign it when setup,\nand ensure that we clear it again for the error path exit.\n\nFixes: 5398ae698525 (\"io_uring: clean file_data access in files_register\")\nReported-by: syzbot+f4ebcc98223dafd8991e@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-17 09:25:44 -0600 io_uring: fix error path cleanup in io_sqe_files_register()"
    },
    {
        "commit": "0918682be432b85ccd49285832221d9b65831ef5",
        "message": "This reverts commit 738277adc81929b3e7c9b63fec6693868cc5f931.\n\nThis change didn't make a lot of sense, and as Linus reports, it actually\nfails on clang:\n\n   /tmp/io_uring-dd40c4.s:26476: Warning: ignoring changed section\n   attributes for .data..read_mostly\n\nThe arrays are already marked const so, by definition, they are not\njust read-mostly, they are read-only.\n\nReported-by: Linus Torvalds <torvalds@linux-foundation.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-17 09:25:43 -0600 Revert \"io_uring: mark io_uring_fops/io_op_defs as __read_mostly\""
    },
    {
        "commit": "216578e55ac932cf5e348d9e65d8e129fc9e34cc",
        "message": "REQ_F_COMP_LOCKED is used and implemented in a buggy way. The problem is\nthat the flag is set before io_put_req() but not cleared after, and if\nthat wasn't the final reference, the request will be freed with the flag\nset from some other context, which may not hold a spinlock. That means\npossible races with removing linked timeouts and unsynchronised\ncompletion (e.g. access to CQ).\n\nInstead of fixing REQ_F_COMP_LOCKED, kill the flag and use\ntask_work_add() to move such requests to a fresh context to free from\nit, as was done with __io_free_req_finish().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-17 09:25:43 -0600 io_uring: fix REQ_F_COMP_LOCKED by killing it"
    },
    {
        "commit": "4edf20f9990230e9b85e79954d5cd28fc93616e9",
        "message": "io_req_clean_work() checks REQ_F_COMP_LOCK to pass this two layers up.\nMove the check up into __io_free_req(), so at least it doesn't looks so\nugly and would facilitate further changes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-17 09:25:43 -0600 io_uring: dig out COMP_LOCK from deep call chain"
    },
    {
        "commit": "6a0af224c21309f24dbb1b79d0744b255d7156a0",
        "message": "Move io_put_req() in io_poll_task_handler() from under spinlock. This\neliminates the need to use REQ_F_COMP_LOCKED, at the expense of\npotentially having to grab the lock again. That's still a better trade\noff than relying on the locked flag.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-17 09:25:42 -0600 io_uring: don't put a poll req under spinlock"
    },
    {
        "commit": "b1b74cfc1967bd0747ff85f650f598e84eeb3d1c",
        "message": "If a request had REQ_F_LINK_TIMEOUT it would've been cleared in\n__io_kill_linked_timeout() by the time of __io_fail_links(), so no need\nto care about it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-17 09:25:42 -0600 io_uring: don't unnecessarily clear F_LINK_TIMEOUT"
    },
    {
        "commit": "368c5481ae7c6a9719c40984faea35480d9f4872",
        "message": "__io_kill_linked_timeout() sets REQ_F_COMP_LOCKED for a linked timeout\neven if it can't cancel it, e.g. it's already running. It not only races\nwith io_link_timeout_fn() for ->flags field, but also leaves the flag\nset and so io_link_timeout_fn() may find it and decide that it holds the\nlock. Hopefully, the second problem is potential.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-17 09:25:42 -0600 io_uring: don't set COMP_LOCKED if won't put"
    },
    {
        "commit": "035fbafc7a54b8c7755b3c508b8f3ab6ff3c8d65",
        "message": "An incorrect sizeof() is being used, sizeof(file_data->table) is not\ncorrect, it should be sizeof(*file_data->table).\n\nFixes: 5398ae698525 (\"io_uring: clean file_data access in files_register\")\nSigned-off-by: Colin Ian King <colin.king@canonical.com>\nAddresses-Coverity: (\"Sizeof not portable (SIZEOF_MISMATCH)\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-17 09:25:41 -0600 io_uring: Fix sizeof() mismatch"
    },
    {
        "commit": "6ad4bf6ea1609fb539a62f10fca87ddbd53a0315",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - Add blkcg accounting for io-wq offload (Dennis)\n\n - A use-after-free fix for io-wq (Hillf)\n\n - Cancelation fixes and improvements\n\n - Use proper files_struct references for offload\n\n - Cleanup of io_uring_get_socket() since that can now go into our own\n   header\n\n - SQPOLL fixes and cleanups, and support for sharing the thread\n\n - Improvement to how page accounting is done for registered buffers and\n   huge pages, accounting the real pinned state\n\n - Series cleaning up the xarray code (Willy)\n\n - Various cleanups, refactoring, and improvements (Pavel)\n\n - Use raw spinlock for io-wq (Sebastian)\n\n - Add support for ring restrictions (Stefano)\n\n* tag 'io_uring-5.10-2020-10-12' of git://git.kernel.dk/linux-block: (62 commits)\n  io_uring: keep a pointer ref_node in file_data\n  io_uring: refactor *files_register()'s error paths\n  io_uring: clean file_data access in files_register\n  io_uring: don't delay io_init_req() error check\n  io_uring: clean leftovers after splitting issue\n  io_uring: remove timeout.list after hrtimer cancel\n  io_uring: use a separate struct for timeout_remove\n  io_uring: improve submit_state.ios_left accounting\n  io_uring: simplify io_file_get()\n  io_uring: kill extra check in fixed io_file_get()\n  io_uring: clean up ->files grabbing\n  io_uring: don't io_prep_async_work() linked reqs\n  io_uring: Convert advanced XArray uses to the normal API\n  io_uring: Fix XArray usage in io_uring_add_task_file\n  io_uring: Fix use of XArray in __io_uring_files_cancel\n  io_uring: fix break condition for __io_uring_register() waiting\n  io_uring: no need to call xa_destroy() on empty xarray\n  io_uring: batch account ->req_issue and task struct references\n  io_uring: kill callback_head argument for io_req_task_work_add()\n  io_uring: move req preps out of io_issue_sqe()\n  ...",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-13 12:36:21 -0700 Merge tag 'io_uring-5.10-2020-10-12' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "b2e9685283127f30e7f2b466af0046ff9bd27a86",
        "message": "->cur_refs of struct fixed_file_data always points to percpu_ref\nembedded into struct fixed_file_ref_node. Don't overuse container_of()\nand offsetting, and point directly to fixed_file_ref_node.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-10 12:49:25 -0600 io_uring: keep a pointer ref_node in file_data"
    },
    {
        "commit": "600cf3f8b3f68ad22ee9af4cf24b0a96512f7fbe",
        "message": "Don't keep repeating cleaning sequences in error paths, write it once\nin the and use labels. It's less error prone and looks cleaner.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-10 12:49:25 -0600 io_uring: refactor *files_register()'s error paths"
    },
    {
        "commit": "5398ae698525572d4eb0531854158ece94385318",
        "message": "Keep file_data in a local var and replace with it complex references\nsuch as ctx->file_data.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-10 12:49:25 -0600 io_uring: clean file_data access in files_register"
    },
    {
        "commit": "692d836351ffa05016521a769543af51c9dc3e9e",
        "message": "Don't postpone io_init_req() error checks and do that right after\ncalling it. There is no control-flow statements or dependencies with\nsqe/submitted accounting, so do those earlier, that makes the code flow\na bit more natural.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-10 12:49:25 -0600 io_uring: don't delay io_init_req() error check"
    },
    {
        "commit": "062d04d73168d1d7109b75600a53a6a361d1fda8",
        "message": "Kill extra if in io_issue_sqe() and place send/recv[msg] calls\nappropriately under switch's cases.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-10 12:49:25 -0600 io_uring: clean leftovers after splitting issue"
    },
    {
        "commit": "a71976f3fa474d0aa9b33fc2ecaa67af6103bb71",
        "message": "Remove timeouts from ctx->timeout_list after hrtimer_try_to_cancel()\nsuccessfully cancels it. With this we don't need to care whether there\nwas a race and it was removed in io_timeout_fn(), and that will be handy\nfor following patches.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-10 12:49:25 -0600 io_uring: remove timeout.list after hrtimer cancel"
    },
    {
        "commit": "0bdf7a2ddb7d8b28d1c9f505e7f32aa2972d461b",
        "message": "Don't use struct io_timeout for both IORING_OP_TIMEOUT and\nIORING_OP_TIMEOUT_REMOVE, they're quite different. Split them in two,\nthat allows to remove an unused field in struct io_timeout, and btw kill\n->flags not used by either. This also easier to follow, especially for\ntimeout remove.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-10 12:49:25 -0600 io_uring: use a separate struct for timeout_remove"
    },
    {
        "commit": "71b547c048eb10d54627932019411549b1e4cfb7",
        "message": "state->ios_left isn't decremented for requests that don't need a file,\nso it might be larger than number of SQEs left. That in some\ncircumstances makes us to grab more files that is needed so imposing\nextra put.\nDeaccount one ios_left for each request.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-10 12:49:25 -0600 io_uring: improve submit_state.ios_left accounting"
    },
    {
        "commit": "8371adf53c3c5ebfde1172608d3f4e126729cb10",
        "message": "Keep ->needs_file_no_error check out of io_file_get(), and let callers\nhandle it. It makes it more straightforward. Also, as the only error it\ncan hand back -EBADF, make it return a file or NULL.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-10 12:49:24 -0600 io_uring: simplify io_file_get()"
    },
    {
        "commit": "479f517be57185087c2ae3c5e5aea0c3c6d4e5cc",
        "message": "ctx->nr_user_files == 0 IFF ctx->file_data == NULL and there fixed files\nare not used. Hence, verifying fds only against ctx->nr_user_files is\nenough. Remove the other check from hot path.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-10 12:49:24 -0600 io_uring: kill extra check in fixed io_file_get()"
    },
    {
        "commit": "233295130e53c8dfe6dbef3f52634c3f7e44cd6a",
        "message": "Move work.files grabbing into io_prep_async_work() to all other work\nresources initialisation. We don't need to keep it separately now, as\n->ring_fd/file are gone. It also allows to not grab it when a request\nis not going to io-wq.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-10 12:49:24 -0600 io_uring: clean up ->files grabbing"
    },
    {
        "commit": "5bf5e464f1acb1c031b4a290d63760bcb074c027",
        "message": "There is no real reason left for preparing io-wq work context for linked\nrequests in advance, remove it as this might become a bottleneck in some\ncases.\n\nReported-by: Roman Gershman <romger@amazon.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-10 12:49:20 -0600 io_uring: don't io_prep_async_work() linked reqs"
    },
    {
        "commit": "5e2ed8c4f45093698855b1f45cdf43efbf6dd498",
        "message": "There are no bugs here that I've spotted, it's just easier to use the\nnormal API and there are no performance advantages to using the more\nverbose advanced API.\n\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-09 09:00:05 -0600 io_uring: Convert advanced XArray uses to the normal API"
    },
    {
        "commit": "236434c3438c4da3dfbd6aeeab807577b85e951a",
        "message": "The xas_store() wasn't paired with an xas_nomem() loop, so if it couldn't\nallocate memory using GFP_NOWAIT, it would leak the reference to the file\ndescriptor.  Also the node pointed to by the xas could be freed between\nthe call to xas_load() under the rcu_read_lock() and the acquisition of\nthe xa_lock.\n\nIt's easier to just use the normal xa_load/xa_store interface here.\n\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\n[axboe: fix missing assign after alloc, cur_uring -> tctx rename]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-09 08:59:40 -0600 io_uring: Fix XArray usage in io_uring_add_task_file"
    },
    {
        "commit": "ce765372bc443573d1d339a2bf4995de385dea3a",
        "message": "We have to drop the lock during each iteration, so there's no advantage\nto using the advanced API.  Convert this to a standard xa_for_each() loop.\n\nReported-by: syzbot+27c12725d8ff0bfe1a13@syzkaller.appspotmail.com\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-09 08:52:26 -0600 io_uring: Fix use of XArray in __io_uring_files_cancel"
    },
    {
        "commit": "ed6930c9201cd1e00f74474da2f095796a0d82f6",
        "message": "Colin reports that there's unreachable code, since we only ever break\nif ret == 0. This is correct, and is due to a reversed logic condition\nin when to break or not.\n\nBreak out of the loop if we don't process any task work, in that case\nwe do want to return -EINTR.\n\nFixes: af9c1a44f8de (\"io_uring: process task work in io_uring_register()\")\nReported-by: Colin Ian King <colin.king@canonical.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-08 20:37:45 -0600 io_uring: fix break condition for __io_uring_register() waiting"
    },
    {
        "commit": "ca6484cd308a671811bf39f3119e81966eb476e3",
        "message": "The kernel test robot reports this lockdep issue:\n\n[child1:659] mbind (274) returned ENOSYS, marking as inactive.\n[child1:659] mq_timedsend (279) returned ENOSYS, marking as inactive.\n[main] 10175 iterations. [F:7781 S:2344 HI:2397]\n[   24.610601]\n[   24.610743] ================================\n[   24.611083] WARNING: inconsistent lock state\n[   24.611437] 5.9.0-rc7-00017-g0f2122045b9462 #5 Not tainted\n[   24.611861] --------------------------------\n[   24.612193] inconsistent {SOFTIRQ-ON-W} -> {IN-SOFTIRQ-W} usage.\n[   24.612660] ksoftirqd/0/7 [HC0[0]:SC1[3]:HE0:SE0] takes:\n[   24.613086] f00ed998 (&xa->xa_lock#4){+.?.}-{2:2}, at: xa_destroy+0x43/0xc1\n[   24.613642] {SOFTIRQ-ON-W} state was registered at:\n[   24.614024]   lock_acquire+0x20c/0x29b\n[   24.614341]   _raw_spin_lock+0x21/0x30\n[   24.614636]   io_uring_add_task_file+0xe8/0x13a\n[   24.614987]   io_uring_create+0x535/0x6bd\n[   24.615297]   io_uring_setup+0x11d/0x136\n[   24.615606]   __ia32_sys_io_uring_setup+0xd/0xf\n[   24.615977]   do_int80_syscall_32+0x53/0x6c\n[   24.616306]   restore_all_switch_stack+0x0/0xb1\n[   24.616677] irq event stamp: 939881\n[   24.616968] hardirqs last  enabled at (939880): [<8105592d>] __local_bh_enable_ip+0x13c/0x145\n[   24.617642] hardirqs last disabled at (939881): [<81b6ace3>] _raw_spin_lock_irqsave+0x1b/0x4e\n[   24.618321] softirqs last  enabled at (939738): [<81b6c7c8>] __do_softirq+0x3f0/0x45a\n[   24.618924] softirqs last disabled at (939743): [<81055741>] run_ksoftirqd+0x35/0x61\n[   24.619521]\n[   24.619521] other info that might help us debug this:\n[   24.620028]  Possible unsafe locking scenario:\n[   24.620028]\n[   24.620492]        CPU0\n[   24.620685]        ----\n[   24.620894]   lock(&xa->xa_lock#4);\n[   24.621168]   <Interrupt>\n[   24.621381]     lock(&xa->xa_lock#4);\n[   24.621695]\n[   24.621695]  *** DEADLOCK ***\n[   24.621695]\n[   24.622154] 1 lock held by ksoftirqd/0/7:\n[   24.622468]  #0: 823bfb94 (rcu_callback){....}-{0:0}, at: rcu_process_callbacks+0xc0/0x155\n[   24.623106]\n[   24.623106] stack backtrace:\n[   24.623454] CPU: 0 PID: 7 Comm: ksoftirqd/0 Not tainted 5.9.0-rc7-00017-g0f2122045b9462 #5\n[   24.624090] Call Trace:\n[   24.624284]  ? show_stack+0x40/0x46\n[   24.624551]  dump_stack+0x1b/0x1d\n[   24.624809]  print_usage_bug+0x17a/0x185\n[   24.625142]  mark_lock+0x11d/0x1db\n[   24.625474]  ? print_shortest_lock_dependencies+0x121/0x121\n[   24.625905]  __lock_acquire+0x41e/0x7bf\n[   24.626206]  lock_acquire+0x20c/0x29b\n[   24.626517]  ? xa_destroy+0x43/0xc1\n[   24.626810]  ? lock_acquire+0x20c/0x29b\n[   24.627110]  _raw_spin_lock_irqsave+0x3e/0x4e\n[   24.627450]  ? xa_destroy+0x43/0xc1\n[   24.627725]  xa_destroy+0x43/0xc1\n[   24.627989]  __io_uring_free+0x57/0x71\n[   24.628286]  ? get_pid+0x22/0x22\n[   24.628544]  __put_task_struct+0xf2/0x163\n[   24.628865]  put_task_struct+0x1f/0x2a\n[   24.629161]  delayed_put_task_struct+0xe2/0xe9\n[   24.629509]  rcu_process_callbacks+0x128/0x155\n[   24.629860]  __do_softirq+0x1a3/0x45a\n[   24.630151]  run_ksoftirqd+0x35/0x61\n[   24.630443]  smpboot_thread_fn+0x304/0x31a\n[   24.630763]  kthread+0x124/0x139\n[   24.631016]  ? sort_range+0x18/0x18\n[   24.631290]  ? kthread_create_worker_on_cpu+0x17/0x17\n[   24.631682]  ret_from_fork+0x1c/0x28\n\nwhich is complaining about xa_destroy() grabbing the xa lock in an\nIRQ disabling fashion, whereas the io_uring uses cases aren't interrupt\nsafe. This is really an xarray issue, since it should not assume the\nlock type. But for our use case, since we know the xarray is empty at\nthis point, there's no need to actually call xa_destroy(). So just get\nrid of it.\n\nFixes: 0f2122045b94 (\"io_uring: don't rely on weak ->files references\")\nReported-by: kernel test robot <lkp@intel.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-08 07:46:52 -0600 io_uring: no need to call xa_destroy() on empty xarray"
    },
    {
        "commit": "faf7b51c06973f947776af6c8f8a513475a2bfa1",
        "message": "Identical to how we handle the ctx reference counts, increase by the\nbatch we're expecting to submit, and handle any slow path residual,\nif any. The request alloc-and-issue path is very hot, and this makes\na noticeable difference by avoiding an two atomic incs for each\nindividual request.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-07 12:55:42 -0600 io_uring: batch account ->req_issue and task struct references"
    },
    {
        "commit": "463c43fcd97e493d8a17242f4f000c86fe642ed6",
        "message": "Linux 5.9-rc7\n\n* tag 'v5.9-rc7': (683 commits)\n  Linux 5.9-rc7\n  mm/thp: Split huge pmds/puds if they're pinned when fork()\n  mm: Do early cow for pinned pages during fork() for ptes\n  mm/fork: Pass new vma pointer into copy_page_range()\n  mm: Introduce mm_struct.has_pinned\n  mm: validate pmd after splitting\n  mm: don't rely on system state to detect hot-plug operations\n  mm: replace memmap_context by meminit_context\n  arch/x86/lib/usercopy_64.c: fix __copy_user_flushcache() cache writeback\n  lib/memregion.c: include memregion.h\n  lib/string.c: implement stpcpy\n  mm/migrate: correct thp migration stats\n  mm/gup: fix gup_fast with dynamic page table folding\n  mm: memcontrol: fix missing suffix of workingset_restore\n  mm, THP, swap: fix allocating cluster for swapfile by mistake\n  mm: slab: fix potential double free in ___cache_free\n  Documentation/llvm: Fix clang target examples\n  io_uring: ensure async buffered read-retry is setup properly\n  KVM: SVM: Add a dedicated INVD intercept routine\n  io_uring: don't unconditionally set plug->nowait = true\n  ...",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-10-04 12:19:12 +0200 Merge tag 'v5.9-rc7' into patchwork"
    },
    {
        "commit": "702bfc891db162b99e880da78cc256dac14cfc7f",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - fix for async buffered reads if read-ahead is fully disabled (Hao)\n\n - double poll match fix\n\n - ->show_fdinfo() potential ABBA deadlock complaint fix\n\n* tag 'io_uring-5.9-2020-10-02' of git://git.kernel.dk/linux-block:\n  io_uring: fix async buffered reads when readahead is disabled\n  io_uring: fix potential ABBA deadlock in ->show_fdinfo()\n  io_uring: always delete double poll wait entry on match",
        "kernel_version": "v5.9-rc8",
        "release_date": "2020-10-02 14:38:10 -0700 Merge tag 'io_uring-5.9-2020-10-02' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "87c4311fd2c28e83545cdfa4702b57db15ed1d9b",
        "message": "We always use &req->task_work anyway, no point in passing it in.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 21:00:16 -0600 io_uring: kill callback_head argument for io_req_task_work_add()"
    },
    {
        "commit": "c1379e247a724b2d4c646f1947d4f78c782c0650",
        "message": "All request preparations are done only during submission, reflect it in\nthe code by moving io_req_prep() much earlier into io_queue_sqe().\n\nThat's much cleaner, because it doen't expose bits to async code which\nit won't ever use. Also it makes the interface harder to misuse, and\nthere are potential places for bugs.\n\nFor instance, __io_queue() doesn't clear @sqe before proceeding to a\nnext linked request, that could have been disastrous, but hopefully\nthere are linked requests IFF sqe==NULL, so not actually a bug.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:38:46 -0600 io_uring: move req preps out of io_issue_sqe()"
    },
    {
        "commit": "bfe76559833d5d76fc4eebdad7658d22522f8a22",
        "message": "io_issue_sqe() does two things at once, trying to prepare request and\nissuing them. Split it in two and deduplicate with io_defer_prep().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:38:46 -0600 io_uring: decouple issuing and req preparation"
    },
    {
        "commit": "73debe68b300ca24fdbbe9b6c3787d3e239deb3d",
        "message": "All io_*_prep() functions including io_{read,write}_prep() are called\nonly during submission where @force_nonblock is always true. Don't keep\npropagating it and instead remove the @force_nonblock argument\nfrom prep() altogether.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:38:46 -0600 io_uring: remove nonblock arg from io_{rw}_prep()"
    },
    {
        "commit": "a88fc400212fc1d8aa9ca4979f898fd04ca3aab5",
        "message": "Move setting IOCB_NOWAIT from io_prep_rw() into io_read()/io_write(), so\nit's set/cleared in a single place. Also remove @force_nonblock\nparameter from io_prep_rw().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:38:46 -0600 io_uring: set/clear IOCB_NOWAIT into io_read/write"
    },
    {
        "commit": "2d199895d231c0a1af3a49d1f0da777499f352c8",
        "message": "REQ_F_NEED_CLEANUP is set only by io_*_prep() and they're guaranteed to\nbe called only once, so there is no one who may have set the flag\nbefore. Kill REQ_F_NEED_CLEANUP check in these *prep() handlers.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:38:46 -0600 io_uring: remove F_NEED_CLEANUP check in *prep()"
    },
    {
        "commit": "5b09e37e27a878eb50f0eb96fbce8419e932a7d5",
        "message": "Put brackets around bitwise ops in a complex expression\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:38:45 -0600 io_uring: io_kiocb_ppos() style change"
    },
    {
        "commit": "291b2821e072e16b062c5a0e83f7642143c4399a",
        "message": "Extract common code from if/else branches. That is cleaner and optimised\neven better.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:38:45 -0600 io_uring: simplify io_alloc_req()"
    },
    {
        "commit": "c4068bf898ddaef791049a366828d9b84b467bda",
        "message": "The smart syzbot has found a reproducer for the following issue:\n\n ==================================================================\n BUG: KASAN: use-after-free in instrument_atomic_write include/linux/instrumented.h:71 [inline]\n BUG: KASAN: use-after-free in atomic_inc include/asm-generic/atomic-instrumented.h:240 [inline]\n BUG: KASAN: use-after-free in io_wqe_inc_running fs/io-wq.c:301 [inline]\n BUG: KASAN: use-after-free in io_wq_worker_running+0xde/0x110 fs/io-wq.c:613\n Write of size 4 at addr ffff8882183db08c by task io_wqe_worker-0/7771\n\n CPU: 0 PID: 7771 Comm: io_wqe_worker-0 Not tainted 5.9.0-rc4-syzkaller #0\n Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\n Call Trace:\n  __dump_stack lib/dump_stack.c:77 [inline]\n  dump_stack+0x198/0x1fd lib/dump_stack.c:118\n  print_address_description.constprop.0.cold+0xae/0x497 mm/kasan/report.c:383\n  __kasan_report mm/kasan/report.c:513 [inline]\n  kasan_report.cold+0x1f/0x37 mm/kasan/report.c:530\n  check_memory_region_inline mm/kasan/generic.c:186 [inline]\n  check_memory_region+0x13d/0x180 mm/kasan/generic.c:192\n  instrument_atomic_write include/linux/instrumented.h:71 [inline]\n  atomic_inc include/asm-generic/atomic-instrumented.h:240 [inline]\n  io_wqe_inc_running fs/io-wq.c:301 [inline]\n  io_wq_worker_running+0xde/0x110 fs/io-wq.c:613\n  schedule_timeout+0x148/0x250 kernel/time/timer.c:1879\n  io_wqe_worker+0x517/0x10e0 fs/io-wq.c:580\n  kthread+0x3b5/0x4a0 kernel/kthread.c:292\n  ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:294\n\n Allocated by task 7768:\n  kasan_save_stack+0x1b/0x40 mm/kasan/common.c:48\n  kasan_set_track mm/kasan/common.c:56 [inline]\n  __kasan_kmalloc.constprop.0+0xbf/0xd0 mm/kasan/common.c:461\n  kmem_cache_alloc_node_trace+0x17b/0x3f0 mm/slab.c:3594\n  kmalloc_node include/linux/slab.h:572 [inline]\n  kzalloc_node include/linux/slab.h:677 [inline]\n  io_wq_create+0x57b/0xa10 fs/io-wq.c:1064\n  io_init_wq_offload fs/io_uring.c:7432 [inline]\n  io_sq_offload_start fs/io_uring.c:7504 [inline]\n  io_uring_create fs/io_uring.c:8625 [inline]\n  io_uring_setup+0x1836/0x28e0 fs/io_uring.c:8694\n  do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n  entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\n Freed by task 21:\n  kasan_save_stack+0x1b/0x40 mm/kasan/common.c:48\n  kasan_set_track+0x1c/0x30 mm/kasan/common.c:56\n  kasan_set_free_info+0x1b/0x30 mm/kasan/generic.c:355\n  __kasan_slab_free+0xd8/0x120 mm/kasan/common.c:422\n  __cache_free mm/slab.c:3418 [inline]\n  kfree+0x10e/0x2b0 mm/slab.c:3756\n  __io_wq_destroy fs/io-wq.c:1138 [inline]\n  io_wq_destroy+0x2af/0x460 fs/io-wq.c:1146\n  io_finish_async fs/io_uring.c:6836 [inline]\n  io_ring_ctx_free fs/io_uring.c:7870 [inline]\n  io_ring_exit_work+0x1e4/0x6d0 fs/io_uring.c:7954\n  process_one_work+0x94c/0x1670 kernel/workqueue.c:2269\n  worker_thread+0x64c/0x1120 kernel/workqueue.c:2415\n  kthread+0x3b5/0x4a0 kernel/kthread.c:292\n  ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:294\n\n The buggy address belongs to the object at ffff8882183db000\n  which belongs to the cache kmalloc-1k of size 1024\n The buggy address is located 140 bytes inside of\n  1024-byte region [ffff8882183db000, ffff8882183db400)\n The buggy address belongs to the page:\n page:000000009bada22b refcount:1 mapcount:0 mapping:0000000000000000 index:0x0 pfn:0x2183db\n flags: 0x57ffe0000000200(slab)\n raw: 057ffe0000000200 ffffea0008604c48 ffffea00086a8648 ffff8880aa040700\n raw: 0000000000000000 ffff8882183db000 0000000100000002 0000000000000000\n page dumped because: kasan: bad access detected\n\n Memory state around the buggy address:\n  ffff8882183daf80: ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff\n  ffff8882183db000: fa fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb\n >ffff8882183db080: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb\n                       ^\n  ffff8882183db100: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb\n  ffff8882183db180: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb\n ==================================================================\n\nwhich is down to the comment below,\n\n\t/* all workers gone, wq exit can proceed */\n\tif (!nr_workers && refcount_dec_and_test(&wqe->wq->refs))\n\t\tcomplete(&wqe->wq->done);\n\nbecause there might be multiple cases of wqe in a wq and we would wait\nfor every worker in every wqe to go home before releasing wq's resources\non destroying.\n\nTo that end, rework wq's refcount by making it independent of the tracking\nof workers because after all they are two different things, and keeping\nit balanced when workers come and go. Note the manager kthread, like\nother workers, now holds a grab to wq during its lifetime.\n\nFinally to help destroy wq, check IO_WQ_BIT_EXIT upon creating worker\nand do nothing for exiting wq.\n\nCc: stable@vger.kernel.org # v5.5+\nReported-by: syzbot+45fa0a195b941764e0f0@syzkaller.appspotmail.com\nReported-by: syzbot+9af99580130003da82b1@syzkaller.appspotmail.com\nCc: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Hillf Danton <hdanton@sina.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:34 -0600 io-wq: fix use-after-free in io_wq_worker_running"
    },
    {
        "commit": "dbbe9c642411c359ad0a0e32442eb2e11d3811b5",
        "message": "In most cases we'll specify IORING_SETUP_SQPOLL and run multiple\nio_uring instances in a host. Since all sqthreads are named\n\"io_uring-sq\", it's hard to distinguish the relations between\napplication process and its io_uring sqthread.\nWith this patch, application can get its corresponding sqthread pid\nand cpu through show_fdinfo.\nSteps:\n1. Get io_uring fd first.\n$ ls -l /proc/<pid>/fd | grep -w io_uring\n2. Then get io_uring instance related info, including corresponding\nsqthread pid and cpu.\n$ cat /proc/<pid>/fdinfo/<io_uring_fd>\n\npos:\t0\nflags:\t02000002\nmnt_id:\t13\nSqThread:\t6929\nSqThreadCpu:\t2\nUserFiles:\t1\n    0: testfile\nUserBufs:\t0\nPollList:\n\nSigned-off-by: Joseph Qi <joseph.qi@linux.alibaba.com>\nReviewed-by: Stefano Garzarella <sgarzare@redhat.com>\n[axboe: fixed for new shared SQPOLL]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:34 -0600 io_uring: show sqthread pid and cpu in fdinfo"
    },
    {
        "commit": "af9c1a44f8dee7a958e07977f24ba40e3c770987",
        "message": "We do this for CQ ring wait, in case task_work completions come in. We\nshould do the same in io_uring_register(), to avoid spurious -EINTR\nif the ring quiescing ends up having to process task_work to complete\nthe operation\n\nReported-by: Dan Melnic <dmm@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:34 -0600 io_uring: process task work in io_uring_register()"
    },
    {
        "commit": "91d8f5191e8fe6fc6a87aa5353b36f5a7409fbec",
        "message": "There are a few operations that are offloaded to the worker threads. In\nthis case, we lose process context and end up in kthread context. This\nresults in ios to be not accounted to the issuing cgroup and\nconsequently end up as issued by root. Just like others, adopt the\npersonality of the blkcg too when issuing via the workqueues.\n\nFor the SQPOLL thread, it will live and attach in the inited cgroup's\ncontext.\n\nSigned-off-by: Dennis Zhou <dennis@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:34 -0600 io_uring: add blkcg accounting to offloaded operations"
    },
    {
        "commit": "de2939388be564836b06f0f06b3787bdedaed822",
        "message": "io_uring does account any registered buffer as pinned/locked memory, and\nchecks limit and fails if the given user doesn't have a big enough limit\nto register the ranges specified. However, if huge pages are used, we\nare potentially under-accounting the memory in terms of what gets pinned\non the vm side.\n\nThis patch rectifies that, by ensuring that we account the full size of\na compound page, regardless of how much of it is being registered. Huge\npages are not accounted mulitple times - if multiple sections of a huge\npage is registered, then the page is only accounted once.\n\nReported-by: Andrea Arcangeli <aarcange@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:34 -0600 io_uring: improve registered buffer accounting for huge pages"
    },
    {
        "commit": "14db84110d489276d9c06ea38d59d3ff0ade2ae1",
        "message": "Fixes coccicheck warning:\n\nfs/io_uring.c:4242:13-14: Unneeded semicolon\n\nSigned-off-by: Zheng Bin <zhengbin13@huawei.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:34 -0600 io_uring: remove unneeded semicolon"
    },
    {
        "commit": "e95eee2dee7862f267a169b10d384c82f71010ce",
        "message": "In the spirit of fairness, cap the max number of SQ entries we'll submit\nfor SQPOLL if we have multiple rings. If we don't do that, we could be\nsubmitting tons of entries for one ring, while others are waiting to get\nservice.\n\nThe value of 8 is somewhat arbitrarily chosen as something that allows\na fair bit of batching, without using an excessive time per ring.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:34 -0600 io_uring: cap SQ submit size for SQPOLL with multiple rings"
    },
    {
        "commit": "e8c2bc1fb6c9495b71efe7af476a351ccfba73c4",
        "message": "There's really no point in having this union, it just means that we're\nalways allocating enough room to cater to any command. But that's\npointless, as the ->io field is request type private anyway.\n\nThis gets rid of the io_async_ctx structure, and fills in the required\nsize in the io_op_defs[] instead.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:34 -0600 io_uring: get rid of req->io/io_async_ctx union"
    },
    {
        "commit": "4be1c615126963ade321492283c05fb653f55099",
        "message": "Testing ctx->user_bufs for NULL in io_import_fixed() is not neccessary,\nbecause in that case ctx->nr_user_bufs would be zero, and the following\ncheck would fail.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:34 -0600 io_uring: kill extra user_bufs check"
    },
    {
        "commit": "ab0b196ce5551a093388bb2ceace30df7891051c",
        "message": "When io_req_map_rw() is called from io_rw_prep_async(), it memcpy()\niorw->iter into itself. Even though it doesn't lead to an error, such a\nmemcpy()'s aliasing rules violation is considered to be a bad practise.\n\nInline io_req_map_rw() into io_rw_prep_async(). We don't really need any\nremapping there, so it's much simpler than the generic implementation.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_uring: fix overlapped memcpy in io_req_map_rw()"
    },
    {
        "commit": "afb87658f89b65e067334e06d833ccd859e659e6",
        "message": "Set rw->free_iovec to @iovec, that gives an identical result and stresses\nthat @iovec param rw->free_iovec play the same role.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_uring: refactor io_req_map_rw()"
    },
    {
        "commit": "f4bff104fffba2f069bc1b19ef0decbca7ff5459",
        "message": "Don't touch iter->iov and iov in between __io_import_iovec() and\nio_req_map_rw(), the former function aleady sets it correctly, because it\ncreates one more case with NULL'ed iov to consider in io_req_map_rw().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_uring: simplify io_rw_prep_async()"
    },
    {
        "commit": "90554200724d5b280439dc361fe7ee92fe459ea7",
        "message": "When using SQPOLL, applications can run into the issue of running out of\nSQ ring entries because the thread hasn't consumed them yet. The only\noption for dealing with that is checking later, or busy checking for the\ncondition.\n\nProvide IORING_ENTER_SQ_WAIT if applications want to wait on this\ncondition.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_uring: provide IORING_ENTER_SQ_WAIT for SQPOLL SQ ring waits"
    },
    {
        "commit": "738277adc81929b3e7c9b63fec6693868cc5f931",
        "message": "These structures are never written, move them appropriately.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_uring: mark io_uring_fops/io_op_defs as __read_mostly"
    },
    {
        "commit": "aa06165de863a09bceebef65ecaf19294b26fd2e",
        "message": "We support using IORING_SETUP_ATTACH_WQ to share async backends between\nrings created by the same process, this now also allows the same to\nhappen with SQPOLL. The setup procedure remains the same, the caller\nsets io_uring_params->wq_fd to the 'parent' context, and then the newly\ncreated ring will attach to that async backend.\n\nThis means that multiple rings can share the same SQPOLL thread, saving\nresources.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_uring: enable IORING_SETUP_ATTACH_WQ to attach to SQPOLL thread too"
    },
    {
        "commit": "69fb21310fd36aad96370e05953f2c2366f492e4",
        "message": "Remove the SQPOLL thread from the ctx, and use the io_sq_data as the\ndata structure we pass in. io_sq_data has a list of ctx's that we can\nthen iterate over and handle.\n\nAs of now we're ready to handle multiple ctx's, though we're still just\nhandling a single one after this patch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_uring: base SQPOLL handling off io_sq_data"
    },
    {
        "commit": "534ca6d684f1feaf2edd90e641129725cba7e86d",
        "message": "Move all the necessary state out of io_ring_ctx, and into a new\nstructure, io_sq_data. The latter now deals with any state or\nvariables associated with the SQPOLL thread itself.\n\nIn preparation for supporting more than one io_ring_ctx per SQPOLL\nthread.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_uring: split SQPOLL data into separate structure"
    },
    {
        "commit": "c8d1ba583fe67c6b5e054d89f1433498a924286f",
        "message": "This is done in preparation for handling more than one ctx, but it also\ncleans up the code a bit since io_sq_thread() was a bit too unwieldy to\nget a get overview on.\n\n__io_sq_thread() is now the main handler, and it returns an enum sq_ret\nthat tells io_sq_thread() what it ended up doing. The parent then makes\na decision on idle, spinning, or work handling based on that.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_uring: split work handling part of SQPOLL into helper"
    },
    {
        "commit": "3f0e64d054114b725569c2481bbc6a8eb538bf15",
        "message": "We need to decouple the clearing on wakeup from the the inline schedule,\nas that is going to be required for handling multiple rings in one\nthread.\n\nWrap our wakeup handler so we can clear it when we get the wakeup, by\ndefinition that is when we no longer need the flag set.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_uring: move SQPOLL post-wakeup ring need wakeup flag into wake handler"
    },
    {
        "commit": "6a7793828fb21c1e0c119d98c2f898d8f83c8a8b",
        "message": "This is in preparation to sharing the poller thread between rings. For\nthat we need per-ring wait_queue_entry storage, and we can't easily put\nthat on the stack if one thread is managing multiple rings.\n\nWe'll also be sharing the wait_queue_head across rings for the purposes\nof wakeups, provide the usual private ring wait_queue_head for now but\nmake it a pointer so we can easily override it when sharing.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_uring: use private ctx wait queue entries for SQPOLL"
    },
    {
        "commit": "e35afcf91230238dc27f98d1cd7cb787474b28cb",
        "message": "We're not handling signals by default in kernel threads, and we never\nuse TWA_SIGNAL for the SQPOLL thread internally. Hence we can never\nhave a signal pending, and we don't need to check for it (nor flush it).\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_uring: io_sq_thread() doesn't need to flush signals"
    },
    {
        "commit": "95da84659226d75698a1ab958be0af21d9cc2a9c",
        "message": "During a context switch the scheduler invokes wq_worker_sleeping() with\ndisabled preemption. Disabling preemption is needed because it protects\naccess to `worker->sleeping'. As an optimisation it avoids invoking\nschedule() within the schedule path as part of possible wake up (thus\npreempt_enable_no_resched() afterwards).\n\nThe io-wq has been added to the mix in the same section with disabled\npreemption. This breaks on PREEMPT_RT because io_wq_worker_sleeping()\nacquires a spinlock_t. Also within the schedule() the spinlock_t must be\nacquired after tsk_is_pi_blocked() otherwise it will block on the\nsleeping lock again while scheduling out.\n\nWhile playing with `io_uring-bench' I didn't notice a significant\nlatency spike after converting io_wqe::lock to a raw_spinlock_t. The\nlatency was more or less the same.\n\nIn order to keep the spinlock_t it would have to be moved after the\ntsk_is_pi_blocked() check which would introduce a branch instruction\ninto the hot path.\n\nThe lock is used to maintain the `work_list' and wakes one task up at\nmost.\nShould io_wqe_cancel_pending_work() cause latency spikes, while\nsearching for a specific item, then it would need to drop the lock\nduring iterations.\nrevert_creds() is also invoked under the lock. According to debug\ncred::non_rcu is 0. Otherwise it should be moved outside of the locked\nsection because put_cred_rcu()->free_uid() acquires a sleeping lock.\n\nConvert io_wqe::lock to a raw_spinlock_t.c\n\nSigned-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_wq: Make io_wqe::lock a raw_spinlock_t"
    },
    {
        "commit": "7e84e1c7566a1df470a9e1f49d3db2ce311261a4",
        "message": "This patch adds a new IORING_SETUP_R_DISABLED flag to start the\nrings disabled, allowing the user to register restrictions,\nbuffers, files, before to start processing SQEs.\n\nWhen IORING_SETUP_R_DISABLED is set, SQE are not processed and\nSQPOLL kthread is not started.\n\nThe restrictions registration are allowed only when the rings\nare disable to prevent concurrency issue while processing SQEs.\n\nThe rings can be enabled using IORING_REGISTER_ENABLE_RINGS\nopcode with io_uring_register(2).\n\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Stefano Garzarella <sgarzare@redhat.com>\nReviewed-by: Kees Cook <keescook@chromium.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_uring: allow disabling rings during the creation"
    },
    {
        "commit": "21b55dbc0653018b8cd4513c37cbca303b0f0d50",
        "message": "The new io_uring_register(2) IOURING_REGISTER_RESTRICTIONS opcode\npermanently installs a feature allowlist on an io_ring_ctx.\nThe io_ring_ctx can then be passed to untrusted code with the\nknowledge that only operations present in the allowlist can be\nexecuted.\n\nThe allowlist approach ensures that new features added to io_uring\ndo not accidentally become available when an existing application\nis launched on a newer kernel version.\n\nCurrently is it possible to restrict sqe opcodes, sqe flags, and\nregister opcodes.\n\nIOURING_REGISTER_RESTRICTIONS can only be made once. Afterwards\nit is not possible to change restrictions anymore.\nThis prevents untrusted code from removing restrictions.\n\nSuggested-by: Stefan Hajnoczi <stefanha@redhat.com>\nSigned-off-by: Stefano Garzarella <sgarzare@redhat.com>\nReviewed-by: Kees Cook <keescook@chromium.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_uring: add IOURING_REGISTER_RESTRICTIONS opcode"
    },
    {
        "commit": "9d4a75efa200a31deabe9ba1c941aef697e6bb30",
        "message": "The enumeration allows us to keep track of the last\nio_uring_register(2) opcode available.\n\nBehaviour and opcodes names don't change.\n\nSigned-off-by: Stefano Garzarella <sgarzare@redhat.com>\nReviewed-by: Kees Cook <keescook@chromium.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_uring: use an enumeration for io_uring_register(2) opcodes"
    },
    {
        "commit": "a3ec60054082ca2c2145ba487f9fc4de904e2b03",
        "message": "Now we have a io_uring kernel header, move this definition out of fs.h\nand into io_uring.h where it belongs.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:33 -0600 io_uring: move io_uring_get_socket() into io_uring.h"
    },
    {
        "commit": "9b8284921513fc1ea57d87777283a59b05862f03",
        "message": "If we don't get and assign the namespace for the async work, then certain\npaths just don't work properly (like /dev/stdin, /proc/mounts, etc).\nAnything that references the current namespace of the given task should\nbe assigned for async work on behalf of that task.\n\nCc: stable@vger.kernel.org # v5.5+\nReported-by: Al Viro <viro@zeniv.linux.org.uk>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:32 -0600 io_uring: reference ->nsproxy for file table commands"
    },
    {
        "commit": "0f2122045b946241a9e549c2a76cea54fa58a7ff",
        "message": "Grab actual references to the files_struct. To avoid circular references\nissues due to this, we add a per-task note that keeps track of what\nio_uring contexts a task has used. When the tasks execs or exits its\nassigned files, we cancel requests based on this tracking.\n\nWith that, we can grab proper references to the files table, and no\nlonger need to rely on stashing away ring_fd and ring_file to check\nif the ring_fd may have been closed.\n\nCc: stable@vger.kernel.org # v5.5+\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:32 -0600 io_uring: don't rely on weak ->files references"
    },
    {
        "commit": "e6c8aa9ac33bd7c968af7816240fc081401fddcd",
        "message": "This allows us to selectively flush out pending overflows, depending on\nthe task and/or files_struct being passed in.\n\nNo intended functional changes in this patch.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:32 -0600 io_uring: enable task/files specific overflow flushing"
    },
    {
        "commit": "76e1b6427fd8246376a97e3227049d49188dfb9c",
        "message": "Return whether we found and canceled requests or not. This is in\npreparation for using this information, no functional changes in this\npatch.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:32 -0600 io_uring: return cancelation status from poll/timeout/files handlers"
    },
    {
        "commit": "e3bc8e9dad7f2f83cc807111d4472164c9210153",
        "message": "Sometimes we assign a weak reference to it, sometimes we grab a\nreference to it. Clean this up and make it unconditional, and drop the\nflag related to tracking this state.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:32 -0600 io_uring: unconditionally grab req->task"
    },
    {
        "commit": "2aede0e417db846793c276c7a1bbf7262c8349b0",
        "message": "We can grab a reference to the task instead of stashing away the task\nfiles_struct. This is doable without creating a circular reference\nbetween the ring fd and the task itself.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:32 -0600 io_uring: stash ctx task reference for SQPOLL"
    },
    {
        "commit": "f573d384456b3025d3f8e58b3eafaeeb0f510784",
        "message": "No functional changes in this patch, prep patch for grabbing references\nto the files_struct.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:32 -0600 io_uring: move dropping of files into separate helper"
    },
    {
        "commit": "f3606e3a92ddd36299642c78592fc87609abb1f6",
        "message": "We currently cancel these when the ring exits, and we cancel all of\nthem. This is in preparation for killing only the ones associated\nwith a given task.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:32 -0600 io_uring: allow timeout/poll/files killing to take task into account"
    },
    {
        "commit": "0f078896911fc6a421b0e708e910b99a28f8a0fa",
        "message": "* io_uring-5.9:\n  io_uring: fix async buffered reads when readahead is disabled\n  io_uring: fix potential ABBA deadlock in ->show_fdinfo()\n  io_uring: always delete double poll wait entry on match",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-30 20:32:25 -0600 Merge branch 'io_uring-5.9' into for-5.10/io_uring"
    },
    {
        "commit": "c8d317aa1887b40b188ec3aaa6e9e524333caed1",
        "message": "The async buffered reads feature is not working when readahead is\nturned off. There are two things to concern:\n\n- when doing retry in io_read, not only the IOCB_WAITQ flag but also\n  the IOCB_NOWAIT flag is still set, which makes it goes to would_block\n  phase in generic_file_buffered_read() and then return -EAGAIN. After\n  that, the io-wq thread work is queued, and later doing the async\n  reads in the old way.\n\n- even if we remove IOCB_NOWAIT when doing retry, the feature is still\n  not running properly, since in generic_file_buffered_read() it goes to\n  lock_page_killable() after calling mapping->a_ops->readpage() to do\n  IO, and thus causing process to sleep.\n\nFixes: 1a0a7853b901 (\"mm: support async buffered reads in generic_file_buffered_read()\")\nFixes: 3b2a4439e0ae (\"io_uring: get rid of kiocb_wait_page_queue_init()\")\nSigned-off-by: Hao Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc8",
        "release_date": "2020-09-29 07:54:00 -0600 io_uring: fix async buffered reads when readahead is disabled"
    },
    {
        "commit": "fad8e0de4426a776c9bcb060555e7c09e2d08db6",
        "message": "syzbot reports a potential lock deadlock between the normal IO path and\n->show_fdinfo():\n\n======================================================\nWARNING: possible circular locking dependency detected\n5.9.0-rc6-syzkaller #0 Not tainted\n------------------------------------------------------\nsyz-executor.2/19710 is trying to acquire lock:\nffff888098ddc450 (sb_writers#4){.+.+}-{0:0}, at: io_write+0x6b5/0xb30 fs/io_uring.c:3296\n\nbut task is already holding lock:\nffff8880a11b8428 (&ctx->uring_lock){+.+.}-{3:3}, at: __do_sys_io_uring_enter+0xe9a/0x1bd0 fs/io_uring.c:8348\n\nwhich lock already depends on the new lock.\n\nthe existing dependency chain (in reverse order) is:\n\n-> #2 (&ctx->uring_lock){+.+.}-{3:3}:\n       __mutex_lock_common kernel/locking/mutex.c:956 [inline]\n       __mutex_lock+0x134/0x10e0 kernel/locking/mutex.c:1103\n       __io_uring_show_fdinfo fs/io_uring.c:8417 [inline]\n       io_uring_show_fdinfo+0x194/0xc70 fs/io_uring.c:8460\n       seq_show+0x4a8/0x700 fs/proc/fd.c:65\n       seq_read+0x432/0x1070 fs/seq_file.c:208\n       do_loop_readv_writev fs/read_write.c:734 [inline]\n       do_loop_readv_writev fs/read_write.c:721 [inline]\n       do_iter_read+0x48e/0x6e0 fs/read_write.c:955\n       vfs_readv+0xe5/0x150 fs/read_write.c:1073\n       kernel_readv fs/splice.c:355 [inline]\n       default_file_splice_read.constprop.0+0x4e6/0x9e0 fs/splice.c:412\n       do_splice_to+0x137/0x170 fs/splice.c:871\n       splice_direct_to_actor+0x307/0x980 fs/splice.c:950\n       do_splice_direct+0x1b3/0x280 fs/splice.c:1059\n       do_sendfile+0x55f/0xd40 fs/read_write.c:1540\n       __do_sys_sendfile64 fs/read_write.c:1601 [inline]\n       __se_sys_sendfile64 fs/read_write.c:1587 [inline]\n       __x64_sys_sendfile64+0x1cc/0x210 fs/read_write.c:1587\n       do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n       entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\n-> #1 (&p->lock){+.+.}-{3:3}:\n       __mutex_lock_common kernel/locking/mutex.c:956 [inline]\n       __mutex_lock+0x134/0x10e0 kernel/locking/mutex.c:1103\n       seq_read+0x61/0x1070 fs/seq_file.c:155\n       pde_read fs/proc/inode.c:306 [inline]\n       proc_reg_read+0x221/0x300 fs/proc/inode.c:318\n       do_loop_readv_writev fs/read_write.c:734 [inline]\n       do_loop_readv_writev fs/read_write.c:721 [inline]\n       do_iter_read+0x48e/0x6e0 fs/read_write.c:955\n       vfs_readv+0xe5/0x150 fs/read_write.c:1073\n       kernel_readv fs/splice.c:355 [inline]\n       default_file_splice_read.constprop.0+0x4e6/0x9e0 fs/splice.c:412\n       do_splice_to+0x137/0x170 fs/splice.c:871\n       splice_direct_to_actor+0x307/0x980 fs/splice.c:950\n       do_splice_direct+0x1b3/0x280 fs/splice.c:1059\n       do_sendfile+0x55f/0xd40 fs/read_write.c:1540\n       __do_sys_sendfile64 fs/read_write.c:1601 [inline]\n       __se_sys_sendfile64 fs/read_write.c:1587 [inline]\n       __x64_sys_sendfile64+0x1cc/0x210 fs/read_write.c:1587\n       do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n       entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\n-> #0 (sb_writers#4){.+.+}-{0:0}:\n       check_prev_add kernel/locking/lockdep.c:2496 [inline]\n       check_prevs_add kernel/locking/lockdep.c:2601 [inline]\n       validate_chain kernel/locking/lockdep.c:3218 [inline]\n       __lock_acquire+0x2a96/0x5780 kernel/locking/lockdep.c:4441\n       lock_acquire+0x1f3/0xaf0 kernel/locking/lockdep.c:5029\n       percpu_down_read include/linux/percpu-rwsem.h:51 [inline]\n       __sb_start_write+0x228/0x450 fs/super.c:1672\n       io_write+0x6b5/0xb30 fs/io_uring.c:3296\n       io_issue_sqe+0x18f/0x5c50 fs/io_uring.c:5719\n       __io_queue_sqe+0x280/0x1160 fs/io_uring.c:6175\n       io_queue_sqe+0x692/0xfa0 fs/io_uring.c:6254\n       io_submit_sqe fs/io_uring.c:6324 [inline]\n       io_submit_sqes+0x1761/0x2400 fs/io_uring.c:6521\n       __do_sys_io_uring_enter+0xeac/0x1bd0 fs/io_uring.c:8349\n       do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n       entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nother info that might help us debug this:\n\nChain exists of:\n  sb_writers#4 --> &p->lock --> &ctx->uring_lock\n\n Possible unsafe locking scenario:\n\n       CPU0                    CPU1\n       ----                    ----\n  lock(&ctx->uring_lock);\n                               lock(&p->lock);\n                               lock(&ctx->uring_lock);\n  lock(sb_writers#4);\n\n *** DEADLOCK ***\n\n1 lock held by syz-executor.2/19710:\n #0: ffff8880a11b8428 (&ctx->uring_lock){+.+.}-{3:3}, at: __do_sys_io_uring_enter+0xe9a/0x1bd0 fs/io_uring.c:8348\n\nstack backtrace:\nCPU: 0 PID: 19710 Comm: syz-executor.2 Not tainted 5.9.0-rc6-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nCall Trace:\n __dump_stack lib/dump_stack.c:77 [inline]\n dump_stack+0x198/0x1fd lib/dump_stack.c:118\n check_noncircular+0x324/0x3e0 kernel/locking/lockdep.c:1827\n check_prev_add kernel/locking/lockdep.c:2496 [inline]\n check_prevs_add kernel/locking/lockdep.c:2601 [inline]\n validate_chain kernel/locking/lockdep.c:3218 [inline]\n __lock_acquire+0x2a96/0x5780 kernel/locking/lockdep.c:4441\n lock_acquire+0x1f3/0xaf0 kernel/locking/lockdep.c:5029\n percpu_down_read include/linux/percpu-rwsem.h:51 [inline]\n __sb_start_write+0x228/0x450 fs/super.c:1672\n io_write+0x6b5/0xb30 fs/io_uring.c:3296\n io_issue_sqe+0x18f/0x5c50 fs/io_uring.c:5719\n __io_queue_sqe+0x280/0x1160 fs/io_uring.c:6175\n io_queue_sqe+0x692/0xfa0 fs/io_uring.c:6254\n io_submit_sqe fs/io_uring.c:6324 [inline]\n io_submit_sqes+0x1761/0x2400 fs/io_uring.c:6521\n __do_sys_io_uring_enter+0xeac/0x1bd0 fs/io_uring.c:8349\n do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\nRIP: 0033:0x45e179\nCode: 3d b2 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 0b b2 fb ff c3 66 2e 0f 1f 84 00 00 00 00\nRSP: 002b:00007f1194e74c78 EFLAGS: 00000246 ORIG_RAX: 00000000000001aa\nRAX: ffffffffffffffda RBX: 00000000000082c0 RCX: 000000000045e179\nRDX: 0000000000000000 RSI: 0000000000000001 RDI: 0000000000000004\nRBP: 000000000118cf98 R08: 0000000000000000 R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000246 R12: 000000000118cf4c\nR13: 00007ffd1aa5756f R14: 00007f1194e759c0 R15: 000000000118cf4c\n\nFix this by just not diving into details if we fail to trylock the\nio_uring mutex. We know the ctx isn't going away during this operation,\nbut we cannot safely iterate buffers/files/personalities if we don't\nhold the io_uring mutex.\n\nReported-by: syzbot+2f8fa4e860edc3066aba@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc8",
        "release_date": "2020-09-28 09:06:08 -0600 io_uring: fix potential ABBA deadlock in ->show_fdinfo()"
    },
    {
        "commit": "8706e04ed7d6c95004d42b22a4db97d5b2eb73b2",
        "message": "syzbot reports a crash with tty polling, which is using the double poll\nhandling:\n\ngeneral protection fault, probably for non-canonical address 0xdffffc0000000009: 0000 [#1] PREEMPT SMP KASAN\nKASAN: null-ptr-deref in range [0x0000000000000048-0x000000000000004f]\nCPU: 0 PID: 6874 Comm: syz-executor749 Not tainted 5.9.0-rc6-next-20200924-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nRIP: 0010:io_poll_get_single fs/io_uring.c:4778 [inline]\nRIP: 0010:io_poll_double_wake+0x51/0x510 fs/io_uring.c:4845\nCode: fc ff df 48 c1 ea 03 80 3c 02 00 0f 85 9e 03 00 00 48 b8 00 00 00 00 00 fc ff df 49 8b 5d 08 48 8d 7b 48 48 89 fa 48 c1 ea 03 <0f> b6 04 02 84 c0 74 06 0f 8e 63 03 00 00 0f b6 6b 48 bf 06 00 00\nRSP: 0018:ffffc90001c1fb70 EFLAGS: 00010006\nRAX: dffffc0000000000 RBX: 0000000000000000 RCX: 0000000000000004\nRDX: 0000000000000009 RSI: ffffffff81d9b3ad RDI: 0000000000000048\nRBP: dffffc0000000000 R08: ffff8880a3cac798 R09: ffffc90001c1fc60\nR10: fffff52000383f73 R11: 0000000000000000 R12: 0000000000000004\nR13: ffff8880a3cac798 R14: ffff8880a3cac7a0 R15: 0000000000000004\nFS:  0000000001f98880(0000) GS:ffff8880ae400000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00007f18886916c0 CR3: 0000000094c5a000 CR4: 00000000001506f0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\nCall Trace:\n __wake_up_common+0x147/0x650 kernel/sched/wait.c:93\n __wake_up_common_lock+0xd0/0x130 kernel/sched/wait.c:123\n tty_ldisc_hangup+0x1cf/0x680 drivers/tty/tty_ldisc.c:735\n __tty_hangup.part.0+0x403/0x870 drivers/tty/tty_io.c:625\n __tty_hangup drivers/tty/tty_io.c:575 [inline]\n tty_vhangup+0x1d/0x30 drivers/tty/tty_io.c:698\n pty_close+0x3f5/0x550 drivers/tty/pty.c:79\n tty_release+0x455/0xf60 drivers/tty/tty_io.c:1679\n __fput+0x285/0x920 fs/file_table.c:281\n task_work_run+0xdd/0x190 kernel/task_work.c:141\n tracehook_notify_resume include/linux/tracehook.h:188 [inline]\n exit_to_user_mode_loop kernel/entry/common.c:165 [inline]\n exit_to_user_mode_prepare+0x1e2/0x1f0 kernel/entry/common.c:192\n syscall_exit_to_user_mode+0x7a/0x2c0 kernel/entry/common.c:267\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\nRIP: 0033:0x401210\n\nwhich is due to a failure in removing the double poll wait entry if we\nhit a wakeup match. This can cause multiple invocations of the wakeup,\nwhich isn't safe.\n\nCc: stable@vger.kernel.org # v5.8\nReported-by: syzbot+81b3883093f772addf6d@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc8",
        "release_date": "2020-09-28 08:38:54 -0600 io_uring: always delete double poll wait entry on match"
    },
    {
        "commit": "692495baa377e373cc9cd930af03e9b8b77eacdf",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two fixes for regressions in this cycle, and one that goes to 5.8\n  stable:\n\n   - fix leak of getname() retrieved filename\n\n   - remove plug->nowait assignment, fixing a regression with btrfs\n\n   - fix for async buffered retry\"\n\n* tag 'io_uring-5.9-2020-09-25' of git://git.kernel.dk/linux-block:\n  io_uring: ensure async buffered read-retry is setup properly\n  io_uring: don't unconditionally set plug->nowait = true\n  io_uring: ensure open/openat2 name is cleaned on cancelation",
        "kernel_version": "v5.9-rc7",
        "release_date": "2020-09-26 11:13:51 -0700 Merge tag 'io_uring-5.9-2020-09-25' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "abc7220b2233df9b13d8e0e312fb6f31e5402d7d",
        "message": "Tag fix up for TI serdes mux definition introduced in 5.9\n\n* tag 'ti-k3-dt-fixes-for-v5.9' of git://git.kernel.org/pub/scm/linux/kernel/git/nmenon/linux: (637 commits)\n  arm64: dts: ti: k3-j721e: Rename mux header and update macro names\n  Linux 5.9-rc3\n  genirq/matrix: Deal with the sillyness of for_each_cpu() on UP\n  fsldma: fix very broken 32-bit ppc ioread64 functionality\n  kernel.h: Silence sparse warning in lower_32_bits\n  cifs: fix check of tcon dfs in smb1\n  KVM: arm64: Set HCR_EL2.PTW to prevent AT taking synchronous exception\n  KVM: arm64: Survive synchronous exceptions caused by AT instructions\n  KVM: arm64: Add kvm_extable for vaxorcism code\n  arm64: vdso32: make vdso32 install conditional\n  arm64: use a common .arch preamble for inline assembly\n  mfd: mfd-core: Ensure disabled devices are ignored without error\n  usb: storage: Add unusual_uas entry for Sony PSZ drives\n  md/raid5: make sure stripe_size as power of two\n  powerpc/32s: Disable VMAP stack which CONFIG_ADB_PMU\n  io_uring: don't bounce block based -EAGAIN retry off task_work\n  io_uring: fix IOPOLL -EAGAIN retries\n  arm64/cpuinfo: Remove unnecessary fallthrough annotation\n  media: dib0700: Fix identation issue in dib8096_set_param_override()\n  hwmon: (gsc-hwmon) Scale temperature to millidegrees\n  ...\n\nLink: https://lore.kernel.org/r/20200921125402.mtwypblhb45a6ssh@akan\nSigned-off-by: Olof Johansson <olof@lixom.net>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-26 10:08:48 -0700 Merge tag 'ti-k3-dt-fixes-for-v5.9' of git://git.kernel.org/pub/scm/linux/kernel/git/nmenon/linux into arm/fixes"
    },
    {
        "commit": "f38c7e3abfba9a9e180b34f642254c43782e7ffe",
        "message": "A previous commit for fixing up short reads botched the async retry\npath, so we ended up going to worker threads more often than we should.\nFix this up, so retries work the way they originally were intended to.\n\nFixes: 227c0c9673d8 (\"io_uring: internally retry short reads\")\nReported-by: Hao_Xu <haoxu@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc7",
        "release_date": "2020-09-25 15:39:13 -0600 io_uring: ensure async buffered read-retry is setup properly"
    },
    {
        "commit": "62c774ed483174b994c79e0c9f596b315f1ddfaf",
        "message": "This causes all the bios to be submitted with REQ_NOWAIT, which can be\nproblematic on either btrfs or on file systems that otherwise use a mix\nof block devices where only some of them support it.\n\nFor now, just remove the setting of plug->nowait = true.\n\nReported-by: Dan Melnic <dmm@fb.com>\nReported-by: Brian Foster <bfoster@redhat.com>\nFixes: b63534c41e20 (\"io_uring: re-issue block requests that failed because of resources\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc7",
        "release_date": "2020-09-25 09:01:53 -0600 io_uring: don't unconditionally set plug->nowait = true"
    },
    {
        "commit": "f3cd4850504ff612d0ea77a0aaf29b66c98fcefe",
        "message": "If we cancel these requests, we'll leak the memory associated with the\nfilename. Add them to the table of ops that need cleaning, if\nREQ_F_NEED_CLEANUP is set.\n\nCc: stable@vger.kernel.org\nFixes: e62753e4e292 (\"io_uring: call statx directly\")\nReviewed-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc7",
        "release_date": "2020-09-25 07:41:46 -0600 io_uring: ensure open/openat2 name is cleaned on cancelation"
    },
    {
        "commit": "0baca070068c58b95e342881d9da4840d5cf3bd1",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A few fixes - most of them regression fixes from this cycle, but also\n  a few stable heading fixes, and a build fix for the included demo tool\n  since some systems now actually have gettid() available\"\n\n* tag 'io_uring-5.9-2020-09-22' of git://git.kernel.dk/linux-block:\n  io_uring: fix openat/openat2 unified prep handling\n  io_uring: mark statx/files_update/epoll_ctl as non-SQPOLL\n  tools/io_uring: fix compile breakage\n  io_uring: don't use retry based buffered reads for non-async bdev\n  io_uring: don't re-setup vecs/iter in io_resumit_prep() is already there\n  io_uring: don't run task work on an exiting task\n  io_uring: drop 'ctx' ref on task work cancelation\n  io_uring: grab any needed state during defer prep",
        "kernel_version": "v5.9-rc7",
        "release_date": "2020-09-22 14:36:50 -0700 Merge tag 'io_uring-5.9-2020-09-22' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "4eb8dded6b82e184c09bb963bea0335fa3f30b55",
        "message": "A previous commit unified how we handle prep for these two functions,\nbut this means that we check the allowed context (SQPOLL, specifically)\nlater than we should. Move the ring type checking into the two parent\nfunctions, instead of doing it after we've done some setup work.\n\nFixes: ec65fea5a8d7 (\"io_uring: deduplicate io_openat{,2}_prep()\")\nReported-by: Andy Lutomirski <luto@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc7",
        "release_date": "2020-09-21 07:51:03 -0600 io_uring: fix openat/openat2 unified prep handling"
    },
    {
        "commit": "6ca56f845955e325033758f90a2cffe150f31bc8",
        "message": "These will naturally fail when attempted through SQPOLL, but either\nwith -EFAULT or -EBADF. Make it explicit that these are not workable\nthrough SQPOLL and return -EINVAL, just like other ops that need to\nuse ->files.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc7",
        "release_date": "2020-09-21 07:51:00 -0600 io_uring: mark statx/files_update/epoll_ctl as non-SQPOLL"
    },
    {
        "commit": "72f04da48a9828ba3ae8ac77bea648bda8b7d0ff",
        "message": "It would seem none of the kernel continuous integration does this:\n    $ cd tools/io_uring\n    $ make\n\nOtherwise it may have noticed:\n   cc -Wall -Wextra -g -D_GNU_SOURCE   -c -o io_uring-bench.o\n\t io_uring-bench.c\nio_uring-bench.c:133:12: error: static declaration of \u2018gettid\u2019\n\t follows non-static declaration\n  133 | static int gettid(void)\n      |            ^~~~~~\nIn file included from /usr/include/unistd.h:1170,\n                 from io_uring-bench.c:27:\n/usr/include/x86_64-linux-gnu/bits/unistd_ext.h:34:16: note:\n\t previous declaration of \u2018gettid\u2019 was here\n   34 | extern __pid_t gettid (void) __THROW;\n      |                ^~~~~~\nmake: *** [<builtin>: io_uring-bench.o] Error 1\n\nThe problem on Ubuntu 20.04 (with lk 5.9.0-rc5) is that unistd.h\nalready defines gettid(). So prefix the local definition with\n\"lk_\".\n\nSigned-off-by: Douglas Gilbert <dgilbert@interlog.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc7",
        "release_date": "2020-09-21 07:50:58 -0600 tools/io_uring: fix compile breakage"
    },
    {
        "commit": "f5cac8b156e8b7b67bb0fdfd19900855bf9569f3",
        "message": "Some block devices, like dm, bubble back -EAGAIN through the completion\nhandler. We check for this in io_read(), but don't honor it for when\nwe have copied the iov. Return -EAGAIN for this case before retrying,\nto force punt to io-wq.\n\nFixes: bcf5a06304d6 (\"io_uring: support true async buffered reads, if file provides it\")\nReported-by: Zorro Lang <zlang@redhat.com>\nTested-by: Zorro Lang <zlang@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc7",
        "release_date": "2020-09-21 07:50:56 -0600 io_uring: don't use retry based buffered reads for non-async bdev"
    },
    {
        "commit": "8f3d749685e48c44dbe877ac9781079d85f914c8",
        "message": "If we already have mapped the necessary data for retry, then don't set\nit up again. It's a pointless operation, and we leak the iovec if it's\na large (non-stack) vec.\n\nFixes: b63534c41e20 (\"io_uring: re-issue block requests that failed because of resources\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc7",
        "release_date": "2020-09-21 07:50:54 -0600 io_uring: don't re-setup vecs/iter in io_resumit_prep() is already there"
    },
    {
        "commit": "a665eec0a22e11cdde708c1c256a465ebe768047",
        "message": "Commit 0cef77c7798a7 (\"powerpc/64s/radix: flush remote CPUs out of\nsingle-threaded mm_cpumask\") added a mechanism to trim the mm_cpumask of\na process under certain conditions. One of the assumptions is that\nmm_users would not be incremented via a reference outside the process\ncontext with mmget_not_zero() then go on to kthread_use_mm() via that\nreference.\n\nThat invariant was broken by io_uring code (see previous sparc64 fix),\nbut I'll point Fixes: to the original powerpc commit because we are\nchanging that assumption going forward, so this will make backports\nmatch up.\n\nFix this by no longer relying on that assumption, but by having each CPU\ncheck the mm is not being used, and clearing their own bit from the mask\nonly if it hasn't been switched-to by the time the IPI is processed.\n\nThis relies on commit 38cf307c1f20 (\"mm: fix kthread_use_mm() vs TLB\ninvalidate\") and ARCH_WANT_IRQS_OFF_ACTIVATE_MM to disable irqs over mm\nswitch sequences.\n\nFixes: 0cef77c7798a7 (\"powerpc/64s/radix: flush remote CPUs out of single-threaded mm_cpumask\")\nSigned-off-by: Nicholas Piggin <npiggin@gmail.com>\nReviewed-by: Michael Ellerman <mpe@ellerman.id.au>\nDepends-on: 38cf307c1f20 (\"mm: fix kthread_use_mm() vs TLB invalidate\")\nSigned-off-by: Michael Ellerman <mpe@ellerman.id.au>\nLink: https://lore.kernel.org/r/20200914045219.3736466-5-npiggin@gmail.com",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-16 12:24:37 +1000 powerpc/64s/radix: Fix mm_cpumask trimming race vs kthread_use_mm"
    },
    {
        "commit": "bafb056ce27940c9994ea905336aa8f27b4f7275",
        "message": "The de facto (and apparently uncommented) standard for using an mm had,\nthanks to this code in sparc if nothing else, been that you must have a\nreference on mm_users *and that reference must have been obtained with\nmmget()*, i.e., from a thread with a reference to mm_users that had used\nthe mm.\n\nThe introduction of mmget_not_zero() in commit d2005e3f41d4\n(\"userfaultfd: don't pin the user memory in userfaultfd_file_create()\")\nallowed mm_count holders to aoperate on user mappings asynchronously\nfrom the actual threads using the mm, but they were not to load those\nmappings into their TLB (i.e., walking vmas and page tables is okay,\nkthread_use_mm() is not).\n\nio_uring 2b188cc1bb857 (\"Add io_uring IO interface\") added code which\ndoes a kthread_use_mm() from a mmget_not_zero() refcount.\n\nThe problem with this is code which previously assumed mm == current->mm\nand mm->mm_users == 1 implies the mm will remain single-threaded at\nleast until this thread creates another mm_users reference, has now\nbroken.\n\narch/sparc/kernel/smp_64.c:\n\n    if (atomic_read(&mm->mm_users) == 1) {\n        cpumask_copy(mm_cpumask(mm), cpumask_of(cpu));\n        goto local_flush_and_out;\n    }\n\nvs fs/io_uring.c\n\n    if (unlikely(!(ctx->flags & IORING_SETUP_SQPOLL) ||\n                 !mmget_not_zero(ctx->sqo_mm)))\n        return -EFAULT;\n    kthread_use_mm(ctx->sqo_mm);\n\nmmget_not_zero() could come in right after the mm_users == 1 test, then\nkthread_use_mm() which sets its CPU in the mm_cpumask. That update could\nbe lost if cpumask_copy() occurs afterward.\n\nI propose we fix this by allowing mmget_not_zero() to be a first-class\nreference, and not have this obscure undocumented and unchecked\nrestriction.\n\nThe basic fix for sparc64 is to remove its mm_cpumask clearing code. The\noptimisation could be effectively restored by sending IPIs to mm_cpumask\nmembers and having them remove themselves from mm_cpumask. This is more\ntricky so I leave it as an exercise for someone with a sparc64 SMP.\npowerpc has a (currently similarly broken) example.\n\nSigned-off-by: Nicholas Piggin <npiggin@gmail.com>\nAcked-by: David S. Miller <davem@davemloft.net>\nSigned-off-by: Michael Ellerman <mpe@ellerman.id.au>\nLink: https://lore.kernel.org/r/20200914045219.3736466-4-npiggin@gmail.com",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-16 12:24:37 +1000 sparc64: remove mm_cpumask clearing to fix kthread_use_mm race"
    },
    {
        "commit": "6200b0ae4ea28a4bfd8eb434e33e6201b7a6a282",
        "message": "This isn't safe, and isn't needed either. We are guaranteed that any\nwork we queue is on a live task (and will be run), or it goes to\nour backup io-wq threads if the task is exiting.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc7",
        "release_date": "2020-09-14 10:22:15 -0600 io_uring: don't run task work on an exiting task"
    },
    {
        "commit": "87ceb6a6b81eca000911403446d4c6043b4e4f82",
        "message": "If task_work ends up being marked for cancelation, we go through a\ncancelation helper instead of the queue path. In converting task_work to\nalways hold a ctx reference, this path was missed. Make sure that\nio_req_task_cancel() puts the reference that is being held against the\nctx.\n\nFixes: 6d816e088c35 (\"io_uring: hold 'ctx' reference around task_work queue + execute\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc7",
        "release_date": "2020-09-14 10:22:14 -0600 io_uring: drop 'ctx' ref on task work cancelation"
    },
    {
        "commit": "202700e18acbed55970dbb9d4d518ac59b1172c8",
        "message": "Always grab work environment for deferred links. The assumption that we\nwill be running it always from the task in question is false, as exiting\ntasks may mean that we're deferring this one to a thread helper. And at\nthat point it's too late to grab the work environment.\n\nFixes: debb85f496c9 (\"io_uring: factor out grab_env() from defer_prep()\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc7",
        "release_date": "2020-09-13 14:47:06 -0600 io_uring: grab any needed state during defer prep"
    },
    {
        "commit": "d13ee586e026e8166231e03e5345718ebf625c02",
        "message": "Linux 5.9-rc4\n\n* tag 'v5.9-rc4': (1001 commits)\n  Linux 5.9-rc4\n  io_uring: fix linked deferred ->files cancellation\n  io_uring: fix cancel of deferred reqs with ->files\n  include/linux/log2.h: add missing () around n in roundup_pow_of_two()\n  mm/khugepaged.c: fix khugepaged's request size in collapse_file\n  mm/hugetlb: fix a race between hugetlb sysctl handlers\n  mm/hugetlb: try preferred node first when alloc gigantic page from cma\n  mm/migrate: preserve soft dirty in remove_migration_pte()\n  mm/migrate: remove unnecessary is_zone_device_page() check\n  mm/rmap: fixup copying of soft dirty and uffd ptes\n  mm/migrate: fixup setting UFFD_WP flag\n  mm: madvise: fix vma user-after-free\n  checkpatch: fix the usage of capture group ( ... )\n  fork: adjust sysctl_max_threads definition to match prototype\n  ipc: adjust proc_ipc_sem_dointvec definition to match prototype\n  mm: track page table modifications in __apply_to_page_range()\n  MAINTAINERS: IA64: mark Status as Odd Fixes only\n  MAINTAINERS: add LLVM maintainers\n  MAINTAINERS: update Cavium/Marvell entries\n  mm: slub: fix conversion of freelist_corrupted()\n  ...",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-09-07 16:13:06 +0200 Merge tag 'v5.9-rc4' into patchwork"
    },
    {
        "commit": "a8205e310011f09cc73cd577d7b0074c57b9bb54",
        "message": "Pull more io_uring fixes from Jens Axboe:\n \"Two followup fixes. One is fixing a regression from this merge window,\n  the other is two commits fixing cancelation of deferred requests.\n\n  Both have gone through full testing, and both spawned a few new\n  regression test additions to liburing.\n\n   - Don't play games with const, properly store the output iovec and\n     assign it as needed.\n\n   - Deferred request cancelation fix (Pavel)\"\n\n* tag 'io_uring-5.9-2020-09-06' of git://git.kernel.dk/linux-block:\n  io_uring: fix linked deferred ->files cancellation\n  io_uring: fix cancel of deferred reqs with ->files\n  io_uring: fix explicit async read/write mapping for large segments",
        "kernel_version": "v5.9-rc4",
        "release_date": "2020-09-06 12:10:27 -0700 Merge tag 'io_uring-5.9-2020-09-06' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "c127a2a1b7baa5eb40a7e2de4b7f0c51ccbbb2ef",
        "message": "While looking for ->files in ->defer_list, consider that requests there\nmay actually be links.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc4",
        "release_date": "2020-09-05 16:02:42 -0600 io_uring: fix linked deferred ->files cancellation"
    },
    {
        "commit": "b7ddce3cbf010edbfac6c6d8cc708560a7bcd7a4",
        "message": "While trying to cancel requests with ->files, it also should look for\nrequests in ->defer_list, otherwise it might end up hanging a thread.\n\nCancel all requests in ->defer_list up to the last request there with\nmatching ->files, that's needed to follow drain ordering semantics.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc4",
        "release_date": "2020-09-05 15:59:51 -0600 io_uring: fix cancel of deferred reqs with ->files"
    },
    {
        "commit": "c183edff33fdcd639d222a8f473bf44602adc655",
        "message": "If we exceed UIO_FASTIOV, we don't handle the transition correctly\nbetween an allocated vec for requests that are queued with IOSQE_ASYNC.\nStore the iovec appropriately and re-set it in the iter iov in case\nit changed.\n\nFixes: ff6165b2d7f6 (\"io_uring: retain iov_iter state over io_read/io_write calls\")\nReported-by: Nick Hill <nick@nickhill.org>\nTested-by: Norman Maurer <norman.maurer@googlemail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc4",
        "release_date": "2020-09-05 09:02:47 -0600 io_uring: fix explicit async read/write mapping for large segments"
    },
    {
        "commit": "d849ca483dba7546ad176da83bf66d1c013725f6",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - EAGAIN with O_NONBLOCK retry fix\n\n - Two small fixes for registered files (Jiufei)\n\n* tag 'io_uring-5.9-2020-09-04' of git://git.kernel.dk/linux-block:\n  io_uring: no read/write-retry on -EAGAIN error and O_NONBLOCK marked file\n  io_uring: set table->files[i] to NULL when io_sqe_file_register failed\n  io_uring: fix removing the wrong file in __io_sqe_files_update()",
        "kernel_version": "v5.9-rc4",
        "release_date": "2020-09-04 12:55:22 -0700 Merge tag 'io_uring-5.9-2020-09-04' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "355afaeb578abac907217c256a844cfafb0337b2",
        "message": "Actually two things that need fixing up here:\n\n- The io_rw_reissue() -EAGAIN retry is explicit to block devices and\n  regular files, so don't ever attempt to do that on other types of\n  files.\n\n- If we hit -EAGAIN on a nonblock marked file, don't arm poll handler for\n  it. It should just complete with -EAGAIN.\n\nCc: stable@vger.kernel.org\nReported-by: Norman Maurer <norman.maurer@googlemail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc4",
        "release_date": "2020-09-02 10:20:41 -0600 io_uring: no read/write-retry on -EAGAIN error and O_NONBLOCK marked file"
    },
    {
        "commit": "95d1c8e5f801e959a89181a2548a3efa60a1a6ce",
        "message": "While io_sqe_file_register() failed in __io_sqe_files_update(),\ntable->files[i] still point to the original file which may freed\nsoon, and that will trigger use-after-free problems.\n\nCc: stable@vger.kernel.org\nFixes: f3bd9dae3708 (\"io_uring: fix memleak in __io_sqe_files_update()\")\nSigned-off-by: Jiufei Xue <jiufei.xue@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc4",
        "release_date": "2020-09-02 09:11:59 -0600 io_uring: set table->files[i] to NULL when io_sqe_file_register failed"
    },
    {
        "commit": "98dfd5024a2e9e170b85c07078e2d89f20a5dfbd",
        "message": "Index here is already the position of the file in fixed_file_table, we\nshould not use io_file_from_index() again to get it. Otherwise, the\nwrong file which still in use may be released unexpectedly.\n\nCc: stable@vger.kernel.org # v5.6\nFixes: 05f3fb3c5397 (\"io_uring: avoid ring quiesce for fixed file set unregister and update\")\nSigned-off-by: Jiufei Xue <jiufei.xue@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc4",
        "release_date": "2020-09-01 08:04:58 -0600 io_uring: fix removing the wrong file in __io_sqe_files_update()"
    },
    {
        "commit": "24148d8648e37f8c15bedddfa50d14a31a0582c5",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A few fixes in here, all based on reports and test cases from folks\n  using it. Most of it is stable material as well:\n\n   - Hashed work cancelation fix (Pavel)\n\n   - poll wakeup signalfd fix\n\n   - memlock accounting fix\n\n   - nonblocking poll retry fix\n\n   - ensure we never return -ERESTARTSYS for reads\n\n   - ensure offset == -1 is consistent with preadv2() as documented\n\n   - IOPOLL -EAGAIN handling fixes\n\n   - remove useless task_work bounce for block based -EAGAIN retry\"\n\n* tag 'io_uring-5.9-2020-08-28' of git://git.kernel.dk/linux-block:\n  io_uring: don't bounce block based -EAGAIN retry off task_work\n  io_uring: fix IOPOLL -EAGAIN retries\n  io_uring: clear req->result on IOPOLL re-issue\n  io_uring: make offset == -1 consistent with preadv2/pwritev2\n  io_uring: ensure read requests go through -ERESTART* transformation\n  io_uring: don't use poll handler if file can't be nonblocking read/written\n  io_uring: fix imbalanced sqo_mm accounting\n  io_uring: revert consumed iov_iter bytes on error\n  io-wq: fix hang after cancelling pending hashed work\n  io_uring: don't recurse on tsk->sighand->siglock with signalfd",
        "kernel_version": "v5.9-rc3",
        "release_date": "2020-08-28 16:23:16 -0700 Merge tag 'io_uring-5.9-2020-08-28' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "fdee946d0925f971f167d2606984426763355e4f",
        "message": "These events happen inline from submission, so there's no need to\nbounce them through the original task. Just set them up for retry\nand issue retry directly instead of going over task_work.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc3",
        "release_date": "2020-08-27 16:48:34 -0600 io_uring: don't bounce block based -EAGAIN retry off task_work"
    },
    {
        "commit": "eefdf30f3dcb5c1d47bee2b3afdb9d4d05343ff3",
        "message": "This normally isn't hit, as polling is mostly done on NVMe with deep\nqueue depths. But if we do run into request starvation, we need to\nensure that retries are properly serialized.\n\nReported-by: Andres Freund <andres@anarazel.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc3",
        "release_date": "2020-08-27 16:40:29 -0600 io_uring: fix IOPOLL -EAGAIN retries"
    },
    {
        "commit": "56450c20fe10d4d93f58019109aa4e06fc0b9206",
        "message": "Make sure we clear req->result, which was set to -EAGAIN for retry\npurposes, when moving it to the reissue list. Otherwise we can end up\nretrying a request more than once, which leads to weird results in\nthe io-wq handling (and other spots).\n\nCc: stable@vger.kernel.org\nReported-by: Andres Freund <andres@anarazel.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc3",
        "release_date": "2020-08-26 18:58:26 -0600 io_uring: clear req->result on IOPOLL re-issue"
    },
    {
        "commit": "0fef948363f62494d779cf9dc3c0a86ea1e5f7cd",
        "message": "The man page for io_uring generally claims were consistent with what\npreadv2 and pwritev2 accept, but turns out there's a slight discrepancy\nin how offset == -1 is handled for pipes/streams. preadv doesn't allow\nit, but preadv2 does. This currently causes io_uring to return -EINVAL\nif that is attempted, but we should allow that as documented.\n\nThis change makes us consistent with preadv2/pwritev2 for just passing\nin a NULL ppos for streams if the offset is -1.\n\nCc: stable@vger.kernel.org # v5.7+\nReported-by: Benedikt Ames <wisp3rwind@posteo.eu>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc3",
        "release_date": "2020-08-26 10:36:20 -0600 io_uring: make offset == -1 consistent with preadv2/pwritev2"
    },
    {
        "commit": "00d23d516e2e7900cd1bd577c1f84794ae7ff3a7",
        "message": "We need to call kiocb_done() for any ret < 0 to ensure that we always\nget the proper -ERESTARTSYS (and friends) transformation done.\n\nAt some point this should be tied into general error handling, so we\ncan get rid of the various (mostly network) related commands that check\nand perform this substitution.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc3",
        "release_date": "2020-08-25 12:59:22 -0600 io_uring: ensure read requests go through -ERESTART* transformation"
    },
    {
        "commit": "9dab14b81807a40dab8e464ec87043935c562c2c",
        "message": "There's no point in using the poll handler if we can't do a nonblocking\nIO attempt of the operation, since we'll need to go async anyway. In\nfact this is actively harmful, as reading from eg pipes won't return 0\nto indicate EOF.\n\nCc: stable@vger.kernel.org # v5.7+\nReported-by: Benedikt Ames <wisp3rwind@posteo.eu>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc3",
        "release_date": "2020-08-25 12:27:50 -0600 io_uring: don't use poll handler if file can't be nonblocking read/written"
    },
    {
        "commit": "6b7898eb180df12767933466b7855b23103ad489",
        "message": "We do the initial accounting of locked_vm and pinned_vm before we have\nsetup ctx->sqo_mm, which means we can end up having not accounted the\nmemory at setup time, but still decrement it when we exit. This causes\nan imbalance in the accounting.\n\nSetup ctx->sqo_mm earlier in io_uring_create(), before we do the first\naccounting of mm->{locked,pinned}_vm. This also unifies the state\ngrabbing for the ctx, and eliminates a failure case in\nio_sq_offload_start().\n\nFixes: f74441e6311a (\"io_uring: account locked memory before potential error case\")\nReported-by: Robert M. Muncrief <rmuncrief@humanavance.com>\nReported-by: Niklas Schnelle <schnelle@linux.ibm.com>\nTested-by: Niklas Schnelle <schnelle@linux.ibm.com>\nTested-by: Robert M. Muncrief <rmuncrief@humanavance.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc3",
        "release_date": "2020-08-25 12:05:57 -0600 io_uring: fix imbalanced sqo_mm accounting"
    },
    {
        "commit": "842163154b87b01d8f516af15ad8916eb1661016",
        "message": "Some consumers of the iov_iter will return an error, but still have\nbytes consumed in the iterator. This is an issue for -EAGAIN, since we\nrely on a sane iov_iter state across retries.\n\nFix this by ensuring that we revert consumed bytes, if any, if the file\noperations have consumed any bytes from iterator. This is similar to what\ngeneric_file_read_iter() does, and is always safe as we have the previous\nbytes count handy already.\n\nFixes: ff6165b2d7f6 (\"io_uring: retain iov_iter state over io_read/io_write calls\")\nReported-by: Dmitry Shulyak <yashulyak@gmail.com>\nReviewed-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc3",
        "release_date": "2020-08-25 07:28:43 -0600 io_uring: revert consumed iov_iter bytes on error"
    },
    {
        "commit": "901341bb9718884f337a92e6c4541227597d1f48",
        "message": "Currently, io_uring's recvmsg subscribes to both POLLERR and POLLIN. In\nthe context of TCP tx zero-copy, this is inefficient since we are only\nreading the error queue and not using recvmsg to read POLLIN responses.\n\nThis patch was tested by using a simple sending program to call recvmsg\nusing io_uring with MSG_ERRQUEUE set and verifying with printks that the\nPOLLIN is correctly unset when the msg flags are MSG_ERRQUEUE.\n\nSigned-off-by: Arjun Roy <arjunroy@google.com>\nSigned-off-by: Soheil Hassas Yeganeh <soheil@google.com>\nAcked-by: Eric Dumazet <edumazet@google.com>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Luke Hsiao <lukehsiao@google.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-08-24 16:16:06 -0700 io_uring: ignore POLLIN for recvmsg on MSG_ERRQUEUE"
    },
    {
        "commit": "583bbf0624dfd8fc45f1049be1d4980be59451ff",
        "message": "For TCP tx zero-copy, the kernel notifies the process of completions by\nqueuing completion notifications on the socket error queue. This patch\nallows reading these notifications via recvmsg to support TCP tx\nzero-copy.\n\nAncillary data was originally disallowed due to privilege escalation\nvia io_uring's offloading of sendmsg() onto a kernel thread with kernel\ncredentials (https://crbug.com/project-zero/1975). So, we must ensure\nthat the socket type is one where the ancillary data types that are\ndelivered on recvmsg are plain data (no file descriptors or values that\nare translated based on the identity of the calling process).\n\nThis was tested by using io_uring to call recvmsg on the MSG_ERRQUEUE\nwith tx zero-copy enabled. Before this patch, we received -EINVALID from\nthis specific code path. After this patch, we could read tcp tx\nzero-copy completion notifications from the MSG_ERRQUEUE.\n\nSigned-off-by: Soheil Hassas Yeganeh <soheil@google.com>\nSigned-off-by: Arjun Roy <arjunroy@google.com>\nAcked-by: Eric Dumazet <edumazet@google.com>\nReviewed-by: Jann Horn <jannh@google.com>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Luke Hsiao <lukehsiao@google.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>",
        "kernel_version": "v5.10-rc1",
        "release_date": "2020-08-24 16:16:06 -0700 io_uring: allow tcp ancillary data for __sys_recvmsg_sock()"
    },
    {
        "commit": "c41c3ec4a2bcd3f24ab753bb337ec342f24bdf94",
        "message": "Pull block fixes from Jens Axboe:\n\n - NVMe pull request from Sagi:\n       - nvme completion rework from Christoph and Chao that mostly came\n         from a bit of divergence of how we classify errors related to\n         pathing/retry etc.\n       - nvmet passthru fixes from Chaitanya\n       - minor nvmet fixes from Amit and I\n       - mpath round-robin path selection fix from Martin\n       - ignore noiob for zoned devices from Keith\n       - minor nvme-fc fix from Tianjia\"\n\n - BFQ cgroup leak fix (Dmitry)\n\n - block layer MAINTAINERS addition (Geert)\n\n - fix null_blk FUA checking (Hou)\n\n - get_max_io_size() size fix (Keith)\n\n - fix block page_is_mergeable() for compound pages (Matthew)\n\n - discard granularity fixes (Ming)\n\n - IO scheduler ordering fix (Ming)\n\n - misc fixes\n\n* tag 'io_uring-5.9-2020-08-23' of git://git.kernel.dk/linux-block: (31 commits)\n  null_blk: fix passing of REQ_FUA flag in null_handle_rq\n  nvmet: Disable keep-alive timer when kato is cleared to 0h\n  nvme: redirect commands on dying queue\n  nvme: just check the status code type in nvme_is_path_error\n  nvme: refactor command completion\n  nvme: rename and document nvme_end_request\n  nvme: skip noiob for zoned devices\n  nvme-pci: fix PRP pool size\n  nvme-pci: Use u32 for nvme_dev.q_depth and nvme_queue.q_depth\n  nvme: Use spin_lock_irq() when taking the ctrl->lock\n  nvmet: call blk_mq_free_request() directly\n  nvmet: fix oops in pt cmd execution\n  nvmet: add ns tear down label for pt-cmd handling\n  nvme: multipath: round-robin: eliminate \"fallback\" variable\n  nvme: multipath: round-robin: fix single non-optimized path case\n  nvme-fc: Fix wrong return value in __nvme_fc_init_request()\n  nvmet-passthru: Reject commands with non-sgl flags set\n  nvmet: fix a memory leak\n  blkcg: fix memleak for iolatency\n  MAINTAINERS: Add missing header files to BLOCK LAYER section\n  ...",
        "kernel_version": "v5.9-rc3",
        "release_date": "2020-08-24 11:53:15 -0700 Merge tag 'io_uring-5.9-2020-08-23' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "fd7d6de2241453fc7d042336d366a939a25bc5a9",
        "message": "If an application is doing reads on signalfd, and we arm the poll handler\nbecause there's no data available, then the wakeup can recurse on the\ntasks sighand->siglock as the signal delivery from task_work_add() will\nuse TWA_SIGNAL and that attempts to lock it again.\n\nWe can detect the signalfd case pretty easily by comparing the poll->head\nwait_queue_head_t with the target task signalfd wait queue. Just use\nnormal task wakeup for this case.\n\nCc: stable@vger.kernel.org # v5.7+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc3",
        "release_date": "2020-08-23 11:03:53 -0600 io_uring: don't recurse on tsk->sighand->siglock with signalfd"
    },
    {
        "commit": "f873db9acd3c92d4741bc3676c9eb511b2f9a6f6",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Make sure the head link cancelation includes async work\n\n - Get rid of kiocb_wait_page_queue_init(), makes no sense to have it as\n   a separate function since you moved it into io_uring itself\n\n - io_import_iovec cleanups (Pavel, me)\n\n - Use system_unbound_wq for ring exit work, to avoid spawning tons of\n   these if we have tons of rings exiting at the same time\n\n - Fix req->flags overflow flag manipulation (Pavel)\n\n* tag 'io_uring-5.9-2020-08-21' of git://git.kernel.dk/linux-block:\n  io_uring: kill extra iovec=NULL in import_iovec()\n  io_uring: comment on kfree(iovec) checks\n  io_uring: fix racy req->flags modification\n  io_uring: use system_unbound_wq for ring exit work\n  io_uring: cleanup io_import_iovec() of pre-mapped request\n  io_uring: get rid of kiocb_wait_page_queue_init()\n  io_uring: find and cancel head link async work on files exit",
        "kernel_version": "v5.9-rc2",
        "release_date": "2020-08-21 14:59:16 -0700 Merge tag 'io_uring-5.9-2020-08-21' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "867a23eab52847d41a0a6eae41a64d76de7782a8",
        "message": "If io_import_iovec() returns an error, return iovec is undefined and\nmust not be used, so don't set it to NULL when failing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc2",
        "release_date": "2020-08-20 05:36:19 -0600 io_uring: kill extra iovec=NULL in import_iovec()"
    },
    {
        "commit": "f261c16861b82951bd2125706660ce4602930563",
        "message": "kfree() handles NULL pointers well, but io_{read,write}() checks it\nbecause of performance reasons. Leave a comment there for those who are\ntempted to patch it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc2",
        "release_date": "2020-08-20 05:36:17 -0600 io_uring: comment on kfree(iovec) checks"
    },
    {
        "commit": "bb175342aa64e6c6f1d04f5235502121d6ff0247",
        "message": "Setting and clearing REQ_F_OVERFLOW in io_uring_cancel_files() and\nio_cqring_overflow_flush() are racy, because they might be called\nasynchronously.\n\nREQ_F_OVERFLOW flag in only needed for files cancellation, so if it can\nbe guaranteed that requests _currently_ marked inflight can't be\noverflown, the problem will be solved with removing the flag\naltogether.\n\nThat's how the patch works, it removes inflight status of a request\nin io_cqring_fill_event() whenever it should be thrown into CQ-overflow\nlist. That's Ok to do, because no opcode specific handling can be done\nafter io_cqring_fill_event(), the same assumption as with \"struct\nio_completion\" patches.\nAnd it already have a good place for such cleanups, which is\nio_clean_op(). A nice side effect of this is removing this inflight\ncheck from the hot path.\n\nnote on synchronisation: now __io_cqring_fill_event() may be taking two\nspinlocks simultaneously, completion_lock and inflight_lock. It's fine,\nbecause we never do that in reverse order, and CQ-overflow of inflight\nrequests shouldn't happen often.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc2",
        "release_date": "2020-08-20 05:36:15 -0600 io_uring: fix racy req->flags modification"
    },
    {
        "commit": "fc666777da9da387354b1a142a6ffc5d43bc4f7e",
        "message": "We currently use system_wq, which is unbounded in terms of number of\nworkers. This means that if we're exiting tons of rings at the same\ntime, then we'll briefly spawn tons of event kworkers just for a very\nshort blocking time as the rings exit.\n\nUse system_unbound_wq instead, which has a sane cap on the concurrency\nlevel.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc2",
        "release_date": "2020-08-19 11:10:51 -0600 io_uring: use system_unbound_wq for ring exit work"
    },
    {
        "commit": "8452fd0ce657a4313dfa784f11320971b67727fd",
        "message": "io_rw_prep_async() goes through a dance of clearing req->io, calling\nthe iovec import, then re-setting req->io. Provide an internal helper\nthat does the right thing without needing state tweaked to get there.\n\nThis enables further cleanups in io_read, io_write, and\nio_resubmit_prep(), but that's left for another time.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc2",
        "release_date": "2020-08-18 14:03:15 -0700 io_uring: cleanup io_import_iovec() of pre-mapped request"
    },
    {
        "commit": "3b2a4439e0ae1732f90877a7160bbf42e1beb4b6",
        "message": "The 5.9 merge moved this function io_uring, which means that we don't\nneed to retain the generic nature of it. Clean up this part by removing\nredundant checks, and just inlining the small remainder in\nio_rw_should_retry().\n\nNo functional changes in this patch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc2",
        "release_date": "2020-08-16 14:36:31 -0700 io_uring: get rid of kiocb_wait_page_queue_init()"
    },
    {
        "commit": "b711d4eaf0c408a811311ee3e94d6e9e5a230a9a",
        "message": "Commit f254ac04c874 (\"io_uring: enable lookup of links holding inflight files\")\nonly handled 2 out of the three head link cases we have, we also need to\nlookup and cancel work that is blocked in io-wq if that work has a link\nthat's holding a reference to the files structure.\n\nPut the \"cancel head links that hold this request pending\" logic into\nio_attempt_cancel(), which will to through the motions of finding and\ncanceling head links that hold the current inflight files stable request\npending.\n\nCc: stable@vger.kernel.org\nReported-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc2",
        "release_date": "2020-08-16 14:36:31 -0700 io_uring: find and cancel head link async work on files exit"
    },
    {
        "commit": "2cc3c4b3c2e9c99e90aaf19cd801ff2c160f283c",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A few differerent things in here.\n\n  Seems like syzbot got some more io_uring bits wired up, and we got a\n  handful of reports and the associated fixes are in here.\n\n  General fixes too, and a lot of them marked for stable.\n\n  Lastly, a bit of fallout from the async buffered reads, where we now\n  more easily trigger short reads. Some applications don't really like\n  that, so the io_read() code now handles short reads internally, and\n  got a cleanup along the way so that it's now easier to read (and\n  documented). We're now passing tests that failed before\"\n\n* tag 'io_uring-5.9-2020-08-15' of git://git.kernel.dk/linux-block:\n  io_uring: short circuit -EAGAIN for blocking read attempt\n  io_uring: sanitize double poll handling\n  io_uring: internally retry short reads\n  io_uring: retain iov_iter state over io_read/io_write calls\n  task_work: only grab task signal lock when needed\n  io_uring: enable lookup of links holding inflight files\n  io_uring: fail poll arm on queue proc failure\n  io_uring: hold 'ctx' reference around task_work queue + execute\n  fs: RWF_NOWAIT should imply IOCB_NOIO\n  io_uring: defer file table grabbing request cleanup for locked requests\n  io_uring: add missing REQ_F_COMP_LOCKED for nested requests\n  io_uring: fix recursive completion locking on oveflow flush\n  io_uring: use TWA_SIGNAL for task_work uncondtionally\n  io_uring: account locked memory before potential error case\n  io_uring: set ctx sq/cq entry count earlier\n  io_uring: Fix NULL pointer dereference in loop_rw_iter()\n  io_uring: add comments on how the async buffered read retry works\n  io_uring: io_async_buf_func() need not test page bit",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-16 10:55:12 -0700 Merge tag 'io_uring-5.9-2020-08-15' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "f91daf565b0e272a33bd3fcd19eaebd331c5cffd",
        "message": "One case was missed in the short IO retry handling, and that's hitting\n-EAGAIN on a blocking attempt read (eg from io-wq context). This is a\nproblem on sockets that are marked as non-blocking when created, they\ndon't carry any REQ_F_NOWAIT information to help us terminate them\ninstead of perpetually retrying.\n\nFixes: 227c0c9673d8 (\"io_uring: internally retry short reads\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-15 15:58:42 -0700 io_uring: short circuit -EAGAIN for blocking read attempt"
    },
    {
        "commit": "d4e7cd36a90e38e0276d6ce0c20f5ccef17ec38c",
        "message": "There's a bit of confusion on the matching pairs of poll vs double poll,\ndepending on if the request is a pure poll (IORING_OP_POLL_ADD) or\npoll driven retry.\n\nAdd io_poll_get_double() that returns the double poll waitqueue, if any,\nand io_poll_get_single() that returns the original poll waitqueue. With\nthat, remove the argument to io_poll_remove_double().\n\nFinally ensure that wait->private is cleared once the double poll handler\nhas run, so that remove knows it's already been seen.\n\nCc: stable@vger.kernel.org # v5.8\nReported-by: syzbot+7f617d4a9369028b8a2c@syzkaller.appspotmail.com\nFixes: 18bceab101ad (\"io_uring: allow POLL_ADD with double poll_wait() users\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-15 11:48:18 -0700 io_uring: sanitize double poll handling"
    },
    {
        "commit": "227c0c9673d86732995474d277f84e08ee763e46",
        "message": "We've had a few application cases of not handling short reads properly,\nand it is understandable as short reads aren't really expected if the\napplication isn't doing non-blocking IO.\n\nNow that we retain the iov_iter over retries, we can implement internal\nretry pretty trivially. This ensures that we don't return a short read,\neven for buffered reads on page cache conflicts.\n\nCleanup the deep nesting and hard to read nature of io_read() as well,\nit's much more straight forward now to read and understand. Added a\nfew comments explaining the logic as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-13 16:00:31 -0700 io_uring: internally retry short reads"
    },
    {
        "commit": "ff6165b2d7f66fccb7095a60ccc7a80813858665",
        "message": "Instead of maintaining (and setting/remembering) iov_iter size and\nsegment counts, just put the iov_iter in the async part of the IO\nstructure.\n\nThis is mostly a preparation patch for doing appropriate internal retries\nfor short reads, but it also cleans up the state handling nicely and\nsimplifies it quite a bit.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-13 13:53:34 -0700 io_uring: retain iov_iter state over io_read/io_write calls"
    },
    {
        "commit": "ebf0d100df0731901c16632f78d78d35f4123bc4",
        "message": "If JOBCTL_TASK_WORK is already set on the targeted task, then we need\nnot go through {lock,unlock}_task_sighand() to set it again and queue\na signal wakeup. This is safe as we're checking it _after_ adding the\nnew task_work with cmpxchg().\n\nThe ordering is as follows:\n\ntask_work_add()\t\t\t\tget_signal()\n--------------------------------------------------------------\nSTORE(task->task_works, new_work);\tSTORE(task->jobctl);\nmb();\t\t\t\t\tmb();\nLOAD(task->jobctl);\t\t\tLOAD(task->task_works);\n\nThis speeds up TWA_SIGNAL handling quite a bit, which is important now\nthat io_uring is relying on it for all task_work deliveries.\n\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Jann Horn <jannh@google.com>\nAcked-by: Oleg Nesterov <oleg@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-13 09:01:38 -0600 task_work: only grab task signal lock when needed"
    },
    {
        "commit": "f254ac04c8744cf7bfed012717eac34eacc65dfb",
        "message": "When a process exits, we cancel whatever requests it has pending that\nare referencing the file table. However, if a link is holding a\nreference, then we cannot find it by simply looking at the inflight\nlist.\n\nEnable checking of the poll and timeout list to find the link, and\ncancel it appropriately.\n\nCc: stable@vger.kernel.org\nReported-by: Josef <josef.grieb@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-12 17:33:30 -0600 io_uring: enable lookup of links holding inflight files"
    },
    {
        "commit": "a36da65c46565d2527eec3efdb546251e38253fd",
        "message": "Check the ipt.error value, it must have been either cleared to zero or\nset to another error than the default -EINVAL if we don't go through the\nwaitqueue proc addition. Just give up on poll at that point and return\nfailure, this will fallback to async work.\n\nio_poll_add() doesn't suffer from this failure case, as it returns the\nerror value directly.\n\nCc: stable@vger.kernel.org # v5.7+\nReported-by: syzbot+a730016dc0bdce4f6ff5@syzkaller.appspotmail.com\nReviewed-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-12 08:29:40 -0600 io_uring: fail poll arm on queue proc failure"
    },
    {
        "commit": "6d816e088c359866f9867057e04f244c608c42fe",
        "message": "We're holding the request reference, but we need to go one higher\nto ensure that the ctx remains valid after the request has finished.\nIf the ring is closed with pending task_work inflight, and the\ngiven io_kiocb finishes sync during issue, then we need a reference\nto the ring itself around the task_work execution cycle.\n\nCc: stable@vger.kernel.org # v5.7+\nReported-by: syzbot+9b260fc33297966f5a8e@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-11 08:09:13 -0600 io_uring: hold 'ctx' reference around task_work queue + execute"
    },
    {
        "commit": "51a4cc112c7a42b62a91bcccdfac42e7c4561729",
        "message": "If we're in the error path failing links and we have a link that has\ngrabbed a reference to the fs_struct, then we cannot safely drop our\nreference to the table if we already hold the completion lock. This\nadds a hardirq dependency to the fs_struct->lock, which it currently\ndoesn't have.\n\nDefer the final cleanup and free of such requests to avoid adding this\ndependency.\n\nReported-by: syzbot+ef4b654b49ed7ff049bf@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-10 15:19:25 -0600 io_uring: defer file table grabbing request cleanup for locked requests"
    },
    {
        "commit": "9b7adba9eaec28e0e4343c96d0dbeb9578802f5f",
        "message": "When we traverse into failing links or timeouts, we need to ensure we\npropagate the REQ_F_COMP_LOCKED flag to ensure that we correctly signal\nto the completion side that we already hold the completion lock.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-10 15:19:25 -0600 io_uring: add missing REQ_F_COMP_LOCKED for nested requests"
    },
    {
        "commit": "7271ef3a93a832180068c7aade3f130b7f39b17e",
        "message": "syszbot reports a scenario where we recurse on the completion lock\nwhen flushing an overflow:\n\n1 lock held by syz-executor287/6816:\n #0: ffff888093cdb4d8 (&ctx->completion_lock){....}-{2:2}, at: io_cqring_overflow_flush+0xc6/0xab0 fs/io_uring.c:1333\n\nstack backtrace:\nCPU: 1 PID: 6816 Comm: syz-executor287 Not tainted 5.8.0-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nCall Trace:\n __dump_stack lib/dump_stack.c:77 [inline]\n dump_stack+0x1f0/0x31e lib/dump_stack.c:118\n print_deadlock_bug kernel/locking/lockdep.c:2391 [inline]\n check_deadlock kernel/locking/lockdep.c:2432 [inline]\n validate_chain+0x69a4/0x88a0 kernel/locking/lockdep.c:3202\n __lock_acquire+0x1161/0x2ab0 kernel/locking/lockdep.c:4426\n lock_acquire+0x160/0x730 kernel/locking/lockdep.c:5005\n __raw_spin_lock_irq include/linux/spinlock_api_smp.h:128 [inline]\n _raw_spin_lock_irq+0x67/0x80 kernel/locking/spinlock.c:167\n spin_lock_irq include/linux/spinlock.h:379 [inline]\n io_queue_linked_timeout fs/io_uring.c:5928 [inline]\n __io_queue_async_work fs/io_uring.c:1192 [inline]\n __io_queue_deferred+0x36a/0x790 fs/io_uring.c:1237\n io_cqring_overflow_flush+0x774/0xab0 fs/io_uring.c:1359\n io_ring_ctx_wait_and_kill+0x2a1/0x570 fs/io_uring.c:7808\n io_uring_release+0x59/0x70 fs/io_uring.c:7829\n __fput+0x34f/0x7b0 fs/file_table.c:281\n task_work_run+0x137/0x1c0 kernel/task_work.c:135\n exit_task_work include/linux/task_work.h:25 [inline]\n do_exit+0x5f3/0x1f20 kernel/exit.c:806\n do_group_exit+0x161/0x2d0 kernel/exit.c:903\n __do_sys_exit_group+0x13/0x20 kernel/exit.c:914\n __se_sys_exit_group+0x10/0x10 kernel/exit.c:912\n __x64_sys_exit_group+0x37/0x40 kernel/exit.c:912\n do_syscall_64+0x31/0x70 arch/x86/entry/common.c:46\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nFix this by passing back the link from __io_queue_async_work(), and\nthen let the caller handle the queueing of the link. Take care to also\npunt the submission reference put to the caller, as we're holding the\ncompletion lock for the __io_queue_defer() case. Hence we need to mark\nthe io_kiocb appropriately for that case.\n\nReported-by: syzbot+996f91b6ec3812c48042@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-10 15:19:25 -0600 io_uring: fix recursive completion locking on oveflow flush"
    },
    {
        "commit": "0ba9c9edcd152158a0e321a4c13ac1dfc571ff3d",
        "message": "An earlier commit:\n\nb7db41c9e03b (\"io_uring: fix regression with always ignoring signals in io_cqring_wait()\")\n\nensured that we didn't get stuck waiting for eventfd reads when it's\nregistered with the io_uring ring for event notification, but we still\nhave cases where the task can be waiting on other events in the kernel and\nneed a bigger nudge to make forward progress. Or the task could be in the\nkernel and running, but on its way to blocking.\n\nThis means that TWA_RESUME cannot reliably be used to ensure we make\nprogress. Use TWA_SIGNAL unconditionally.\n\nCc: stable@vger.kernel.org # v5.7+\nReported-by: Josef <josef.grieb@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-10 15:17:46 -0600 io_uring: use TWA_SIGNAL for task_work uncondtionally"
    },
    {
        "commit": "f74441e6311a28f0ee89b9c8e296a33730f812fc",
        "message": "The tear down path will always unaccount the memory, so ensure that we\nhave accounted it before hitting any of them.\n\nReported-by: Tom\u00e1\u0161 Chaloupka <chalucha@gmail.com>\nReviewed-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-06 07:39:29 -0600 io_uring: account locked memory before potential error case"
    },
    {
        "commit": "bd74048108c179cea0ff52979506164c80f29da7",
        "message": "If we hit an earlier error path in io_uring_create(), then we will have\naccounted memory, but not set ctx->{sq,cq}_entries yet. Then when the\nring is torn down in error, we use those values to unaccount the memory.\n\nEnsure we set the ctx entries before we're able to hit a potential error\npath.\n\nCc: stable@vger.kernel.org\nReported-by: Tom\u00e1\u0161 Chaloupka <chalucha@gmail.com>\nTested-by: Tom\u00e1\u0161 Chaloupka <chalucha@gmail.com>\nReviewed-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-06 07:18:06 -0600 io_uring: set ctx sq/cq entry count earlier"
    },
    {
        "commit": "2dd2111d0d383df104b144e0d1f6b5a00cb7cd88",
        "message": "loop_rw_iter() does not check whether the file has a read or\nwrite function. This can lead to NULL pointer dereference\nwhen the user passes in a file descriptor that does not have\nread or write function.\n\nThe crash log looks like this:\n\n[   99.834071] BUG: kernel NULL pointer dereference, address: 0000000000000000\n[   99.835364] #PF: supervisor instruction fetch in kernel mode\n[   99.836522] #PF: error_code(0x0010) - not-present page\n[   99.837771] PGD 8000000079d62067 P4D 8000000079d62067 PUD 79d8c067 PMD 0\n[   99.839649] Oops: 0010 [#2] SMP PTI\n[   99.840591] CPU: 1 PID: 333 Comm: io_wqe_worker-0 Tainted: G      D           5.8.0 #2\n[   99.842622] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1 04/01/2014\n[   99.845140] RIP: 0010:0x0\n[   99.845840] Code: Bad RIP value.\n[   99.846672] RSP: 0018:ffffa1c7c01ebc08 EFLAGS: 00010202\n[   99.848018] RAX: 0000000000000000 RBX: ffff92363bd67300 RCX: ffff92363d461208\n[   99.849854] RDX: 0000000000000010 RSI: 00007ffdbf696bb0 RDI: ffff92363bd67300\n[   99.851743] RBP: ffffa1c7c01ebc40 R08: 0000000000000000 R09: 0000000000000000\n[   99.853394] R10: ffffffff9ec692a0 R11: 0000000000000000 R12: 0000000000000010\n[   99.855148] R13: 0000000000000000 R14: ffff92363d461208 R15: ffffa1c7c01ebc68\n[   99.856914] FS:  0000000000000000(0000) GS:ffff92363dd00000(0000) knlGS:0000000000000000\n[   99.858651] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n[   99.860032] CR2: ffffffffffffffd6 CR3: 000000007ac66000 CR4: 00000000000006e0\n[   99.861979] Call Trace:\n[   99.862617]  loop_rw_iter.part.0+0xad/0x110\n[   99.863838]  io_write+0x2ae/0x380\n[   99.864644]  ? kvm_sched_clock_read+0x11/0x20\n[   99.865595]  ? sched_clock+0x9/0x10\n[   99.866453]  ? sched_clock_cpu+0x11/0xb0\n[   99.867326]  ? newidle_balance+0x1d4/0x3c0\n[   99.868283]  io_issue_sqe+0xd8f/0x1340\n[   99.869216]  ? __switch_to+0x7f/0x450\n[   99.870280]  ? __switch_to_asm+0x42/0x70\n[   99.871254]  ? __switch_to_asm+0x36/0x70\n[   99.872133]  ? lock_timer_base+0x72/0xa0\n[   99.873155]  ? switch_mm_irqs_off+0x1bf/0x420\n[   99.874152]  io_wq_submit_work+0x64/0x180\n[   99.875192]  ? kthread_use_mm+0x71/0x100\n[   99.876132]  io_worker_handle_work+0x267/0x440\n[   99.877233]  io_wqe_worker+0x297/0x350\n[   99.878145]  kthread+0x112/0x150\n[   99.878849]  ? __io_worker_unuse+0x100/0x100\n[   99.879935]  ? kthread_park+0x90/0x90\n[   99.880874]  ret_from_fork+0x22/0x30\n[   99.881679] Modules linked in:\n[   99.882493] CR2: 0000000000000000\n[   99.883324] ---[ end trace 4453745f4673190b ]---\n[   99.884289] RIP: 0010:0x0\n[   99.884837] Code: Bad RIP value.\n[   99.885492] RSP: 0018:ffffa1c7c01ebc08 EFLAGS: 00010202\n[   99.886851] RAX: 0000000000000000 RBX: ffff92363acd7f00 RCX: ffff92363d461608\n[   99.888561] RDX: 0000000000000010 RSI: 00007ffe040d9e10 RDI: ffff92363acd7f00\n[   99.890203] RBP: ffffa1c7c01ebc40 R08: 0000000000000000 R09: 0000000000000000\n[   99.891907] R10: ffffffff9ec692a0 R11: 0000000000000000 R12: 0000000000000010\n[   99.894106] R13: 0000000000000000 R14: ffff92363d461608 R15: ffffa1c7c01ebc68\n[   99.896079] FS:  0000000000000000(0000) GS:ffff92363dd00000(0000) knlGS:0000000000000000\n[   99.898017] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n[   99.899197] CR2: ffffffffffffffd6 CR3: 000000007ac66000 CR4: 00000000000006e0\n\nFixes: 32960613b7c3 (\"io_uring: correctly handle non ->{read,write}_iter() file_operations\")\nCc: stable@vger.kernel.org\nSigned-off-by: Guoyu Huang <hgy5945@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-05 06:48:25 -0600 io_uring: Fix NULL pointer dereference in loop_rw_iter()"
    },
    {
        "commit": "c1dd91d16246b168b80af9b64c5cc35a66410455",
        "message": "The retry based logic here isn't easy to follow unless you're already\nfamiliar with how io_uring does task_work based retries. Add some\ncomments explaining the flow a little better.\n\nSuggested-by: Linus Torvalds <torvalds@linux-foundation.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-03 17:48:15 -0600 io_uring: add comments on how the async buffered read retry works"
    },
    {
        "commit": "cbd287c09351f1d3a4b3cb9167a2616a11390d32",
        "message": "Since we don't do exclusive waits or wakeups, we know that the bit is\nalways going to be set. Kill the test. Also see commit:\n\n2a9127fcf229 (\"mm: rewrite wait_on_page_bit_common() logic\")\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-03 17:39:37 -0600 io_uring: io_async_buf_func() need not test page bit"
    },
    {
        "commit": "cdc8fcb49905c0b67e355e027cb462ee168ffaa3",
        "message": "Pull io_uring updates from Jens Axboe:\n \"Lots of cleanups in here, hardening the code and/or making it easier\n  to read and fixing bugs, but a core feature/change too adding support\n  for real async buffered reads. With the latter in place, we just need\n  buffered write async support and we're done relying on kthreads for\n  the fast path. In detail:\n\n   - Cleanup how memory accounting is done on ring setup/free (Bijan)\n\n   - sq array offset calculation fixup (Dmitry)\n\n   - Consistently handle blocking off O_DIRECT submission path (me)\n\n   - Support proper async buffered reads, instead of relying on kthread\n     offload for that. This uses the page waitqueue to drive retries\n     from task_work, like we handle poll based retry. (me)\n\n   - IO completion optimizations (me)\n\n   - Fix race with accounting and ring fd install (me)\n\n   - Support EPOLLEXCLUSIVE (Jiufei)\n\n   - Get rid of the io_kiocb unionizing, made possible by shrinking\n     other bits (Pavel)\n\n   - Completion side cleanups (Pavel)\n\n   - Cleanup REQ_F_ flags handling, and kill off many of them (Pavel)\n\n   - Request environment grabbing cleanups (Pavel)\n\n   - File and socket read/write cleanups (Pavel)\n\n   - Improve kiocb_set_rw_flags() (Pavel)\n\n   - Tons of fixes and cleanups (Pavel)\n\n   - IORING_SQ_NEED_WAKEUP clear fix (Xiaoguang)\"\n\n* tag 'for-5.9/io_uring-20200802' of git://git.kernel.dk/linux-block: (127 commits)\n  io_uring: flip if handling after io_setup_async_rw\n  fs: optimise kiocb_set_rw_flags()\n  io_uring: don't touch 'ctx' after installing file descriptor\n  io_uring: get rid of atomic FAA for cq_timeouts\n  io_uring: consolidate *_check_overflow accounting\n  io_uring: fix stalled deferred requests\n  io_uring: fix racy overflow count reporting\n  io_uring: deduplicate __io_complete_rw()\n  io_uring: de-unionise io_kiocb\n  io-wq: update hash bits\n  io_uring: fix missing io_queue_linked_timeout()\n  io_uring: mark ->work uninitialised after cleanup\n  io_uring: deduplicate io_grab_files() calls\n  io_uring: don't do opcode prep twice\n  io_uring: clear IORING_SQ_NEED_WAKEUP after executing task works\n  io_uring: batch put_task_struct()\n  tasks: add put_task_struct_many()\n  io_uring: return locked and pinned page accounting\n  io_uring: don't miscount pinned memory\n  io_uring: don't open-code recv kbuf managment\n  ...",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-03 13:01:22 -0700 Merge tag 'for-5.9/io_uring-20200802' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "fa15bafb71fd7a4d6018dae87cfaf890fd4ab47f",
        "message": "As recently done with with send/recv, flip the if after\nrw_verify_aread() in io_{read,write}() and tabulise left bits left.\nThis removes mispredicted by a compiler jump on the success/fast path.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-08-01 11:02:57 -0600 io_uring: flip if handling after io_setup_async_rw"
    },
    {
        "commit": "d1719f70d0a5b83b12786a7dbc5b9fe396469016",
        "message": "As soon as we install the file descriptor, we have to assume that it\ncan get arbitrarily closed. We currently account memory (and note that\nwe did) after installing the ring fd, which means that it could be a\npotential use-after-free condition if the fd is closed right after\nbeing installed, but before we fiddle with the ctx.\n\nIn fact, syzbot reported this exact scenario:\n\nBUG: KASAN: use-after-free in io_account_mem fs/io_uring.c:7397 [inline]\nBUG: KASAN: use-after-free in io_uring_create fs/io_uring.c:8369 [inline]\nBUG: KASAN: use-after-free in io_uring_setup+0x2797/0x2910 fs/io_uring.c:8400\nRead of size 1 at addr ffff888087a41044 by task syz-executor.5/18145\n\nCPU: 0 PID: 18145 Comm: syz-executor.5 Not tainted 5.8.0-rc7-next-20200729-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nCall Trace:\n __dump_stack lib/dump_stack.c:77 [inline]\n dump_stack+0x18f/0x20d lib/dump_stack.c:118\n print_address_description.constprop.0.cold+0xae/0x497 mm/kasan/report.c:383\n __kasan_report mm/kasan/report.c:513 [inline]\n kasan_report.cold+0x1f/0x37 mm/kasan/report.c:530\n io_account_mem fs/io_uring.c:7397 [inline]\n io_uring_create fs/io_uring.c:8369 [inline]\n io_uring_setup+0x2797/0x2910 fs/io_uring.c:8400\n do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\nRIP: 0033:0x45c429\nCode: 8d b6 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 5b b6 fb ff c3 66 2e 0f 1f 84 00 00 00 00\nRSP: 002b:00007f8f121d0c78 EFLAGS: 00000246 ORIG_RAX: 00000000000001a9\nRAX: ffffffffffffffda RBX: 0000000000008540 RCX: 000000000045c429\nRDX: 0000000000000000 RSI: 0000000020000040 RDI: 0000000000000196\nRBP: 000000000078bf38 R08: 0000000000000000 R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000246 R12: 000000000078bf0c\nR13: 00007fff86698cff R14: 00007f8f121d19c0 R15: 000000000078bf0c\n\nMove the accounting of the ring used locked memory before we get and\ninstall the ring file descriptor.\n\nCc: stable@vger.kernel.org\nReported-by: syzbot+9d46305e76057f30c74e@syzkaller.appspotmail.com\nFixes: 309758254ea6 (\"io_uring: report pinned memory usage\")\nReviewed-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-31 08:25:06 -0600 io_uring: don't touch 'ctx' after installing file descriptor"
    },
    {
        "commit": "01cec8c18f5ad9c27eee9f21439072832181039e",
        "message": "If ->cq_timeouts modifications are done under ->completion_lock, we\ndon't really nee any fetch-and-add and other complex atomics. Replace it\nwith non-atomic FAA, that saves an implicit full memory barrier.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-30 11:42:21 -0600 io_uring: get rid of atomic FAA for cq_timeouts"
    },
    {
        "commit": "4693014340808e7f099e302c1dc40e9d79ff7667",
        "message": "Add a helper to mark ctx->{cq,sq}_check_overflow to get rid of\nduplicates, and it's clearer to check cq_overflow_list directly anyway.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-30 11:42:21 -0600 io_uring: consolidate *_check_overflow accounting"
    },
    {
        "commit": "dd9dfcdf5a603680458f5e7b0d2273c66e5417db",
        "message": "Always do io_commit_cqring() after completing a request, even if it was\naccounted as overflowed on the CQ side. Failing to do that may lead to\nnot to pushing deferred requests when needed, and so stalling the whole\nring.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-30 11:42:21 -0600 io_uring: fix stalled deferred requests"
    },
    {
        "commit": "b2bd1cf99f3e7c8fbf12ea07af2c6998e1209e25",
        "message": "All ->cq_overflow modifications should be under completion_lock,\notherwise it can report a wrong number to the userspace. Fix it in\nio_uring_cancel_files().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-30 11:42:21 -0600 io_uring: fix racy overflow count reporting"
    },
    {
        "commit": "81b68a5ca0ab5d92229a7b76332b9ce88bd6dbd1",
        "message": "Call __io_complete_rw() in io_iopoll_queue() instead of hand coding it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-30 11:42:21 -0600 io_uring: deduplicate __io_complete_rw()"
    },
    {
        "commit": "010e8e6be2194678f7e4bb3044c088bbee779f57",
        "message": "As io_kiocb have enough space, move ->work out of a union. It's safer\nthis way and removes ->work memcpy bouncing.\nBy the way make tabulation in struct io_kiocb consistent.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-30 11:42:21 -0600 io_uring: de-unionise io_kiocb"
    },
    {
        "commit": "0513b9d75c07cbcdfda3778b636d3d131d679eb1",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two small fixes for corner/error cases\"\n\n* tag 'io_uring-5.8-2020-07-30' of git://git.kernel.dk/linux-block:\n  io_uring: fix lockup in io_fail_links()\n  io_uring: fix ->work corruption with poll_add",
        "kernel_version": "v5.8",
        "release_date": "2020-07-30 09:47:07 -0700 Merge tag 'io_uring-5.8-2020-07-30' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "f063c5477eb392c315aa25ad538b4920b367ea05",
        "message": "Whoever called io_prep_linked_timeout() should also do\nio_queue_linked_timeout(). __io_queue_sqe() doesn't follow that for the\npunting path leaving linked timeouts prepared but never queued.\n\nFixes: 6df1db6b54243 (\"io_uring: fix mis-refcounting linked timeouts\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-25 09:47:44 -0600 io_uring: fix missing io_queue_linked_timeout()"
    },
    {
        "commit": "b65e0dd6a2de050d3fc4c0db4969a245f4e7273e",
        "message": "Remove REQ_F_WORK_INITIALIZED after io_req_clean_work(). That's a cold\npath but is safer for those using io_req_clean_work() out of\n*dismantle_req()/*io_free(). And for the same reason zero work.fs\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-25 09:47:44 -0600 io_uring: mark ->work uninitialised after cleanup"
    },
    {
        "commit": "1f68f31b51507e1ad647aa3a43c295eb024490ad",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix discrepancy in how sqe->flags are treated for a few requests,\n   this makes it consistent (Daniele)\n\n - Ensure that poll driven retry works with double waitqueue poll users\n\n - Fix a missing io_req_init_async() (Pavel)\n\n* tag 'io_uring-5.8-2020-07-24' of git://git.kernel.dk/linux-block:\n  io_uring: missed req_init_async() for IOSQE_ASYNC\n  io_uring: always allow drain/link/hardlink/async sqe flags\n  io_uring: ensure double poll additions work with both request types",
        "kernel_version": "v5.8-rc7",
        "release_date": "2020-07-24 14:02:41 -0700 Merge tag 'io_uring-5.8-2020-07-24' of git://git.kernel.dk/linux-block into master"
    },
    {
        "commit": "f56040b81999871973d21f334b4657957422c90e",
        "message": "Move io_req_init_async() into io_grab_files(), it's safer this way. Note\nthat io_queue_async_work() does *init_async(), so it's valid to move out\nof __io_queue_sqe() punt path. Also, add a helper around io_grab_files().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:46 -0600 io_uring: deduplicate io_grab_files() calls"
    },
    {
        "commit": "ae34817bd93e373a03203a4c6892735c430a14e1",
        "message": "Calling into opcode prep handlers may be dangerous, as they re-read\nSQE but might not re-initialise requests completely. If io_req_defer()\npassed fast checks and is done with preparations, punt it async.\n\nAs all other cases are covered with nulling @sqe, this guarantees that\nio_[opcode]_prep() are visited only once per request.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:46 -0600 io_uring: don't do opcode prep twice"
    },
    {
        "commit": "23b3628e45924419399da48c2b3a522b05557c91",
        "message": "In io_sq_thread(), if there are task works to handle, current codes\nwill skip schedule() and go on polling sq again, but forget to clear\nIORING_SQ_NEED_WAKEUP flag, fix this issue. Also add two helpers to\nset and clear IORING_SQ_NEED_WAKEUP flag,\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:46 -0600 io_uring: clear IORING_SQ_NEED_WAKEUP after executing task works"
    },
    {
        "commit": "5af1d13e8f0d8839db04a71ec786f369b0e67234",
        "message": "As every iopoll request have a task ref, it becomes expensive to put\nthem one by one, instead we can put several at once integrating that\ninto io_req_free_batch().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:46 -0600 io_uring: batch put_task_struct()"
    },
    {
        "commit": "cbcf72148da4af55ea81cfb351ea7c026ff1014f",
        "message": "Locked and pinned memory accounting in io_{,un}account_mem() depends on\nhaving ->sqo_mm, which is NULL after a recent change for non SQPOLL'ed\nio_ring. That disables the accounting.\n\nReturn ->sqo_mm initialisation back, and do __io_sq_thread_acquire_mm()\nbased on IORING_SETUP_SQPOLL flag.\n\nFixes: 8eb06d7e8dd85 (\"io_uring: fix missing ->mm on exit\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:45 -0600 io_uring: return locked and pinned page accounting"
    },
    {
        "commit": "5dbcad51f78434e782d0470b8b5fc4380700c35f",
        "message": "io_sqe_buffer_unregister() uses cxt->sqo_mm for memory accounting, but\nio_ring_ctx_free() drops ->sqo_mm before leaving pinned_vm\nover-accounted. Postpone mm cleanup for when it's not needed anymore.\n\nFixes: 309758254ea62 (\"io_uring: report pinned memory usage\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:45 -0600 io_uring: don't miscount pinned memory"
    },
    {
        "commit": "7fbb1b541f4286cc337b9bca1e5bad0ce4ee978c",
        "message": "Don't implement fast path of kbuf freeing and management inlined into\nio_recv{,msg}(), that's error prone and duplicates handling. Replace it\nwith a helper io_put_recv_kbuf(), which mimics io_put_rw_kbuf() in the\nio_read/write().\n\nThis also keeps cflags calculation in one place, removing duplication\nbetween rw and recv/send.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:45 -0600 io_uring: don't open-code recv kbuf managment"
    },
    {
        "commit": "8ff069bf2efd7b7aeb90b56ea8edc165c93d8940",
        "message": "Extract a common helper for cleaning up a selected buffer, this will be\nused shortly. By the way, correct cflags types to unsigned and, as kbufs\nare anyway tracked by a flag, remove useless zeroing req->rw.addr.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:45 -0600 io_uring: extract io_put_kbuf() helper"
    },
    {
        "commit": "bc02ef3325e3ef524ef29b65681ca4207b781224",
        "message": "Move REQ_F_BUFFER_SELECT flag check out of io_recv_buffer_select(), and\ndo that in its call sites That saves us from double error checking and\npossibly an extra function call.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:45 -0600 io_uring: move BUFFER_SELECT check into *recv[msg]"
    },
    {
        "commit": "0e1b6fe3d1e5f1b79c5bec37881c98febfba7718",
        "message": "io_clean_op() may be skipped even if there is a selected io_buffer,\nthat's because *select_buffer() funcions never set REQ_F_NEED_CLEANUP.\n\nTrigger io_clean_op() when REQ_F_BUFFER_SELECTED is set as well, and\nand clear the flag if was freed out of it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:45 -0600 io_uring: free selected-bufs if error'ed"
    },
    {
        "commit": "14c32eee9286621dd437b53460e44bd11e5bc08d",
        "message": "Instead of returning error from io_recv(), go through generic cleanup\npath, because it'll retain cflags for userspace. Do the same for\nio_send() for consistency.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:45 -0600 io_uring: don't forget cflags in io_recv()"
    },
    {
        "commit": "6b754c8b912a164fbb15b7b839d51709c3d9ee6f",
        "message": "With the return on a bad socket, kmsg is always non-null by the end\nof the function, prune left extra checks and initialisations.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:45 -0600 io_uring: remove extra checks in send/recv"
    },
    {
        "commit": "7a7cacba8b4560403615b04d57bdcd1f93f90f10",
        "message": "Flip over \"if (sock)\" condition with return on error, the upper layer\nwill take care. That change will be handy later, but already removes\nan extra jump from hot path.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:44 -0600 io_uring: indent left {send,recv}[msg]()"
    },
    {
        "commit": "06ef3608b0eed673fcbc62cf74c8d3ad0007a337",
        "message": "Currently, file refs in struct io_submit_state are tracked with 2 vars:\n@has_refs -- how many refs were initially taken\n@used_refs -- number of refs used\n\nReplace it with a single variable counting how many refs left at the\ncurrent moment.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:44 -0600 io_uring: simplify file ref tracking in submission state"
    },
    {
        "commit": "57f1a64958543fe18a7fe0addbfb31bb2ceeaea2",
        "message": "RLIMIT_SIZE in needed only for execution from an io-wq context, hence\nmove all preparations from hot path to io-wq work setup.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:44 -0600 io_uring/io-wq: move RLIMIT_FSIZE to io-wq"
    },
    {
        "commit": "327d6d968b195cfc48ff97c49b56520aac922f65",
        "message": "Every call to io_req_defer_prep() is prepended with allocating ->io,\njust do that in the function. And while we're at it, mark error paths\nwith unlikey and replace \"if (ret < 0)\" with \"if (ret)\".\n\nThere is only one change in the observable behaviour, that's instead of\nkilling the head request right away on error, it postpones it until the\nlink is assembled, that looks more preferable.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:44 -0600 io_uring: alloc ->io in io_req_defer_prep()"
    },
    {
        "commit": "1c2da9e8839d6437b43f2c805411d1a0cbd70165",
        "message": "A switch in __io_clean_op() doesn't have default, it's pointless to list\nopcodes that doesn't do any cleanup. Remove IORING_OP_OPEN* from there.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:44 -0600 io_uring: remove empty cleanup of OP_OPEN* reqs"
    },
    {
        "commit": "dca9cf8b87f55c96f072c1fc6bc90e2b97a8e19f",
        "message": "The only caller of io_req_work_grab_env() is io_prep_async_work(), and\nthey are both initialising req->work. Inline grab_env(), it's easier\nto keep this way, moreover there already were bugs with misplacing\nio_req_init_async().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 13:00:40 -0600 io_uring: inline io_req_work_grab_env()"
    },
    {
        "commit": "0f7e466b393abab86be96ffcf00af383afddc0d1",
        "message": "req->cflags is used only for defer-completion path, just use completion\ndata to store it. With the 4 bytes from the ->sequence patch and\ncompacting io_kiocb, this frees 8 bytes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:45 -0600 io_uring: place cflags into completion data"
    },
    {
        "commit": "9cf7c104deaef52d6fd7c103a716e31d9815ede8",
        "message": "req->sequence is used only for deferred (i.e. DRAIN) requests, but\ninitialised for every request. Remove req->sequence from io_kiocb\ntogether with its initialisation in io_init_req().\n\nReplace it with a new field in struct io_defer_entry, that will be\ncalculated only when needed in io_req_defer(), which is a slow path.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:45 -0600 io_uring: remove sequence from io_kiocb"
    },
    {
        "commit": "27dc8338e5fb0e0ed5b272e792f4ffad7f3bc03e",
        "message": "The only left user of req->list is DRAIN, hence instead of keeping a\nseparate per request list for it, do that with old fashion non-intrusive\nlists allocated on demand. That's a really slow path, so that's OK.\n\nThis removes req->list and so sheds 16 bytes from io_kiocb.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:45 -0600 io_uring: use non-intrusive list for defer"
    },
    {
        "commit": "7d6ddea6beaf6639cf3a2b291dcdac6fe1edc584",
        "message": "poll*() doesn't use req->list, don't init it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:45 -0600 io_uring: remove init for unused list"
    },
    {
        "commit": "135fcde8496b03d31648171dbc038990112e41d5",
        "message": "Instead of using shared req->list, hang timeouts up on their own list\nentry. struct io_timeout have enough extra space for it, but if that\nwill be a problem ->inflight_entry can reused for that.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:45 -0600 io_uring: add req->timeout.list"
    },
    {
        "commit": "40d8ddd4facb80760d5a0c61a7cf026d5ff73ff0",
        "message": "As with the completion path, also use compl.list for overflowed\nrequests. If cleaned up properly, nobody needs per-op data there\nanymore.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:44 -0600 io_uring: use completion list for CQ overflow"
    },
    {
        "commit": "d21ffe7eca82d47b489760899912f81e30456e2e",
        "message": "req->inflight_entry is used to track requests that grabbed files_struct.\nLet's share it with iopoll list, because the only iopoll'ed ops are\nreads and writes, which don't need a file table.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:44 -0600 io_uring: use inflight_entry list for iopoll'ing"
    },
    {
        "commit": "540e32a0855e700affa29b1112bf2dbb1fa7702a",
        "message": "It supports both polling and I/O polling. Rename ctx->poll to clearly\nshow that it's only in I/O poll case.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:44 -0600 io_uring: rename ctx->poll into ctx->iopoll"
    },
    {
        "commit": "3ca405ebfc1c3445b049dd25ca3338cbc99837d1",
        "message": "Calling io_req_complete(req) means that the request is done, and there\nis nothing left but to clean it up. That also means that per-op data\nafter that should not be used, so we're free to reuse it in completion\npath, e.g. to store overflow_list as done in this patch.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:44 -0600 io_uring: share completion list w/ per-op space"
    },
    {
        "commit": "252917c30f551e8e4377faac81d7fcf8e9629df1",
        "message": "As for import_iovec(), return !=NULL iovec from io_import_iovec() only\nwhen it should be freed. That includes returning NULL when iovec is\nalready in req->io, because it should be deallocated by other means,\ne.g. inside op handler. After io_setup_async_rw() local iovec to ->io,\njust mark it NULL, to follow the idea in io_{read,write} as well.\n\nThat's easier to follow, and especially useful if we want to reuse\nper-op space for completion data.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\n[axboe: only call kfree() on non-NULL pointer]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:44 -0600 io_uring: follow **iovec idiom in io_import_iovec"
    },
    {
        "commit": "c3e330a493740a2a8312dcb7b1cffceaec7f619a",
        "message": "Preparing reads/writes for async is a bit tricky. Extract a helper to\nnot repeat it twice.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:44 -0600 io_uring: add a helper for async rw iovec prep"
    },
    {
        "commit": "b64e3444d4e1c71fe148a4f4535395b1fdd73200",
        "message": "Don't deref req->io->rw every time, but put it in a local variable. This\nlooks prettier, generates less instructions, and doesn't break alias\nanalysis.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:44 -0600 io_uring: simplify io_req_map_rw()"
    },
    {
        "commit": "e73751225bae1e9b67e957afb273366fbb6ca136",
        "message": "io_kiocb::task_work was de-unionised, and is not planned to be shared\nback, because it's too useful and commonly used. Hence, instead of\nkeeping a separate task_work in struct io_async_rw just reuse\nreq->task_work.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:44 -0600 io_uring: replace rw->task_work with rq->task_work"
    },
    {
        "commit": "2ae523ed07f14391d685651f671a7858fe8c368a",
        "message": "Don't repeat send msg initialisation code, it's error prone.\nExtract and use a helper function.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:44 -0600 io_uring: extract io_sendmsg_copy_hdr()"
    },
    {
        "commit": "1400e69705baf98d1c9cb73b592a3a68aab1d852",
        "message": "send/recv msghdr initialisation works with struct io_async_msghdr, but\npulls the whole struct io_async_ctx for no reason. That complicates it\nwith composite accessing, e.g. io->msg.\n\nUse and pass the most specific type, which is struct io_async_msghdr.\nIt is the larget field in union io_async_ctx and doesn't save stack\nspace, but looks clearer.\nThe most of the changes are replacing \"io->msg.\" with \"iomsg->\"\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:44 -0600 io_uring: use more specific type in rcv/snd msg cp"
    },
    {
        "commit": "270a5940700bb6cf9abf36ea10cf1fa0d453aa7a",
        "message": "Every second field in send/recv is called msg, make it a bit more\nunderstandable by renaming ->msg, which is a user provided ptr,\nto ->umsg.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:44 -0600 io_uring: rename sr->msg into umsg"
    },
    {
        "commit": "b36200f543ff07a1cb346aa582349141df2c8068",
        "message": "rings_size() sets sq_offset to the total size of the rings (the returned\nvalue which is used for memory allocation). This is wrong: sq array should\nbe located within the rings, not after them. Set sq_offset to where it\nshould be.\n\nFixes: 75b28affdd6a (\"io_uring: allocate the two rings together\")\nSigned-off-by: Dmitry Vyukov <dvyukov@google.com>\nAcked-by: Hristo Venev <hristo@venev.name>\nCc: io-uring@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:55:44 -0600 io_uring: fix sq array offset calculation"
    },
    {
        "commit": "760618f7a8e3b63aa06266efb301719c374e29d4",
        "message": "Merge in io_uring-5.8 fixes, as changes/cleanups to how we do locked\nmem accounting require a fixup, and only one of the spots are noticed\nby git as the other merges cleanly. The flags fix from io_uring-5.8\nalso causes a merge conflict, the leak fix for recvmsg, the double poll\nfix, and the link failure locking fix.\n\n* io_uring-5.8:\n  io_uring: fix lockup in io_fail_links()\n  io_uring: fix ->work corruption with poll_add\n  io_uring: missed req_init_async() for IOSQE_ASYNC\n  io_uring: always allow drain/link/hardlink/async sqe flags\n  io_uring: ensure double poll additions work with both request types\n  io_uring: fix recvmsg memory leak with buffer selection\n  io_uring: fix not initialised work->flags\n  io_uring: fix missing msg_name assignment\n  io_uring: account user memory freed when exit has been queued\n  io_uring: fix memleak in io_sqe_files_register()\n  io_uring: fix memleak in __io_sqe_files_update()\n  io_uring: export cq overflow status to userspace\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-24 12:53:31 -0600 Merge branch 'io_uring-5.8' into for-5.9/io_uring"
    },
    {
        "commit": "4ae6dbd683860b9edc254ea8acf5e04b5ae242e5",
        "message": "io_fail_links() doesn't consider REQ_F_COMP_LOCKED leading to nested\nspin_lock(completion_lock) and lockup.\n\n[  197.680409] rcu: INFO: rcu_preempt detected expedited stalls on\n\tCPUs/tasks: { 6-... } 18239 jiffies s: 1421 root: 0x40/.\n[  197.680411] rcu: blocking rcu_node structures:\n[  197.680412] Task dump for CPU 6:\n[  197.680413] link-timeout    R  running task        0  1669\n\t1 0x8000008a\n[  197.680414] Call Trace:\n[  197.680420]  ? io_req_find_next+0xa0/0x200\n[  197.680422]  ? io_put_req_find_next+0x2a/0x50\n[  197.680423]  ? io_poll_task_func+0xcf/0x140\n[  197.680425]  ? task_work_run+0x67/0xa0\n[  197.680426]  ? do_exit+0x35d/0xb70\n[  197.680429]  ? syscall_trace_enter+0x187/0x2c0\n[  197.680430]  ? do_group_exit+0x43/0xa0\n[  197.680448]  ? __x64_sys_exit_group+0x18/0x20\n[  197.680450]  ? do_syscall_64+0x52/0xa0\n[  197.680452]  ? entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8",
        "release_date": "2020-07-24 12:51:33 -0600 io_uring: fix lockup in io_fail_links()"
    },
    {
        "commit": "d5e16d8e23825304c6a9945116cc6b6f8d51f28c",
        "message": "req->work might be already initialised by the time it gets into\n__io_arm_poll_handler(), which will corrupt it by using fields that are\nin an union with req->work. Luckily, the only side effect is missing\nput_creds(). Clean req->work before going there.\n\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8",
        "release_date": "2020-07-24 12:51:33 -0600 io_uring: fix ->work corruption with poll_add"
    },
    {
        "commit": "3e863ea3bb1a2203ae648eb272db0ce6a1a2072c",
        "message": "IOSQE_ASYNC branch of io_queue_sqe() is another place where an\nunitialised req->work can be accessed (i.e. prior io_req_init_async()).\nNothing really bad though, it just looses IO_WQ_WORK_CONCURRENT flag.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc7",
        "release_date": "2020-07-23 11:20:55 -0600 io_uring: missed req_init_async() for IOSQE_ASYNC"
    },
    {
        "commit": "61710e437f2807e26a3402543bdbb7217a9c8620",
        "message": "We currently filter these for timeout_remove/async_cancel/files_update,\nbut we only should be filtering for fixed file and buffer select. This\nalso causes a second read of sqe->flags, which isn't needed.\n\nJust check req->flags for the relevant bits. This then allows these\ncommands to be used in links, for example, like everything else.\n\nSigned-off-by: Daniele Albano <d.albano@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc7",
        "release_date": "2020-07-18 14:15:16 -0600 io_uring: always allow drain/link/hardlink/async sqe flags"
    },
    {
        "commit": "807abcb0883439af5ead73f3308310453b97b624",
        "message": "The double poll additions were centered around doing POLL_ADD on file\ndescriptors that use more than one waitqueue (typically one for read,\none for write) when being polled. However, it can also end up being\ntriggered for when we use poll triggered retry. For that case, we cannot\nsafely use req->io, as that could be used by the request type itself.\n\nAdd a second io_poll_iocb pointer in the structure we allocate for poll\nbased retry, and ensure we use the right one from the two paths.\n\nFixes: 18bceab101ad (\"io_uring: allow POLL_ADD with double poll_wait() users\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc7",
        "release_date": "2020-07-17 19:41:05 -0600 io_uring: ensure double poll additions work with both request types"
    },
    {
        "commit": "4ebf8d7649cd86c41c41bf48da4b7761da2d5009",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Fix for a case where, with automatic buffer selection, we can leak the\n  buffer descriptor for recvmsg\"\n\n* tag 'io_uring-5.8-2020-07-17' of git://git.kernel.dk/linux-block:\n  io_uring: fix recvmsg memory leak with buffer selection",
        "kernel_version": "v5.8-rc6",
        "release_date": "2020-07-17 10:47:51 -0700 Merge tag 'io_uring-5.8-2020-07-17' of git://git.kernel.dk/linux-block into master"
    },
    {
        "commit": "681fda8d27a66f7e65ff7f2d200d7635e64a8d05",
        "message": "io_recvmsg() doesn't free memory allocated for struct io_buffer. This can\ncauses a leak when used with automatic buffer selection.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc6",
        "release_date": "2020-07-15 13:35:56 -0600 io_uring: fix recvmsg memory leak with buffer selection"
    },
    {
        "commit": "4437dd6e8f71e8b4b5546924a6e48e7edaf4a8dc",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two late fixes again:\n\n   - Fix missing msg_name assignment in certain cases (Pavel)\n\n   - Correct a previous fix for full coverage (Pavel)\"\n\n* tag 'io_uring-5.8-2020-07-12' of git://git.kernel.dk/linux-block:\n  io_uring: fix not initialised work->flags\n  io_uring: fix missing msg_name assignment",
        "kernel_version": "v5.8-rc5",
        "release_date": "2020-07-12 12:17:58 -0700 Merge tag 'io_uring-5.8-2020-07-12' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "16d598030a37853a7a6b4384cad19c9c0af2f021",
        "message": "59960b9deb535 (\"io_uring: fix lazy work init\") tried to fix missing\nio_req_init_async(), but left out work.flags and hash. Do it earlier.\n\nFixes: 7cdaf587de7c (\"io_uring: avoid whole io_wq_work copy for requests completed inline\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc5",
        "release_date": "2020-07-12 09:40:50 -0600 io_uring: fix not initialised work->flags"
    },
    {
        "commit": "dd821e0c95a64b5923a0c57f07d3f7563553e756",
        "message": "Ensure to set msg.msg_name for the async portion of send/recvmsg,\nas the header copy will copy to/from it.\n\nCc: stable@vger.kernel.org # v5.5+\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc5",
        "release_date": "2020-07-12 09:40:25 -0600 io_uring: fix missing msg_name assignment"
    },
    {
        "commit": "a581387e415bbb0085e7e67906c8f4a99746590e",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix memleak for error path in registered files (Yang)\n\n - Export CQ overflow state in flags, necessary to fix a case where\n   liburing doesn't know if it needs to enter the kernel (Xiaoguang)\n\n - Fix for a regression in when user memory is accounted freed, causing\n   issues with back-to-back ring exit + init if the ulimit -l setting is\n   very tight.\n\n* tag 'io_uring-5.8-2020-07-10' of git://git.kernel.dk/linux-block:\n  io_uring: account user memory freed when exit has been queued\n  io_uring: fix memleak in io_sqe_files_register()\n  io_uring: fix memleak in __io_sqe_files_update()\n  io_uring: export cq overflow status to userspace",
        "kernel_version": "v5.8-rc5",
        "release_date": "2020-07-10 09:57:57 -0700 Merge tag 'io_uring-5.8-2020-07-10' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "309fc03a3284af62eb6082fb60327045a1dabf57",
        "message": "We currently account the memory after the exit work has been run, but\nthat leaves a gap where a process has closed its ring and until the\nmemory has been accounted as freed. If the memlocked ulimit is\nborderline, then that can introduce spurious setup errors returning\n-ENOMEM because the free work hasn't been run yet.\n\nAccount this as freed when we close the ring, as not to expose a tiny\ngap where setting up a new ring can fail.\n\nFixes: 85faa7b8346e (\"io_uring: punt final io_ring_ctx wait-and-free to workqueue\")\nCc: stable@vger.kernel.org # v5.7\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc5",
        "release_date": "2020-07-10 09:18:35 -0600 io_uring: account user memory freed when exit has been queued"
    },
    {
        "commit": "667e57da358f61b6966e12e925a69e42d912e8bb",
        "message": "I got a memleak report when doing some fuzz test:\n\nBUG: memory leak\nunreferenced object 0x607eeac06e78 (size 8):\n  comm \"test\", pid 295, jiffies 4294735835 (age 31.745s)\n  hex dump (first 8 bytes):\n    00 00 00 00 00 00 00 00                          ........\n  backtrace:\n    [<00000000932632e6>] percpu_ref_init+0x2a/0x1b0\n    [<0000000092ddb796>] __io_uring_register+0x111d/0x22a0\n    [<00000000eadd6c77>] __x64_sys_io_uring_register+0x17b/0x480\n    [<00000000591b89a6>] do_syscall_64+0x56/0xa0\n    [<00000000864a281d>] entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nCall percpu_ref_exit() on error path to avoid\nrefcount memleak.\n\nFixes: 05f3fb3c5397 (\"io_uring: avoid ring quiesce for fixed file set unregister and update\")\nCc: stable@vger.kernel.org\nReported-by: Hulk Robot <hulkci@huawei.com>\nSigned-off-by: Yang Yingliang <yangyingliang@huawei.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc5",
        "release_date": "2020-07-10 07:50:21 -0600 io_uring: fix memleak in io_sqe_files_register()"
    },
    {
        "commit": "4349f30ecb8068d146a1e57bb12f46e745323b4c",
        "message": "We don't use 'ctx' at all in io_sq_thread_drop_mm(), it just works\non the mm of the current task. Drop the argument.\n\nMove io_file_put_work() to where we have the other forward declarations\nof functions.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-09 15:07:01 -0600 io_uring: remove dead 'ctx' argument and move forward declaration"
    },
    {
        "commit": "2bc9930e78fe0cb3e7b7e3169de0a40baee38d29",
        "message": "We just have one caller of this, req_need_defer(), just inline the\ncode in there instead.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-09 09:43:27 -0600 io_uring: get rid of __req_need_defer()"
    },
    {
        "commit": "f3bd9dae3708a0ff6b067e766073ffeb853301f9",
        "message": "I got a memleak report when doing some fuzz test:\n\nBUG: memory leak\nunreferenced object 0xffff888113e02300 (size 488):\ncomm \"syz-executor401\", pid 356, jiffies 4294809529 (age 11.954s)\nhex dump (first 32 bytes):\n00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................\na0 a4 ce 19 81 88 ff ff 60 ce 09 0d 81 88 ff ff ........`.......\nbacktrace:\n[<00000000129a84ec>] kmem_cache_zalloc include/linux/slab.h:659 [inline]\n[<00000000129a84ec>] __alloc_file+0x25/0x310 fs/file_table.c:101\n[<000000003050ad84>] alloc_empty_file+0x4f/0x120 fs/file_table.c:151\n[<000000004d0a41a3>] alloc_file+0x5e/0x550 fs/file_table.c:193\n[<000000002cb242f0>] alloc_file_pseudo+0x16a/0x240 fs/file_table.c:233\n[<00000000046a4baa>] anon_inode_getfile fs/anon_inodes.c:91 [inline]\n[<00000000046a4baa>] anon_inode_getfile+0xac/0x1c0 fs/anon_inodes.c:74\n[<0000000035beb745>] __do_sys_perf_event_open+0xd4a/0x2680 kernel/events/core.c:11720\n[<0000000049009dc7>] do_syscall_64+0x56/0xa0 arch/x86/entry/common.c:359\n[<00000000353731ca>] entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nBUG: memory leak\nunreferenced object 0xffff8881152dd5e0 (size 16):\ncomm \"syz-executor401\", pid 356, jiffies 4294809529 (age 11.954s)\nhex dump (first 16 bytes):\n01 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00 ................\nbacktrace:\n[<0000000074caa794>] kmem_cache_zalloc include/linux/slab.h:659 [inline]\n[<0000000074caa794>] lsm_file_alloc security/security.c:567 [inline]\n[<0000000074caa794>] security_file_alloc+0x32/0x160 security/security.c:1440\n[<00000000c6745ea3>] __alloc_file+0xba/0x310 fs/file_table.c:106\n[<000000003050ad84>] alloc_empty_file+0x4f/0x120 fs/file_table.c:151\n[<000000004d0a41a3>] alloc_file+0x5e/0x550 fs/file_table.c:193\n[<000000002cb242f0>] alloc_file_pseudo+0x16a/0x240 fs/file_table.c:233\n[<00000000046a4baa>] anon_inode_getfile fs/anon_inodes.c:91 [inline]\n[<00000000046a4baa>] anon_inode_getfile+0xac/0x1c0 fs/anon_inodes.c:74\n[<0000000035beb745>] __do_sys_perf_event_open+0xd4a/0x2680 kernel/events/core.c:11720\n[<0000000049009dc7>] do_syscall_64+0x56/0xa0 arch/x86/entry/common.c:359\n[<00000000353731ca>] entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nIf io_sqe_file_register() failed, we need put the file that get by fget()\nto avoid the memleak.\n\nFixes: c3a31e605620 (\"io_uring: add support for IORING_REGISTER_FILES_UPDATE\")\nCc: stable@vger.kernel.org\nReported-by: Hulk Robot <hulkci@huawei.com>\nSigned-off-by: Yang Yingliang <yangyingliang@huawei.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc5",
        "release_date": "2020-07-08 20:16:19 -0600 io_uring: fix memleak in __io_sqe_files_update()"
    },
    {
        "commit": "6d5f904904608a9cd32854d7d0a4dd65b27f9935",
        "message": "For those applications which are not willing to use io_uring_enter()\nto reap and handle cqes, they may completely rely on liburing's\nio_uring_peek_cqe(), but if cq ring has overflowed, currently because\nio_uring_peek_cqe() is not aware of this overflow, it won't enter\nkernel to flush cqes, below test program can reveal this bug:\n\nstatic void test_cq_overflow(struct io_uring *ring)\n{\n        struct io_uring_cqe *cqe;\n        struct io_uring_sqe *sqe;\n        int issued = 0;\n        int ret = 0;\n\n        do {\n                sqe = io_uring_get_sqe(ring);\n                if (!sqe) {\n                        fprintf(stderr, \"get sqe failed\\n\");\n                        break;;\n                }\n                ret = io_uring_submit(ring);\n                if (ret <= 0) {\n                        if (ret != -EBUSY)\n                                fprintf(stderr, \"sqe submit failed: %d\\n\", ret);\n                        break;\n                }\n                issued++;\n        } while (ret > 0);\n        assert(ret == -EBUSY);\n\n        printf(\"issued requests: %d\\n\", issued);\n\n        while (issued) {\n                ret = io_uring_peek_cqe(ring, &cqe);\n                if (ret) {\n                        if (ret != -EAGAIN) {\n                                fprintf(stderr, \"peek completion failed: %s\\n\",\n                                        strerror(ret));\n                                break;\n                        }\n                        printf(\"left requets: %d\\n\", issued);\n                        continue;\n                }\n                io_uring_cqe_seen(ring, cqe);\n                issued--;\n                printf(\"left requets: %d\\n\", issued);\n        }\n}\n\nint main(int argc, char *argv[])\n{\n        int ret;\n        struct io_uring ring;\n\n        ret = io_uring_queue_init(16, &ring, 0);\n        if (ret) {\n                fprintf(stderr, \"ring setup failed: %d\\n\", ret);\n                return 1;\n        }\n\n        test_cq_overflow(&ring);\n        return 0;\n}\n\nTo fix this issue, export cq overflow status to userspace by adding new\nIORING_SQ_CQ_OVERFLOW flag, then helper functions() in liburing, such as\nio_uring_peek_cqe, can be aware of this cq overflow and do flush accordingly.\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc5",
        "release_date": "2020-07-08 19:17:06 -0600 io_uring: export cq overflow status to userspace"
    },
    {
        "commit": "5acbbc8ed3a9aef71c6eb5f19ba24f7321200220",
        "message": "It's safe to call kfree() with a NULL pointer, but it's also pointless.\nMost of the time we don't have any data to free, and at millions of\nrequests per second, the redundant function call adds noticeable\noverhead (about 1.3% of the runtime).\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-08 15:15:26 -0600 io_uring: only call kfree() for a non-zero pointer"
    },
    {
        "commit": "aa340845ae6f019e0a12321a1741c14679bb0664",
        "message": "The \"apoll\" variable is freed and then used on the next line.  We need\nto move the free down a few lines.\n\nFixes: 0be0b0e33b0b (\"io_uring: simplify io_async_task_func()\")\nSigned-off-by: Dan Carpenter <dan.carpenter@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-08 13:15:04 -0600 io_uring: fix a use after free in io_async_task_func()"
    },
    {
        "commit": "482c6b614a4750f71ed9c928bb5b2007a05dd694",
        "message": "Merge in 5.8-rc4 for-5.9/block to setup for-5.9/drivers, to provide\na clean base and making the life for the NVMe changes easier.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\n\n* tag 'v5.8-rc4': (732 commits)\n  Linux 5.8-rc4\n  x86/ldt: use \"pr_info_once()\" instead of open-coding it badly\n  MIPS: Do not use smp_processor_id() in preemptible code\n  MIPS: Add missing EHB in mtc0 -> mfc0 sequence for DSPen\n  .gitignore: Do not track `defconfig` from `make savedefconfig`\n  io_uring: fix regression with always ignoring signals in io_cqring_wait()\n  x86/ldt: Disable 16-bit segments on Xen PV\n  x86/entry/32: Fix #MC and #DB wiring on x86_32\n  x86/entry/xen: Route #DB correctly on Xen PV\n  x86/entry, selftests: Further improve user entry sanity checks\n  x86/entry/compat: Clear RAX high bits on Xen PV SYSENTER\n  i2c: mlxcpld: check correct size of maximum RECV_LEN packet\n  i2c: add Kconfig help text for slave mode\n  i2c: slave-eeprom: update documentation\n  i2c: eg20t: Load module automatically if ID matches\n  i2c: designware: platdrv: Set class based on DMI\n  i2c: algo-pca: Add 0x78 as SCL stuck low status for PCA9665\n  mm/page_alloc: fix documentation error\n  vmalloc: fix the owner argument for the new __vmalloc_node_range callers\n  mm/cma.c: use exact_nid true to fix possible per-numa cma leak\n  ...",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-08 08:02:13 -0600 Merge tag 'v5.8-rc4' into for-5.9/drivers"
    },
    {
        "commit": "b2edc0a77fac19bbdef63cedb2ea34aec1a9a499",
        "message": "First of all don't spin in io_ring_ctx_wait_and_kill() on iopoll.\nRequests won't complete faster because of that, but only lengthen\nio_uring_release().\n\nThe same goes for offloaded cleanup in io_ring_exit_work() -- it\nalready has waiting loop, don't do blocking active spinning.\n\nFor that, pass min=0 into io_iopoll_[try_]reap_events(), so it won't\nactively spin. Leave the function if io_do_iopoll() there can't\ncomplete a request to sleep in io_ring_exit_work().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-07 12:00:03 -0600 io_uring: don't burn CPU for iopoll on exit"
    },
    {
        "commit": "7668b92a69b8201e2dd16a47a08efb93e909f419",
        "message": "Nobody checks io_iopoll_check()'s output parameter @nr_events.\nRemove the parameter and declare it further down the stack.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-07 12:00:03 -0600 io_uring: remove nr_events arg from iopoll_check()"
    },
    {
        "commit": "9dedd56301564acdbb1dd37cf09250a4c7b783c9",
        "message": "io_iopoll_reap_events() doesn't care about returned valued of\nio_iopoll_getevents() and does the same checks for list emptiness\nand need_resched(). Just use io_do_iopoll().\n\nio_sq_thread() doesn't check return value as well. It also passes min=0,\nso there never be the second iteration inside io_poll_getevents().\nInline it there too.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-07 12:00:03 -0600 io_uring: partially inline io_iopoll_getevents()"
    },
    {
        "commit": "3fcee5a6d5414df8ff4ee22f2477bde76d34527c",
        "message": "It's not nice to hold @uring_lock for too long io_iopoll_reap_events().\nFor instance, the lock is needed to publish requests to @poll_list, and\nthat locks out tasks doing that for no good reason. Loose it\noccasionally.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-06 09:06:20 -0600 io_uring: briefly loose locks while reaping events"
    },
    {
        "commit": "eba0a4dd2aa5c47ca5b0c56ffb6d6665e047ff72",
        "message": "Nobody adjusts *nr_events (number of completed requests) before calling\nio_iopoll_getevents(), so the passed @min shouldn't be adjusted as well.\nOthewise it can return less than initially asked @min without hitting\nneed_resched().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-06 09:06:20 -0600 io_uring: fix stopping iopoll'ing too early"
    },
    {
        "commit": "3aadc23e6054353ca056bf14e87250c79efbd7ed",
        "message": "->iopoll() may have completed current request, but instead of reaping\nit, io_do_iopoll() just continues with the next request in the list.\nAs a result it can leave just polled and completed request in the list\nup until next syscall. Even outer loop in io_iopoll_getevents() doesn't\nhelp the situation.\n\nE.g. poll_list: req0 -> req1\nIf req0->iopoll() completed both requests, and @min<=1,\nthen @req0 will be left behind.\n\nCheck whether a req was completed after ->iopoll().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-06 09:06:20 -0600 io_uring: don't delay iopoll'ed req completion"
    },
    {
        "commit": "8b3656af2a37dc538d21e144a5a94bacae05e9f1",
        "message": "Don't forget to fill cqe->flags properly in io_submit_flush_completions()\n\nFixes: a1d7c393c4711 (\"io_uring: enable READ/WRITE to use deferred completions\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-05 15:07:50 -0600 io_uring: fix lost cqe->flags"
    },
    {
        "commit": "652532ad459524d32c6bf1522e0b88d83b084d1a",
        "message": "A preparation path, extracts error path into a separate block. It looks\nsaner then calling req_set_fail_links() after io_put_req_find_next(), even\nthough it have been working well.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-05 15:07:37 -0600 io_uring: keep queue_sqe()'s fail path separately"
    },
    {
        "commit": "6df1db6b542436c6d429caa66e1045862fa36155",
        "message": "io_prep_linked_timeout() sets REQ_F_LINK_TIMEOUT altering refcounting of\nthe following linked request. After that someone should call\nio_queue_linked_timeout(), otherwise a submission reference of the linked\ntimeout won't be ever dropped.\n\nThat's what happens in io_steal_work() if io-wq decides to postpone linked\nrequest with io_wqe_enqueue(). io_queue_linked_timeout() can also be\npotentially called twice without synchronisation during re-submission,\ne.g. io_rw_resubmit().\n\nThere are the rules, whoever did io_prep_linked_timeout() must also call\nio_queue_linked_timeout(). To not do it twice, io_prep_linked_timeout()\nwill return non NULL only for the first call. That's controlled by\nREQ_F_LINK_TIMEOUT flag.\n\nAlso kill REQ_F_QUEUE_TIMEOUT.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-05 15:07:35 -0600 io_uring: fix mis-refcounting linked timeouts"
    },
    {
        "commit": "c2c4c83c58cbca23527fee93b49738a5a84272a1",
        "message": "Since we now have that in the 5.9 branch, convert the existing users of\ntask_work_add() to use this new helper.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-05 15:07:31 -0600 io_uring: use new io_req_task_work_add() helper throughout"
    },
    {
        "commit": "4c6e277c4cc4a6b3b2b9c66a7b014787ae757cc1",
        "message": "Provide a helper to run task_work instead of checking and running\nmanually in a bunch of different spots. While doing so, also move the\ntask run state setting where we run the task work. Then we can move it\nout of the callback helpers. This also helps ensure we only do this once\nper task_work list run, not per task_work item.\n\nSuggested-by: Oleg Nesterov <oleg@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-05 15:05:22 -0600 io_uring: abstract out task work running"
    },
    {
        "commit": "58c6a581decbcdd7f49eb7cab27ee14cef247fd5",
        "message": "Pull in task_work changes from the 5.8 series, as we'll need to apply\nthe same kind of changes to other parts in the 5.9 branch.\n\n* io_uring-5.8:\n  io_uring: fix regression with always ignoring signals in io_cqring_wait()\n  io_uring: use signal based task_work running\n  task_work: teach task_work_add() to do signal_wake_up()",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-07-05 15:04:17 -0600 Merge branch 'io_uring-5.8' into for-5.9/io_uring"
    },
    {
        "commit": "9fbe565cb78d10ca2619ee23eac27262cff45629",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Andres reported a regression with the fix that was merged earlier this\n  week, where his setup of using signals to interrupt io_uring CQ waits\n  no longer worked correctly.\n\n  Fix this, and also limit our use of TWA_SIGNAL to the case where we\n  need it, and continue using TWA_RESUME for task_work as before.\n\n  Since the original is marked for 5.7 stable, let's flush this one out\n  early\"\n\n* tag 'io_uring-5.8-2020-07-05' of git://git.kernel.dk/linux-block:\n  io_uring: fix regression with always ignoring signals in io_cqring_wait()",
        "kernel_version": "v5.8-rc4",
        "release_date": "2020-07-05 10:41:33 -0700 Merge tag 'io_uring-5.8-2020-07-05' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "b7db41c9e03b5189bc94993bd50e4506ac9e34c1",
        "message": "When switching to TWA_SIGNAL for task_work notifications, we also made\nany signal based condition in io_cqring_wait() return -ERESTARTSYS.\nThis breaks applications that rely on using signals to abort someone\nwaiting for events.\n\nCheck if we have a signal pending because of queued task_work, and\nrepeat the signal check once we've run the task_work. This provides a\nreliable way of telling the two apart.\n\nAdditionally, only use TWA_SIGNAL if we are using an eventfd. If not,\nwe don't have the dependency situation described in the original commit,\nand we can get by with just using TWA_RESUME like we previously did.\n\nFixes: ce593a6c480a (\"io_uring: use signal based task_work running\")\nCc: stable@vger.kernel.org # v5.7\nReported-by: Andres Freund <andres@anarazel.de>\nTested-by: Andres Freund <andres@anarazel.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc4",
        "release_date": "2020-07-04 13:44:45 -0600 io_uring: fix regression with always ignoring signals in io_cqring_wait()"
    },
    {
        "commit": "c93493b7cd40c20708e3373a7cc8e8049460d7ce",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"One fix in here, for a regression in 5.7 where a task is waiting in\n  the kernel for a condition, but that condition won't become true until\n  task_work is run. And the task_work can't be run exactly because the\n  task is waiting in the kernel, so we'll never make any progress.\n\n  One example of that is registering an eventfd and queueing io_uring\n  work, and then the task goes and waits in eventfd read with the\n  expectation that it'll get woken (and read an event) when the io_uring\n  request completes. The io_uring request is finished through task_work,\n  which won't get run while the task is looping in eventfd read\"\n\n* tag 'io_uring-5.8-2020-07-01' of git://git.kernel.dk/linux-block:\n  io_uring: use signal based task_work running\n  task_work: teach task_work_add() to do signal_wake_up()",
        "kernel_version": "v5.8-rc4",
        "release_date": "2020-07-02 14:56:22 -0700 Merge tag 'io_uring-5.8-2020-07-01' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "ce593a6c480a22acba08795be313c0c6d49dd35d",
        "message": "Since 5.7, we've been using task_work to trigger async running of\nrequests in the context of the original task. This generally works\ngreat, but there's a case where if the task is currently blocked\nin the kernel waiting on a condition to become true, it won't process\ntask_work. Even though the task is woken, it just checks whatever\ncondition it's waiting on, and goes back to sleep if it's still false.\n\nThis is a problem if that very condition only becomes true when that\ntask_work is run. An example of that is the task registering an eventfd\nwith io_uring, and it's now blocked waiting on an eventfd read. That\nread could depend on a completion event, and that completion event\nwon't get trigged until task_work has been run.\n\nUse the TWA_SIGNAL notification for task_work, so that we ensure that\nthe task always runs the work when queued.\n\nCc: stable@vger.kernel.org # v5.7\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc4",
        "release_date": "2020-06-30 12:39:05 -0600 io_uring: use signal based task_work running"
    },
    {
        "commit": "8eb06d7e8dd853d70668617dda57de4f6cebe651",
        "message": "There is a fancy bug, where exiting user task may not have ->mm,\nthat makes task_works to try to do kthread_use_mm(ctx->sqo_mm).\n\nDon't do that if sqo_mm is NULL.\n\n[  290.460558] WARNING: CPU: 6 PID: 150933 at kernel/kthread.c:1238\n\tkthread_use_mm+0xf3/0x110\n[  290.460579] CPU: 6 PID: 150933 Comm: read-write2 Tainted: G\n\tI E     5.8.0-rc2-00066-g9b21720607cf #531\n[  290.460580] RIP: 0010:kthread_use_mm+0xf3/0x110\n...\n[  290.460584] Call Trace:\n[  290.460584]  __io_sq_thread_acquire_mm.isra.0.part.0+0x25/0x30\n[  290.460584]  __io_req_task_submit+0x64/0x80\n[  290.460584]  io_req_task_submit+0x15/0x20\n[  290.460585]  task_work_run+0x67/0xa0\n[  290.460585]  do_exit+0x35d/0xb70\n[  290.460585]  do_group_exit+0x43/0xa0\n[  290.460585]  get_signal+0x140/0x900\n[  290.460586]  do_signal+0x37/0x780\n[  290.460586]  __prepare_exit_to_usermode+0x126/0x1c0\n[  290.460586]  __syscall_return_slowpath+0x3b/0x1c0\n[  290.460587]  do_syscall_64+0x5f/0xa0\n[  290.460587]  entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nfollowing with faults.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-30 09:33:02 -0600 io_uring: fix missing ->mm on exit"
    },
    {
        "commit": "3fa5e0f331280237af918ab2e7a160f5a68d3e7d",
        "message": "gcc 9.2.0 compiles io_req_find_next() as a separate function leaving\nthe first REQ_F_LINK_HEAD fast check not inlined. Help it by splitting\nout the check from the function.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-30 09:32:04 -0600 io_uring: optimise io_req_find_next() fast check"
    },
    {
        "commit": "0be0b0e33b0bfd08264b108512e44b3907fe987b",
        "message": "Greatly simplify io_async_task_func() removing duplicated functionality\nof __io_req_task_submit(). This do one extra spin lock/unlock for\ncancelled poll case, but that shouldn't happen often.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-30 09:32:04 -0600 io_uring: simplify io_async_task_func()"
    },
    {
        "commit": "ea1164e574e9af0a15ab730ead0861a4c7724142",
        "message": "io_poll_task_func() hand-coded link submission forgetting to set\nTASK_RUNNING, acquire mm, etc. Call existing helper for that,\ni.e. __io_req_task_submit().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-30 09:32:04 -0600 io_uring: fix NULL mm in io_poll_task_func()"
    },
    {
        "commit": "cf2f54255d0342cfbd273cbb964ad6bc7674f587",
        "message": "Actually, io_iopoll_queue() may have NULL ->mm, that's if SQ thread\ndidn't grabbed mm before doing iopoll. Don't fail reqs there, as after\nrecent changes it won't be punted directly but rather through task_work.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-30 09:32:04 -0600 io_uring: don't fail iopoll requeue without ->mm"
    },
    {
        "commit": "ab0b6451db2a8ed630b89ef3826b8ea994149444",
        "message": "Avoid jumping through hoops to silence unused variable warnings, and\nalso fix sparse rightfully complaining about the locking context:\n\nfs/io_uring.c:1593:39: warning: context imbalance in 'io_kill_linked_timeout' - unexpected unlock\n\nProvide the functional helper as __io_kill_linked_timeout(), and have\nseparate the locking from it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-30 08:43:15 -0600 io_uring: clean up io_kill_linked_timeout() locking"
    },
    {
        "commit": "cbdcb4357c000861b77369c34e110fa893d23607",
        "message": "Currently io_steal_work() is disabled, and every linked request should\ngo through task_work for initialisation. Do io_req_work_grab_env()\njust before io-wq punting and for the whole link, so any request\nreachable by io_steal_work() is prepared.\n\nThis is also interesting for another reason -- it localises\nio_req_work_grab_env() into one place just before io-wq punting, helping\nto to better manage req->work lifetime and add some neat\ncleanup/optimisations later.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-30 08:40:00 -0600 io_uring: do grab_env() just before punting"
    },
    {
        "commit": "debb85f496c9cc70663eac31d3ad9153839c844c",
        "message": "Remove io_req_work_grab_env() call from io_req_defer_prep(), just call\nit when neccessary.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-30 08:39:59 -0600 io_uring: factor out grab_env() from defer_prep()"
    },
    {
        "commit": "edcdfcc149a8d0c11d4dd2b23b5338af22e31a5f",
        "message": "Place io_req_init_async() in io_req_work_grab_env() so it won't be\nforgotten.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-30 08:39:59 -0600 io_uring: do init work in grab_env()"
    },
    {
        "commit": "351fd53595a3ceb88756a005e3b864f7c8cb86e4",
        "message": "Remove struct io_op_def *def parameter from io_req_work_grab_env(),\nit's trivially deducible from req->opcode and fast. The API is\ncleaner this way, and also helps the complier to understand\nthat it's a real constant and could be register-cached.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-30 08:39:59 -0600 io_uring: don't pass def into io_req_work_grab_env"
    },
    {
        "commit": "ecfc51777487da4da530710e0b13de4c8cb4a6d2",
        "message": "After __io_free_req() puts a ctx ref, it should be assumed that the ctx\nmay already be gone. However, it can be accessed when putting the\nfallback req. Free the req first and then put the ctx.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-30 08:39:59 -0600 io_uring: fix potential use after free on fallback request free"
    },
    {
        "commit": "8eb7e2d00763367f345ef0b2a2eb4f8001ae40ce",
        "message": "There are too many useless flags, kill REQ_F_TIMEOUT_NOSEQ, which can be\neasily infered from req.timeout itself.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-30 08:39:59 -0600 io_uring: kill REQ_F_TIMEOUT_NOSEQ"
    },
    {
        "commit": "a1a4661691c5f1a3af4c04f56ad68e2d1dbee3af",
        "message": "Now REQ_F_TIMEOUT is set but never used, kill it\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-30 08:39:59 -0600 io_uring: kill REQ_F_TIMEOUT"
    },
    {
        "commit": "9b5f7bd93272689ec8dc2cfd40a812265c23414e",
        "message": "Generally, it's better to return a value directly than having out\nparameter. It's cleaner and saves from some kinds of ugly bugs.\nMay also be faster.\n\nReturn next request from io_req_find_next() and friends directly\ninstead of passing out parameter.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-30 08:39:57 -0600 io_uring: replace find_next() out param with ret"
    },
    {
        "commit": "7c86ffeeed303187f266ed17bd87a9b375955709",
        "message": "Linked timeout cancellation code is repeated in in io_req_link_next()\nand io_fail_links(), and they differ in details even though shouldn't.\nBasing on the fact that there is maximum one armed linked timeout in\na link, and it immediately follows the head, extract a function that\nwill check for it and defuse.\n\nJustification:\n- DRY and cleaner\n- better inlining for io_req_link_next() (just 1 call site now)\n- isolates linked_timeouts from common path\n- reduces time under spinlock for failed links\n- actually less code\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\n[axboe: fold in locking fix for io_fail_links()]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-30 08:38:58 -0600 io_uring: deduplicate freeing linked timeouts"
    },
    {
        "commit": "fb49278624f75e15d36c3c43d322ca8961fb40e9",
        "message": "Don't forget to wake up a process to which io_rw_reissue() added\ntask_work.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-29 07:43:03 -0600 io_uring: fix missing wake_up io_rw_reissue()"
    },
    {
        "commit": "f3a6fa2267480d7f19fbde8316372be46055e548",
        "message": "req->iopoll() is not necessarily called by a task that submitted a\nrequest. Because of that, it's dangerous to grab_env() and punt async on\n-EGAIN, potentially grabbing another task's mm and corrupting its\nmemory.\n\nDo resubmit from the submitter task context.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-28 08:13:03 -0600 io_uring: fix iopoll -EAGAIN handling"
    },
    {
        "commit": "3adfecaa647ff8afa4b6f5907193cf751a0f8351",
        "message": "There are a lot of new users of task_work, and some of task_work_add()\nmay happen while we do io polling, thus make iopoll from time to time\nto do task_work_run(), so it doesn't poll for sitting there reqs.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-28 08:13:03 -0600 io_uring: do task_work_run() during iopoll"
    },
    {
        "commit": "6795c5aba247653f99d1f336ff496dd74659b322",
        "message": "Assign req->result to io_size early in io_{read,write}(), it's enough\nand makes it more straightforward.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-28 08:10:17 -0600 io_uring: clean up req->result setting by rw"
    },
    {
        "commit": "9b0d911acce00b67f7e7336f838b732de7d917d6",
        "message": "After pulling nxt from a request, it's no more a links head, so clear\nREQ_F_LINK_HEAD. Absence of this flag also indicates that there are no\nlinked requests, so replacing REQ_F_LINK_NEXT, which can be killed.\n\nLinked timeouts also behave leaving the flag intact when necessary.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-28 08:10:17 -0600 io_uring: kill REQ_F_LINK_NEXT"
    },
    {
        "commit": "2d6500d44c1374808040d120e625a22b013c9f0d",
        "message": "Move all batch free bits close to each other and rename in a consistent\nway.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-28 08:10:17 -0600 io_uring: cosmetic changes for batch free"
    },
    {
        "commit": "c3524383333e4ff2f720ab0c02b3a329f72de78b",
        "message": "There is no reason to not batch deallocation of linked requests. Take\naway its next req first and handle it as everything else in\nio_req_multi_free().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-28 08:10:17 -0600 io_uring: batch-free linked requests as well"
    },
    {
        "commit": "2757a23e7f6441eabf605ca59eeb88c34071757d",
        "message": "Every request in io_req_multi_free() is has ->file set. Instead of\npointlessly defering and counting reqs with file, dismantle it on place\nand save for batch dealloc.\n\nIt also saves us from potentially skipping io_cleanup_req(), put_task(),\netc. Never happens though, becacuse ->file is always there.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-28 08:10:17 -0600 io_uring: dismantle req early and remove need_iter"
    },
    {
        "commit": "e6543a816edca00b6b4c48625d142059d7211059",
        "message": "io_free_req_many() is used only for iopoll requests, i.e. reads/writes.\nHence no need to batch inflight unhooking. For safety, it'll be done by\nio_dismantle_req(), which replaces __io_req_aux_free(), and looks more\nsolid and cleaner.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-28 08:10:17 -0600 io_uring: remove inflight batching in free_many()"
    },
    {
        "commit": "8c9cb6cd9a46ae6fb7cb6c39cf6a48a53440feef",
        "message": "Now io_complete_rw_common() puts a ref, extra io_req_put() in\nio_iopoll_queue() causes undeflow. Remove it.\n\n[  455.998620] refcount_t: underflow; use-after-free.\n[  455.998743] WARNING: CPU: 6 PID: 285394 at lib/refcount.c:28\n\trefcount_warn_saturate+0xae/0xf0\n[  455.998772] CPU: 6 PID: 285394 Comm: read-write2 Tainted: G\n          I E     5.8.0-rc2-00048-g1b1aa738f167-dirty #509\n[  455.998772] RIP: 0010:refcount_warn_saturate+0xae/0xf0\n...\n[  455.998778] Call Trace:\n[  455.998778]  io_put_req+0x44/0x50\n[  455.998778]  io_iopoll_complete+0x245/0x370\n[  455.998779]  io_iopoll_getevents+0x12f/0x1a0\n[  455.998779]  io_iopoll_reap_events.part.0+0x5e/0xa0\n[  455.998780]  io_ring_ctx_wait_and_kill+0x132/0x1c0\n[  455.998780]  io_uring_release+0x20/0x30\n[  455.998780]  __fput+0xcd/0x230\n[  455.998781]  ____fput+0xe/0x10\n[  455.998781]  task_work_run+0x67/0xa0\n[  455.998781]  do_exit+0x35d/0xb70\n[  455.998782]  do_group_exit+0x43/0xa0\n[  455.998783]  get_signal+0x140/0x900\n[  455.998783]  do_signal+0x37/0x780\n[  455.998784]  __prepare_exit_to_usermode+0x126/0x1c0\n[  455.998785]  __syscall_return_slowpath+0x3b/0x1c0\n[  455.998785]  do_syscall_64+0x5f/0xa0\n[  455.998785]  entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nFixes: a1d7c393c47 (\"io_uring: enable READ/WRITE to use deferred completions\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-28 08:10:17 -0600 io_uring: fix refs underflow in io_iopoll_queue()"
    },
    {
        "commit": "710c2bfb66474a186b0196e3342d43db0e6c04e1",
        "message": "We won't have valid ring_fd, ring_file in task work. Grab files early.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-28 08:10:17 -0600 io_uring: fix missing io_grab_files()"
    },
    {
        "commit": "a6d45dd0d43e6d1275e002704540688b6768bc22",
        "message": "No reason to mark a head of a link as for-async in io_req_defer_prep().\ngrab_env(), etc. That will be done further during submission if\nneccessary.\n\nMark for_async=false saving extra grab_env() in many cases.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-28 08:10:17 -0600 io_uring: don't mark link's head for_async"
    },
    {
        "commit": "1bcb8c5d65a845e0ecb9e82237c399b29b8d15ea",
        "message": "io_steal_work() can't be sure that @nxt has req->work properly set, so we\ncan't pass it to io-wq as is.\n\nA dirty quick fix -- drag it through io_req_task_queue(), and always\nreturn NULL from io_steal_work().\n\ne.g.\n\n[   50.770161] BUG: kernel NULL pointer dereference, address: 00000000\n[   50.770164] #PF: supervisor write access in kernel mode\n[   50.770164] #PF: error_code(0x0002) - not-present page\n[   50.770168] CPU: 1 PID: 1448 Comm: io_wqe_worker-0 Tainted: G\n\tI       5.8.0-rc2-00035-g2237d76530eb-dirty #494\n[   50.770172] RIP: 0010:override_creds+0x19/0x30\n...\n[   50.770183]  io_worker_handle_work+0x25c/0x430\n[   50.770185]  io_wqe_worker+0x2a0/0x350\n[   50.770190]  kthread+0x136/0x180\n[   50.770194]  ret_from_fork+0x22/0x30\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-28 08:10:17 -0600 io_uring: fix feeding io-wq with uninit reqs"
    },
    {
        "commit": "906a8c3fdbc367325d4200e39212a2a7715b7b0e",
        "message": "It's not enough to check for REQ_F_WORK_INITIALIZED and punt async\nassuming that io_req_work_grab_env() was called, it may not have been.\nE.g. io_close_prep() and personality path set the flag without further\nasync init.\n\nAs a quick fix, always pass next work through io_req_task_queue().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-28 08:10:16 -0600 io_uring: fix punting req w/o grabbed env"
    },
    {
        "commit": "8ef77766ba8694968ed4ba24311b4bacee14f235",
        "message": "req->work and req->task_work are in a union, so io_req_task_queue() screws\neverything that was in work. De-union them for now.\n\n[  704.367253] BUG: unable to handle page fault for address:\n\tffffffffaf7330d0\n[  704.367256] #PF: supervisor write access in kernel mode\n[  704.367256] #PF: error_code(0x0003) - permissions violation\n[  704.367261] CPU: 6 PID: 1654 Comm: io_wqe_worker-0 Tainted: G\nI       5.8.0-rc2-00038-ge28d0bdc4863-dirty #498\n[  704.367265] RIP: 0010:_raw_spin_lock+0x1e/0x36\n...\n[  704.367276]  __alloc_fd+0x35/0x150\n[  704.367279]  __get_unused_fd_flags+0x25/0x30\n[  704.367280]  io_openat2+0xcb/0x1b0\n[  704.367283]  io_issue_sqe+0x36a/0x1320\n[  704.367294]  io_wq_submit_work+0x58/0x160\n[  704.367295]  io_worker_handle_work+0x2a3/0x430\n[  704.367296]  io_wqe_worker+0x2a0/0x350\n[  704.367301]  kthread+0x136/0x180\n[  704.367304]  ret_from_fork+0x22/0x30\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-28 08:10:10 -0600 io_uring: fix req->work corruption"
    },
    {
        "commit": "ab0f2473d374f0dc4e3cc3f386abfafd8cf08ed2",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Three small fixes:\n\n   - Close a corner case for polled IO resubmission (Pavel)\n\n   - Toss commands when exiting (Pavel)\n\n   - Fix SQPOLL conditional reschedule on perpetually busy submit\n     (Xuan)\"\n\n* tag 'io_uring-5.8-2020-06-26' of git://git.kernel.dk/linux-block:\n  io_uring: fix current->mm NULL dereference on exit\n  io_uring: fix hanging iopoll in case of -EAGAIN\n  io_uring: fix io_sq_thread no schedule when busy",
        "kernel_version": "v5.8-rc3",
        "release_date": "2020-06-27 09:02:49 -0700 Merge tag 'io_uring-5.8-2020-06-26' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "1e16c2f917a59d27fb6b540c44d66978c8ad29ef",
        "message": "Fix build errors when CONFIG_NET is not set/enabled:\n\n../fs/io_uring.c:5472:10: error: too many arguments to function \u2018io_sendmsg\u2019\n../fs/io_uring.c:5474:10: error: too many arguments to function \u2018io_send\u2019\n../fs/io_uring.c:5484:10: error: too many arguments to function \u2018io_recvmsg\u2019\n../fs/io_uring.c:5486:10: error: too many arguments to function \u2018io_recv\u2019\n../fs/io_uring.c:5510:9: error: too many arguments to function \u2018io_accept\u2019\n../fs/io_uring.c:5518:9: error: too many arguments to function \u2018io_connect\u2019\n\nSigned-off-by: Randy Dunlap <rdunlap@infradead.org>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: io-uring@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-26 19:46:18 -0600 io_uring: fix function args for !CONFIG_NET"
    },
    {
        "commit": "2237d76530ebf8e4fc3a0c6048d74cff808dee8f",
        "message": "Merge in changes that went into 5.8-rc3. GIT will silently do the\nmerge, but we still need a tweak on top of that since\nio_complete_rw_common() was modified to take a io_comp_state pointer.\nThe auto-merge fails on that, and we end up with something that\ndoesn't compile.\n\n* io_uring-5.8:\n  io_uring: fix current->mm NULL dereference on exit\n  io_uring: fix hanging iopoll in case of -EAGAIN\n  io_uring: fix io_sq_thread no schedule when busy\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-26 13:44:16 -0600 Merge branch 'io_uring-5.8' into for-5.9/io_uring"
    },
    {
        "commit": "c40f63790ec957e9449056fb78d8c2523eff96b5",
        "message": "Currently links are always done in an async fashion, unless we catch them\ninline after we successfully complete a request without having to resort\nto blocking. This isn't necessarily the most efficient approach, it'd be\nmore ideal if we could just use the task_work handling for this.\n\nOutside of saving an async jump, we can also do less prep work for these\nkinds of requests.\n\nRunning dependent links from the task_work handler yields some nice\nperformance benefits. As an example, examples/link-cp from the liburing\nrepository uses read+write links to implement a copy operation. Without\nthis patch, the a cache fold 4G file read from a VM runs in about 3\nseconds:\n\n$ time examples/link-cp /data/file /dev/null\n\nreal\t0m2.986s\nuser\t0m0.051s\nsys\t0m2.843s\n\nand a subsequent cache hot run looks like this:\n\n$ time examples/link-cp /data/file /dev/null\n\nreal\t0m0.898s\nuser\t0m0.069s\nsys\t0m0.797s\n\nWith this patch in place, the cold case takes about 2.4 seconds:\n\n$ time examples/link-cp /data/file /dev/null\n\nreal\t0m2.400s\nuser\t0m0.020s\nsys\t0m2.366s\n\nand the cache hot case looks like this:\n\n$ time examples/link-cp /data/file /dev/null\n\nreal\t0m0.676s\nuser\t0m0.010s\nsys\t0m0.665s\n\nAs expected, the (mostly) cache hot case yields the biggest improvement,\nrunning about 25% faster with this change, while the cache cold case\nyields about a 20% increase in performance. Outside of the performance\nincrease, we're using less CPU as well, as we're not using the async\noffload threads at all for this anymore.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-26 10:34:23 -0600 io_uring: use task_work for links if possible"
    },
    {
        "commit": "a1d7c393c4711a9ce6c239c3ab053a50dc96505a",
        "message": "A bit more surgery required here, as completions are generally done\nthrough the kiocb->ki_complete() callback, even if they complete inline.\nThis enables the regular read/write path to use the io_comp_state\nlogic to batch inline completions.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-25 07:23:49 -0600 io_uring: enable READ/WRITE to use deferred completions"
    },
    {
        "commit": "229a7b63507a3e84afb17c3bbb67505a81d28a1d",
        "message": "Provide the completion state to the handlers that we know can complete\ninline, so they can utilize this for batching completions.\n\nCap the max batch count at 32. This should be enough to provide a good\namortization of the cost of the lock+commit dance for completions, while\nstill being low enough not to cause any real latency issues for SQPOLL\napplications.\n\nXuan Zhuo <xuanzhuo@linux.alibaba.com> reports that this changes his\nprofile from:\n\n17.97% [kernel] [k] copy_user_generic_unrolled\n13.92% [kernel] [k] io_commit_cqring\n11.04% [kernel] [k] __io_cqring_fill_event\n10.33% [kernel] [k] udp_recvmsg\n 5.94% [kernel] [k] skb_release_data\n 4.31% [kernel] [k] udp_rmem_release\n 2.68% [kernel] [k] __check_object_size\n 2.24% [kernel] [k] __slab_free\n 2.22% [kernel] [k] _raw_spin_lock_bh\n 2.21% [kernel] [k] kmem_cache_free\n 2.13% [kernel] [k] free_pcppages_bulk\n 1.83% [kernel] [k] io_submit_sqes\n 1.38% [kernel] [k] page_frag_free\n 1.31% [kernel] [k] inet_recvmsg\n\nto\n\n19.99% [kernel] [k] copy_user_generic_unrolled\n11.63% [kernel] [k] skb_release_data\n 9.36% [kernel] [k] udp_rmem_release\n 8.64% [kernel] [k] udp_recvmsg\n 6.21% [kernel] [k] __slab_free\n 4.39% [kernel] [k] __check_object_size\n 3.64% [kernel] [k] free_pcppages_bulk\n 2.41% [kernel] [k] kmem_cache_free\n 2.00% [kernel] [k] io_submit_sqes\n 1.95% [kernel] [k] page_frag_free\n 1.54% [kernel] [k] io_put_req\n[...]\n 0.07% [kernel] [k] io_commit_cqring\n 0.44% [kernel] [k] __io_cqring_fill_event\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-25 07:23:46 -0600 io_uring: pass in completion state to appropriate issue side handlers"
    },
    {
        "commit": "f13fad7ba41cef806358885fbb3f9004f3214b2d",
        "message": "No functional changes in this patch, just in preparation for having the\ncompletion state be available on the issue side. Later on, this will\nallow requests that complete inline to be completed in batches.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-25 07:23:44 -0600 io_uring: pass down completion state on the issue side"
    },
    {
        "commit": "013538bd65fd3cdbf3ca8b0c99b962c70473c803",
        "message": "No functional changes in this patch, just in preparation for passing back\npending completions to the caller and completing them in a batched\nfashion.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-25 07:22:50 -0600 io_uring: add 'io_comp_state' to struct io_submit_state"
    },
    {
        "commit": "e1e16097e265daac918ce355bf1a0d1677adf0c7",
        "message": "We have lots of callers of:\n\nio_cqring_add_event(req, result);\nio_put_req(req);\n\nProvide a helper that does this for us. It helps clean up the code, and\nalso provides a more convenient location for us to change the completion\nhandling.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-25 07:22:41 -0600 io_uring: provide generic io_req_complete() helper"
    },
    {
        "commit": "d3cac64c498c4fb2df46b97ee6f4c7d6d75f5e3d",
        "message": "__io_queue_sqe() tries to handle all request of a link,\nso it's not enough to grab mm in io_sq_thread_acquire_mm()\nbased just on the head.\n\nDon't check req->needs_mm and do it always.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-25 07:22:38 -0600 io_uring: fix NULL-mm for linked reqs"
    },
    {
        "commit": "d60b5fbc1ce8210759b568da49d149b868e7c6d3",
        "message": "Don't reissue requests from io_iopoll_reap_events(), the task may not\nhave mm, which ends up with NULL. It's better to kill everything off on\nexit anyway.\n\n[  677.734670] RIP: 0010:io_iopoll_complete+0x27e/0x630\n...\n[  677.734679] Call Trace:\n[  677.734695]  ? __send_signal+0x1f2/0x420\n[  677.734698]  ? _raw_spin_unlock_irqrestore+0x24/0x40\n[  677.734699]  ? send_signal+0xf5/0x140\n[  677.734700]  io_iopoll_getevents+0x12f/0x1a0\n[  677.734702]  io_iopoll_reap_events.part.0+0x5e/0xa0\n[  677.734703]  io_ring_ctx_wait_and_kill+0x132/0x1c0\n[  677.734704]  io_uring_release+0x20/0x30\n[  677.734706]  __fput+0xcd/0x230\n[  677.734707]  ____fput+0xe/0x10\n[  677.734709]  task_work_run+0x67/0xa0\n[  677.734710]  do_exit+0x35d/0xb70\n[  677.734712]  do_group_exit+0x43/0xa0\n[  677.734713]  get_signal+0x140/0x900\n[  677.734715]  do_signal+0x37/0x780\n[  677.734717]  ? enqueue_hrtimer+0x41/0xb0\n[  677.734718]  ? recalibrate_cpu_khz+0x10/0x10\n[  677.734720]  ? ktime_get+0x3e/0xa0\n[  677.734721]  ? lapic_next_deadline+0x26/0x30\n[  677.734723]  ? tick_program_event+0x4d/0x90\n[  677.734724]  ? __hrtimer_get_next_event+0x4d/0x80\n[  677.734726]  __prepare_exit_to_usermode+0x126/0x1c0\n[  677.734741]  prepare_exit_to_usermode+0x9/0x40\n[  677.734742]  idtentry_exit_cond_rcu+0x4c/0x60\n[  677.734743]  sysvec_reschedule_ipi+0x92/0x160\n[  677.734744]  ? asm_sysvec_reschedule_ipi+0xa/0x20\n[  677.734745]  asm_sysvec_reschedule_ipi+0x12/0x20\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc3",
        "release_date": "2020-06-25 07:20:43 -0600 io_uring: fix current->mm NULL dereference on exit"
    },
    {
        "commit": "cd664b0e35cb1202f40c259a1a5ea791d18c879d",
        "message": "io_do_iopoll() won't do anything with a request unless\nreq->iopoll_completed is set. So io_complete_rw_iopoll() has to set\nit, otherwise io_do_iopoll() will poll a file again and again even\nthough the request of interest was completed long time ago.\n\nAlso, remove -EAGAIN check from io_issue_sqe() as it races with\nthe changed lines. The request will take the long way and be\nresubmitted from io_iopoll*().\n\nio_kiocb's result and iopoll_completed\")\n\nFixes: bbde017a32b3 (\"io_uring: add memory barrier to synchronize\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc3",
        "release_date": "2020-06-25 07:20:43 -0600 io_uring: fix hanging iopoll in case of -EAGAIN"
    },
    {
        "commit": "b772f07add1c0b22e02c0f1e96f647560679d3a9",
        "message": "When the user consumes and generates sqe at a fast rate,\nio_sqring_entries can always get sqe, and ret will not be equal to -EBUSY,\nso that io_sq_thread will never call cond_resched or schedule, and then\nwe will get the following system error prompt:\n\nrcu: INFO: rcu_sched self-detected stall on CPU\nor\nwatchdog: BUG: soft lockup-CPU#23 stuck for 112s! [io_uring-sq:1863]\n\nThis patch checks whether need to call cond_resched() by checking\nthe need_resched() function every cycle.\n\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Xuan Zhuo <xuanzhuo@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc3",
        "release_date": "2020-06-23 11:54:30 -0600 io_uring: fix io_sq_thread no schedule when busy"
    },
    {
        "commit": "f6b6c7d6a9600bdbf5826f57137630e1670e2d87",
        "message": "After recent changes, io_submit_sqes() always passes valid submit state,\nso kill leftovers checking it for NULL.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-21 20:46:05 -0600 io_uring: kill NULL checks for submit state"
    },
    {
        "commit": "b90cd197f9315f968d5ee4e6ee9f4e3067f2c883",
        "message": "It's a good practice to modify fields of a struct after but not before\nit was initialised. Even though io_init_poll_iocb() doesn't touch\npoll->file, call it first.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-21 20:46:05 -0600 io_uring: set @poll->file after @poll init"
    },
    {
        "commit": "24c74678634b3cbdb325b3b7706366c83811b311",
        "message": "REQ_F_MUST_PUNT may seem looking good and clear, but it's the same\nas not having REQ_F_NOWAIT set. That rather creates more confusion.\nMoreover, it doesn't even affect any behaviour (e.g. see the patch\nremoving it from io_{read,write}).\n\nKill theg flag and update already outdated comments.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-21 20:46:05 -0600 io_uring: remove REQ_F_MUST_PUNT"
    },
    {
        "commit": "62ef73165091476d31f31e33d9d0d48b088c129d",
        "message": "io_{read,write}() {\n\t...\ncopy_iov: // prep async\n  \tif (!(flags & REQ_F_NOWAIT) && !file_can_poll(file))\n\t\tflags |= REQ_F_MUST_PUNT;\n}\n\nREQ_F_MUST_PUNT there is pointless, because if it happens then\nREQ_F_NOWAIT is known to be _not_ set, and the request will go\nasync path in __io_queue_sqe() anyway. file_can_poll() check\nis also repeated in arm_poll*(), so don't need it.\n\nRemove the mentioned assignment REQ_F_MUST_PUNT in preparation\nfor killing the flag.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-21 20:46:03 -0600 io_uring: remove setting REQ_F_MUST_PUNT in rw"
    },
    {
        "commit": "895aa7b1a0cc42de985b310478688138719bc293",
        "message": "Pull in async buffered reads branch.\n\n* async-buffered.8:\n  io_uring: support true async buffered reads, if file provides it\n  mm: add kiocb_wait_page_queue_init() helper\n  btrfs: flag files as supporting buffered async reads\n  xfs: flag files as supporting buffered async reads\n  block: flag block devices as supporting IOCB_WAITQ\n  fs: add FMODE_BUF_RASYNC\n  mm: support async buffered reads in generic_file_buffered_read()\n  mm: add support for async page locking\n  mm: abstract out wake_page_match() from wake_page_function()\n  mm: allow read-ahead with IOCB_NOWAIT set\n  io_uring: re-issue block requests that failed because of resources\n  io_uring: catch -EIO from buffered issue request failure\n  io_uring: always plug for any number of IOs\n  block: provide plug based way of signaling forced no-wait semantics",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-21 20:44:36 -0600 Merge branch 'async-buffered.8' into for-5.9/io_uring"
    },
    {
        "commit": "bcf5a06304d69a3bb194a494d87b532d5e90b01c",
        "message": "If the file is flagged with FMODE_BUF_RASYNC, then we don't have to punt\nthe buffered read to an io-wq worker. Instead we can rely on page\nunlocking callbacks to support retry based async IO. This is a lot more\nefficient than doing async thread offload.\n\nThe retry is done similarly to how we handle poll based retry. From\nthe unlock callback, we simply queue the retry to a task_work based\nhandler.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-21 20:44:26 -0600 io_uring: support true async buffered reads, if file provides it"
    },
    {
        "commit": "b63534c41e20b474483b4ddf47efc858c17352e0",
        "message": "Mark the plug with nowait == true, which will cause requests to avoid\nblocking on request allocation. If they do, we catch them and reissue\nthem from a task_work based handler.\n\nNormally we can catch -EAGAIN directly, but the hard case is for split\nrequests. As an example, the application issues a 512KB request. The\nblock core will split this into 128KB if that's the max size for the\ndevice. The first request issues just fine, but we run into -EAGAIN for\nsome latter splits for the same request. As the bio is split, we don't\nget to see the -EAGAIN until one of the actual reads complete, and hence\nwe cannot handle it inline as part of submission.\n\nThis does potentially cause re-reads of parts of the range, as the whole\nrequest is reissued. There's currently no better way to handle this.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-21 20:44:25 -0600 io_uring: re-issue block requests that failed because of resources"
    },
    {
        "commit": "4503b7676a2e0abe69c2f2c0d8b03aec53f2f048",
        "message": "-EIO bubbles up like -EAGAIN if we fail to allocate a request at the\nlower level. Play it safe and treat it like -EAGAIN in terms of sync\nretry, to avoid passing back an errant -EIO.\n\nCatch some of these early for block based file, as non-mq devices\ngenerally do not support NOWAIT. That saves us some overhead by\nnot first trying, then retrying from async context. We can go straight\nto async punt instead.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-21 20:44:25 -0600 io_uring: catch -EIO from buffered issue request failure"
    },
    {
        "commit": "ac8691c415e0ce0b8734cb6d9df2df18608eebed",
        "message": "Currently we only plug if we're doing more than two request. We're going\nto be relying on always having the plug there to pass down information,\nso plug unconditionally.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-21 20:44:25 -0600 io_uring: always plug for any number of IOs"
    },
    {
        "commit": "2e0464d48f32a9e78e2aa85cbbedc77ecbb6ed60",
        "message": "Ring pages are not pinned so it is more appropriate to report them\nas locked.\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-21 20:44:01 -0600 io_uring: separate reporting of ring pages from registered pages"
    },
    {
        "commit": "309758254ea62e07471abcaeca5b5c2173f4ebc2",
        "message": "Report pinned memory usage always, regardless of whether locked memory\nlimit is enforced.\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-21 20:44:01 -0600 io_uring: report pinned memory usage"
    },
    {
        "commit": "aad5d8da1b301fe399d65f2dcb84df2ec60caaa3",
        "message": "Rename account_mem to limit_name to clarify its purpose.\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-21 20:44:01 -0600 io_uring: rename ctx->account_mem field"
    },
    {
        "commit": "a087e2b519929152fdde8299457e32d5a8994a7c",
        "message": "Facilitate separation of locked memory usage reporting vs. limiting for\nupcoming patches.  No functional changes.\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\n[axboe: kill unnecessary () around return in io_account_mem()]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-21 20:44:00 -0600 io_uring: add wrappers for memory accounting"
    },
    {
        "commit": "a31eb4a2f1650fa578082ad9e9845487ecd90abe",
        "message": "Applications can pass this flag in to avoid accept thundering herd.\n\nSigned-off-by: Jiufei Xue <jiufei.xue@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-21 20:44:00 -0600 io_uring: use EPOLLEXCLUSIVE flag to aoid thundering herd type behavior"
    },
    {
        "commit": "5769a351b89cd4d82016f18fa5f6c4077403564d",
        "message": "poll events should be 32-bits to cover EPOLLEXCLUSIVE.\n\nExplicit word-swap the poll32_events for big endian to make sure the ABI\nis not changed.  We call this feature IORING_FEAT_POLL_32BITS,\napplications who want to use EPOLLEXCLUSIVE should check the feature bit\nfirst.\n\nSigned-off-by: Jiufei Xue <jiufei.xue@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.9-rc1",
        "release_date": "2020-06-21 20:44:00 -0600 io_uring: change the poll type to be 32-bits"
    },
    {
        "commit": "4333a9b0b67bb4e8bcd91bdd80da80b0ec151162",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Catch a case where io_sq_thread() didn't do proper mm acquire\n\n - Ensure poll completions are reaped on shutdown\n\n - Async cancelation and run fixes (Pavel)\n\n - io-poll race fixes (Xiaoguang)\n\n - Request cleanup race fix (Xiaoguang)\n\n* tag 'io_uring-5.8-2020-06-19' of git://git.kernel.dk/linux-block:\n  io_uring: fix possible race condition against REQ_F_NEED_CLEANUP\n  io_uring: reap poll completions while waiting for refs to drop on exit\n  io_uring: acquire 'mm' for task_work for SQPOLL\n  io_uring: add memory barrier to synchronize io_kiocb's result and iopoll_completed\n  io_uring: don't fail links for EAGAIN error in IOPOLL mode\n  io_uring: cancel by ->task not pid\n  io_uring: lazy get task\n  io_uring: batch cancel in io_uring_cancel_files()\n  io_uring: cancel all task's requests on exit\n  io-wq: add an option to cancel all matched reqs\n  io-wq: reorder cancellation pending -> running\n  io_uring: fix lazy work init",
        "kernel_version": "v5.8-rc2",
        "release_date": "2020-06-19 13:16:58 -0700 Merge tag 'io_uring-5.8-2020-06-19' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "6f2cc1664db20676069cff27a461ccc97dbfd114",
        "message": "In io_read() or io_write(), when io request is submitted successfully,\nit'll go through the below sequence:\n\n    kfree(iovec);\n    req->flags &= ~REQ_F_NEED_CLEANUP;\n    return ret;\n\nBut clearing REQ_F_NEED_CLEANUP might be unsafe. The io request may\nalready have been completed, and then io_complete_rw_iopoll()\nand io_complete_rw() will be called, both of which will also modify\nreq->flags if needed. This causes a race condition, with concurrent\nnon-atomic modification of req->flags.\n\nTo eliminate this race, in io_read() or io_write(), if io request is\nsubmitted successfully, we don't remove REQ_F_NEED_CLEANUP flag. If\nREQ_F_NEED_CLEANUP is set, we'll leave __io_req_aux_free() to the\niovec cleanup work correspondingly.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc2",
        "release_date": "2020-06-18 08:32:44 -0600 io_uring: fix possible race condition against REQ_F_NEED_CLEANUP"
    },
    {
        "commit": "56952e91acc93ed624fe9da840900defb75f1323",
        "message": "If we're doing polled IO and end up having requests being submitted\nasync, then completions can come in while we're waiting for refs to\ndrop. We need to reap these manually, as nobody else will be looking\nfor them.\n\nBreak the wait into 1/20th of a second time waits, and check for done\npoll completions if we time out. Otherwise we can have done poll\ncompletions sitting in ctx->poll_list, which needs us to reap them but\nwe're just waiting for them.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc2",
        "release_date": "2020-06-17 15:05:08 -0600 io_uring: reap poll completions while waiting for refs to drop on exit"
    },
    {
        "commit": "9d8426a09195e2dcf2aa249de2aaadd792d491c7",
        "message": "If we're unlucky with timing, we could be running task_work after\nhaving dropped the memory context in the sq thread. Since dropping\nthe context requires a runnable task state, we cannot reliably drop\nit as part of our check-for-work loop in io_sq_thread(). Instead,\nabstract out the mm acquire for the sq thread into a helper, and call\nit from the async task work handler.\n\nCc: stable@vger.kernel.org # v5.7\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc2",
        "release_date": "2020-06-17 12:49:16 -0600 io_uring: acquire 'mm' for task_work for SQPOLL"
    },
    {
        "commit": "bbde017a32b32d2fa8d5fddca25fade20132abf8",
        "message": "In io_complete_rw_iopoll(), stores to io_kiocb's result and iopoll\ncompleted are two independent store operations, to ensure that once\niopoll_completed is ture and then req->result must been perceived by\nthe cpu executing io_do_iopoll(), proper memory barrier should be used.\n\nAnd in io_do_iopoll(), we check whether req->result is EAGAIN, if it is,\nwe'll need to issue this io request using io-wq again. In order to just\nissue a single smp_rmb() on the completion side, move the re-submit work\nto io_iopoll_complete().\n\nCc: stable@vger.kernel.org\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\n[axboe: don't set ->iopoll_completed for -EAGAIN retry]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc2",
        "release_date": "2020-06-17 12:49:09 -0600 io_uring: add memory barrier to synchronize io_kiocb's result and iopoll_completed"
    },
    {
        "commit": "2d7d67920e5c8e0854df23ca77da2dd5880ce5dd",
        "message": "In IOPOLL mode, for EAGAIN error, we'll try to submit io request\nagain using io-wq, so don't fail rest of links if this io request\nhas links.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc2",
        "release_date": "2020-06-17 12:49:01 -0600 io_uring: don't fail links for EAGAIN error in IOPOLL mode"
    },
    {
        "commit": "801dd57bd1d8c2c253f43635a3045bfa32a810b3",
        "message": "For an exiting process it tries to cancel all its inflight requests. Use\nreq->task to match such instead of work.pid. We always have req->task\nset, and it will be valid because we're matching only current exiting\ntask.\n\nAlso, remove work.pid and everything related, it's useless now.\n\nReported-by: Eric W. Biederman <ebiederm@xmission.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc2",
        "release_date": "2020-06-15 08:51:38 -0600 io_uring: cancel by ->task not pid"
    },
    {
        "commit": "4dd2824d6d5914949b5fe589538bc2622d84c5dd",
        "message": "There will be multiple places where req->task is used, so refcount-pin\nit lazily with introduced *io_{get,put}_req_task(). We need to always\nhave valid ->task for cancellation reasons, but don't care about pinning\nit in some cases. That's why it sets req->task in io_req_init() and\nimplements get/put laziness with a flag.\n\nThis also removes using @current from polling io_arm_poll_handler(),\netc., but doesn't change observable behaviour.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc2",
        "release_date": "2020-06-15 08:51:35 -0600 io_uring: lazy get task"
    },
    {
        "commit": "67c4d9e693e3bb7fb968af24e3584f821a78ba56",
        "message": "Instead of waiting for each request one by one, first try to cancel all\nof them in a batched manner, and then go over inflight_list/etc to reap\nleftovers.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc2",
        "release_date": "2020-06-15 08:51:34 -0600 io_uring: batch cancel in io_uring_cancel_files()"
    },
    {
        "commit": "44e728b8aae0bb6d4229129083974f9dea43f50b",
        "message": "If a process is going away, io_uring_flush() will cancel only 1\nrequest with a matching pid. Cancel all of them\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc2",
        "release_date": "2020-06-15 08:51:34 -0600 io_uring: cancel all task's requests on exit"
    },
    {
        "commit": "59960b9deb5354e4cdb0b6ed3a3b653a2b4eb602",
        "message": "Don't leave garbage in req.work before punting async on -EAGAIN\nin io_iopoll_queue().\n\n[  140.922099] general protection fault, probably for non-canonical\n     address 0xdead000000000100: 0000 [#1] PREEMPT SMP PTI\n...\n[  140.922105] RIP: 0010:io_worker_handle_work+0x1db/0x480\n...\n[  140.922114] Call Trace:\n[  140.922118]  ? __next_timer_interrupt+0xe0/0xe0\n[  140.922119]  io_wqe_worker+0x2a9/0x360\n[  140.922121]  ? _raw_spin_unlock_irqrestore+0x24/0x40\n[  140.922124]  kthread+0x12c/0x170\n[  140.922125]  ? io_worker_handle_work+0x480/0x480\n[  140.922126]  ? kthread_park+0x90/0x90\n[  140.922127]  ret_from_fork+0x22/0x30\n\nFixes: 7cdaf587de7c (\"io_uring: avoid whole io_wq_work copy for requests completed inline\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc2",
        "release_date": "2020-06-15 08:37:55 -0600 io_uring: fix lazy work init"
    },
    {
        "commit": "b961f8dc8976c091180839f4483d67b7c2ca2578",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A few late stragglers in here. In particular:\n\n   - Validate full range for provided buffers (Bijan)\n\n   - Fix bad use of kfree() in buffer registration failure (Denis)\n\n   - Don't allow close of ring itself, it's not fully safe. Making it\n     fully safe would require making the system call more expensive,\n     which isn't worth it.\n\n   - Buffer selection fix\n\n   - Regression fix for O_NONBLOCK retry\n\n   - Make IORING_OP_ACCEPT honor O_NONBLOCK (Jiufei)\n\n   - Restrict opcode handling for SQ/IOPOLL (Pavel)\n\n   - io-wq work handling cleanups and improvements (Pavel, Xiaoguang)\n\n   - IOPOLL race fix (Xiaoguang)\"\n\n* tag 'io_uring-5.8-2020-06-11' of git://git.kernel.dk/linux-block:\n  io_uring: fix io_kiocb.flags modification race in IOPOLL mode\n  io_uring: check file O_NONBLOCK state for accept\n  io_uring: avoid unnecessary io_wq_work copy for fast poll feature\n  io_uring: avoid whole io_wq_work copy for requests completed inline\n  io_uring: allow O_NONBLOCK async retry\n  io_wq: add per-wq work handler instead of per work\n  io_uring: don't arm a timeout through work.func\n  io_uring: remove custom ->func handlers\n  io_uring: don't derive close state from ->func\n  io_uring: use kvfree() in io_sqe_buffer_register()\n  io_uring: validate the full range of provided buffers for access\n  io_uring: re-set iov base/len for buffer select retry\n  io_uring: move send/recv IOPOLL check into prep\n  io_uring: deduplicate io_openat{,2}_prep()\n  io_uring: do build_open_how() only once\n  io_uring: fix {SQ,IO}POLL with unsupported opcodes\n  io_uring: disallow close of ring itself",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-11 16:10:08 -0700 Merge tag 'io_uring-5.8-2020-06-11' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "65a6543da386838f935d2f03f452c5c0acff2a68",
        "message": "While testing io_uring in arm, we found sometimes io_sq_thread() keeps\npolling io requests even though there are not inflight io requests in\nblock layer. After some investigations, found a possible race about\nio_kiocb.flags, see below race codes:\n  1) in the end of io_write() or io_read()\n    req->flags &= ~REQ_F_NEED_CLEANUP;\n    kfree(iovec);\n    return ret;\n\n  2) in io_complete_rw_iopoll()\n    if (res != -EAGAIN)\n        req->flags |= REQ_F_IOPOLL_COMPLETED;\n\nIn IOPOLL mode, io requests still maybe completed by interrupt, then\nabove codes are not safe, concurrent modifications to req->flags, which\nis not protected by lock or is not atomic modifications. I also had\ndisassemble io_complete_rw_iopoll() in arm:\n   req->flags |= REQ_F_IOPOLL_COMPLETED;\n   0xffff000008387b18 <+76>:    ldr     w0, [x19,#104]\n   0xffff000008387b1c <+80>:    orr     w0, w0, #0x1000\n   0xffff000008387b20 <+84>:    str     w0, [x19,#104]\n\nSeems that the \"req->flags |= REQ_F_IOPOLL_COMPLETED;\" is  load and\nmodification, two instructions, which obviously is not atomic.\n\nTo fix this issue, add a new iopoll_completed in io_kiocb to indicate\nwhether io request is completed.\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-11 09:45:21 -0600 io_uring: fix io_kiocb.flags modification race in IOPOLL mode"
    },
    {
        "commit": "37c54f9bd48663f7657a9178fe08c47e4f5b537b",
        "message": "Some architectures like arm64 and s390 require USER_DS to be set for\nkernel threads to access user address space, which is the whole purpose of\nkthread_use_mm, but other like x86 don't.  That has lead to a huge mess\nwhere some callers are fixed up once they are tested on said\narchitectures, while others linger around and yet other like io_uring try\nto do \"clever\" optimizations for what usually is just a trivial asignment\nto a member in the thread_struct for most architectures.\n\nMake kthread_use_mm set USER_DS, and kthread_unuse_mm restore to the\nprevious value instead.\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nTested-by: Jens Axboe <axboe@kernel.dk>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nAcked-by: Michael S. Tsirkin <mst@redhat.com>\nCc: Alex Deucher <alexander.deucher@amd.com>\nCc: Al Viro <viro@zeniv.linux.org.uk>\nCc: Felipe Balbi <balbi@kernel.org>\nCc: Felix Kuehling <Felix.Kuehling@amd.com>\nCc: Jason Wang <jasowang@redhat.com>\nCc: Zhenyu Wang <zhenyuw@linux.intel.com>\nCc: Zhi Wang <zhi.a.wang@intel.com>\nCc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\nLink: http://lkml.kernel.org/r/20200404094101.672954-7-hch@lst.de\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-10 19:14:18 -0700 kernel: set USER_DS in kthread_use_mm"
    },
    {
        "commit": "e697deed834de15d2322d0619d51893022c90ea2",
        "message": "If the socket is O_NONBLOCK, we should complete the accept request\nwith -EAGAIN when data is not ready.\n\nSigned-off-by: Jiufei Xue <jiufei.xue@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-10 18:06:16 -0600 io_uring: check file O_NONBLOCK state for accept"
    },
    {
        "commit": "405a5d2b2762f2a9813efdee93274d4e7bf607a1",
        "message": "Basically IORING_OP_POLL_ADD command and async armed poll handlers\nfor regular commands don't touch io_wq_work, so only REQ_F_WORK_INITIALIZED\nis set, can we do io_wq_work copy and restore.\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-10 17:58:46 -0600 io_uring: avoid unnecessary io_wq_work copy for fast poll feature"
    },
    {
        "commit": "7cdaf587de7c6f494b8433fded19f7728e70e1ef",
        "message": "If requests can be submitted and completed inline, we don't need to\ninitialize whole io_wq_work in io_init_req(), which is an expensive\noperation, add a new 'REQ_F_WORK_INITIALIZED' to determine whether\nio_wq_work is initialized and add a helper io_req_init_async(), users\nmust call io_req_init_async() for the first time touching any members\nof io_wq_work.\n\nI use /dev/nullb0 to evaluate performance improvement in my physical\nmachine:\n  modprobe null_blk nr_devices=1 completion_nsec=0\n  sudo taskset -c 60 fio  -name=fiotest -filename=/dev/nullb0 -iodepth=128\n  -thread -rw=read -ioengine=io_uring -direct=1 -bs=4k -size=100G -numjobs=1\n  -time_based -runtime=120\n\nbefore this patch:\nRun status group 0 (all jobs):\n   READ: bw=724MiB/s (759MB/s), 724MiB/s-724MiB/s (759MB/s-759MB/s),\n   io=84.8GiB (91.1GB), run=120001-120001msec\n\nWith this patch:\nRun status group 0 (all jobs):\n   READ: bw=761MiB/s (798MB/s), 761MiB/s-761MiB/s (798MB/s-798MB/s),\n   io=89.2GiB (95.8GB), run=120001-120001msec\n\nAbout 5% improvement.\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-10 17:58:46 -0600 io_uring: avoid whole io_wq_work copy for requests completed inline"
    },
    {
        "commit": "c5b856255cbc3b664d686a83fa9397a835e063de",
        "message": "We can assume that O_NONBLOCK is always honored, even if we don't\nhave a ->read/write_iter() for the file type. Also unify the read/write\nchecking for allowing async punt, having the write side factoring in the\nREQ_F_NOWAIT flag as well.\n\nCc: stable@vger.kernel.org\nFixes: 490e89676a52 (\"io_uring: only force async punt if poll based retry can't handle it\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-09 19:38:24 -0600 io_uring: allow O_NONBLOCK async retry"
    },
    {
        "commit": "f5fa38c59cb0b40633dee5cdf7465801be3e4928",
        "message": "io_uring is the only user of io-wq, and now it uses only io-wq callback\nfor all its requests, namely io_wq_submit_work(). Instead of storing\nwork->runner callback in each instance of io_wq_work, keep it in io-wq\nitself.\n\npros:\n- reduces io_wq_work size\n- more robust -- ->func won't be invalidated with mem{cpy,set}(req)\n- helps other work\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-08 13:47:37 -0600 io_wq: add per-wq work handler instead of per work"
    },
    {
        "commit": "d4c81f38522f3e7f4be1b472ef9988d0ed7f3696",
        "message": "Remove io_link_work_cb() -- the last custom work.func.\nNot the prettiest thing, but works. Instead of queueing a linked timeout\nin io_link_work_cb() mark a request with REQ_F_QUEUE_TIMEOUT and do\nenqueueing based on the flag in io_wq_submit_work().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-08 13:47:37 -0600 io_uring: don't arm a timeout through work.func"
    },
    {
        "commit": "ac45abc0e2a8ed16ecc0eea039fe762ddfefbcad",
        "message": "In preparation of getting rid of work.func, this removes almost all\ncustom instances of it, leaving only io_wq_submit_work() and\nio_link_work_cb(). And the last one will be dealt later.\n\nNothing fancy, just routinely remove *_finish() function and inline\nwhat's left. E.g. remove io_fsync_finish() + inline __io_fsync() into\nio_fsync().\n\nAs no users of io_req_cancelled() are left, delete it as well. The patch\nadds extra switch lookup on cold-ish path, but that's overweighted by\nnice diffstat and other benefits of the following patches.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-08 13:47:37 -0600 io_uring: remove custom ->func handlers"
    },
    {
        "commit": "3af73b286ccee493dc055fc58da02b2dc7a5304d",
        "message": "Relying on having a specific work.func is dangerous, even if an opcode\nhandler set it itself. E.g. io_wq_assign_next() can modify it.\n\nio_close() sets a custom work.func to indicate that\n__close_fd_get_file() was already called. Fortunately, there is no bugs\nwith io_wq_assign_next() and close yet.\n\nStill, do it safe and always be prepared to be called through\nio_wq_submit_work(). Zero req->close.put_file in prep, and call\n__close_fd_get_file() IFF it's NULL.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-08 13:47:37 -0600 io_uring: don't derive close state from ->func"
    },
    {
        "commit": "a8c73c1a614f6da6c0b04c393f87447e28cb6de4",
        "message": "Use kvfree() to free the pages and vmas, since they are allocated by\nkvmalloc_array() in a loop.\n\nFixes: d4ef647510b1 (\"io_uring: avoid page allocation warnings\")\nSigned-off-by: Denis Efremov <efremov@linux.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/r/20200605093203.40087-1-efremov@linux.com",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-08 09:39:13 -0600 io_uring: use kvfree() in io_sqe_buffer_register()"
    },
    {
        "commit": "efe68c1ca8f49e8c06afd74b699411bfbb8ba1ff",
        "message": "Account for the number of provided buffers when validating the address\nrange.\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-08 09:39:13 -0600 io_uring: validate the full range of provided buffers for access"
    },
    {
        "commit": "dddb3e26f6d88c5344d28cb5ff9d3d6fa05c4f7a",
        "message": "We already have the buffer selected, but we should set the iter list\nagain.\n\nCc: stable@vger.kernel.org # v5.7\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-04 11:45:29 -0600 io_uring: re-set iov base/len for buffer select retry"
    },
    {
        "commit": "d2b6f48b691ed67569786c332f0173b918d3fd1b",
        "message": "Fail recv/send in case of IORING_SETUP_IOPOLL earlier during prep,\nso it'd be done only once. Removes duplication as well\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-04 11:14:19 -0600 io_uring: move send/recv IOPOLL check into prep"
    },
    {
        "commit": "ec65fea5a8d7a82d3137dd2a44197eb577da111f",
        "message": "io_openat_prep() and io_openat2_prep() are identical except for how\nstruct open_how is built. Deduplicate it with a helper.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-04 11:14:19 -0600 io_uring: deduplicate io_openat{,2}_prep()"
    },
    {
        "commit": "25e72d1012b30bdff712b563e6141a4f311d28d6",
        "message": "build_open_how() is just adjusting open_flags/mode. Do it once during\nprep. It looks better than storing raw values for the future.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-04 11:14:19 -0600 io_uring: do build_open_how() only once"
    },
    {
        "commit": "3232dd02af65f2d01be641120d2a710176b0c7a7",
        "message": "IORING_SETUP_IOPOLL is defined only for read/write, other opcodes should\nbe disallowed, otherwise it'll get an error as below. Also refuse\nopen/close with SQPOLL, as the polling thread wouldn't know which file\ntable to use.\n\nRIP: 0010:io_iopoll_getevents+0x111/0x5a0\nCall Trace:\n ? _raw_spin_unlock_irqrestore+0x24/0x40\n ? do_send_sig_info+0x64/0x90\n io_iopoll_reap_events.part.0+0x5e/0xa0\n io_ring_ctx_wait_and_kill+0x132/0x1c0\n io_uring_release+0x20/0x30\n __fput+0xcd/0x230\n ____fput+0xe/0x10\n task_work_run+0x67/0xa0\n do_exit+0x353/0xb10\n ? handle_mm_fault+0xd4/0x200\n ? syscall_trace_enter+0x18c/0x2c0\n do_group_exit+0x43/0xa0\n __x64_sys_exit_group+0x18/0x20\n do_syscall_64+0x60/0x1e0\n entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\n[axboe: allow provide/remove buffers and files update]\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-04 11:13:53 -0600 io_uring: fix {SQ,IO}POLL with unsupported opcodes"
    },
    {
        "commit": "fd2206e4e97b5bae422d9f2f9ebbc79bc97e44a5",
        "message": "A previous commit enabled this functionality, which also enabled O_PATH\nto work correctly with io_uring. But we can't safely close the ring\nitself, as the file handle isn't reference counted inside\nio_uring_enter(). Instead of jumping through hoops to enable ring\nclosure, add a \"soft\" ->needs_file option, ->needs_file_no_error. This\nenables O_PATH file descriptors to work, but still catches the case of\ntrying to close the ring itself.\n\nReported-by: Jann Horn <jannh@google.com>\nFixes: 904fbcb115c8 (\"io_uring: remove 'fd is io_uring' from close path\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-02 17:22:24 -0600 io_uring: disallow close of ring itself"
    },
    {
        "commit": "1ee08de1e234d95b5b4f866878b72fceb5372904",
        "message": "Pull io_uring updates from Jens Axboe:\n \"A relatively quiet round, mostly just fixes and code improvements. In\nparticular:\n\n   - Make statx just use the generic statx handler, instead of open\n     coding it. We don't need that anymore, as we always call it async\n     safe (Bijan)\n\n   - Enable closing of the ring itself. Also fixes O_PATH closure (me)\n\n   - Properly name completion members (me)\n\n   - Batch reap of dead file registrations (me)\n\n   - Allow IORING_OP_POLL with double waitqueues (me)\n\n   - Add tee(2) support (Pavel)\n\n   - Remove double off read (Pavel)\n\n   - Fix overflow cancellations (Pavel)\n\n   - Improve CQ timeouts (Pavel)\n\n   - Async defer drain fixes (Pavel)\n\n   - Add support for enabling/disabling notifications on a registered\n     eventfd (Stefano)\n\n   - Remove dead state parameter (Xiaoguang)\n\n   - Disable SQPOLL submit on dying ctx (Xiaoguang)\n\n   - Various code cleanups\"\n\n* tag 'for-5.8/io_uring-2020-06-01' of git://git.kernel.dk/linux-block: (29 commits)\n  io_uring: fix overflowed reqs cancellation\n  io_uring: off timeouts based only on completions\n  io_uring: move timeouts flushing to a helper\n  statx: hide interfaces no longer used by io_uring\n  io_uring: call statx directly\n  statx: allow system call to be invoked from io_uring\n  io_uring: add io_statx structure\n  io_uring: get rid of manual punting in io_close\n  io_uring: separate DRAIN flushing into a cold path\n  io_uring: don't re-read sqe->off in timeout_prep()\n  io_uring: simplify io_timeout locking\n  io_uring: fix flush req->refs underflow\n  io_uring: don't submit sqes when ctx->refs is dying\n  io_uring: async task poll trigger cleanup\n  io_uring: add tee(2) support\n  splice: export do_tee()\n  io_uring: don't repeat valid flag list\n  io_uring: rename io_file_put()\n  io_uring: remove req->needs_fixed_files\n  io_uring: cleanup io_poll_remove_one() logic\n  ...",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-02 15:42:50 -0700 Merge tag 'for-5.8/io_uring-2020-06-01' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "457f44363a8894135c85b7a9afd2bd8196db24ab",
        "message": "This commit adds a new MPSC ring buffer implementation into BPF ecosystem,\nwhich allows multiple CPUs to submit data to a single shared ring buffer. On\nthe consumption side, only single consumer is assumed.\n\nMotivation\n----------\nThere are two distinctive motivators for this work, which are not satisfied by\nexisting perf buffer, which prompted creation of a new ring buffer\nimplementation.\n  - more efficient memory utilization by sharing ring buffer across CPUs;\n  - preserving ordering of events that happen sequentially in time, even\n  across multiple CPUs (e.g., fork/exec/exit events for a task).\n\nThese two problems are independent, but perf buffer fails to satisfy both.\nBoth are a result of a choice to have per-CPU perf ring buffer.  Both can be\nalso solved by having an MPSC implementation of ring buffer. The ordering\nproblem could technically be solved for perf buffer with some in-kernel\ncounting, but given the first one requires an MPSC buffer, the same solution\nwould solve the second problem automatically.\n\nSemantics and APIs\n------------------\nSingle ring buffer is presented to BPF programs as an instance of BPF map of\ntype BPF_MAP_TYPE_RINGBUF. Two other alternatives considered, but ultimately\nrejected.\n\nOne way would be to, similar to BPF_MAP_TYPE_PERF_EVENT_ARRAY, make\nBPF_MAP_TYPE_RINGBUF could represent an array of ring buffers, but not enforce\n\"same CPU only\" rule. This would be more familiar interface compatible with\nexisting perf buffer use in BPF, but would fail if application needed more\nadvanced logic to lookup ring buffer by arbitrary key. HASH_OF_MAPS addresses\nthis with current approach. Additionally, given the performance of BPF\nringbuf, many use cases would just opt into a simple single ring buffer shared\namong all CPUs, for which current approach would be an overkill.\n\nAnother approach could introduce a new concept, alongside BPF map, to\nrepresent generic \"container\" object, which doesn't necessarily have key/value\ninterface with lookup/update/delete operations. This approach would add a lot\nof extra infrastructure that has to be built for observability and verifier\nsupport. It would also add another concept that BPF developers would have to\nfamiliarize themselves with, new syntax in libbpf, etc. But then would really\nprovide no additional benefits over the approach of using a map.\nBPF_MAP_TYPE_RINGBUF doesn't support lookup/update/delete operations, but so\ndoesn't few other map types (e.g., queue and stack; array doesn't support\ndelete, etc).\n\nThe approach chosen has an advantage of re-using existing BPF map\ninfrastructure (introspection APIs in kernel, libbpf support, etc), being\nfamiliar concept (no need to teach users a new type of object in BPF program),\nand utilizing existing tooling (bpftool). For common scenario of using\na single ring buffer for all CPUs, it's as simple and straightforward, as\nwould be with a dedicated \"container\" object. On the other hand, by being\na map, it can be combined with ARRAY_OF_MAPS and HASH_OF_MAPS map-in-maps to\nimplement a wide variety of topologies, from one ring buffer for each CPU\n(e.g., as a replacement for perf buffer use cases), to a complicated\napplication hashing/sharding of ring buffers (e.g., having a small pool of\nring buffers with hashed task's tgid being a look up key to preserve order,\nbut reduce contention).\n\nKey and value sizes are enforced to be zero. max_entries is used to specify\nthe size of ring buffer and has to be a power of 2 value.\n\nThere are a bunch of similarities between perf buffer\n(BPF_MAP_TYPE_PERF_EVENT_ARRAY) and new BPF ring buffer semantics:\n  - variable-length records;\n  - if there is no more space left in ring buffer, reservation fails, no\n    blocking;\n  - memory-mappable data area for user-space applications for ease of\n    consumption and high performance;\n  - epoll notifications for new incoming data;\n  - but still the ability to do busy polling for new data to achieve the\n    lowest latency, if necessary.\n\nBPF ringbuf provides two sets of APIs to BPF programs:\n  - bpf_ringbuf_output() allows to *copy* data from one place to a ring\n    buffer, similarly to bpf_perf_event_output();\n  - bpf_ringbuf_reserve()/bpf_ringbuf_commit()/bpf_ringbuf_discard() APIs\n    split the whole process into two steps. First, a fixed amount of space is\n    reserved. If successful, a pointer to a data inside ring buffer data area\n    is returned, which BPF programs can use similarly to a data inside\n    array/hash maps. Once ready, this piece of memory is either committed or\n    discarded. Discard is similar to commit, but makes consumer ignore the\n    record.\n\nbpf_ringbuf_output() has disadvantage of incurring extra memory copy, because\nrecord has to be prepared in some other place first. But it allows to submit\nrecords of the length that's not known to verifier beforehand. It also closely\nmatches bpf_perf_event_output(), so will simplify migration significantly.\n\nbpf_ringbuf_reserve() avoids the extra copy of memory by providing a memory\npointer directly to ring buffer memory. In a lot of cases records are larger\nthan BPF stack space allows, so many programs have use extra per-CPU array as\na temporary heap for preparing sample. bpf_ringbuf_reserve() avoid this needs\ncompletely. But in exchange, it only allows a known constant size of memory to\nbe reserved, such that verifier can verify that BPF program can't access\nmemory outside its reserved record space. bpf_ringbuf_output(), while slightly\nslower due to extra memory copy, covers some use cases that are not suitable\nfor bpf_ringbuf_reserve().\n\nThe difference between commit and discard is very small. Discard just marks\na record as discarded, and such records are supposed to be ignored by consumer\ncode. Discard is useful for some advanced use-cases, such as ensuring\nall-or-nothing multi-record submission, or emulating temporary malloc()/free()\nwithin single BPF program invocation.\n\nEach reserved record is tracked by verifier through existing\nreference-tracking logic, similar to socket ref-tracking. It is thus\nimpossible to reserve a record, but forget to submit (or discard) it.\n\nbpf_ringbuf_query() helper allows to query various properties of ring buffer.\nCurrently 4 are supported:\n  - BPF_RB_AVAIL_DATA returns amount of unconsumed data in ring buffer;\n  - BPF_RB_RING_SIZE returns the size of ring buffer;\n  - BPF_RB_CONS_POS/BPF_RB_PROD_POS returns current logical possition of\n    consumer/producer, respectively.\nReturned values are momentarily snapshots of ring buffer state and could be\noff by the time helper returns, so this should be used only for\ndebugging/reporting reasons or for implementing various heuristics, that take\ninto account highly-changeable nature of some of those characteristics.\n\nOne such heuristic might involve more fine-grained control over poll/epoll\nnotifications about new data availability in ring buffer. Together with\nBPF_RB_NO_WAKEUP/BPF_RB_FORCE_WAKEUP flags for output/commit/discard helpers,\nit allows BPF program a high degree of control and, e.g., more efficient\nbatched notifications. Default self-balancing strategy, though, should be\nadequate for most applications and will work reliable and efficiently already.\n\nDesign and implementation\n-------------------------\nThis reserve/commit schema allows a natural way for multiple producers, either\non different CPUs or even on the same CPU/in the same BPF program, to reserve\nindependent records and work with them without blocking other producers. This\nmeans that if BPF program was interruped by another BPF program sharing the\nsame ring buffer, they will both get a record reserved (provided there is\nenough space left) and can work with it and submit it independently. This\napplies to NMI context as well, except that due to using a spinlock during\nreservation, in NMI context, bpf_ringbuf_reserve() might fail to get a lock,\nin which case reservation will fail even if ring buffer is not full.\n\nThe ring buffer itself internally is implemented as a power-of-2 sized\ncircular buffer, with two logical and ever-increasing counters (which might\nwrap around on 32-bit architectures, that's not a problem):\n  - consumer counter shows up to which logical position consumer consumed the\n    data;\n  - producer counter denotes amount of data reserved by all producers.\n\nEach time a record is reserved, producer that \"owns\" the record will\nsuccessfully advance producer counter. At that point, data is still not yet\nready to be consumed, though. Each record has 8 byte header, which contains\nthe length of reserved record, as well as two extra bits: busy bit to denote\nthat record is still being worked on, and discard bit, which might be set at\ncommit time if record is discarded. In the latter case, consumer is supposed\nto skip the record and move on to the next one. Record header also encodes\nrecord's relative offset from the beginning of ring buffer data area (in\npages). This allows bpf_ringbuf_commit()/bpf_ringbuf_discard() to accept only\nthe pointer to the record itself, without requiring also the pointer to ring\nbuffer itself. Ring buffer memory location will be restored from record\nmetadata header. This significantly simplifies verifier, as well as improving\nAPI usability.\n\nProducer counter increments are serialized under spinlock, so there is\na strict ordering between reservations. Commits, on the other hand, are\ncompletely lockless and independent. All records become available to consumer\nin the order of reservations, but only after all previous records where\nalready committed. It is thus possible for slow producers to temporarily hold\noff submitted records, that were reserved later.\n\nReservation/commit/consumer protocol is verified by litmus tests in\nDocumentation/litmus-test/bpf-rb.\n\nOne interesting implementation bit, that significantly simplifies (and thus\nspeeds up as well) implementation of both producers and consumers is how data\narea is mapped twice contiguously back-to-back in the virtual memory. This\nallows to not take any special measures for samples that have to wrap around\nat the end of the circular buffer data area, because the next page after the\nlast data page would be first data page again, and thus the sample will still\nappear completely contiguous in virtual memory. See comment and a simple ASCII\ndiagram showing this visually in bpf_ringbuf_area_alloc().\n\nAnother feature that distinguishes BPF ringbuf from perf ring buffer is\na self-pacing notifications of new data being availability.\nbpf_ringbuf_commit() implementation will send a notification of new record\nbeing available after commit only if consumer has already caught up right up\nto the record being committed. If not, consumer still has to catch up and thus\nwill see new data anyways without needing an extra poll notification.\nBenchmarks (see tools/testing/selftests/bpf/benchs/bench_ringbuf.c) show that\nthis allows to achieve a very high throughput without having to resort to\ntricks like \"notify only every Nth sample\", which are necessary with perf\nbuffer. For extreme cases, when BPF program wants more manual control of\nnotifications, commit/discard/output helpers accept BPF_RB_NO_WAKEUP and\nBPF_RB_FORCE_WAKEUP flags, which give full control over notifications of data\navailability, but require extra caution and diligence in using this API.\n\nComparison to alternatives\n--------------------------\nBefore considering implementing BPF ring buffer from scratch existing\nalternatives in kernel were evaluated, but didn't seem to meet the needs. They\nlargely fell into few categores:\n  - per-CPU buffers (perf, ftrace, etc), which don't satisfy two motivations\n    outlined above (ordering and memory consumption);\n  - linked list-based implementations; while some were multi-producer designs,\n    consuming these from user-space would be very complicated and most\n    probably not performant; memory-mapping contiguous piece of memory is\n    simpler and more performant for user-space consumers;\n  - io_uring is SPSC, but also requires fixed-sized elements. Naively turning\n    SPSC queue into MPSC w/ lock would have subpar performance compared to\n    locked reserve + lockless commit, as with BPF ring buffer. Fixed sized\n    elements would be too limiting for BPF programs, given existing BPF\n    programs heavily rely on variable-sized perf buffer already;\n  - specialized implementations (like a new printk ring buffer, [0]) with lots\n    of printk-specific limitations and implications, that didn't seem to fit\n    well for intended use with BPF programs.\n\n  [0] https://lwn.net/Articles/779550/\n\nSigned-off-by: Andrii Nakryiko <andriin@fb.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20200529075424.3139988-2-andriin@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-06-01 14:38:22 -0700 bpf: Implement BPF ring buffer and verifier support for it"
    },
    {
        "commit": "7b53d59859bc932b37895d2d37388e7fa29af7a5",
        "message": "Overflowed requests in io_uring_cancel_files() should be shed only of\ninflight and overflowed refs. All other left references are owned by\nsomeone else.\n\nIf refcount_sub_and_test() fails, it will go further and put put extra\nref, don't do that. Also, don't need to do io_wq_cancel_work()\nfor overflowed reqs, they will be let go shortly anyway.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-30 07:38:32 -0600 io_uring: fix overflowed reqs cancellation"
    },
    {
        "commit": "bfe68a221905de37e65394a6d58c1e5f3e545d2f",
        "message": "Offset timeouts wait not for sqe->off non-timeout CQEs, but rather\nsqe->off + number of prior inflight requests. Wait exactly for\nsqe->off non-timeout completions\n\nReported-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-30 07:38:18 -0600 io_uring: off timeouts based only on completions"
    },
    {
        "commit": "360428f8c0cd857006a8a3f515946285370489ac",
        "message": "Separate flushing offset timeouts io_commit_cqring() by moving it into a\nhelper. Just a preparation, makes following patches clearer.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-30 07:38:17 -0600 io_uring: move timeouts flushing to a helper"
    },
    {
        "commit": "b0beb28097fa04177b3769f4bb7a0d0d9c4ae76e",
        "message": "This reverts commit c58c1f83436b501d45d4050fd1296d71a9760bcb.\n\nio_uring does do the right thing for this case, and we're still returning\n-EAGAIN to userspace for the cases we don't support. Revert this change\nto avoid doing endless spins of resubmits.\n\nCc: stable@vger.kernel.org # v5.6\nReported-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7",
        "release_date": "2020-05-28 13:20:39 -0600 Revert \"block: end bio with BLK_STS_AGAIN in case of non-mq devs and REQ_NOWAIT\""
    },
    {
        "commit": "6f88cc176a3358c54bb6c38c8afee3f3a42faf54",
        "message": "The io_uring interfaces have been replaced by do_statx() and are no\nlonger needed.\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-26 16:48:06 -0600 statx: hide interfaces no longer used by io_uring"
    },
    {
        "commit": "e62753e4e2926f249d088cc0517be5ed4efec6d6",
        "message": "Calling statx directly both simplifies the interface and avoids potential\nincompatibilities between sync and async invokations.\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-26 16:48:06 -0600 io_uring: call statx directly"
    },
    {
        "commit": "0018784fc84f636d473a0d2a65a34f9d01893c0a",
        "message": "This is a prepatory patch to allow io_uring to invoke statx directly.\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-26 16:48:06 -0600 statx: allow system call to be invoked from io_uring"
    },
    {
        "commit": "1d9e1288039a47dc1189c3c1fed5cf3c215e94b7",
        "message": "Separate statx data from open in io_kiocb. No functional changes.\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-26 16:48:06 -0600 io_uring: add io_statx structure"
    },
    {
        "commit": "0bf0eefdab52d9f9f3a1eeda32a4fc7afe4e9219",
        "message": "io_close() was punting async manually to skip grabbing files. Use\nREQ_F_NO_FILE_TABLE instead, and pass it through the generic path\nwith -EAGAIN.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-26 13:31:09 -0600 io_uring: get rid of manual punting in io_close"
    },
    {
        "commit": "0451894522108d6c72934aff6ef89023743a9ed4",
        "message": "io_commit_cqring() assembly doesn't look good with extra code handling\ndrained requests. IOSQE_IO_DRAIN is slow and discouraged to be used in\na hot path, so try to minimise its impact by putting it into a helper\nand doing a fast check.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-26 13:31:09 -0600 io_uring: separate DRAIN flushing into a cold path"
    },
    {
        "commit": "56080b02ed6e71fbc0add2d05a32ed7361dd736a",
        "message": "SQEs are user writable, don't read sqe->off twice in io_timeout_prep()\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-26 13:31:08 -0600 io_uring: don't re-read sqe->off in timeout_prep()"
    },
    {
        "commit": "733f5c95e6fdabd05b8dfc15e04512809c9652c2",
        "message": "Move spin_lock_irq() earlier to have only 1 call site of it in\nio_timeout(). It makes the flow easier.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-26 13:31:08 -0600 io_uring: simplify io_timeout locking"
    },
    {
        "commit": "4518a3cc273cf82efdd36522fb1f13baad173c70",
        "message": "In io_uring_cancel_files(), after refcount_sub_and_test() leaves 0\nreq->refs, it calls io_put_req(), which would also put a ref. Call\nio_free_req() instead.\n\nCc: stable@vger.kernel.org\nFixes: 2ca10259b418 (\"io_uring: prune request from overflow list on flush\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-26 13:31:08 -0600 io_uring: fix flush req->refs underflow"
    },
    {
        "commit": "18f855e574d9799a0e7489f8ae6fd8447d0dd74a",
        "message": "Stefano reported a crash with using SQPOLL with io_uring:\n\n  BUG: kernel NULL pointer dereference, address: 00000000000003b0\n  CPU: 2 PID: 1307 Comm: io_uring-sq Not tainted 5.7.0-rc7 #11\n  RIP: 0010:task_numa_work+0x4f/0x2c0\n  Call Trace:\n   task_work_run+0x68/0xa0\n   io_sq_thread+0x252/0x3d0\n   kthread+0xf9/0x130\n   ret_from_fork+0x35/0x40\n\nwhich is task_numa_work() oopsing on current->mm being NULL.\n\nThe task work is queued by task_tick_numa(), which checks if current->mm is\nNULL at the time of the call. But this state isn't necessarily persistent,\nif the kthread is using use_mm() to temporarily adopt the mm of a task.\n\nChange the task_tick_numa() check to exclude kernel threads in general,\nas it doesn't make sense to attempt ot balance for kthreads anyway.\n\nReported-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Ingo Molnar <mingo@kernel.org>\nAcked-by: Peter Zijlstra <peterz@infradead.org>\nLink: https://lore.kernel.org/r/865de121-8190-5d30-ece5-3b097dc74431@kernel.dk",
        "kernel_version": "v5.7",
        "release_date": "2020-05-26 18:34:58 +0200 sched/fair: Don't NUMA balance for kthreads"
    },
    {
        "commit": "444565650a5fe9c63ddf153e6198e31705dedeb2",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A small collection of small fixes that should go into this release:\n\n   - Two fixes for async request preparation (Pavel)\n\n   - Busy clear fix for SQPOLL (Xiaoguang)\n\n   - Don't use kiocb->private for O_DIRECT buf index, some file systems\n     use it (Bijan)\n\n   - Kill dead check in io_splice()\n\n   - Ensure sqo_wait is initialized early\n\n   - Cancel task_work if we fail adding to original process\n\n   - Only add (IO)pollable requests to iopoll list, fixing a regression\n     in this merge window\"\n\n* tag 'io_uring-5.7-2020-05-22' of git://git.kernel.dk/linux-block:\n  io_uring: reset -EBUSY error when io sq thread is waken up\n  io_uring: don't add non-IO requests to iopoll pending list\n  io_uring: don't use kiocb.private to store buf_index\n  io_uring: cancel work if task_work_add() fails\n  io_uring: remove dead check in io_splice()\n  io_uring: fix FORCE_ASYNC req preparation\n  io_uring: don't prepare DRAIN reqs twice\n  io_uring: initialize ctx->sqo_wait earlier",
        "kernel_version": "v5.7-rc7",
        "release_date": "2020-05-22 11:12:30 -0700 Merge tag 'io_uring-5.7-2020-05-22' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "6b668c9b7fc6fc0c313cdaee8b75d17f4d954ab5",
        "message": "When IORING_SETUP_SQPOLL is enabled, io_ring_ctx_wait_and_kill() will wait\nfor sq thread to idle by busy loop:\n\n    while (ctx->sqo_thread && !wq_has_sleeper(&ctx->sqo_wait))\n        cond_resched();\n\nAbove loop isn't very CPU friendly, it may introduce a short cpu burst on\nthe current cpu.\n\nIf ctx->refs is dying, we forbid sq_thread from submitting any further\nSQEs. Instead they just get discarded when we exit.\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-20 08:41:26 -0600 io_uring: don't submit sqes when ctx->refs is dying"
    },
    {
        "commit": "d4ae271dfaae2a5f41c015f2f20d62a1deeec734",
        "message": "In io_sq_thread(), currently if we get an -EBUSY error and go to sleep,\nwe will won't clear it again, which will result in io_sq_thread() will\nnever have a chance to submit sqes again. Below test program test.c\ncan reveal this bug:\n\nint main(int argc, char *argv[])\n{\n        struct io_uring ring;\n        int i, fd, ret;\n        struct io_uring_sqe *sqe;\n        struct io_uring_cqe *cqe;\n        struct iovec *iovecs;\n        void *buf;\n        struct io_uring_params p;\n\n        if (argc < 2) {\n                printf(\"%s: file\\n\", argv[0]);\n                return 1;\n        }\n\n        memset(&p, 0, sizeof(p));\n        p.flags = IORING_SETUP_SQPOLL;\n        ret = io_uring_queue_init_params(4, &ring, &p);\n        if (ret < 0) {\n                fprintf(stderr, \"queue_init: %s\\n\", strerror(-ret));\n                return 1;\n        }\n\n        fd = open(argv[1], O_RDONLY | O_DIRECT);\n        if (fd < 0) {\n                perror(\"open\");\n                return 1;\n        }\n\n        iovecs = calloc(10, sizeof(struct iovec));\n        for (i = 0; i < 10; i++) {\n                if (posix_memalign(&buf, 4096, 4096))\n                        return 1;\n                iovecs[i].iov_base = buf;\n                iovecs[i].iov_len = 4096;\n        }\n\n        ret = io_uring_register_files(&ring, &fd, 1);\n        if (ret < 0) {\n                fprintf(stderr, \"%s: register %d\\n\", __FUNCTION__, ret);\n                return ret;\n        }\n\n        for (i = 0; i < 10; i++) {\n                sqe = io_uring_get_sqe(&ring);\n                if (!sqe)\n                        break;\n\n                io_uring_prep_readv(sqe, 0, &iovecs[i], 1, 0);\n                sqe->flags |= IOSQE_FIXED_FILE;\n\n                ret = io_uring_submit(&ring);\n                sleep(1);\n                printf(\"submit %d\\n\", i);\n        }\n\n        for (i = 0; i < 10; i++) {\n                io_uring_wait_cqe(&ring, &cqe);\n                printf(\"receive: %d\\n\", i);\n                if (cqe->res != 4096) {\n                        fprintf(stderr, \"ret=%d, wanted 4096\\n\", cqe->res);\n                        ret = 1;\n                }\n                io_uring_cqe_seen(&ring, cqe);\n        }\n\n        close(fd);\n        io_uring_queue_exit(&ring);\n        return 0;\n}\nsudo ./test testfile\nabove command will hang on the tenth request, to fix this bug, when io\nsq_thread is waken up, we reset the variable 'ret' to be zero.\n\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc7",
        "release_date": "2020-05-20 07:26:47 -0600 io_uring: reset -EBUSY error when io sq thread is waken up"
    },
    {
        "commit": "b532576ed39efe3b351ae8320b2ab67a4c4c3719",
        "message": "We normally disable any commands that aren't specifically poll commands\nfor a ring that is setup for polling, but we do allow buffer provide and\nremove commands to support buffer selection for polled IO. Once a\nrequest is issued, we add it to the poll list to poll for completion. But\nwe should not do that for non-IO commands, as those request complete\ninline immediately and aren't pollable. If we do, we can leave requests\non the iopoll list after they are freed.\n\nFixes: ddf0322db79c (\"io_uring: add IORING_OP_PROVIDE_BUFFERS\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc7",
        "release_date": "2020-05-19 21:20:27 -0600 io_uring: don't add non-IO requests to iopoll pending list"
    },
    {
        "commit": "4f4eeba87cc731b200bff9372d14a80f5996b277",
        "message": "kiocb.private is used in iomap_dio_rw() so store buf_index separately.\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\n\nMove 'buf_index' to a hole in io_kiocb.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc7",
        "release_date": "2020-05-19 16:19:49 -0600 io_uring: don't use kiocb.private to store buf_index"
    },
    {
        "commit": "e3aabf9554fd04eb14cd44ae7583fc9d40edd250",
        "message": "We currently move it to the io_wqe_manager for execution, but we cannot\nsafely do so as we may lack some of the state to execute it out of\ncontext. As we cancel work anyway when the ring/task exits, just mark\nthis request as canceled and io_async_task_func() will do the right\nthing.\n\nFixes: aa96bf8a9ee3 (\"io_uring: use io-wq manager as backup task if task is exiting\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc7",
        "release_date": "2020-05-18 11:14:22 -0600 io_uring: cancel work if task_work_add() fails"
    },
    {
        "commit": "310672552f4aea2ad50704711aa3cdd45f5441e9",
        "message": "If the request is still hashed in io_async_task_func(), then it cannot\nhave been canceled and it's pointless to check. So save that check.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-17 17:43:31 -0600 io_uring: async task poll trigger cleanup"
    },
    {
        "commit": "948a7749454b1712f1b2f2429f9493eb3e4a89b0",
        "message": "We checked for 'force_nonblock' higher up, so it's definitely false\nat this point. Kill the check, it's a remnant of when we tried to do\ninline splice without always punting to async context.\n\nFixes: 2fb3e82284fc (\"io_uring: punt splice async because of inode mutex\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc7",
        "release_date": "2020-05-17 14:21:38 -0600 io_uring: remove dead check in io_splice()"
    },
    {
        "commit": "f2a8d5c7a218b9c24befb756c4eb30aa550ce822",
        "message": "Add IORING_OP_TEE implementing tee(2) support. Almost identical to\nsplice bits, but without offsets.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-17 14:10:07 -0600 io_uring: add tee(2) support"
    },
    {
        "commit": "9dafdfc2f0a3ae551711098de3d7b621a469f11a",
        "message": "export do_tee() for use in io_uring\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-17 14:10:07 -0600 splice: export do_tee()"
    },
    {
        "commit": "c11368a57be460de889696f6ff8815fbcacf4db2",
        "message": "req->flags stores all sqe->flags. After checking that sqe->flags are\nvalid set if IOSQE* flags, no need to double check it, just forward them\nall.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-17 14:10:07 -0600 io_uring: don't repeat valid flag list"
    },
    {
        "commit": "9f13c35b33fddb186beab9ef21c555a01e45f4d7",
        "message": "io_file_put() deals with flushing state's file refs, adding \"state\" to\nits name makes it a bit clearer. Also, avoid double check of\nstate->file in __io_file_get() in some cases.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-17 14:10:07 -0600 io_uring: rename io_file_put()"
    },
    {
        "commit": "0cdaf760f42eb8e8a714c1cc017423e5da6d4936",
        "message": "A submission is \"async\" IIF it's done by SQPOLL thread. Instead of\npassing @async flag into io_submit_sqes(), deduce it from ctx->flags.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-17 14:10:07 -0600 io_uring: remove req->needs_fixed_files"
    },
    {
        "commit": "3bfa5bcb26f0b52d7ae8416aa0618fff21aceaaf",
        "message": "We only need apoll in the one section, do the juggling with the work\nrestoration there. This removes a special case further down as well.\n\nNo functional changes in this patch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-17 14:10:01 -0600 io_uring: cleanup io_poll_remove_one() logic"
    },
    {
        "commit": "bd2ab18a1d6267446eae1b47dd839050452bdf7f",
        "message": "As for other not inlined requests, alloc req->io for FORCE_ASYNC reqs,\nso they can be prepared properly.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc7",
        "release_date": "2020-05-17 09:22:09 -0600 io_uring: fix FORCE_ASYNC req preparation"
    },
    {
        "commit": "650b548129b60b0d23508351800108196f4aa89f",
        "message": "If req->io is not NULL, it's already prepared. Don't do it again,\nit's dangerous.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc7",
        "release_date": "2020-05-17 09:22:09 -0600 io_uring: don't prepare DRAIN reqs twice"
    },
    {
        "commit": "583863ed918136412ddf14de2e12534f17cfdc6f",
        "message": "Ensure that ctx->sqo_wait is initialized as soon as the ctx is allocated,\ninstead of deferring it to the offload setup. This fixes a syzbot\nreported lockdep complaint, which is really due to trying to wake_up\non an uninitialized wait queue:\n\nRSP: 002b:00007fffb1fb9aa8 EFLAGS: 00000246 ORIG_RAX: 00000000000001a9\nRAX: ffffffffffffffda RBX: 0000000000000000 RCX: 0000000000441319\nRDX: 0000000000000001 RSI: 0000000020000140 RDI: 000000000000047b\nRBP: 0000000000010475 R08: 0000000000000001 R09: 00000000004002c8\nR10: 0000000000000000 R11: 0000000000000246 R12: 0000000000402260\nR13: 00000000004022f0 R14: 0000000000000000 R15: 0000000000000000\nINFO: trying to register non-static key.\nthe code is fine but needs lockdep annotation.\nturning off the locking correctness validator.\nCPU: 1 PID: 7090 Comm: syz-executor222 Not tainted 5.7.0-rc1-next-20200415-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nCall Trace:\n __dump_stack lib/dump_stack.c:77 [inline]\n dump_stack+0x188/0x20d lib/dump_stack.c:118\n assign_lock_key kernel/locking/lockdep.c:913 [inline]\n register_lock_class+0x1664/0x1760 kernel/locking/lockdep.c:1225\n __lock_acquire+0x104/0x4c50 kernel/locking/lockdep.c:4234\n lock_acquire+0x1f2/0x8f0 kernel/locking/lockdep.c:4934\n __raw_spin_lock_irqsave include/linux/spinlock_api_smp.h:110 [inline]\n _raw_spin_lock_irqsave+0x8c/0xbf kernel/locking/spinlock.c:159\n __wake_up_common_lock+0xb4/0x130 kernel/sched/wait.c:122\n io_cqring_ev_posted+0xa5/0x1e0 fs/io_uring.c:1160\n io_poll_remove_all fs/io_uring.c:4357 [inline]\n io_ring_ctx_wait_and_kill+0x2bc/0x5a0 fs/io_uring.c:7305\n io_uring_create fs/io_uring.c:7843 [inline]\n io_uring_setup+0x115e/0x22b0 fs/io_uring.c:7870\n do_syscall_64+0xf6/0x7d0 arch/x86/entry/common.c:295\n entry_SYSCALL_64_after_hwframe+0x49/0xb3\nRIP: 0033:0x441319\nCode: e8 5c ae 02 00 48 83 c4 18 c3 0f 1f 80 00 00 00 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 bb 0a fc ff c3 66 2e 0f 1f 84 00 00 00 00\nRSP: 002b:00007fffb1fb9aa8 EFLAGS: 00000246 ORIG_RAX: 00000000000001a9\n\nReported-by: syzbot+8c91f5d054e998721c57@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc7",
        "release_date": "2020-05-17 09:20:00 -0600 io_uring: initialize ctx->sqo_wait earlier"
    },
    {
        "commit": "18e70f3a7651b420bf5d8ce0a3fd3d1cd9e5b689",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two small fixes that should go into this release:\n\n   - Check and handle zero length splice (Pavel)\n\n   - Fix a regression in this merge window for fixed files used with\n     polled block IO\"\n\n* tag 'io_uring-5.7-2020-05-15' of git://git.kernel.dk/linux-block:\n  io_uring: polled fixed file must go through free iteration\n  io_uring: fix zero len do_splice()",
        "kernel_version": "v5.7-rc6",
        "release_date": "2020-05-16 13:17:41 -0700 Merge tag 'io_uring-5.7-2020-05-15' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "6a4d07cde5778174a35ffc445c1d1388479563ee",
        "message": "There's no point in using list_del_init() on entries that are going\naway, and the associated lock is always used in process context so\nlet's not use the IRQ disabling+saving variant of the spinlock.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-15 14:37:14 -0600 io_uring: file registration list and lock optimization"
    },
    {
        "commit": "7e55a19cf6e70ce08964b46dbbfbdb07fbc995fc",
        "message": "This new flag should be set/clear from the application to\ndisable/enable eventfd notifications when a request is completed\nand queued to the CQ ring.\n\nBefore this patch, notifications were always sent if an eventfd is\nregistered, so IORING_CQ_EVENTFD_DISABLED is not set during the\ninitialization.\n\nIt will be up to the application to set the flag after initialization\nif no notifications are required at the beginning.\n\nSigned-off-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-15 12:16:59 -0600 io_uring: add IORING_CQ_EVENTFD_DISABLED to the CQ ring flags"
    },
    {
        "commit": "0d9b5b3af134cddfdc1dd31d41946a0ad389bbf2",
        "message": "This patch adds the new 'cq_flags' field that should be written by\nthe application and read by the kernel.\n\nThis new field is available to the userspace application through\n'cq_off.flags'.\nWe are using 4-bytes previously reserved and set to zero. This means\nthat if the application finds this field to zero, then the new\nfunctionality is not supported.\n\nIn the next patch we will introduce the first flag available.\n\nSigned-off-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-15 12:16:59 -0600 io_uring: add 'cq_flags' field for the CQ ring"
    },
    {
        "commit": "18bceab101adde8f38de76016bc77f3f25cf22f4",
        "message": "Some file descriptors use separate waitqueues for their f_ops->poll()\nhandler, most commonly one for read and one for write. The io_uring\npoll implementation doesn't work with that, as the 2nd poll_wait()\ncall will cause the io_uring poll request to -EINVAL.\n\nThis affects (at least) tty devices and /dev/random as well. This is a\nbig problem for event loops where some file descriptors work, and others\ndon't.\n\nWith this fix, io_uring handles multiple waitqueues.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-15 11:56:54 -0600 io_uring: allow POLL_ADD with double poll_wait() users"
    },
    {
        "commit": "4a38aed2a0a729ccecd84dca5b76d827b9e1294d",
        "message": "We currently embed and queue a work item per fixed_file_ref_node that\nwe update, but if the workload does a lot of these, then the associated\nkworker-events overhead can become quite noticeable.\n\nSince we rarely need to wait on these, batch them at 1 second intervals\ninstead. If we do need to wait for them, we just flush the pending\ndelayed work.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-15 11:56:18 -0600 io_uring: batch reap of dead file registrations"
    },
    {
        "commit": "0f158b4cf20e7983d5b33878a6aad118cfac4f05",
        "message": "We used to have three completions, now we just have two. With the two,\nlet's not allocate them dynamically, just embed then in the ctx and\nname them appropriately.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-14 17:18:39 -0600 io_uring: name sq thread and ref completions"
    },
    {
        "commit": "9d9e88a24c1f20ebfc2f28b1762ce78c0b9e1cb3",
        "message": "When we changed the file registration handling, it became important to\niterate the bulk request freeing list for fixed files as well, or we\nmiss dropping the fixed file reference. If not, we're leaking references,\nand we'll get a kworker stuck waiting for file references to disappear.\n\nThis also means we can remove the special casing of fixed vs non-fixed\nfiles, we need to iterate for both and we can just rely on\n__io_req_aux_free() doing io_put_file() instead of doing it manually.\n\nFixes: 055895537302 (\"io_uring: refactor file register/unregister/update handling\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc6",
        "release_date": "2020-05-13 13:00:00 -0600 io_uring: polled fixed file must go through free iteration"
    },
    {
        "commit": "8469508951d4a324b2df3b5bad75e99922c3b798",
        "message": "Remove duplicate semicolon at the end of line in io_file_from_index()\n\nSigned-off-by: Xiaoming Ni <nixiaoming@huawei.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-11 09:04:05 -0600 io_uring: remove duplicate semicolon at the end of line"
    },
    {
        "commit": "1d3962ae3b3d3a945f7fd5c651cf170a27521a35",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix finish_wait() balancing in file cancelation (Xiaoguang)\n\n - Ensure early cleanup of resources in ring map failure (Xiaoguang)\n\n - Ensure IORING_OP_SLICE does the right file mode checks (Pavel)\n\n - Remove file opening from openat/openat2/statx, it's not needed and\n   messes with O_PATH\n\n* tag 'io_uring-5.7-2020-05-08' of git://git.kernel.dk/linux-block:\n  io_uring: don't use 'fd' for openat/openat2/statx\n  splice: move f_mode checks to do_{splice,tee}()\n  io_uring: handle -EFAULT properly in io_uring_setup()\n  io_uring: fix mismatched finish_wait() calls in io_uring_cancel_files()",
        "kernel_version": "v5.7-rc5",
        "release_date": "2020-05-09 12:02:09 -0700 Merge tag 'io_uring-5.7-2020-05-08' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "c96874265cd04b4bd4a8e114ac9af039a6d83cfe",
        "message": "do_splice() doesn't expect len to be 0. Just always return 0 in this\ncase as splice(2) does.\n\nFixes: 7d67af2c0134 (\"io_uring: add splice(2) support\")\nReported-by: Jann Horn <jannh@google.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc6",
        "release_date": "2020-05-09 10:16:10 -0600 io_uring: fix zero len do_splice()"
    },
    {
        "commit": "7d01bd745a8f52ff2883f661235139ab6e7d23e6",
        "message": "The \"struct io_submit_state *state\" parameter is not used, remove it.\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-08 21:29:47 -0600 io_uring: remove obsolete 'state' parameter"
    },
    {
        "commit": "904fbcb115c85090484dfdffaf7f461d96fe8e53",
        "message": "The attempt protecting us from closing the ring itself wasn't really\ncomplete, and we actually don't need it. The referencing of requests\nthemselve, and the references they hold on the ring, ensures that the\nlife time of the ring is sane. With the check removed, we can also\nremove the need to have the close operation fget() the file.\n\nReported-by: Al Viro <viro@zeniv.linux.org.uk>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-08 21:27:24 -0600 io_uring: remove 'fd is io_uring' from close path"
    },
    {
        "commit": "63ff822358b276137059520cf16e587e8073e80f",
        "message": "We currently make some guesses as when to open this fd, but in reality\nwe have no business (or need) to do so at all. In fact, it makes certain\nthings fail, like O_PATH.\n\nRemove the fd lookup from these opcodes, we're just passing the 'fd' to\ngeneric helpers anyway. With that, we can also remove the special casing\nof fd values in io_req_needs_file(), and the 'fd_non_neg' check that\nwe have. And we can ensure that we only read sqe->fd once.\n\nThis fixes O_PATH usage with openat/openat2, and ditto statx path side\noddities.\n\nCc: stable@vger.kernel.org: # v5.6\nReported-by: Max Kellermann <mk@cm4all.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc5",
        "release_date": "2020-05-07 14:56:15 -0600 io_uring: don't use 'fd' for openat/openat2/statx"
    },
    {
        "commit": "90da2e3f25c8b4d742b2687b8fed8fc4eb8851da",
        "message": "do_splice() is used by io_uring, as will be do_tee(). Move f_mode\nchecks from sys_{splice,tee}() to do_{splice,tee}(), so they're\nenforced for io_uring as well.\n\nFixes: 7d67af2c0134 (\"io_uring: add splice(2) support\")\nReported-by: Jann Horn <jannh@google.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc5",
        "release_date": "2020-05-07 09:45:07 -0600 splice: move f_mode checks to do_{splice,tee}()"
    },
    {
        "commit": "12aceb89b0bce19eb89735f9de7a9983e4f0adae",
        "message": "eventfd is using ->read() as it's file_operations read handler, but\nthis prevents passing in information about whether a given IO operation\nis blocking or not. We can only use the file flags for that. To support\nasync (-EAGAIN/poll based) retries for io_uring, we need ->read_iter()\nsupport. Convert eventfd to using ->read_iter().\n\nWith ->read_iter(), we can support IOCB_NOWAIT. Ensure the fd setup\nis done such that we set file->f_mode with FMODE_NOWAIT.\n\n[missing include added]\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Al Viro <viro@zeniv.linux.org.uk>",
        "kernel_version": "v5.8-rc1",
        "release_date": "2020-05-06 22:33:43 -0400 eventfd: convert to f_op->read_iter()"
    },
    {
        "commit": "7f13657d141346125f4d0bb93eab4777f40c406e",
        "message": "If copy_to_user() in io_uring_setup() failed, we'll leak many kernel\nresources, which will be recycled until process terminates. This bug\ncan be reproduced by using mprotect to set params to PROT_READ. To fix\nthis issue, refactor io_uring_create() a bit to add a new 'struct\nio_uring_params __user *params' parameter and move the copy_to_user()\nin io_uring_setup() to io_uring_setup(), if copy_to_user() failed,\nwe can free kernel resource properly.\n\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc5",
        "release_date": "2020-05-05 13:18:11 -0600 io_uring: handle -EFAULT properly in io_uring_setup()"
    },
    {
        "commit": "d8f1b9716cfd1a1f74c0fedad40c5f65a25aa208",
        "message": "The prepare_to_wait() and finish_wait() calls in io_uring_cancel_files()\nare mismatched. Currently I don't see any issues related this bug, just\nfind it by learning codes.\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc5",
        "release_date": "2020-05-04 09:07:14 -0600 io_uring: fix mismatched finish_wait() calls in io_uring_cancel_files()"
    },
    {
        "commit": "cf0185308c41a307a4e7b37b6690d30735fa16a6",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix for statx not grabbing the file table, making AT_EMPTY_PATH fail\n\n - Cover a few cases where async poll can handle retry, eliminating the\n   need for an async thread\n\n - fallback request busy/free fix (Bijan)\n\n - syzbot reported SQPOLL thread exit fix for non-preempt (Xiaoguang)\n\n - Fix extra put of req for sync_file_range (Pavel)\n\n - Always punt splice async. We'll improve this for 5.8, but wanted to\n   eliminate the inode mutex lock from the non-blocking path for 5.7\n   (Pavel)\n\n* tag 'io_uring-5.7-2020-05-01' of git://git.kernel.dk/linux-block:\n  io_uring: punt splice async because of inode mutex\n  io_uring: check non-sync defer_list carefully\n  io_uring: fix extra put in sync_file_range()\n  io_uring: use cond_resched() in io_ring_ctx_wait_and_kill()\n  io_uring: use proper references for fallback_req locking\n  io_uring: only force async punt if poll based retry can't handle it\n  io_uring: enable poll retry for any file with ->read_iter / ->write_iter\n  io_uring: statx must grab the file table for valid fd",
        "kernel_version": "v5.7-rc4",
        "release_date": "2020-05-01 17:03:06 -0700 Merge tag 'io_uring-5.7-2020-05-01' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "2fb3e82284fca40afbde5351907f0a5b3be717f9",
        "message": "Nonblocking do_splice() still may wait for some time on an inode mutex.\nLet's play safe and always punt it async.\n\nReported-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc4",
        "release_date": "2020-05-01 08:50:57 -0600 io_uring: punt splice async because of inode mutex"
    },
    {
        "commit": "4ee3631451c9a62e6b6bc7ee51fb9a5b34e33509",
        "message": "io_req_defer() do double-checked locking. Use proper helpers for that,\ni.e. list_empty_careful().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc4",
        "release_date": "2020-05-01 08:50:30 -0600 io_uring: check non-sync defer_list carefully"
    },
    {
        "commit": "7759a0bfadceef3910d0e50f86d63b6ed58b4e70",
        "message": "[   40.179474] refcount_t: underflow; use-after-free.\n[   40.179499] WARNING: CPU: 6 PID: 1848 at lib/refcount.c:28 refcount_warn_saturate+0xae/0xf0\n...\n[   40.179612] RIP: 0010:refcount_warn_saturate+0xae/0xf0\n[   40.179617] Code: 28 44 0a 01 01 e8 d7 01 c2 ff 0f 0b 5d c3 80 3d 15 44 0a 01 00 75 91 48 c7 c7 b8 f5 75 be c6 05 05 44 0a 01 01 e8 b7 01 c2 ff <0f> 0b 5d c3 80 3d f3 43 0a 01 00 0f 85 6d ff ff ff 48 c7 c7 10 f6\n[   40.179619] RSP: 0018:ffffb252423ebe18 EFLAGS: 00010286\n[   40.179623] RAX: 0000000000000000 RBX: ffff98d65e929400 RCX: 0000000000000000\n[   40.179625] RDX: 0000000000000001 RSI: 0000000000000086 RDI: 00000000ffffffff\n[   40.179627] RBP: ffffb252423ebe18 R08: 0000000000000001 R09: 000000000000055d\n[   40.179629] R10: 0000000000000c8c R11: 0000000000000001 R12: 0000000000000000\n[   40.179631] R13: ffff98d68c434400 R14: ffff98d6a9cbaa20 R15: ffff98d6a609ccb8\n[   40.179634] FS:  0000000000000000(0000) GS:ffff98d6af580000(0000) knlGS:0000000000000000\n[   40.179636] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n[   40.179638] CR2: 00000000033e3194 CR3: 000000006480a003 CR4: 00000000003606e0\n[   40.179641] Call Trace:\n[   40.179652]  io_put_req+0x36/0x40\n[   40.179657]  io_free_work+0x15/0x20\n[   40.179661]  io_worker_handle_work+0x2f5/0x480\n[   40.179667]  io_wqe_worker+0x2a9/0x360\n[   40.179674]  ? _raw_spin_unlock_irqrestore+0x24/0x40\n[   40.179681]  kthread+0x12c/0x170\n[   40.179685]  ? io_worker_handle_work+0x480/0x480\n[   40.179690]  ? kthread_park+0x90/0x90\n[   40.179695]  ret_from_fork+0x35/0x40\n[   40.179702] ---[ end trace 85027405f00110aa ]---\n\nOpcode handler must never put submission ref, but that's what\nio_sync_file_range_finish() do. use io_steal_work() there.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc4",
        "release_date": "2020-05-01 08:50:30 -0600 io_uring: fix extra put in sync_file_range()"
    },
    {
        "commit": "3fd44c86711f71156b586c22b0495c58f69358bb",
        "message": "While working on to make io_uring sqpoll mode support syscalls that need\nstruct files_struct, I got cpu soft lockup in io_ring_ctx_wait_and_kill(),\n\n    while (ctx->sqo_thread && !wq_has_sleeper(&ctx->sqo_wait))\n        cpu_relax();\n\nabove loop never has an chance to exit, it's because preempt isn't enabled\nin the kernel, and the context calling io_ring_ctx_wait_and_kill() and\nio_sq_thread() run in the same cpu, if io_sq_thread calls a cond_resched()\nyield cpu and another context enters above loop, then io_sq_thread() will\nalways in runqueue and never exit.\n\nUse cond_resched() can fix this issue.\n\n Reported-by: syzbot+66243bb7126c410cefe6@syzkaller.appspotmail.com\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc4",
        "release_date": "2020-04-30 22:24:27 -0600 io_uring: use cond_resched() in io_ring_ctx_wait_and_kill()"
    },
    {
        "commit": "dd461af65946de060bff2dab08a63676d2731afe",
        "message": "Use ctx->fallback_req address for test_and_set_bit_lock() and\nclear_bit_unlock().\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc4",
        "release_date": "2020-04-30 22:24:27 -0600 io_uring: use proper references for fallback_req locking"
    },
    {
        "commit": "490e89676a523c688343d6cb8ca5f5dc476414df",
        "message": "We do blocking retry from our poll handler, if the file supports polled\nnotifications. Only mark the request as needing an async worker if we\ncan't poll for it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc4",
        "release_date": "2020-04-30 22:24:27 -0600 io_uring: only force async punt if poll based retry can't handle it"
    },
    {
        "commit": "af197f50ac53fff1241598c73ca606754a3bb808",
        "message": "We can have files like eventfd where it's perfectly fine to do poll\nbased retry on them, right now io_file_supports_async() doesn't take\nthat into account.\n\nPass in data direction and check the f_op instead of just always needing\nan async worker.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc4",
        "release_date": "2020-04-30 22:24:22 -0600 io_uring: enable poll retry for any file with ->read_iter / ->write_iter"
    },
    {
        "commit": "5b0bbee4732cbd58aa98213d4c11a366356bba3d",
        "message": "Clay reports that OP_STATX fails for a test case with a valid fd\nand empty path:\n\n -- Test 0: statx:fd 3: SUCCEED, file mode 100755\n -- Test 1: statx:path ./uring_statx: SUCCEED, file mode 100755\n -- Test 2: io_uring_statx:fd 3: FAIL, errno 9: Bad file descriptor\n -- Test 3: io_uring_statx:path ./uring_statx: SUCCEED, file mode 100755\n\nThis is due to statx not grabbing the process file table, hence we can't\nlookup the fd in async context. If the fd is valid, ensure that we grab\nthe file table so we can grab the file from async context.\n\nCc: stable@vger.kernel.org # v5.6\nReported-by: Clay Harris <bugs@claycon.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc4",
        "release_date": "2020-04-27 10:41:22 -0600 io_uring: statx must grab the file table for valid fd"
    },
    {
        "commit": "bc0c4d1e176eeb614dc8734fc3ace34292771f11",
        "message": "IORING_OP_MADVISE can end up basically doing mprotect() on the VM of\nanother process, which means that it can race with our crazy core dump\nhandling which accesses the VM state without holding the mmap_sem\n(because it incorrectly thinks that it is the final user).\n\nThis is clearly a core dumping problem, but we've never fixed it the\nright way, and instead have the notion of \"check that the mm is still\nok\" using mmget_still_valid() after getting the mmap_sem for writing in\nany situation where we're not the original VM thread.\n\nSee commit 04f5866e41fb (\"coredump: fix race condition between\nmmget_not_zero()/get_task_mm() and core dumping\") for more background on\nthis whole mmget_still_valid() thing.  You might want to have a barf bag\nhandy when you do.\n\nWe're discussing just fixing this properly in the only remaining core\ndumping routines.  But even if we do that, let's make do_madvise() do\nthe right thing, and then when we fix core dumping, we can remove all\nthese mmget_still_valid() checks.\n\nReported-and-tested-by: Jann Horn <jannh@google.com>\nFixes: c1ca757bd6f4 (\"io_uring: add IORING_OP_MADVISE\")\nAcked-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
        "kernel_version": "v5.7-rc3",
        "release_date": "2020-04-24 13:28:03 -0700 mm: check that mm is still valid in madvise()"
    },
    {
        "commit": "aee1a009c9d247756b368890e3f886174776d3db",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Single fixup for a change that went into -rc2\"\n\n* tag 'io_uring-5.7-2020-04-24' of git://git.kernel.dk/linux-block:\n  io_uring: only restore req->work for req that needs do completion",
        "kernel_version": "v5.7-rc3",
        "release_date": "2020-04-24 12:58:22 -0700 Merge tag 'io_uring-5.7-2020-04-24' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "44575a67314b3768d4415252271e8f60c5c70118",
        "message": "When testing io_uring IORING_FEAT_FAST_POLL feature, I got below panic:\nBUG: kernel NULL pointer dereference, address: 0000000000000030\nPGD 0 P4D 0\nOops: 0000 [#1] SMP PTI\nCPU: 5 PID: 2154 Comm: io_uring_echo_s Not tainted 5.6.0+ #359\nHardware name: QEMU Standard PC (i440FX + PIIX, 1996),\nBIOS rel-1.11.1-0-g0551a4be2c-prebuilt.qemu-project.org 04/01/2014\nRIP: 0010:io_wq_submit_work+0xf/0xa0\nCode: ff ff ff be 02 00 00 00 e8 ae c9 19 00 e9 58 ff ff ff 66 0f 1f\n84 00 00 00 00 00 0f 1f 44 00 00 41 54 49 89 fc 55 53 48 8b 2f <8b>\n45 30 48 8d 9d 48 ff ff ff 25 01 01 00 00 83 f8 01 75 07 eb 2a\nRSP: 0018:ffffbef543e93d58 EFLAGS: 00010286\nRAX: ffffffff84364f50 RBX: ffffa3eb50f046b8 RCX: 0000000000000000\nRDX: ffffa3eb0efc1840 RSI: 0000000000000006 RDI: ffffa3eb50f046b8\nRBP: 0000000000000000 R08: 00000000fffd070d R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000000 R12: ffffa3eb50f046b8\nR13: ffffa3eb0efc2088 R14: ffffffff85b69be0 R15: ffffa3eb0effa4b8\nFS:  00007fe9f69cc4c0(0000) GS:ffffa3eb5ef40000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 0000000000000030 CR3: 0000000020410000 CR4: 00000000000006e0\nCall Trace:\n task_work_run+0x6d/0xa0\n do_exit+0x39a/0xb80\n ? get_signal+0xfe/0xbc0\n do_group_exit+0x47/0xb0\n get_signal+0x14b/0xbc0\n ? __x64_sys_io_uring_enter+0x1b7/0x450\n do_signal+0x2c/0x260\n ? __x64_sys_io_uring_enter+0x228/0x450\n exit_to_usermode_loop+0x87/0xf0\n do_syscall_64+0x209/0x230\n entry_SYSCALL_64_after_hwframe+0x49/0xb3\nRIP: 0033:0x7fe9f64f8df9\nCode: Bad RIP value.\n\ntask_work_run calls io_wq_submit_work unexpectedly, it's obvious that\nstruct callback_head's func member has been changed. After looking into\ncodes, I found this issue is still due to the union definition:\n    union {\n        /*\n         * Only commands that never go async can use the below fields,\n         * obviously. Right now only IORING_OP_POLL_ADD uses them, and\n         * async armed poll handlers for regular commands. The latter\n         * restore the work, if needed.\n         */\n        struct {\n            struct callback_head\ttask_work;\n            struct hlist_node\thash_node;\n            struct async_poll\t*apoll;\n        };\n        struct io_wq_work\twork;\n    };\n\nWhen task_work_run has multiple work to execute, the work that calls\nio_poll_remove_all() will do req->work restore for  non-poll request\nalways, but indeed if a non-poll request has been added to a new\ncallback_head, subsequent callback will call io_async_task_func() to\nhandle this request, that means we should not do the restore work\nfor such non-poll request. Meanwhile in io_async_task_func(), we should\ndrop submit ref when req has been canceled.\n\nFix both issues.\n\nFixes: b1f573bd15fd (\"io_uring: restore req->work when canceling poll request\")\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\n\nUse io_double_put_req()\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc3",
        "release_date": "2020-04-19 13:55:27 -0600 io_uring: only restore req->work for req that needs do completion"
    },
    {
        "commit": "a2286a449baf11624d393aad28c1081cee3f47fb",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - wrap up the init/setup cleanup (Pavel)\n\n - fix some issues around deferral sequences (Pavel)\n\n - fix splice punt check using the wrong struct file member\n\n - apply poll re-arm logic for pollable retry too\n\n - pollable retry should honor cancelation\n\n - fix setup time error handling syzbot reported crash\n\n - restore work state when poll is canceled\n\n* tag 'io_uring-5.7-2020-04-17' of git://git.kernel.dk/linux-block:\n  io_uring: don't count rqs failed after current one\n  io_uring: kill already cached timeout.seq_offset\n  io_uring: fix cached_sq_head in io_timeout()\n  io_uring: only post events in io_poll_remove_all() if we completed some\n  io_uring: io_async_task_func() should check and honor cancelation\n  io_uring: check for need to re-wait in polled async handling\n  io_uring: correct O_NONBLOCK check for splice punt\n  io_uring: restore req->work when canceling poll request\n  io_uring: move all request init code in one place\n  io_uring: keep all sqe->flags in req->flags\n  io_uring: early submission req fail code\n  io_uring: track mm through current->mm\n  io_uring: remove obsolete @mm_fault",
        "kernel_version": "v5.7-rc2",
        "release_date": "2020-04-17 10:12:26 -0700 Merge tag 'io_uring-5.7-2020-04-17' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "31af27c7cc9f675d93a135dca99e6413f9096f1d",
        "message": "When checking for draining with __req_need_defer(), it tries to match\nhow many requests were sent before a current one with number of already\ncompleted. Dropped SQEs are included in req->sequence, and they won't\never appear in CQ. To compensate for that, __req_need_defer() substracts\nctx->cached_sq_dropped.\nHowever, what it should really use is number of SQEs dropped __before__\nthe current one. In other words, any submitted request shouldn't\nshouldn't affect dequeueing from the drain queue of previously submitted\nones.\n\nInstead of saving proper ctx->cached_sq_dropped in each request,\nsubstract from req->sequence it at initialisation, so it includes number\nof properly submitted requests.\n\nnote: it also changes behaviour of timeouts, but\n1. it's already diverge from the description because of using SQ\n2. the description is ambiguous regarding dropped SQEs\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc2",
        "release_date": "2020-04-14 19:16:59 -0600 io_uring: don't count rqs failed after current one"
    },
    {
        "commit": "b55ce732004989c85bf9d858c03e6d477cf9023b",
        "message": "req->timeout.count and req->io->timeout.seq_offset store the same value,\nwhich is sqe->off. Kill the second one\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc2",
        "release_date": "2020-04-14 19:16:57 -0600 io_uring: kill already cached timeout.seq_offset"
    },
    {
        "commit": "22cad1585c6bc6caf2688701004cf2af6865cbe0",
        "message": "io_timeout() can be executed asynchronously by a worker and without\nholding ctx->uring_lock\n\n1. using ctx->cached_sq_head there is racy there\n2. it should count events from a moment of timeout's submission, but\nnot execution\n\nUse req->sequence.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc2",
        "release_date": "2020-04-14 19:16:55 -0600 io_uring: fix cached_sq_head in io_timeout()"
    },
    {
        "commit": "8e2e1faf28b3e66430f55f0b0ee83370ecc150af",
        "message": "syzbot reports this crash:\n\nBUG: unable to handle page fault for address: ffffffffffffffe8\nPGD f96e17067 P4D f96e17067 PUD f96e19067 PMD 0\nOops: 0000 [#1] SMP DEBUG_PAGEALLOC KASAN PTI\nCPU: 55 PID: 211750 Comm: trinity-c127 Tainted: G    B        L    5.7.0-rc1-next-20200413 #4\nHardware name: HP ProLiant DL380 Gen9/ProLiant DL380 Gen9, BIOS P89 04/12/2017\nRIP: 0010:__wake_up_common+0x98/0x290\nel/sched/wait.c:87\nCode: 40 4d 8d 78 e8 49 8d 7f 18 49 39 fd 0f 84 80 00 00 00 e8 6b bd 2b 00 49 8b 5f 18 45 31 e4 48 83 eb 18 4c 89 ff e8 08 bc 2b 00 <45> 8b 37 41 f6 c6 04 75 71 49 8d 7f 10 e8 46 bd 2b 00 49 8b 47 10\nRSP: 0018:ffffc9000adbfaf0 EFLAGS: 00010046\nRAX: 0000000000000000 RBX: ffffffffffffffe8 RCX: ffffffffaa9636b8\nRDX: 0000000000000003 RSI: dffffc0000000000 RDI: ffffffffffffffe8\nRBP: ffffc9000adbfb40 R08: fffffbfff582c5fd R09: fffffbfff582c5fd\nR10: ffffffffac162fe3 R11: fffffbfff582c5fc R12: 0000000000000000\nR13: ffff888ef82b0960 R14: ffffc9000adbfb80 R15: ffffffffffffffe8\nFS:  00007fdcba4c4740(0000) GS:ffff889033780000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: ffffffffffffffe8 CR3: 0000000f776a0004 CR4: 00000000001606e0\nCall Trace:\n __wake_up_common_lock+0xea/0x150\nommon_lock at kernel/sched/wait.c:124\n ? __wake_up_common+0x290/0x290\n ? lockdep_hardirqs_on+0x16/0x2c0\n __wake_up+0x13/0x20\n io_cqring_ev_posted+0x75/0xe0\nv_posted at fs/io_uring.c:1160\n io_ring_ctx_wait_and_kill+0x1c0/0x2f0\nl at fs/io_uring.c:7305\n io_uring_create+0xa8d/0x13b0\n ? io_req_defer_prep+0x990/0x990\n ? __kasan_check_write+0x14/0x20\n io_uring_setup+0xb8/0x130\n ? io_uring_create+0x13b0/0x13b0\n ? check_flags.part.28+0x220/0x220\n ? lockdep_hardirqs_on+0x16/0x2c0\n __x64_sys_io_uring_setup+0x31/0x40\n do_syscall_64+0xcc/0xaf0\n ? syscall_return_slowpath+0x580/0x580\n ? lockdep_hardirqs_off+0x1f/0x140\n ? entry_SYSCALL_64_after_hwframe+0x3e/0xb3\n ? trace_hardirqs_off_caller+0x3a/0x150\n ? trace_hardirqs_off_thunk+0x1a/0x1c\n entry_SYSCALL_64_after_hwframe+0x49/0xb3\nRIP: 0033:0x7fdcb9dd76ed\nCode: 00 c3 66 2e 0f 1f 84 00 00 00 00 00 90 f3 0f 1e fa 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 8b 0d 6b 57 2c 00 f7 d8 64 89 01 48\nRSP: 002b:00007ffe7fd4e4f8 EFLAGS: 00000246 ORIG_RAX: 00000000000001a9\nRAX: ffffffffffffffda RBX: 00000000000001a9 RCX: 00007fdcb9dd76ed\nRDX: fffffffffffffffc RSI: 0000000000000000 RDI: 0000000000005d54\nRBP: 00000000000001a9 R08: 0000000e31d3caa7 R09: 0082400004004000\nR10: ffffffffffffffff R11: 0000000000000246 R12: 0000000000000002\nR13: 00007fdcb842e058 R14: 00007fdcba4c46c0 R15: 00007fdcb842e000\nModules linked in: bridge stp llc nfnetlink cn brd vfat fat ext4 crc16 mbcache jbd2 loop kvm_intel kvm irqbypass intel_cstate intel_uncore dax_pmem intel_rapl_perf dax_pmem_core ip_tables x_tables xfs sd_mod tg3 firmware_class libphy hpsa scsi_transport_sas dm_mirror dm_region_hash dm_log dm_mod [last unloaded: binfmt_misc]\nCR2: ffffffffffffffe8\n---[ end trace f9502383d57e0e22 ]---\nRIP: 0010:__wake_up_common+0x98/0x290\nCode: 40 4d 8d 78 e8 49 8d 7f 18 49 39 fd 0f 84 80 00 00 00 e8 6b bd 2b 00 49 8b 5f 18 45 31 e4 48 83 eb 18 4c 89 ff e8 08 bc 2b 00 <45> 8b 37 41 f6 c6 04 75 71 49 8d 7f 10 e8 46 bd 2b 00 49 8b 47 10\nRSP: 0018:ffffc9000adbfaf0 EFLAGS: 00010046\nRAX: 0000000000000000 RBX: ffffffffffffffe8 RCX: ffffffffaa9636b8\nRDX: 0000000000000003 RSI: dffffc0000000000 RDI: ffffffffffffffe8\nRBP: ffffc9000adbfb40 R08: fffffbfff582c5fd R09: fffffbfff582c5fd\nR10: ffffffffac162fe3 R11: fffffbfff582c5fc R12: 0000000000000000\nR13: ffff888ef82b0960 R14: ffffc9000adbfb80 R15: ffffffffffffffe8\nFS:  00007fdcba4c4740(0000) GS:ffff889033780000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: ffffffffffffffe8 CR3: 0000000f776a0004 CR4: 00000000001606e0\nKernel panic - not syncing: Fatal exception\nKernel Offset: 0x29800000 from 0xffffffff81000000 (relocation range: 0xffffffff80000000-0xffffffffbfffffff)\n---[ end Kernel panic - not syncing: Fatal exception ]\u2014\n\nwhich is due to error injection (or allocation failure) preventing the\nrings from being setup. On shutdown, we attempt to remove any pending\nrequests, and for poll request, we call io_cqring_ev_posted() when we've\nkilled poll requests. However, since the rings aren't setup, we won't\nfind any poll requests. Make the calling of io_cqring_ev_posted()\ndependent on actually having completed requests. This fixes this setup\ncorner case, and removes spurious calls if we remove poll requests and\ndon't find any.\n\nReported-by: Qian Cai <cai@lca.pw>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc2",
        "release_date": "2020-04-13 17:05:14 -0600 io_uring: only post events in io_poll_remove_all() if we completed some"
    },
    {
        "commit": "2bae047ec9576da72d5003487de0bb93e747fff7",
        "message": "If the request has been marked as canceled, don't try and issue it.\nInstead just fill a canceled event and finish the request.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc2",
        "release_date": "2020-04-13 11:22:52 -0600 io_uring: io_async_task_func() should check and honor cancelation"
    },
    {
        "commit": "74ce6ce43d4fc6ce15efb21378d9ef26125c298b",
        "message": "We added this for just the regular poll requests in commit a6ba632d2c24\n(\"io_uring: retry poll if we got woken with non-matching mask\"), we\nshould do the same for the poll handler used pollable async requests.\nMove the re-wait check and arm into a helper, and call it from\nio_async_task_func() as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc2",
        "release_date": "2020-04-13 11:09:12 -0600 io_uring: check for need to re-wait in polled async handling"
    },
    {
        "commit": "88357580854aab29d27e1a443575caaedd081612",
        "message": "The splice file punt check uses file->f_mode to check for O_NONBLOCK,\nbut it should be checking file->f_flags. This leads to punting even\nfor files that have O_NONBLOCK set, which isn't necessary. This equates\nto checking for FMODE_PATH, which will never be set on the fd in\nquestion.\n\nFixes: 7d67af2c0134 (\"io_uring: add splice(2) support\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc2",
        "release_date": "2020-04-12 21:16:01 -0600 io_uring: correct O_NONBLOCK check for splice punt"
    },
    {
        "commit": "b1f573bd15fda2e19ea66a4d26fae8be1b12791d",
        "message": "When running liburing test case 'accept', I got below warning:\nRED: Invalid credentials\nRED: At include/linux/cred.h:285\nRED: Specified credentials: 00000000d02474a0\nRED: ->magic=4b, put_addr=000000005b4f46e9\nRED: ->usage=-1699227648, subscr=-25693\nRED: ->*uid = { 256,-25693,-25693,65534 }\nRED: ->*gid = { 0,-1925859360,-1789740800,-1827028688 }\nRED: ->security is 00000000258c136e\neneral protection fault, probably for non-canonical address 0xdead4ead00000000: 0000 [#1] SMP PTI\nPU: 21 PID: 2037 Comm: accept Not tainted 5.6.0+ #318\nardware name: QEMU Standard PC (i440FX + PIIX, 1996),\nBIOS rel-1.11.1-0-g0551a4be2c-prebuilt.qemu-project.org 04/01/2014\nIP: 0010:dump_invalid_creds+0x16f/0x184\node: 48 8b 83 88 00 00 00 48 3d ff 0f 00 00 76 29 48 89 c2 81 e2 00 ff ff ff 48\n81 fa 00 6b 6b 6b 74 17 5b 48 c7 c7 4b b1 10 8e 5d <8b> 50 04 41 5c 8b 30 41 5d\ne9 67 e3 04 00 5b 5d 41 5c 41 5d c3 0f\nSP: 0018:ffffacc1039dfb38 EFLAGS: 00010087\nAX: dead4ead00000000 RBX: ffff9ba39319c100 RCX: 0000000000000007\nDX: 0000000000000000 RSI: 0000000000000000 RDI: ffffffff8e10b14b\nBP: ffffffff8e108476 R08: 0000000000000000 R09: 0000000000000001\n10: 0000000000000000 R11: ffffacc1039df9e5 R12: 000000009552b900\n13: 000000009319c130 R14: ffff9ba39319c100 R15: 0000000000000246\nS:  00007f96b2bfc4c0(0000) GS:ffff9ba39f340000(0000) knlGS:0000000000000000\nS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nR2: 0000000000401870 CR3: 00000007db7a4000 CR4: 00000000000006e0\nall Trace:\n__invalid_creds+0x48/0x4a\n__io_req_aux_free+0x2e8/0x3b0\n? io_poll_remove_one+0x2a/0x1d0\n__io_free_req+0x18/0x200\nio_free_req+0x31/0x350\nio_poll_remove_one+0x17f/0x1d0\nio_poll_cancel.isra.80+0x6c/0x80\nio_async_find_and_cancel+0x111/0x120\nio_issue_sqe+0x181/0x10e0\n? __lock_acquire+0x552/0xae0\n? lock_acquire+0x8e/0x310\n? fs_reclaim_acquire.part.97+0x5/0x30\n__io_queue_sqe.part.100+0xc4/0x580\n? io_submit_sqes+0x751/0xbd0\n? rcu_read_lock_sched_held+0x32/0x40\nio_submit_sqes+0x9ba/0xbd0\n? __x64_sys_io_uring_enter+0x2b2/0x460\n? __x64_sys_io_uring_enter+0xaf/0x460\n? find_held_lock+0x2d/0x90\n? __x64_sys_io_uring_enter+0x111/0x460\n__x64_sys_io_uring_enter+0x2d7/0x460\ndo_syscall_64+0x5a/0x230\nentry_SYSCALL_64_after_hwframe+0x49/0xb3\n\nAfter looking into codes, it turns out that this issue is because we didn't\nrestore the req->work, which is changed in io_arm_poll_handler(), req->work\nis a union with below struct:\n\tstruct {\n\t\tstruct callback_head\ttask_work;\n\t\tstruct hlist_node\thash_node;\n\t\tstruct async_poll\t*apoll;\n\t};\nIf we forget to restore, members in struct io_wq_work would be invalid,\nrestore the req->work to fix this issue.\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\n\nGet rid of not needed 'need_restore' variable.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc2",
        "release_date": "2020-04-12 08:46:50 -0600 io_uring: restore req->work when canceling poll request"
    },
    {
        "commit": "ef4ff581102a917a69877feca2e5347e2f3e458c",
        "message": "Requests initialisation is scattered across several functions, namely\nio_init_req(), io_submit_sqes(), io_submit_sqe(). Put it\nin io_init_req() for better data locality and code clarity.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc2",
        "release_date": "2020-04-12 08:46:50 -0600 io_uring: move all request init code in one place"
    },
    {
        "commit": "dea3b49c7fb09b4f6b6a574c0485ffeb9df7b69c",
        "message": "It's a good idea to not read sqe->flags twice, as it's prone to security\nbugs. Instead of passing it around, embeed them in req->flags. It's\nalready so except for IOSQE_IO_LINK.\n1. rename former REQ_F_LINK -> REQ_F_LINK_HEAD\n2. introduce and copy REQ_F_LINK, which mimics IO_IOSQE_LINK\n\nAnd leave req_set_fail_links() using new REQ_F_LINK, because it's more\nsensible.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc2",
        "release_date": "2020-04-12 08:46:49 -0600 io_uring: keep all sqe->flags in req->flags"
    },
    {
        "commit": "1d4240cc9e7bb101dac58f30283fa24a809f5606",
        "message": "Having only one place for cleaning up a request after a link assembly/\nsubmission failure will play handy in the future. At least it allows\nto remove duplicated cleanup sequence.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc2",
        "release_date": "2020-04-12 08:46:44 -0600 io_uring: early submission req fail code"
    },
    {
        "commit": "bf9c2f1cdcc718b6d2d41172f6ca005fe22cc7ff",
        "message": "As a preparation for extracting request init bits, remove self-coded mm\ntracking from io_submit_sqes(), but rely on current->mm. It's more\nconvenient, than passing this piece of state in other functions.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc2",
        "release_date": "2020-04-12 08:46:30 -0600 io_uring: track mm through current->mm"
    },
    {
        "commit": "dccc587f6c07ccc734588226fdf62f685558e89f",
        "message": "If io_submit_sqes() can't grab an mm, it fails and exits right away.\nThere is no need to track the fact of the failure. Remove @mm_fault.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc2",
        "release_date": "2020-04-12 08:46:30 -0600 io_uring: remove obsolete @mm_fault"
    },
    {
        "commit": "172edde9604941f61d75bb3b4f88068204f8c086",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Here's a set of fixes that either weren't quite ready for the first,\n  or came about from some intensive testing on memcached with 350K+\n  sockets.\n\n  Summary:\n\n   - Fixes for races or deadlocks around poll handling\n\n   - Don't double account fixed files against RLIMIT_NOFILE\n\n   - IORING_OP_OPENAT LFS fix\n\n   - Poll retry handling (Bijan)\n\n   - Missing finish_wait() for SQPOLL (Hillf)\n\n   - Cleanup/split of io_kiocb alloc vs ctx references (Pavel)\n\n   - Fixed file unregistration and init fixes (Xiaoguang)\n\n   - Various little fixes (Xiaoguang, Pavel, Colin)\"\n\n* tag 'io_uring-5.7-2020-04-09' of git://git.kernel.dk/linux-block:\n  io_uring: punt final io_ring_ctx wait-and-free to workqueue\n  io_uring: fix fs cleanup on cqe overflow\n  io_uring: don't read user-shared sqe flags twice\n  io_uring: remove req init from io_get_req()\n  io_uring: alloc req only after getting sqe\n  io_uring: simplify io_get_sqring\n  io_uring: do not always copy iovec in io_req_map_rw()\n  io_uring: ensure openat sets O_LARGEFILE if needed\n  io_uring: initialize fixed_file_data lock\n  io_uring: remove redundant variable pointer nxt and io_wq_assign_next call\n  io_uring: fix ctx refcounting in io_submit_sqes()\n  io_uring: process requests completed with -EAGAIN on poll list\n  io_uring: remove bogus RLIMIT_NOFILE check in file registration\n  io_uring: use io-wq manager as backup task if task is exiting\n  io_uring: grab task reference for poll requests\n  io_uring: retry poll if we got woken with non-matching mask\n  io_uring: add missing finish_wait() in io_sq_thread()\n  io_uring: refactor file register/unregister/update handling",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-10 10:02:21 -0700 Merge tag 'io_uring-5.7-2020-04-09' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "85faa7b8346ebef0606d2d0df6d3f8c76acb3654",
        "message": "We can't reliably wait in io_ring_ctx_wait_and_kill(), since the\ntask_works list isn't ordered (in fact it's LIFO ordered). We could\neither fix this with a separate task_works list for io_uring work, or\njust punt the wait-and-free to async context. This ensures that\ntask_work that comes in while we're shutting down is processed\ncorrectly. If we don't go async, we could have work past the fput()\nwork for the ring that depends on work that won't be executed until\nafter we're done with the wait-and-free. But as this operation is\nblocking, it'll never get a chance to run.\n\nThis was reproduced with hundreds of thousands of sockets running\nmemcached, haven't been able to reproduce this synthetically.\n\nReported-by: Dan Melnic <dmm@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-09 18:45:27 -0600 io_uring: punt final io_ring_ctx wait-and-free to workqueue"
    },
    {
        "commit": "c398ecb3d611925e4a5411afdf7489914a5c0460",
        "message": "If completion queue overflow occurs, __io_cqring_fill_event() will\nupdate req->cflags, which is in a union with req->work and happens to\nbe aliased to req->work.fs. Following io_free_req() ->\nio_req_work_drop_env() may get a bunch of different problems (miscount\nfs->users, segfault, etc) on cleaning @fs.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-09 09:38:18 -0600 io_uring: fix fs cleanup on cqe overflow"
    },
    {
        "commit": "9c280f9087118099f50566e906b9d9d5a0fb4529",
        "message": "Don't re-read userspace-shared sqe->flags, it can be exploited.\nsqe->flags are copied into req->flags in io_submit_sqe(), check them\nthere instead.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-08 09:26:51 -0600 io_uring: don't read user-shared sqe flags twice"
    },
    {
        "commit": "0553b8bda8709c47863eab3fff7ac32ad04ca52b",
        "message": "io_get_req() do two different things: io_kiocb allocation and\ninitialisation. Move init part out of it and rename into\nio_alloc_req(). It's simpler this way and also have better data\nlocality.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-08 09:26:28 -0600 io_uring: remove req init from io_get_req()"
    },
    {
        "commit": "b1e50e549b1372d9742509230dc4af7dd521d984",
        "message": "As io_get_sqe() split into 2 stage get/consume, get an sqe before\nallocating io_kiocb, so no free_req*() for a failure case is needed,\nand inline back __io_req_do_free(), which has only 1 user.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-08 09:26:16 -0600 io_uring: alloc req only after getting sqe"
    },
    {
        "commit": "709b302faddfac757d87df2080f900eccb1dc9e2",
        "message": "Make io_get_sqring() care only about sqes themselves, not initialising\nthe io_kiocb. Also, split it into get + consume, that will be helpful in\nthe future.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-08 09:25:18 -0600 io_uring: simplify io_get_sqring"
    },
    {
        "commit": "45097daea2f4e89bdb1c98359f78d0d6feb8e5c8",
        "message": "In io_read_prep() or io_write_prep(), io_req_map_rw() takes\nstruct io_async_rw's fast_iov as argument to call io_import_iovec(),\nand if io_import_iovec() uses struct io_async_rw's fast_iov as\nvalid iovec array, later indeed io_req_map_rw() does not need\nto do the memcpy operation, because they are same pointers.\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-08 09:23:42 -0600 io_uring: do not always copy iovec in io_req_map_rw()"
    },
    {
        "commit": "08a1d26eb894a9dcf79f674558a284ad1ffef517",
        "message": "OPENAT2 correctly sets O_LARGEFILE if it has to, but that escaped the\nOPENAT opcode. Dmitry reports that his test case that compares openat()\nand IORING_OP_OPENAT sees failures on large files:\n\n*** sync openat\nopenat succeeded\nsync write at offset 0\nwrite succeeded\nsync write at offset 4294967296\nwrite succeeded\n\n*** sync openat\nopenat succeeded\nio_uring write at offset 0\nwrite succeeded\nio_uring write at offset 4294967296\nwrite succeeded\n\n*** io_uring openat\nopenat succeeded\nsync write at offset 0\nwrite succeeded\nsync write at offset 4294967296\nwrite failed: File too large\n\n*** io_uring openat\nopenat succeeded\nio_uring write at offset 0\nwrite succeeded\nio_uring write at offset 4294967296\nwrite failed: File too large\n\nEnsure we set O_LARGEFILE, if force_o_largefile() is true.\n\nCc: stable@vger.kernel.org # v5.6\nFixes: 15b71abe7b52 (\"io_uring: add support for IORING_OP_OPENAT\")\nReported-by: Dmitry Kadashev <dkadashev@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-08 09:20:54 -0600 io_uring: ensure openat sets O_LARGEFILE if needed"
    },
    {
        "commit": "f7fe9346869a12efe3af3cc9be2e45a1b6ff8761",
        "message": "syzbot reports below warning:\nINFO: trying to register non-static key.\nthe code is fine but needs lockdep annotation.\nturning off the locking correctness validator.\nCPU: 1 PID: 7099 Comm: syz-executor897 Not tainted 5.6.0-next-20200406-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nCall Trace:\n __dump_stack lib/dump_stack.c:77 [inline]\n dump_stack+0x188/0x20d lib/dump_stack.c:118\n assign_lock_key kernel/locking/lockdep.c:913 [inline]\n register_lock_class+0x1664/0x1760 kernel/locking/lockdep.c:1225\n __lock_acquire+0x104/0x4e00 kernel/locking/lockdep.c:4223\n lock_acquire+0x1f2/0x8f0 kernel/locking/lockdep.c:4923\n __raw_spin_lock_irqsave include/linux/spinlock_api_smp.h:110 [inline]\n _raw_spin_lock_irqsave+0x8c/0xbf kernel/locking/spinlock.c:159\n io_sqe_files_register fs/io_uring.c:6599 [inline]\n __io_uring_register+0x1fe8/0x2f00 fs/io_uring.c:8001\n __do_sys_io_uring_register fs/io_uring.c:8081 [inline]\n __se_sys_io_uring_register fs/io_uring.c:8063 [inline]\n __x64_sys_io_uring_register+0x192/0x560 fs/io_uring.c:8063\n do_syscall_64+0xf6/0x7d0 arch/x86/entry/common.c:295\n entry_SYSCALL_64_after_hwframe+0x49/0xb3\nRIP: 0033:0x440289\nCode: 18 89 d0 c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 00 48 89 f8 48 89 f7\n48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff\nff 0f 83 fb 13 fc ff c3 66 2e 0f 1f 84 00 00 00 00\nRSP: 002b:00007ffff1bbf558 EFLAGS: 00000246 ORIG_RAX: 00000000000001ab\nRAX: ffffffffffffffda RBX: 00000000004002c8 RCX: 0000000000440289\nRDX: 0000000020000280 RSI: 0000000000000002 RDI: 0000000000000003\nRBP: 00000000006ca018 R08: 0000000000000000 R09: 00000000004002c8\nR10: 0000000000000001 R11: 0000000000000246 R12: 0000000000401b10\nR13: 0000000000401ba0 R14: 0000000000000000 R15: 0000000000000000\n\nInitialize struct fixed_file_data's lock to fix this issue.\n\nReported-by: syzbot+e6eeca4a035da76b3065@syzkaller.appspotmail.com\nFixes: 055895537302 (\"io_uring: refactor file register/unregister/update handling\")\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-07 09:45:51 -0600 io_uring: initialize fixed_file_data lock"
    },
    {
        "commit": "211fea18a7bb9b8d51cb5d2b9cbe5583af256609",
        "message": "An earlier commit \"io_uring: remove @nxt from handlers\" removed the\nsetting of pointer nxt and now it is always null, hence the non-null\ncheck and call to io_wq_assign_next is redundant and can be removed.\n\nAddresses-Coverity: (\"'Constant' variable guard\")\nReviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>\nSigned-off-by: Colin Ian King <colin.king@canonical.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-07 09:45:33 -0600 io_uring: remove redundant variable pointer nxt and io_wq_assign_next call"
    },
    {
        "commit": "48bdd849e967f1c573d2b2bc24308e24a83f39c2",
        "message": "If io_get_req() fails, it drops a ref. Then, awhile keeping @submitted\nunmodified, io_submit_sqes() breaks the loop and puts @nr - @submitted\nrefs. For each submitted req a ref is dropped in io_put_req() and\nfriends. So, for @nr taken refs there will be\n(@nr - @submitted + @submitted + 1) dropped.\n\nRemove ctx refcounting from io_get_req(), that at the same time makes\nit clearer.\n\nFixes: 2b85edfc0c90 (\"io_uring: batch getting pcpu references\")\nCc: stable@vger.kernel.org # v5.6\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-05 16:23:48 -0600 io_uring: fix ctx refcounting in io_submit_sqes()"
    },
    {
        "commit": "581f981034890dfd27be7e98946e8f0461f3967a",
        "message": "A request that completes with an -EAGAIN result after it has been added\nto the poll list, will not be removed from that list in io_do_iopoll()\nbecause the f_op->iopoll() will not succeed for that request.\n\nMaintain a retryable local list similar to the done list, and explicity\nreissue requests with an -EAGAIN result.\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-03 14:55:39 -0600 io_uring: process requests completed with -EAGAIN on poll list"
    },
    {
        "commit": "c336e992cb1cb1db9ee608dfb30342ae781057ab",
        "message": "We already checked this limit when the file was opened, and we keep it\nopen in the file table. Hence when we added unit_inflight to the count\nwe want to register, we're doubly accounting these files. This results\nin -EMFILE for file registration, if we're at half the limit.\n\nCc: stable@vger.kernel.org # v5.1+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-03 13:56:44 -0600 io_uring: remove bogus RLIMIT_NOFILE check in file registration"
    },
    {
        "commit": "aa96bf8a9ee33457b7e3ea43e97dfa1e3a15ab20",
        "message": "If the original task is (or has) exited, then the task work will not get\nqueued properly. Allow for using the io-wq manager task to queue this\nwork for execution, and ensure that the io-wq manager notices and runs\nthis work if woken up (or exiting).\n\nReported-by: Dan Melnic <dmm@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-03 11:35:57 -0600 io_uring: use io-wq manager as backup task if task is exiting"
    },
    {
        "commit": "3537b6a7c65434d0d2cc0c9862e69be11c367fdc",
        "message": "We can have a task exit if it's not the owner of the ring. Be safe and\ngrab an actual reference to it, to avoid a potential use-after-free.\n\nReported-by: Dan Melnic <dmm@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-03 11:35:57 -0600 io_uring: grab task reference for poll requests"
    },
    {
        "commit": "a6ba632d2c249a4390289727c07b8b55eb02a41d",
        "message": "If we get woken and the poll doesn't match our mask, re-add the task\nto the poll waitqueue and try again instead of completing the request\nwith a mask of 0.\n\nReported-by: Dan Melnic <dmm@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-03 11:35:48 -0600 io_uring: retry poll if we got woken with non-matching mask"
    },
    {
        "commit": "10bea96dcc13ad841d53bdcc9d8e731e9e0ad58f",
        "message": "Add it to pair with prepare_to_wait() in an attempt to avoid\nanything weird in the field.\n\nFixes: b41e98524e42 (\"io_uring: add per-task callback handler\")\nReported-by: syzbot+0c3370f235b74b3cfd97@syzkaller.appspotmail.com\nSigned-off-by: Hillf Danton <hdanton@sina.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-04-01 07:02:55 -0600 io_uring: add missing finish_wait() in io_sq_thread()"
    },
    {
        "commit": "0558955373023b08f638c9ede36741b0e4200f58",
        "message": "While diving into io_uring fileset register/unregister/update codes, we\nfound one bug in the fileset update handling. io_uring fileset update\nuse a percpu_ref variable to check whether we can put the previously\nregistered file, only when the refcnt of the perfcpu_ref variable\nreaches zero, can we safely put these files. But this doesn't work so\nwell. If applications always issue requests continually, this\nperfcpu_ref will never have an chance to reach zero, and it'll always be\nin atomic mode, also will defeat the gains introduced by fileset\nregister/unresiger/update feature, which are used to reduce the atomic\noperation overhead of fput/fget.\n\nTo fix this issue, while applications do IORING_REGISTER_FILES or\nIORING_REGISTER_FILES_UPDATE operations, we allocate a new percpu_ref\nand kill the old percpu_ref, new requests will use the new percpu_ref.\nOnce all previous old requests complete, old percpu_refs will be dropped\nand registered files will be put safely.\n\nLink: https://lore.kernel.org/io-uring/5a8dac33-4ca2-4847-b091-f7dcd3ad0ff3@linux.alibaba.com/T/#t\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-31 08:47:54 -0600 io_uring: refactor file register/unregister/update handling"
    },
    {
        "commit": "e59cd88028dbd41472453e5883f78330aa73c56e",
        "message": "Pull io_uring updates from Jens Axboe:\n \"Here are the io_uring changes for this merge window. Light on new\n  features this time around (just splice + buffer selection), lots of\n  cleanups, fixes, and improvements to existing support. In particular,\n  this contains:\n\n   - Cleanup fixed file update handling for stack fallback (Hillf)\n\n   - Re-work of how pollable async IO is handled, we no longer require\n     thread offload to handle that. Instead we rely using poll to drive\n     this, with task_work execution.\n\n   - In conjunction with the above, allow expendable buffer selection,\n     so that poll+recv (for example) no longer has to be a split\n     operation.\n\n   - Make sure we honor RLIMIT_FSIZE for buffered writes\n\n   - Add support for splice (Pavel)\n\n   - Linked work inheritance fixes and optimizations (Pavel)\n\n   - Async work fixes and cleanups (Pavel)\n\n   - Improve io-wq locking (Pavel)\n\n   - Hashed link write improvements (Pavel)\n\n   - SETUP_IOPOLL|SETUP_SQPOLL improvements (Xiaoguang)\"\n\n* tag 'for-5.7/io_uring-2020-03-29' of git://git.kernel.dk/linux-block: (54 commits)\n  io_uring: cleanup io_alloc_async_ctx()\n  io_uring: fix missing 'return' in comment\n  io-wq: handle hashed writes in chains\n  io-uring: drop 'free_pfile' in struct io_file_put\n  io-uring: drop completion when removing file\n  io_uring: Fix ->data corruption on re-enqueue\n  io-wq: close cancel gap for hashed linked work\n  io_uring: make spdxcheck.py happy\n  io_uring: honor original task RLIMIT_FSIZE\n  io-wq: hash dependent work\n  io-wq: split hashing and enqueueing\n  io-wq: don't resched if there is no work\n  io-wq: remove duplicated cancel code\n  io_uring: fix truncated async read/readv and write/writev retry\n  io_uring: dual license io_uring.h uapi header\n  io_uring: io_uring_enter(2) don't poll while SETUP_IOPOLL|SETUP_SQPOLL enabled\n  io_uring: Fix unused function warnings\n  io_uring: add end-of-bits marker and build time verify it\n  io_uring: provide means of removing buffers\n  io_uring: add IOSQE_BUFFER_SELECT support for IORING_OP_RECVMSG\n  ...",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-30 12:18:49 -0700 Merge tag 'for-5.7/io_uring-2020-03-29' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "3d9932a8b240c9019f48358e8a6928c53c2c7f6b",
        "message": "Cleanup io_alloc_async_ctx() a bit, add a new __io_alloc_async_ctx(),\nso io_setup_async_rw() won't need to check whether async_ctx is true\nor false again.\n\nReviewed-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-27 08:54:06 -0600 io_uring: cleanup io_alloc_async_ctx()"
    },
    {
        "commit": "40510a639ec08db81d5ff9c79856baf9dda94748",
        "message": "Currently, queue io_cpu assignment is done sequentially for default,\nread and poll queues based on queue id. This causes miss-alignment between\ncontext of CPU initiating I/O and the I/O worker thread processing\nqueued requests or completions.\n\nChange to modify queue io_cpu assignment to take into account queue\nmaps offset. Each queue io_cpu will start at zero for each queue map.\nThis essentially aligns read/poll queues to start over the same range as\ndefault queues.\n\nTesting performed by Mark with:\n- ram device (nvmet)\n- single CPU core (pinned)\n- 100% 4k reads\n- engine io_uring (not using sq_thread option)\n- hipri flag set\n\nMicro-benchmark results show a net gain of:\n- increase of 18%-29% in IOPs\n- reduction of 16%-22% in average latency\n- reduction of 7%-23% in 99.99% latency\n\nBaseline:\n========\nQDepth/Batch\t| IOPs [k]\t| Avg. Lat [us]\t| 99.99% Lat [us]\n-----------------------------------------------------------------\n1/1 \t\t| 32.4\t\t| 30.11\t\t| 50.94\n32/8\t\t| 179\t\t| 168.20\t| 371\n\nCPU alignment:\n=============\nQDepth/Batch\t| IOPs [k]\t| Avg. Lat [us]\t| 99.99% Lat [us]\n-----------------------------------------------------------------\n1/1 \t\t| 38.5\t\t|   25.18\t| 39.16\n32/8\t\t| 231\t\t|   130.75\t| 343\n\nReported-by: Mark Wunderlich <mark.wunderlich@intel.com>\nSigned-off-by: Sagi Grimberg <sagi@grimberg.me>\nSigned-off-by: Keith Busch <kbusch@kernel.org>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-26 04:48:06 +0900 nvme-tcp: optimize queue io_cpu assignment for multiple queue maps"
    },
    {
        "commit": "bff6035d0c40fa1dd195aa41f61814d622883420",
        "message": "The missing 'return' work may make it hard for other developers to\nunderstand it.\n\nSigned-off-by: Chucheng Luo <luochucheng@vivo.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-24 21:46:36 -0600 io_uring: fix missing 'return' in comment"
    },
    {
        "commit": "4afdb733b1606c6cb86e7833f9335f4870cf7ddd",
        "message": "A case of task hung was reported by syzbot,\n\nINFO: task syz-executor975:9880 blocked for more than 143 seconds.\n      Not tainted 5.6.0-rc6-syzkaller #0\n\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nsyz-executor975 D27576  9880   9878 0x80004000\nCall Trace:\n schedule+0xd0/0x2a0 kernel/sched/core.c:4154\n schedule_timeout+0x6db/0xba0 kernel/time/timer.c:1871\n do_wait_for_common kernel/sched/completion.c:83 [inline]\n __wait_for_common kernel/sched/completion.c:104 [inline]\n wait_for_common kernel/sched/completion.c:115 [inline]\n wait_for_completion+0x26a/0x3c0 kernel/sched/completion.c:136\n io_queue_file_removal+0x1af/0x1e0 fs/io_uring.c:5826\n __io_sqe_files_update.isra.0+0x3a1/0xb00 fs/io_uring.c:5867\n io_sqe_files_update fs/io_uring.c:5918 [inline]\n __io_uring_register+0x377/0x2c00 fs/io_uring.c:7131\n __do_sys_io_uring_register fs/io_uring.c:7202 [inline]\n __se_sys_io_uring_register fs/io_uring.c:7184 [inline]\n __x64_sys_io_uring_register+0x192/0x560 fs/io_uring.c:7184\n do_syscall_64+0xf6/0x7d0 arch/x86/entry/common.c:294\n entry_SYSCALL_64_after_hwframe+0x49/0xbe\n\nand bisect pointed to 05f3fb3c5397 (\"io_uring: avoid ring quiesce for\nfixed file set unregister and update\").\n\nIt is down to the order that we wait for work done before flushing it\nwhile nobody is likely going to wake us up.\n\nWe can drop that completion on stack as flushing work itself is a sync\noperation we need and no more is left behind it.\n\nTo that end, io_file_put::done is re-used for indicating if it can be\nfreed in the workqueue worker context.\n\nReported-and-Inspired-by: syzbot <syzbot+538d1957ce178382a394@syzkaller.appspotmail.com>\nSigned-off-by: Hillf Danton <hdanton@sina.com>\n\nRename ->done to ->free_pfile\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-23 09:21:06 -0600 io-uring: drop completion when removing file"
    },
    {
        "commit": "18a542ff19ad149fac9e5a36a4012e3cac7b3b3b",
        "message": "work->data and work->list are shared in union. io_wq_assign_next() sets\n->data if a req having a linked_timeout, but then io-wq may want to use\nwork->list, e.g. to do re-enqueue of a request, so corrupting ->data.\n\n->data is not necessary, just remove it and extract linked_timeout\nthrough @link_list.\n\nFixes: 60cf46ae6054 (\"io-wq: hash dependent work\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-22 19:31:27 -0600 io_uring: Fix ->data corruption on re-enqueue"
    },
    {
        "commit": "9f5834c868e901b00f1bfe4d0052b5906b4a2b7f",
        "message": "Commit bbbdeb4720a0 (\"io_uring: dual license io_uring.h uapi header\")\nuses a nested SPDX-License-Identifier to dual license the header.\n\nSince then, ./scripts/spdxcheck.py complains:\n\n  include/uapi/linux/io_uring.h: 1:60 Missing parentheses: OR\n\nAdd parentheses to make spdxcheck.py happy.\n\nSigned-off-by: Lukas Bulwahn <lukas.bulwahn@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-21 14:03:46 -0600 io_uring: make spdxcheck.py happy"
    },
    {
        "commit": "1ab7ea1f83d16489142bcfa1b7670ac7ca86cd81",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Two different fixes in here:\n\n   - Fix for a potential NULL pointer deref for links with async or\n     drain marked (Pavel)\n\n   - Fix for not properly checking RLIMIT_NOFILE for async punted\n     operations.\n\n     This affects openat/openat2, which were added this cycle, and\n     accept4. I did a full audit of other cases where we might check\n     current->signal->rlim[] and found only RLIMIT_FSIZE for buffered\n     writes and fallocate. That one is fixed and queued for 5.7 and\n     marked stable\"\n\n* tag 'io_uring-5.6-20200320' of git://git.kernel.dk/linux-block:\n  io_uring: make sure accept honor rlimit nofile\n  io_uring: make sure openat/openat2 honor rlimit nofile\n  io_uring: NULL-deref for IOSQE_{ASYNC,DRAIN}",
        "kernel_version": "v5.6-rc7",
        "release_date": "2020-03-21 11:54:47 -0700 Merge tag 'io_uring-5.6-20200320' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "4ed734b0d0913e566a9d871e15d24eb240f269f7",
        "message": "With the previous fixes for number of files open checking, I added some\ndebug code to see if we had other spots where we're checking rlimit()\nagainst the async io-wq workers. The only one I found was file size\nchecking, which we should also honor.\n\nDuring write and fallocate prep, store the max file size and override\nthat for the current ask if we're in io-wq worker context.\n\nCc: stable@vger.kernel.org # 5.1+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-20 11:41:23 -0600 io_uring: honor original task RLIMIT_FSIZE"
    },
    {
        "commit": "09952e3e7826119ddd4357c453d54bcc7ef25156",
        "message": "Just like commit 4022e7af86be, this fixes the fact that\nIORING_OP_ACCEPT ends up using get_unused_fd_flags(), which checks\ncurrent->signal->rlim[] for limits.\n\nAdd an extra argument to __sys_accept4_file() that allows us to pass\nin the proper nofile limit, and grab it at request prep time.\n\nAcked-by: David S. Miller <davem@davemloft.net>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc7",
        "release_date": "2020-03-20 08:48:36 -0600 io_uring: make sure accept honor rlimit nofile"
    },
    {
        "commit": "4022e7af86be2dd62975dedb6b7ea551d108695e",
        "message": "Dmitry reports that a test case shows that io_uring isn't honoring a\nmodified rlimit nofile setting. get_unused_fd_flags() checks the task\nsignal->rlimi[] for the limits. As this isn't easily inheritable,\nprovide a __get_unused_fd_flags() that takes the value instead. Then we\ncan grab it when the request is prepared (from the original task), and\npass that in when we do the async part part of the open.\n\nReported-by: Dmitry Kadashev <dkadashev@gmail.com>\nTested-by: Dmitry Kadashev <dkadashev@gmail.com>\nAcked-by: David S. Miller <davem@davemloft.net>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc7",
        "release_date": "2020-03-20 08:47:27 -0600 io_uring: make sure openat/openat2 honor rlimit nofile"
    },
    {
        "commit": "f1d96a8fcbbbb22d4fbc1d69eaaa678bbb0ff6e2",
        "message": "Processing links, io_submit_sqe() prepares requests, drops sqes, and\npasses them with sqe=NULL to io_queue_sqe(). There IOSQE_DRAIN and/or\nIOSQE_ASYNC requests will go through the same prep, which doesn't expect\nsqe=NULL and fail with NULL pointer deference.\n\nAlways do full prepare including io_alloc_async_ctx() for linked\nrequests, and then it can skip the second preparation.\n\nCc: stable@vger.kernel.org # 5.5\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc7",
        "release_date": "2020-03-14 16:57:41 -0600 io_uring: NULL-deref for IOSQE_{ASYNC,DRAIN}"
    },
    {
        "commit": "5007928eaeb7681501e94ac7516f6c6200f993fa",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Just a single fix here, improving the RCU callback ordering from last\n  week. After a bit more perusing by Paul, he poked a hole in the\n  original\"\n\n* tag 'io_uring-5.6-2020-03-13' of git://git.kernel.dk/linux-block:\n  io_uring: ensure RCU callback ordering with rcu_barrier()",
        "kernel_version": "v5.6-rc6",
        "release_date": "2020-03-13 13:00:08 -0700 Merge tag 'io_uring-5.6-2020-03-13' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "3f9d64415fdaa73017fcb168930006648617b488",
        "message": "Ensure we keep the truncated value, if we did truncate it. If not, we\nmight read/write more than the registered buffer size.\n\nAlso for retry, ensure that we return the truncated mapped value for\nthe vectorized versions of the read/write commands.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-11 12:29:15 -0600 io_uring: fix truncated async read/readv and write/writev retry"
    },
    {
        "commit": "bbbdeb4720a0759ec90e3bcb20ad28d19e531346",
        "message": "This just syncs the header it with the liburing version, so there's no\nconfusion on the license of the header parts.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-11 07:45:46 -0600 io_uring: dual license io_uring.h uapi header"
    },
    {
        "commit": "32b2244a840a90ea94ba42392de5c48d53f521f5",
        "message": "When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, applications don't need\nto do io completion events polling again, they can rely on io_sq_thread to do\npolling work, which can reduce cpu usage and uring_lock contention.\n\nI modify fio io_uring engine codes a bit to evaluate the performance:\nstatic int fio_ioring_getevents(struct thread_data *td, unsigned int min,\n                        continue;\n                }\n\n-               if (!o->sqpoll_thread) {\n+               if (o->sqpoll_thread && o->hipri) {\n                        r = io_uring_enter(ld, 0, actual_min,\n                                                IORING_ENTER_GETEVENTS);\n                        if (r < 0) {\n\nand use \"fio  -name=fiotest -filename=/dev/nvme0n1 -iodepth=$depth -thread\n-rw=read -ioengine=io_uring  -hipri=1 -sqthread_poll=1  -direct=1 -bs=4k\n-size=10G -numjobs=1  -time_based -runtime=120\"\n\noriginal codes\n--------------------------------------------------------------------\niodepth       |        4 |        8 |       16 |       32 |       64\nbw            | 1133MB/s | 1519MB/s | 2090MB/s | 2710MB/s | 3012MB/s\nfio cpu usage |     100% |     100% |     100% |     100% |     100%\n--------------------------------------------------------------------\n\nwith patch\n--------------------------------------------------------------------\niodepth       |        4 |        8 |       16 |       32 |       64\nbw            | 1196MB/s | 1721MB/s | 2351MB/s | 2977MB/s | 3357MB/s\nfio cpu usage |    63.8% |   74.4%% |    81.1% |    83.7% |    82.4%\n--------------------------------------------------------------------\nbw improve    |     5.5% |    13.2% |    12.3% |     9.8% |    11.5%\n--------------------------------------------------------------------\n\nFrom above test results, we can see that bw has above 5.5%~13%\nimprovement, and fio process's cpu usage also drops much. Note this\nwon't improve io_sq_thread's cpu usage when SETUP_IOPOLL|SETUP_SQPOLL\nare both enabled, in this case, io_sq_thread always has 100% cpu usage.\nI think this patch will be friendly to applications which will often use\nio_uring_wait_cqe() or similar from liburing.\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-11 07:14:12 -0600 io_uring: io_uring_enter(2) don't poll while SETUP_IOPOLL|SETUP_SQPOLL enabled"
    },
    {
        "commit": "469956e853ccdba72bb82ad2eea6e8ab6b15791f",
        "message": "If CONFIG_NET is not set, gcc warns:\n\nfs/io_uring.c:3110:12: warning: io_setup_async_msg defined but not used [-Wunused-function]\n static int io_setup_async_msg(struct io_kiocb *req,\n            ^~~~~~~~~~~~~~~~~~\n\nThere are many funcions wraped by CONFIG_NET, move them\ntogether to simplify code, also fix this warning.\n\nReported-by: Hulk Robot <hulkci@huawei.com>\nSigned-off-by: YueHaibing <yuehaibing@huawei.com>\n\nMinor tweaks.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-10 09:12:56 -0600 io_uring: Fix unused function warnings"
    },
    {
        "commit": "84557871f2ff332edd445d70349c8724c313c683",
        "message": "Not easy to tell if we're going over the size of bits we can shove\nin req->flags, so add an end-of-bits marker and a BUILD_BUG_ON()\ncheck for it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-10 09:12:56 -0600 io_uring: add end-of-bits marker and build time verify it"
    },
    {
        "commit": "067524e914cb23e20d59480b318fe2625eaee7c8",
        "message": "We have IORING_OP_PROVIDE_BUFFERS, but the only way to remove buffers\nis to trigger IO on them. The usual case of shrinking a buffer pool\nwould be to just not replenish the buffers when IO completes, and\ninstead just free it. But it may be nice to have a way to manually\nremove a number of buffers from a given group, and\nIORING_OP_REMOVE_BUFFERS provides that functionality.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-10 09:12:56 -0600 io_uring: provide means of removing buffers"
    },
    {
        "commit": "52de1fe122408d7a62b6cff9ed3895ebb882d71f",
        "message": "Like IORING_OP_READV, this is limited to supporting just a single\nsegment in the iovec passed in.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-10 09:12:51 -0600 io_uring: add IOSQE_BUFFER_SELECT support for IORING_OP_RECVMSG"
    },
    {
        "commit": "4d954c258a0c365a85a2d1b1cccf63aec38fca4c",
        "message": "This adds support for the vectored read. This is limited to supporting\njust 1 segment in the iov, and is provided just for convenience for\napplications that use IORING_OP_READV already.\n\nThe iov helpers will be used for IORING_OP_RECVMSG as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-10 09:12:48 -0600 io_uring: add IOSQE_BUFFER_SELECT support for IORING_OP_READV"
    },
    {
        "commit": "bcda7baaa3f15c7a95db3c024bb046d6e298f76b",
        "message": "If a server process has tons of pending socket connections, generally\nit uses epoll to wait for activity. When the socket is ready for reading\n(or writing), the task can select a buffer and issue a recv/send on the\ngiven fd.\n\nNow that we have fast (non-async thread) support, a task can have tons\nof pending reads or writes pending. But that means they need buffers to\nback that data, and if the number of connections is high enough, having\nthem preallocated for all possible connections is unfeasible.\n\nWith IORING_OP_PROVIDE_BUFFERS, an application can register buffers to\nuse for any request. The request then sets IOSQE_BUFFER_SELECT in the\nsqe, and a given group ID in sqe->buf_group. When the fd becomes ready,\na free buffer from the specified group is selected. If none are\navailable, the request is terminated with -ENOBUFS. If successful, the\nCQE on completion will contain the buffer ID chosen in the cqe->flags\nmember, encoded as:\n\n\t(buffer_id << IORING_CQE_BUFFER_SHIFT) | IORING_CQE_F_BUFFER;\n\nOnce a buffer has been consumed by a request, it is no longer available\nand must be registered again with IORING_OP_PROVIDE_BUFFERS.\n\nRequests need to support this feature. For now, IORING_OP_READ and\nIORING_OP_RECV support it. This is checked on SQE submission, a CQE with\nres == -EOPNOTSUPP will be posted if attempted on unsupported requests.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-10 09:12:45 -0600 io_uring: support buffer selection for OP_READ and OP_RECV"
    },
    {
        "commit": "ddf0322db79c5984dc1a1db890f946dd19b7d6d9",
        "message": "IORING_OP_PROVIDE_BUFFERS uses the buffer registration infrastructure to\nsupport passing in an addr/len that is associated with a buffer ID and\nbuffer group ID. The group ID is used to index and lookup the buffers,\nwhile the buffer ID can be used to notify the application which buffer\nin the group was used. The addr passed in is the starting buffer address,\nand length is each buffer length. A number of buffers to add with can be\nspecified, in which case addr is incremented by length for each addition,\nand each buffer increments the buffer ID specified.\n\nNo validation is done of the buffer ID. If the application provides\nbuffers within the same group with identical buffer IDs, then it'll have\na hard time telling which buffer ID was used. The only restriction is\nthat the buffer ID can be a max of 16-bits in size, so USHRT_MAX is the\nmaximum ID that can be used.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-10 09:12:14 -0600 io_uring: add IORING_OP_PROVIDE_BUFFERS"
    },
    {
        "commit": "805b13adde3964c78cba125a15527e88c19f87b3",
        "message": "After more careful studying, Paul informs me that we cannot rely on\nordering of RCU callbacks in the way that the the tagged commit did.\nThe current construct looks like this:\n\n\tvoid C(struct rcu_head *rhp)\n\t{\n\t\tdo_something(rhp);\n\t\tcall_rcu(&p->rh, B);\n\t}\n\n\tcall_rcu(&p->rh, A);\n\tcall_rcu(&p->rh, C);\n\nand we're relying on ordering between A and B, which isn't guaranteed.\nMake this explicit instead, and have a work item issue the rcu_barrier()\nto ensure that A has run before we manually execute B.\n\nWhile thorough testing never showed this issue, it's dependent on the\nper-cpu load in terms of RCU callbacks. The updated method simplifies\nthe code as well, and eliminates the need to maintain an rcu_head in\nthe fileset data.\n\nFixes: c1e2148f8ecb (\"io_uring: free fixed_file_data after RCU grace period\")\nReported-by: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc6",
        "release_date": "2020-03-08 20:07:28 -0600 io_uring: ensure RCU callback ordering with rcu_barrier()"
    },
    {
        "commit": "c20037652700024cffeb6b0f74306ce9b391248f",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Here are a few io_uring fixes that should go into this release. This\n  contains:\n\n   - Removal of (now) unused io_wq_flush() and associated flag (Pavel)\n\n   - Fix cancelation lockup with linked timeouts (Pavel)\n\n   - Fix for potential use-after-free when freeing percpu ref for fixed\n     file sets\n\n   - io-wq cancelation fixups (Pavel)\"\n\n* tag 'io_uring-5.6-2020-03-07' of git://git.kernel.dk/linux-block:\n  io_uring: fix lockup with timeouts\n  io_uring: free fixed_file_data after RCU grace period\n  io-wq: remove io_wq_flush and IO_WQ_WORK_INTERNAL\n  io-wq: fix IO_WQ_WORK_NO_CANCEL cancellation",
        "kernel_version": "v5.6-rc5",
        "release_date": "2020-03-07 14:20:29 -0600 Merge tag 'io_uring-5.6-2020-03-07' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "f0e20b8943509d81200cef5e30af2adfddba0f5c",
        "message": "There is a recipe to deadlock the kernel: submit a timeout sqe with a\nlinked_timeout (e.g.  test_single_link_timeout_ception() from liburing),\nand SIGKILL the process.\n\nThen, io_kill_timeouts() takes @ctx->completion_lock, but the timeout\nisn't flagged with REQ_F_COMP_LOCKED, and will try to double grab it\nduring io_put_free() to cancel the linked timeout. Probably, the same\ncan happen with another io_kill_timeout() call site, that is\nio_commit_cqring().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc5",
        "release_date": "2020-03-07 08:35:56 -0700 io_uring: fix lockup with timeouts"
    },
    {
        "commit": "c1e2148f8ecb26863b899d402a823dab8e26efd1",
        "message": "The percpu refcount protects this structure, and we can have an atomic\nswitch in progress when exiting. This makes it unsafe to just free the\nstruct normally, and can trigger the following KASAN warning:\n\nBUG: KASAN: use-after-free in percpu_ref_switch_to_atomic_rcu+0xfa/0x1b0\nRead of size 1 at addr ffff888181a19a30 by task swapper/0/0\n\nCPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.6.0-rc4+ #5747\nHardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.10.2-1ubuntu1 04/01/2014\nCall Trace:\n <IRQ>\n dump_stack+0x76/0xa0\n print_address_description.constprop.0+0x3b/0x60\n ? percpu_ref_switch_to_atomic_rcu+0xfa/0x1b0\n ? percpu_ref_switch_to_atomic_rcu+0xfa/0x1b0\n __kasan_report.cold+0x1a/0x3d\n ? percpu_ref_switch_to_atomic_rcu+0xfa/0x1b0\n percpu_ref_switch_to_atomic_rcu+0xfa/0x1b0\n rcu_core+0x370/0x830\n ? percpu_ref_exit+0x50/0x50\n ? rcu_note_context_switch+0x7b0/0x7b0\n ? run_rebalance_domains+0x11d/0x140\n __do_softirq+0x10a/0x3e9\n irq_exit+0xd5/0xe0\n smp_apic_timer_interrupt+0x86/0x200\n apic_timer_interrupt+0xf/0x20\n </IRQ>\nRIP: 0010:default_idle+0x26/0x1f0\n\nFix this by punting the final exit and free of the struct to RCU, then\nwe know that it's safe to do so. Jann suggested the approach of using a\ndouble rcu callback to achieve this. It's important that we do a nested\ncall_rcu() callback, as otherwise the free could be ordered before the\natomic switch, even if the latter was already queued.\n\nReported-by: syzbot+e017e49c39ab484ac87a@syzkaller.appspotmail.com\nSuggested-by: Jann Horn <jannh@google.com>\nReviewed-by: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc5",
        "release_date": "2020-03-06 10:15:21 -0700 io_uring: free fixed_file_data after RCU grace period"
    },
    {
        "commit": "5a2e745d4d430c4dbeeeb448c3d5c0c3109e511e",
        "message": "This just prepares the ring for having lists of buffers associated with\nit, that the application can provide for SQEs to consume instead of\nproviding their own.\n\nThe buffers are organized by group ID.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-04 11:49:14 -0700 io_uring: buffer registration infrastructure"
    },
    {
        "commit": "e9fd939654f17651ff65e7e55aa6934d29eb4335",
        "message": "First it changes io-wq interfaces. It replaces {get,put}_work() with\nfree_work(), which guaranteed to be called exactly once. It also enforces\nfree_work() callback to be non-NULL.\n\nio_uring follows the changes and instead of putting a submission reference\nin io_put_req_async_completion(), it will be done in io_free_work(). As\nremoves io_get_work() with corresponding refcount_inc(), the ref balance\nis maintained.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-04 11:39:07 -0700 io_uring/io-wq: forward submission ref to async"
    },
    {
        "commit": "7a743e225b2a9da772b28a50031e1ccd8a8ce404",
        "message": "If after dropping the submission reference req->refs == 1, the request\nis done, because this one is for io_put_work() and will be dropped\nsynchronously shortly after. In this case it's safe to steal a next\nwork from the request.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-03 20:02:49 -0700 io_uring: get next work with submission ref drop"
    },
    {
        "commit": "014db0073cc6a12e1f421b9231d6f3aa35735823",
        "message": "There will be no use for @nxt in the handlers, and it's doesn't work\nanyway, so purge it\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-03 20:02:49 -0700 io_uring: remove @nxt from handlers"
    },
    {
        "commit": "594506fec5faec2b1ec82ad6fb0c8132512fc459",
        "message": "The rule is simple, any async handler gets a submission ref and should\nput it at the end. Make them all follow it, and so more consistent.\n\nThis is a preparation patch, and as io_wq_assign_next() currently won't\never work, this doesn't care to use io_put_req_find_next() instead of\nio_put_req().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\n\nrefcount_inc_not_zero() -> refcount_inc() fix.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-03 20:02:35 -0700 io_uring: make submission ref putting consistent"
    },
    {
        "commit": "820d15632ec10bc0cf79595c5a635b795d149520",
        "message": "arm64: dts: agilex: fix gmac compatible\n- The compatible for Agilex GMAC should be \"altr,socfpga-stmmac-a10-s10\"\n\n* tag 'socfpga_dts_fix_for_v5.6_v2' of git://git.kernel.org/pub/scm/linux/kernel/git/dinguyen/linux: (578 commits)\n  arm64: dts: socfpga: agilex: Fix gmac compatible\n  Linux 5.6-rc4\n  KVM: VMX: check descriptor table exits on instruction emulation\n  ext4: potential crash on allocation error in ext4_alloc_flex_bg_array()\n  macintosh: therm_windtunnel: fix regression when instantiating devices\n  jbd2: fix data races at struct journal_head\n  kvm: x86: Limit the number of \"kvm: disabled by bios\" messages\n  KVM: x86: avoid useless copy of cpufreq policy\n  KVM: allow disabling -Werror\n  KVM: x86: allow compiling as non-module with W=1\n  KVM: Pre-allocate 1 cpumask variable per cpu for both pv tlb and pv ipis\n  KVM: Introduce pv check helpers\n  KVM: let declaration of kvm_get_running_vcpus match implementation\n  KVM: SVM: allocate AVIC data structures based on kvm_amd module parameter\n  MAINTAINERS: Correct Cadence PCI driver path\n  io_uring: fix 32-bit compatability with sendmsg/recvmsg\n  net: dsa: mv88e6xxx: Fix masking of egress port\n  mlxsw: pci: Wait longer before accessing the device after reset\n  sfc: fix timestamp reconstruction at 16-bit rollover points\n  vsock: fix potential deadlock in transport->release()\n  ...\n\nLink: https://lore.kernel.org/r/20200303153509.28248-1-dinguyen@kernel.org\nSigned-off-by: Olof Johansson <olof@lixom.net>",
        "kernel_version": "v5.6-rc5",
        "release_date": "2020-03-03 16:40:56 -0800 Merge tag 'socfpga_dts_fix_for_v5.6_v2' of git://git.kernel.org/pub/scm/linux/kernel/git/dinguyen/linux into arm/fixes"
    },
    {
        "commit": "a2100672f3b2afdd55ccc2e640d1a8bd99ff6338",
        "message": "Don't abuse labels for plain and straightworward code.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 21:26:56 -0700 io_uring: clean up io_close"
    },
    {
        "commit": "8755d97a09fed0de206772bcad1838301293c4d8",
        "message": "Clang warns:\n\nfs/io_uring.c:4178:6: warning: variable 'mask' is used uninitialized\nwhenever 'if' condition is false [-Wsometimes-uninitialized]\n        if (def->pollin)\n            ^~~~~~~~~~~\nfs/io_uring.c:4182:2: note: uninitialized use occurs here\n        mask |= POLLERR | POLLPRI;\n        ^~~~\nfs/io_uring.c:4178:2: note: remove the 'if' if its condition is always\ntrue\n        if (def->pollin)\n        ^~~~~~~~~~~~~~~~\nfs/io_uring.c:4154:15: note: initialize the variable 'mask' to silence\nthis warning\n        __poll_t mask, ret;\n                     ^\n                      = 0\n1 warning generated.\n\nio_op_defs has many definitions where pollin is not set so mask indeed\nmight be uninitialized. Initialize it to zero and change the next\nassignment to |=, in case further masks are added in the future to avoid\nmissing changing the assignment then.\n\nFixes: d7718a9d25a6 (\"io_uring: use poll driven retry for files that support it\")\nLink: https://github.com/ClangBuiltLinux/linux/issues/916\nSigned-off-by: Nathan Chancellor <natechancellor@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 16:13:24 -0700 io_uring: Ensure mask is initialized in io_arm_poll_handler"
    },
    {
        "commit": "3b17cf5a58f2a38e23ee980b5dece717d0464fb7",
        "message": "io-wq cares about IO_WQ_WORK_UNBOUND flag only while enqueueing, so\nit's useless setting it for a next req of a link. Thus, removed it\nfrom io_prep_linked_timeout(), and inline the function.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 14:06:41 -0700 io_uring: remove io_prep_next_work()"
    },
    {
        "commit": "4bc4494ec7c97ee38e2aa3d1cd76e289c49ac083",
        "message": "After __io_queue_sqe() ended up in io_queue_async_work(), it's already\nknown that there is no @nxt req, so skip the check and return from the\nfunction.\n\nAlso, @nxt initialisation now can be done just before\nio_put_req_find_next(), as there is no jumping until it's checked.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 14:06:40 -0700 io_uring: remove extra nxt check after punt"
    },
    {
        "commit": "d7718a9d25a61442da8ee8aeeff6a0097f0ccfd6",
        "message": "Currently io_uring tries any request in a non-blocking manner, if it can,\nand then retries from a worker thread if we get -EAGAIN. Now that we have\na new and fancy poll based retry backend, use that to retry requests if\nthe file supports it.\n\nThis means that, for example, an IORING_OP_RECVMSG on a socket no longer\nrequires an async thread to complete the IO. If we get -EAGAIN reading\nfrom the socket in a non-blocking manner, we arm a poll handler for\nnotification on when the socket becomes readable. When it does, the\npending read is executed directly by the task again, through the io_uring\ntask work handlers. Not only is this faster and more efficient, it also\nmeans we're not generating potentially tons of async threads that just\nsit and block, waiting for the IO to complete.\n\nThe feature is marked with IORING_FEAT_FAST_POLL, meaning that async\npollable IO is fast, and that poll<link>other_op is fast as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 14:06:38 -0700 io_uring: use poll driven retry for files that support it"
    },
    {
        "commit": "8a72758c51f8a5501a0e01ea95069630edb9ca07",
        "message": "Add a pollin/pollout field to the request table, and have commands that\nwe can safely poll for properly marked.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 14:06:37 -0700 io_uring: mark requests that we can do poll async in io_op_defs"
    },
    {
        "commit": "b41e98524e424d104aa7851d54fd65820759875a",
        "message": "For poll requests, it's not uncommon to link a read (or write) after\nthe poll to execute immediately after the file is marked as ready.\nSince the poll completion is called inside the waitqueue wake up handler,\nwe have to punt that linked request to async context. This slows down\nthe processing, and actually means it's faster to not use a link for this\nuse case.\n\nWe also run into problems if the completion_lock is contended, as we're\ndoing a different lock ordering than the issue side is. Hence we have\nto do trylock for completion, and if that fails, go async. Poll removal\nneeds to go async as well, for the same reason.\n\neventfd notification needs special case as well, to avoid stack blowing\nrecursion or deadlocks.\n\nThese are all deficiencies that were inherited from the aio poll\nimplementation, but I think we can do better. When a poll completes,\nsimply queue it up in the task poll list. When the task completes the\nlist, we can run dependent links inline as well. This means we never\nhave to go async, and we can remove a bunch of code associated with\nthat, and optimizations to try and make that run faster. The diffstat\nspeaks for itself.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 14:06:36 -0700 io_uring: add per-task callback handler"
    },
    {
        "commit": "c2f2eb7d2c1cdc37fa9633bae96f381d33ee7a14",
        "message": "Store the io_kiocb in the private field instead of the poll entry, this\nis in preparation for allowing multiple waitqueues.\n\nNo functional changes in this patch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 14:06:34 -0700 io_uring: store io_kiocb in wait->private"
    },
    {
        "commit": "5eae8619907a1389dbd1b4a1049caf52782c0916",
        "message": "IO_WQ_WORK_CB is used only for linked timeouts, which will be armed\nbefore the work setup (i.e. mm, override creds, etc). The setup\nshouldn't take long, so it's ok to arm it a bit later and get rid\nof IO_WQ_WORK_CB.\n\nMake io-wq call work->func() only once, callbacks will handle the rest.\ni.e. the linked timeout handler will do the actual issue. And as a\nbonus, it removes an extra indirect call.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 14:06:29 -0700 io_uring: remove IO_WQ_WORK_CB"
    },
    {
        "commit": "02d27d895323c4baa3234e4bed015eb3a196e1dd",
        "message": "io_recvmsg() and io_sendmsg() duplicate nonblock -EAGAIN finilising\npart, so add helper for that.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 14:04:41 -0700 io_uring: extract kmsg copy helper"
    },
    {
        "commit": "b0a20349f212dc725f5ddfd060e426fe6181d9c5",
        "message": "Deduplicate call to io_cqring_fill_event(), plain and easy\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 14:04:37 -0700 io_uring: clean io_poll_complete"
    },
    {
        "commit": "7d67af2c013402537385dae343a2d0f6a4cb3bfd",
        "message": "Add support for splice(2).\n\n- output file is specified as sqe->fd, so it's handled by generic code\n- hash_reg_file handled by generic code as well\n- len is 32bit, but should be fine\n- the fd_in is registered file, when SPLICE_F_FD_IN_FIXED is set, which\nis a splice flag (i.e. sqe->splice_flags).\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 14:04:37 -0700 io_uring: add splice(2) support"
    },
    {
        "commit": "8da11c19940ddbc22fc835bce3f361f4d2417fb0",
        "message": "Preparation without functional changes. Adds io_get_file(), that allows\nto grab files not only into req->file.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 14:04:37 -0700 io_uring: add interface for getting files"
    },
    {
        "commit": "bcaec089c5b64953f96a59089598643911765a43",
        "message": "req->in_async is not really needed, it only prevents propagation of\n@nxt for fast not-blocked submissions. Remove it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 14:04:29 -0700 io_uring: remove req->in_async"
    },
    {
        "commit": "deb6dc0544884067b93bbf9a4716be323103b911",
        "message": "io_prep_async_worker() called io_wq_assign_next() do many useless checks:\nio_req_work_grab_env() was already called during prep, and @do_hashed\nis not ever used. Add io_prep_next_work() -- simplified version, that\ncan be called io-wq.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 14:04:28 -0700 io_uring: don't do full *prep_worker() from io-wq"
    },
    {
        "commit": "5ea62161167eb8297249d3f4dc63741016f01413",
        "message": "Many operations define custom work.func before getting into an io-wq.\nThere are several points against:\n- it calls io_wq_assign_next() from outside io-wq, that may be confusing\n- sync context would go unnecessary through io_req_cancelled()\n- prototypes are quite different, so work!=old_work looks strange\n- makes async/sync responsibilities fuzzy\n- adds extra overhead\n\nDon't call generic path and io-wq handlers from each other, but use\nhelpers instead\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 14:04:24 -0700 io_uring: don't call work.func from sync ctx"
    },
    {
        "commit": "e441d1cf20e1b9fc443e6130488d41e1941aae82",
        "message": "Don't drop an early reference, hang on to it and let the caller drop\nit. This makes it behave more like \"regular\" requests.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 14:04:24 -0700 io_uring: io_accept() should hold on to submit reference on retry"
    },
    {
        "commit": "29de5f6a350778a621a748cecc7efbb8f0cfa5a7",
        "message": "If the -EAGAIN happens because of a static condition, then a poll\nor later retry won't fix it. We must call it again from blocking\ncondition. Play it safe and ensure that any -EAGAIN condition from read\nor write must retry from async context.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.7-rc1",
        "release_date": "2020-03-02 14:04:24 -0700 io_uring: consider any io_read/write -EAGAIN as final"
    },
    {
        "commit": "80ad894382bf1d73eb688c29714fa10c0afcf2e7",
        "message": "io_wq_flush() is buggy, during cancelation of a flush, the associated\nwork may be passed to the caller's (i.e. io_uring) @match callback. That\ncallback is expecting it to be embedded in struct io_kiocb. Cancelation\nof internal work probably doesn't make a lot of sense to begin with.\n\nAs the flush helper is no longer used, just delete it and the associated\nwork flag.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc5",
        "release_date": "2020-03-02 14:03:24 -0700 io-wq: remove io_wq_flush and IO_WQ_WORK_INTERNAL"
    },
    {
        "commit": "74dea5d99d19236608914d2c556134e4cdc21c60",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Fix for a race with IOPOLL used with SQPOLL (Xiaoguang)\n\n - Only show ->fdinfo if procfs is enabled (Tobias)\n\n - Fix for a chain with multiple personalities in the SQEs\n\n - Fix for a missing free of personality idr on exit\n\n - Removal of the spin-for-work optimization\n\n - Fix for next work lookup on request completion\n\n - Fix for non-vec read/write result progation in case of links\n\n - Fix for a fileset references on switch\n\n - Fix for a recvmsg/sendmsg 32-bit compatability mode\n\n* tag 'io_uring-5.6-2020-02-28' of git://git.kernel.dk/linux-block:\n  io_uring: fix 32-bit compatability with sendmsg/recvmsg\n  io_uring: define and set show_fdinfo only if procfs is enabled\n  io_uring: drop file set ref put/get on switch\n  io_uring: import_single_range() returns 0/-ERROR\n  io_uring: pick up link work on submit reference drop\n  io-wq: ensure work->task_pid is cleared on init\n  io-wq: remove spin-for-work optimization\n  io_uring: fix poll_list race for SETUP_IOPOLL|SETUP_SQPOLL\n  io_uring: fix personality idr leak\n  io_uring: handle multiple personalities in link chains",
        "kernel_version": "v5.6-rc4",
        "release_date": "2020-02-28 11:39:14 -0800 Merge tag 'io_uring-5.6-2020-02-28' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "d876836204897b6d7d911f942084f69a1e9d5c4d",
        "message": "We must set MSG_CMSG_COMPAT if we're in compatability mode, otherwise\nthe iovec import for these commands will not do the right thing and fail\nthe command with -EINVAL.\n\nFound by running the test suite compiled as 32-bit.\n\nCc: stable@vger.kernel.org\nFixes: aa1fa28fc73e (\"io_uring: add support for recvmsg()\")\nFixes: 0fa03c624d8f (\"io_uring: add support for sendmsg()\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc4",
        "release_date": "2020-02-27 14:17:49 -0700 io_uring: fix 32-bit compatability with sendmsg/recvmsg"
    },
    {
        "commit": "bebdb65e077267957f48e43d205d4a16cc7b8161",
        "message": "Follow the pattern used with other *_show_fdinfo functions and only\ndefine and use io_uring_show_fdinfo and its helper functions if\nCONFIG_PROC_FS is set.\n\nFixes: 87ce955b24c9 (\"io_uring: add ->show_fdinfo() for the io_uring file descriptor\")\nSigned-off-by: Tobias Klauser <tklauser@distanz.ch>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc4",
        "release_date": "2020-02-27 06:56:21 -0700 io_uring: define and set show_fdinfo only if procfs is enabled"
    },
    {
        "commit": "dd3db2a34cff14e152da7c8e320297719a35abf9",
        "message": "Dan reports that he triggered a warning on ring exit doing some testing:\n\npercpu ref (io_file_data_ref_zero) <= 0 (0) after switching to atomic\nWARNING: CPU: 3 PID: 0 at lib/percpu-refcount.c:160 percpu_ref_switch_to_atomic_rcu+0xe8/0xf0\nModules linked in:\nCPU: 3 PID: 0 Comm: swapper/3 Not tainted 5.6.0-rc3+ #5648\nHardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.10.2-1ubuntu1 04/01/2014\nRIP: 0010:percpu_ref_switch_to_atomic_rcu+0xe8/0xf0\nCode: e7 ff 55 e8 eb d2 80 3d bd 02 d2 00 00 75 8b 48 8b 55 d8 48 c7 c7 e8 70 e6 81 c6 05 a9 02 d2 00 01 48 8b 75 e8 e8 3a d0 c5 ff <0f> 0b e9 69 ff ff ff 90 55 48 89 fd 53 48 89 f3 48 83 ec 28 48 83\nRSP: 0018:ffffc90000110ef8 EFLAGS: 00010292\nRAX: 0000000000000045 RBX: 7fffffffffffffff RCX: 0000000000000000\nRDX: 0000000000000045 RSI: ffffffff825be7a5 RDI: ffffffff825bc32c\nRBP: ffff8881b75eac38 R08: 000000042364b941 R09: 0000000000000045\nR10: ffffffff825beb40 R11: ffffffff825be78a R12: 0000607e46005aa0\nR13: ffff888107dcdd00 R14: 0000000000000000 R15: 0000000000000009\nFS:  0000000000000000(0000) GS:ffff8881b9d80000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00007f49e6a5ea20 CR3: 00000001b747c004 CR4: 00000000001606e0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\nCall Trace:\n <IRQ>\n rcu_core+0x1e4/0x4d0\n __do_softirq+0xdb/0x2f1\n irq_exit+0xa0/0xb0\n smp_apic_timer_interrupt+0x60/0x140\n apic_timer_interrupt+0xf/0x20\n </IRQ>\nRIP: 0010:default_idle+0x23/0x170\nCode: ff eb ab cc cc cc cc 0f 1f 44 00 00 41 54 55 53 65 8b 2d 10 96 92 7e 0f 1f 44 00 00 e9 07 00 00 00 0f 00 2d 21 d0 51 00 fb f4 <65> 8b 2d f6 95 92 7e 0f 1f 44 00 00 5b 5d 41 5c c3 65 8b 05 e5 95\n\nTurns out that this is due to percpu_ref_switch_to_atomic() only\ngrabbing a reference to the percpu refcount if it's not already in\natomic mode. io_uring drops a ref and re-gets it when switching back to\npercpu mode. We attempt to protect against this with the FFD_F_ATOMIC\nbit, but that isn't reliable.\n\nWe don't actually need to juggle these refcounts between atomic and\npercpu switch, we can just do them when we've switched to atomic mode.\nThis removes the need for FFD_F_ATOMIC, which wasn't reliable.\n\nFixes: 05f3fb3c5397 (\"io_uring: avoid ring quiesce for fixed file set unregister and update\")\nReported-by: Dan Melnic <dmm@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc4",
        "release_date": "2020-02-26 10:53:33 -0700 io_uring: drop file set ref put/get on switch"
    },
    {
        "commit": "3a9015988b3d41027cda61f4fdeaaeee73be8b24",
        "message": "Unlike the other core import helpers, import_single_range() returns 0 on\nsuccess, not the length imported. This means that links that depend on\nthe result of non-vec based IORING_OP_{READ,WRITE} that were added for\n5.5 get errored when they should not be.\n\nFixes: 3a6820f2bb8a (\"io_uring: add non-vectored read/write commands\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc4",
        "release_date": "2020-02-26 07:06:57 -0700 io_uring: import_single_range() returns 0/-ERROR"
    },
    {
        "commit": "2a44f46781617c5040372b59da33553a02b1f46d",
        "message": "If work completes inline, then we should pick up a dependent link item\nin __io_queue_sqe() as well. If we don't do so, we're forced to go async\nwith that item, which is suboptimal.\n\nThis also fixes an issue with io_put_req_find_next(), which always looks\nup the next work item. That should only be done if we're dropping the\nlast reference to the request, to prevent multiple lookups of the same\nwork item.\n\nOutside of being a fix, this also enables a good cleanup series for 5.7,\nwhere we never have to pass 'nxt' around or into the work handlers.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc4",
        "release_date": "2020-02-26 07:05:30 -0700 io_uring: pick up link work on submit reference drop"
    },
    {
        "commit": "bdcd3eab2a9ae0ac93f27275b6895dd95e5bf360",
        "message": "After making ext4 support iopoll method:\n  let ext4_file_operations's iopoll method be iomap_dio_iopoll(),\nwe found fio can easily hang in fio_ioring_getevents() with below fio\njob:\n    rm -f testfile; sync;\n    sudo fio -name=fiotest -filename=testfile -iodepth=128 -thread\n-rw=write -ioengine=io_uring  -hipri=1 -sqthread_poll=1 -direct=1\n-bs=4k -size=10G -numjobs=8 -runtime=2000 -group_reporting\nwith IORING_SETUP_SQPOLL and IORING_SETUP_IOPOLL enabled.\n\nThere are two issues that results in this hang, one reason is that\nwhen IORING_SETUP_SQPOLL and IORING_SETUP_IOPOLL are enabled, fio\ndoes not use io_uring_enter to get completed events, it relies on\nkernel io_sq_thread to poll for completed events.\n\nAnother reason is that there is a race: when io_submit_sqes() in\nio_sq_thread() submits a batch of sqes, variable 'inflight' will\nrecord the number of submitted reqs, then io_sq_thread will poll for\nreqs which have been added to poll_list. But note, if some previous\nreqs have been punted to io worker, these reqs will won't be in\npoll_list timely. io_sq_thread() will only poll for a part of previous\nsubmitted reqs, and then find poll_list is empty, reset variable\n'inflight' to be zero. If app just waits these deferred reqs and does\nnot wake up io_sq_thread again, then hang happens.\n\nFor app that entirely relies on io_sq_thread to poll completed requests,\nlet io_iopoll_req_issued() wake up io_sq_thread properly when adding new\nelement to poll_list, and when io_sq_thread prepares to sleep, check\nwhether poll_list is empty again, if not empty, continue to poll.\n\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc4",
        "release_date": "2020-02-25 08:40:43 -0700 io_uring: fix poll_list race for SETUP_IOPOLL|SETUP_SQPOLL"
    },
    {
        "commit": "41726c9a50e7464beca7112d0aebf3a0090c62d2",
        "message": "We somehow never free the idr, even though we init it for every ctx.\nFree it when the rest of the ring data is freed.\n\nFixes: 071698e13ac6 (\"io_uring: allow registering credentials\")\nReviewed-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc4",
        "release_date": "2020-02-24 08:31:51 -0700 io_uring: fix personality idr leak"
    },
    {
        "commit": "193155c8c9429f57400daf1f2ef0075016767112",
        "message": "If we have a chain of requests and they don't all use the same\ncredentials, then the head of the chain will be issued with the\ncredentails of the tail of the chain.\n\nEnsure __io_queue_sqe() overrides the credentials, if they are different.\n\nOnce we do that, we can clean up the creds handling as well, by only\nhaving io_submit_sqe() do the lookup of a personality. It doesn't need\nto assign it, since __io_queue_sqe() now always does the right thing.\n\nFixes: 75c6a03904e0 (\"io_uring: support using a registered personality for commands\")\nReported-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc4",
        "release_date": "2020-02-23 19:46:13 -0700 io_uring: handle multiple personalities in link chains"
    },
    {
        "commit": "b88025ea47ec8aea47f0c283d182ab26bae2970d",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Here's a small collection of fixes that were queued up:\n\n   - Remove unnecessary NULL check (Dan)\n\n   - Missing io_req_cancelled() call in fallocate (Pavel)\n\n   - Put the cleanup check for aux data in the right spot (Pavel)\n\n   - Two fixes for SQPOLL (Stefano, Xiaoguang)\"\n\n* tag 'io_uring-5.6-2020-02-22' of git://git.kernel.dk/linux-block:\n  io_uring: fix __io_iopoll_check deadlock in io_sq_thread\n  io_uring: prevent sq_thread from spinning when it should stop\n  io_uring: fix use-after-free by io_cleanup_req()\n  io_uring: remove unnecessary NULL checks\n  io_uring: add missing io_req_cancelled()",
        "kernel_version": "v5.6-rc3",
        "release_date": "2020-02-22 11:12:55 -0800 Merge tag 'io_uring-5.6-2020-02-22' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "c7849be9cc2dd2754c48ddbaca27c2de6d80a95d",
        "message": "Since commit a3a0e43fd770 (\"io_uring: don't enter poll loop if we have\nCQEs pending\"), if we already events pending, we won't enter poll loop.\nIn case SETUP_IOPOLL and SETUP_SQPOLL are both enabled, if app has\nbeen terminated and don't reap pending events which are already in cq\nring, and there are some reqs in poll_list, io_sq_thread will enter\n__io_iopoll_check(), and find pending events, then return, this loop\nwill never have a chance to exit.\n\nI have seen this issue in fio stress tests, to fix this issue, let\nio_sq_thread call io_iopoll_getevents() with argument 'min' being zero,\nand remove __io_iopoll_check().\n\nFixes: a3a0e43fd770 (\"io_uring: don't enter poll loop if we have CQEs pending\")\nSigned-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc3",
        "release_date": "2020-02-22 07:45:03 -0700 io_uring: fix __io_iopoll_check deadlock in io_sq_thread"
    },
    {
        "commit": "7143b5ac5750f404ff3a594b34fdf3fc2f99f828",
        "message": "This patch drops 'cur_mm' before calling cond_resched(), to prevent\nthe sq_thread from spinning even when the user process is finished.\n\nBefore this patch, if the user process ended without closing the\nio_uring fd, the sq_thread continues to spin until the\n'sq_thread_idle' timeout ends.\n\nIn the worst case where the 'sq_thread_idle' parameter is bigger than\nINT_MAX, the sq_thread will spin forever.\n\nFixes: 6c271ce2f1d5 (\"io_uring: add submission polling\")\nSigned-off-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc3",
        "release_date": "2020-02-21 09:16:10 -0700 io_uring: prevent sq_thread from spinning when it should stop"
    },
    {
        "commit": "929a3af90f0f4bd7132d83552c1a98c83f60ef7e",
        "message": "io_cleanup_req() should be called before req->io is freed, and so\nshouldn't be after __io_free_req() -> __io_req_aux_free(). Also,\nit will be ignored for in io_free_req_many(), which use\n__io_req_aux_free().\n\nPlace cleanup_req() into __io_req_aux_free().\n\nFixes: 99bc4c38537d774 (\"io_uring: fix iovec leaks\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc3",
        "release_date": "2020-02-18 17:12:23 -0700 io_uring: fix use-after-free by io_cleanup_req()"
    },
    {
        "commit": "297a31e3e8318f533cff4fe33ffaefb74f72c6e2",
        "message": "The \"kmsg\" pointer can't be NULL and we have already dereferenced it so\na check here would be useless.\n\nReviewed-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Dan Carpenter <dan.carpenter@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc3",
        "release_date": "2020-02-18 11:22:02 -0700 io_uring: remove unnecessary NULL checks"
    },
    {
        "commit": "7fbeb95d0f68e21e6ca61284f1ac681630976947",
        "message": "fallocate_finish() is missing cancellation check. Add it.\nIt's safe to do that, as only flags setup and sqe fields copy are done\nbefore it gets into __io_fallocate().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc3",
        "release_date": "2020-02-16 10:09:37 -0700 io_uring: add missing io_req_cancelled()"
    },
    {
        "commit": "ca60ad6a6bc4aa88c02c6f103dd80df54689ea4d",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Here's a set of fixes for io_uring:\n\n   - Various fixes with cleanups from Pavel, fixing corner cases where\n     we're not correctly dealing with iovec cleanup.\n\n   - Clarify that statx/openat/openat2 don't accept fixed files\n\n   - Buffered raw device write EOPTNOTSUPP fix\n\n   - Ensure async workers grab current->fs\n\n   - A few task exit fixes with pending requests that grab the file\n     table\n\n   - send/recvmsg async load fix\n\n   - io-wq offline node setup fix\n\n   - CQ overflow flush in poll\"\n\n* tag 'io_uring-5.6-2020-02-14' of git://git.kernel.dk/linux-block: (21 commits)\n  io_uring: prune request from overflow list on flush\n  io-wq: don't call kXalloc_node() with non-online node\n  io_uring: retain sockaddr_storage across send/recvmsg async punt\n  io_uring: cancel pending async work if task exits\n  io-wq: add io_wq_cancel_pid() to cancel based on a specific pid\n  io-wq: make io_wqe_cancel_work() take a match handler\n  io_uring: fix openat/statx's filename leak\n  io_uring: fix double prep iovec leak\n  io_uring: fix async close() with f_op->flush()\n  io_uring: allow AT_FDCWD for non-file openat/openat2/statx\n  io_uring: grab ->fs as part of async preparation\n  io-wq: add support for inheriting ->fs\n  io_uring: retry raw bdev writes if we hit -EOPNOTSUPP\n  io_uring: add cleanup for openat()/statx()\n  io_uring: fix iovec leaks\n  io_uring: remove unused struct io_async_open\n  io_uring: flush overflowed CQ events in the io_uring_poll()\n  io_uring: statx/openat/openat2 don't support fixed files\n  io_uring: fix deferred req iovec leak\n  io_uring: fix 1-bit bitfields to be unsigned\n  ...",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-14 13:47:02 -0800 Merge tag 'io_uring-5.6-2020-02-14' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "2ca10259b4189a433c309054496dd6af1415f992",
        "message": "Carter reported an issue where he could produce a stall on ring exit,\nwhen we're cleaning up requests that match the given file table. For\nthis particular test case, a combination of a few things caused the\nissue:\n\n- The cq ring was overflown\n- The request being canceled was in the overflow list\n\nThe combination of the above means that the cq overflow list holds a\nreference to the request. The request is canceled correctly, but since\nthe overflow list holds a reference to it, the final put won't happen.\nSince the final put doesn't happen, the request remains in the inflight.\nHence we never finish the cancelation flush.\n\nFix this by removing requests from the overflow list if we're canceling\nthem.\n\nCc: stable@vger.kernel.org # 5.5\nReported-by: Carter Li \u674e\u901a\u6d32 <carter.li@eoitek.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-13 17:25:01 -0700 io_uring: prune request from overflow list on flush"
    },
    {
        "commit": "7563439adfae153b20331f1567c8b5d0e5cbd8a7",
        "message": "Glauber reports a crash on init on a box he has:\n\n RIP: 0010:__alloc_pages_nodemask+0x132/0x340\n Code: 18 01 75 04 41 80 ce 80 89 e8 48 8b 54 24 08 8b 74 24 1c c1 e8 0c 48 8b 3c 24 83 e0 01 88 44 24 20 48 85 d2 0f 85 74 01 00 00 <3b> 77 08 0f 82 6b 01 00 00 48 89 7c 24 10 89 ea 48 8b 07 b9 00 02\n RSP: 0018:ffffb8be4d0b7c28 EFLAGS: 00010246\n RAX: 0000000000000000 RBX: 0000000000000000 RCX: 000000000000e8e8\n RDX: 0000000000000000 RSI: 0000000000000002 RDI: 0000000000002080\n RBP: 0000000000012cc0 R08: 0000000000000000 R09: 0000000000000002\n R10: 0000000000000dc0 R11: ffff995c60400100 R12: 0000000000000000\n R13: 0000000000012cc0 R14: 0000000000000001 R15: ffff995c60db00f0\n FS:  00007f4d115ca900(0000) GS:ffff995c60d80000(0000) knlGS:0000000000000000\n CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n CR2: 0000000000002088 CR3: 00000017cca66002 CR4: 00000000007606e0\n DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n PKRU: 55555554\n Call Trace:\n  alloc_slab_page+0x46/0x320\n  new_slab+0x9d/0x4e0\n  ___slab_alloc+0x507/0x6a0\n  ? io_wq_create+0xb4/0x2a0\n  __slab_alloc+0x1c/0x30\n  kmem_cache_alloc_node_trace+0xa6/0x260\n  io_wq_create+0xb4/0x2a0\n  io_uring_setup+0x97f/0xaa0\n  ? io_remove_personalities+0x30/0x30\n  ? io_poll_trigger_evfd+0x30/0x30\n  do_syscall_64+0x5b/0x1c0\n  entry_SYSCALL_64_after_hwframe+0x44/0xa9\n RIP: 0033:0x7f4d116cb1ed\n\nwhich is due to the 'wqe' and 'worker' allocation being node affine.\nBut it isn't valid to call the node affine allocation if the node isn't\nonline.\n\nSetup structures for even offline nodes, as usual, but skip them in\nterms of thread setup to not waste resources. If the node isn't online,\njust alloc memory with NUMA_NO_NODE.\n\nReported-by: Glauber Costa <glauber@scylladb.com>\nTested-by: Glauber Costa <glauber@scylladb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-12 17:43:22 -0700 io-wq: don't call kXalloc_node() with non-online node"
    },
    {
        "commit": "b537916ca5107c3a8714b8ab3099c0ec205aec12",
        "message": "Jonas reports that he sometimes sees -97/-22 error returns from\nsendmsg, if it gets punted async. This is due to not retaining the\nsockaddr_storage between calls. Include that in the state we copy when\ngoing async.\n\nCc: stable@vger.kernel.org # 5.3+\nReported-by: Jonas Bonn <jonas@norrbonn.se>\nTested-by: Jonas Bonn <jonas@norrbonn.se>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-09 11:32:10 -0700 io_uring: retain sockaddr_storage across send/recvmsg async punt"
    },
    {
        "commit": "6ab231448fdc5e37c15a94a4700fca11e80007f7",
        "message": "Normally we cancel all work we track, but for untracked work we could\nleave the async worker behind until that work completes. This is totally\nfine, but does leave resources pending after the task is gone until that\nwork completes.\n\nCancel work that this task queued up when it goes away.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-09 09:55:38 -0700 io_uring: cancel pending async work if task exits"
    },
    {
        "commit": "36282881a795cbf717aca79392ae9cdf0fef59c9",
        "message": "Add a helper that allows the caller to cancel work based on what mm\nit belongs to. This allows io_uring to cancel work from a given\ntask or thread when it exits.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-09 09:55:38 -0700 io-wq: add io_wq_cancel_pid() to cancel based on a specific pid"
    },
    {
        "commit": "0bdbdd08a8f991bdaee54465a168c0795ea5d28b",
        "message": "As in the previous patch, make openat*_prep() and statx_prep() handle\ndouble preparation to avoid resource leakage.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-08 13:07:00 -0700 io_uring: fix openat/statx's filename leak"
    },
    {
        "commit": "5f798beaf35d79355cbf18019c1993a84475a2c3",
        "message": "Requests may be prepared multiple times with ->io allocated (i.e. async\nprepared). Preparation functions don't handle it and forget about\npreviously allocated resources. This may happen in case of:\n- spurious defer_check\n- non-head (i.e. async prepared) request executed in sync (via nxt).\n\nMake the handlers check, whether they already allocated resources, which\nis true IFF REQ_F_NEED_CLEANUP is set.\n\nCc: stable@vger.kernel.org # 5.5\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-08 13:07:00 -0700 io_uring: fix double prep iovec leak"
    },
    {
        "commit": "a93b33312f63ef6d5997f45d6fdf4de84c5396cc",
        "message": "First, io_close() misses filp_close() and io_cqring_add_event(), when\nf_op->flush is defined. That's because in this case it will\nio_queue_async_work() itself not grabbing files, so the corresponding\nchunk in io_close_finish() won't be executed.\n\nSecond, when submitted through io_wq_submit_work(), it will do\nfilp_close() and *_add_event() twice: first inline in io_close(),\nand the second one in call to io_close_finish() from io_close().\nThe second one will also fire, because it was submitted async through\ngeneric path, and so have grabbed files.\n\nAnd the last nice thing is to remove this weird pilgrimage with checking\nwork/old_work and casting it to nxt. Just use a helper instead.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-08 13:07:00 -0700 io_uring: fix async close() with f_op->flush()"
    },
    {
        "commit": "0b5faf6ba7fb78bb1fe7336d23ea1978386a6c3a",
        "message": "Don't just check for dirfd == -1, we should allow AT_FDCWD as well for\nrelative lookups.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-08 13:07:00 -0700 io_uring: allow AT_FDCWD for non-file openat/openat2/statx"
    },
    {
        "commit": "ff002b30181d30cdfbca316dadd099c3ca0d739c",
        "message": "This passes it in to io-wq, so it assumes the right fs_struct when\nexecuting async work that may need to do lookups.\n\nCc: stable@vger.kernel.org # 5.3+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-08 13:07:00 -0700 io_uring: grab ->fs as part of async preparation"
    },
    {
        "commit": "faac996ccd5da95bc56b91aa80f2643c2d0a1c56",
        "message": "For non-blocking issue, we set IOCB_NOWAIT in the kiocb. However, on a\nraw block device, this yields an -EOPNOTSUPP return, as non-blocking\nwrites aren't supported. Turn this -EOPNOTSUPP into -EAGAIN, so we retry\nfrom blocking context with IOCB_NOWAIT cleared.\n\nCc: stable@vger.kernel.org # 5.5\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-08 13:06:58 -0700 io_uring: retry raw bdev writes if we hit -EOPNOTSUPP"
    },
    {
        "commit": "8fef80bf56a49c60b457dedb99fd6c5279a5dbe1",
        "message": "openat() and statx() may have allocated ->open.filename, which should be\nbe put. Add cleanup handlers for them.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-08 13:06:58 -0700 io_uring: add cleanup for openat()/statx()"
    },
    {
        "commit": "99bc4c38537d774e667d043c520914082da19abf",
        "message": "Allocated iovec is freed only in io_{read,write,send,recv)(), and just\nleaves it if an error occured. There are plenty of such cases:\n- cancellation of non-head requests\n- fail grabbing files in __io_queue_sqe()\n- set REQ_F_NOWAIT and returning in __io_queue_sqe()\n\nAdd REQ_F_NEED_CLEANUP, which will force such requests with custom\nallocated resourses go through cleanup handlers on put.\n\nCc: stable@vger.kernel.org # 5.5\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-08 13:06:58 -0700 io_uring: fix iovec leaks"
    },
    {
        "commit": "e96e977992d0ea40b6e70cb63dede85c9078e744",
        "message": "struct io_async_open is unused, remove it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-08 13:06:58 -0700 io_uring: remove unused struct io_async_open"
    },
    {
        "commit": "63e5d81f72af1bf370bf8a6745b0a8d71a7bb37d",
        "message": "In io_uring_poll() we must flush overflowed CQ events before to\ncheck if there are CQ events available, to avoid missing events.\n\nWe call the io_cqring_events() that checks and flushes any overflow\nand returns the number of CQ events available.\n\nSigned-off-by: Stefano Garzarella <sgarzare@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-08 13:06:58 -0700 io_uring: flush overflowed CQ events in the io_uring_poll()"
    },
    {
        "commit": "cf3040ca55f2085b0a372a620ee2cb93ae19b686",
        "message": "All of these opcodes take a directory file descriptor. We can't easily\nsupport fixed files for these operations, and the use case for that\nprobably isn't all that clear (or sensible) anyway.\n\nDisable IOSQE_FIXED_FILE for these operations.\n\nReported-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-08 13:06:33 -0700 io_uring: statx/openat/openat2 don't support fixed files"
    },
    {
        "commit": "1e95081cb5b4cf77065d37866f57cf3c90a3df78",
        "message": "After defer, a request will be prepared, that includes allocating iovec\nif needed, and then submitted through io_wq_submit_work() but not custom\nhandler (e.g. io_rw_async()/io_sendrecv_async()). However, it'll leak\niovec, as it's in io-wq and the code goes as follows:\n\nio_read() {\n\tif (!io_wq_current_is_worker())\n\t\tkfree(iovec);\n}\n\nPut all deallocation logic in io_{read,write,send,recv}(), which will\nleave the memory, if going async with -EAGAIN.\n\nIt also fixes a leak after failed io_alloc_async_ctx() in\nio_{recv,send}_msg().\n\nCc: stable@vger.kernel.org # 5.5\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-06 13:58:57 -0700 io_uring: fix deferred req iovec leak"
    },
    {
        "commit": "e1d85334d62386e9503e4a0d5d022e2d8e0011a0",
        "message": "Make bitfields of size 1 bit be unsigned (since there is no room\nfor the sign bit).\nThis clears up the sparse warnings:\n\n  CHECK   ../fs/io_uring.c\n../fs/io_uring.c:207:50: error: dubious one-bit signed bitfield\n../fs/io_uring.c:208:55: error: dubious one-bit signed bitfield\n../fs/io_uring.c:209:63: error: dubious one-bit signed bitfield\n../fs/io_uring.c:210:54: error: dubious one-bit signed bitfield\n../fs/io_uring.c:211:57: error: dubious one-bit signed bitfield\n\nFound by sight and then verified with sparse.\n\nFixes: 69b3e546139a (\"io_uring: change io_ring_ctx bool fields into bit fields\")\nSigned-off-by: Randy Dunlap <rdunlap@infradead.org>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: io-uring@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-06 13:41:00 -0700 io_uring: fix 1-bit bitfields to be unsigned"
    },
    {
        "commit": "1cb1edb2f5ba8a3e8d47ded391007c6fe3ac0ad7",
        "message": "Fail fast if can't grab mm, so past that requests always have an mm\nwhen required. This allows us to remove req->user altogether.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc2",
        "release_date": "2020-02-06 12:53:10 -0700 io_uring: get rid of delayed mm check"
    },
    {
        "commit": "c1ef57a3a3f5e69e98baf89055b423da62791c13",
        "message": "Pull io_uring updates from Jens Axboe:\n \"Some later fixes for io_uring:\n\n   - Small cleanup series from Pavel\n\n   - Belt and suspenders build time check of sqe size and layout\n     (Stefan)\n\n   - Addition of ->show_fdinfo() on request of Jann Horn, to aid in\n     understanding mapped personalities\n\n   - eventfd recursion/deadlock fix, for both io_uring and aio\n\n   - Fixup for send/recv handling\n\n   - Fixup for double deferral of read/write request\n\n   - Fix for potential double completion event for close request\n\n   - Adjust fadvise advice async/inline behavior\n\n   - Fix for shutdown hang with SQPOLL thread\n\n   - Fix for potential use-after-free of fixed file table\"\n\n* tag 'io_uring-5.6-2020-02-05' of git://git.kernel.dk/linux-block:\n  io_uring: cleanup fixed file data table references\n  io_uring: spin for sq thread to idle on shutdown\n  aio: prevent potential eventfd recursion on poll\n  io_uring: put the flag changing code in the same spot\n  io_uring: iterate req cache backwards\n  io_uring: punt even fadvise() WILLNEED to async context\n  io_uring: fix sporadic double CQE entry for close\n  io_uring: remove extra ->file check\n  io_uring: don't map read/write iovec potentially twice\n  io_uring: use the proper helpers for io_send/recv\n  io_uring: prevent potential eventfd recursion on poll\n  eventfd: track eventfd_signal() recursion depth\n  io_uring: add BUILD_BUG_ON() to assert the layout of struct io_uring_sqe\n  io_uring: add ->show_fdinfo() for the io_uring file descriptor",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-02-06 06:33:17 +0000 Merge tag 'io_uring-5.6-2020-02-05' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "2faf852d1be8a4960d328492298da6448cca0279",
        "message": "syzbot reports a use-after-free in io_ring_file_ref_switch() when it\ntries to switch back to percpu mode. When we put the final reference to\nthe table by calling percpu_ref_kill_and_confirm(), we don't want the\nzero reference to queue async work for flushing the potentially queued\nup items. We currently do a few flush_work(), but they merely paper\naround the issue, since the work item may not have been queued yet\ndepending on the when the percpu-ref callback gets run.\n\nComing into the file unregister, we know we have the ring quiesced.\nio_ring_file_ref_switch() can check for whether or not the ref is dying\nor not, and not queue anything async at that point. Once the ref has\nbeen confirmed killed, flush any potential items manually.\n\nReported-by: syzbot+7caeaea49c2c8a591e3d@syzkaller.appspotmail.com\nFixes: 05f3fb3c5397 (\"io_uring: avoid ring quiesce for fixed file set unregister and update\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-02-04 20:04:18 -0700 io_uring: cleanup fixed file data table references"
    },
    {
        "commit": "df069d80c8e38c19531c392322e9a16617475c44",
        "message": "As part of io_uring shutdown, we cancel work that is pending and won't\nnecessarily complete on its own. That includes requests like poll\ncommands and timeouts.\n\nIf we're using SQPOLL for kernel side submission and we shutdown the\nring immediately after queueing such work, we can race with the sqthread\ndoing the submission. This means we may miss cancelling some work, which\nresults in the io_uring shutdown hanging forever.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-02-04 16:48:34 -0700 io_uring: spin for sq thread to idle on shutdown"
    },
    {
        "commit": "3e577dcd73a1fdc641bf45e5ea4a37869de221b5",
        "message": "Both iocb_flags() and kiocb_set_rw_flags() are inline and modify\nkiocb->ki_flags. Place them close, so they can be potentially better\noptimised.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-02-03 17:27:47 -0700 io_uring: put the flag changing code in the same spot"
    },
    {
        "commit": "6c8a31346925cbb373f84a18428ab3df59d3950e",
        "message": "Grab requests from cache-array from the end, so can get by only\nfree_reqs.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-02-03 17:27:47 -0700 io_uring: iterate req cache backwards"
    },
    {
        "commit": "3e69426da2599677ebbe76e2d97a606c4797bd74",
        "message": "Andres correctly points out that read-ahead can block, if it needs to\nread in meta data (or even just through the page cache page allocations).\nPlay it safe for now and just ensure WILLNEED is also punted to async\ncontext.\n\nWhile in there, allow the file settings hints from non-blocking\ncontext. They don't need to start/do IO, and we can safely do them\ninline.\n\nFixes: 4840e418c2fc (\"io_uring: add IORING_OP_FADVISE\")\nReported-by: Andres Freund <andres@anarazel.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-02-03 17:27:47 -0700 io_uring: punt even fadvise() WILLNEED to async context"
    },
    {
        "commit": "1a417f4e618e05fba29ba222f1e8555c302376ce",
        "message": "We punt close to async for the final fput(), but we log the completion\neven before that even in that case. We rely on the request not having\na files table assigned to detect what the final async close should do.\nHowever, if we punt the async queue to __io_queue_sqe(), we'll get\n->files assigned and this makes io_close_finish() think it should both\nclose the filp again (which does no harm) AND log a new CQE event for\nthis request. This causes duplicate CQEs.\n\nQueue the request up for async manually so we don't grab files\nneedlessly and trigger this condition.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-02-03 17:27:47 -0700 io_uring: fix sporadic double CQE entry for close"
    },
    {
        "commit": "9250f9ee194dc3dcee28a42a1533fa2cc0edd215",
        "message": "It won't ever get into io_prep_rw() when req->file haven't been set in\nio_req_set_file(), hence remove the check.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-02-03 17:27:47 -0700 io_uring: remove extra ->file check"
    },
    {
        "commit": "5d204bcfa09330972ad3428a8f81c23f371d3e6d",
        "message": "If we have a read/write that is deferred, we already setup the async IO\ncontext for that request, and mapped it. When we later try and execute\nthe request and we get -EAGAIN, we don't want to attempt to re-map it.\nIf we do, we end up with garbage in the iovec, which typically leads\nto an -EFAULT or -EINVAL completion.\n\nCc: stable@vger.kernel.org # 5.5\nReported-by: Dan Melnic <dmm@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-02-03 17:27:47 -0700 io_uring: don't map read/write iovec potentially twice"
    },
    {
        "commit": "0b7b21e42ba2d6ac9595a4358a9354249605a3af",
        "message": "Don't use the recvmsg/sendmsg helpers, use the same helpers that the\nrecv(2) and send(2) system calls use.\n\nReported-by: \u674e\u901a\u6d32 <carter.li@eoitek.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-02-03 17:27:47 -0700 io_uring: use the proper helpers for io_send/recv"
    },
    {
        "commit": "f0b493e6b9a8959356983f57112229e69c2f7b8c",
        "message": "If we have nested or circular eventfd wakeups, then we can deadlock if\nwe run them inline from our poll waitqueue wakeup handler. It's also\npossible to have very long chains of notifications, to the extent where\nwe could risk blowing the stack.\n\nCheck the eventfd recursion count before calling eventfd_signal(). If\nit's non-zero, then punt the signaling to async context. This is always\nsafe, as it takes us out-of-line in terms of stack and locking context.\n\nCc: stable@vger.kernel.org # 5.1+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-02-03 17:27:47 -0700 io_uring: prevent potential eventfd recursion on poll"
    },
    {
        "commit": "b5e683d5cab8cd433b06ae178621f083cabd4f63",
        "message": "eventfd use cases from aio and io_uring can deadlock due to circular\nor resursive calling, when eventfd_signal() tries to grab the waitqueue\nlock. On top of that, it's also possible to construct notification\nchains that are deep enough that we could blow the stack.\n\nAdd a percpu counter that tracks the percpu recursion depth, warn if we\nexceed it. The counter is also exposed so that users of eventfd_signal()\ncan do the right thing if it's non-zero in the context where it is\ncalled.\n\nCc: stable@vger.kernel.org # 4.19+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-02-03 17:27:38 -0700 eventfd: track eventfd_signal() recursion depth"
    },
    {
        "commit": "2113b05d039e9213216ec647df58c5a43593631b",
        "message": "Convert fs/io_uring to use the new pin_user_pages() call, which sets\nFOLL_PIN.  Setting FOLL_PIN is now required for code that requires\ntracking of pinned pages, and therefore for any code that calls\nput_user_page().\n\nIn partial anticipation of this work, the io_uring code was already\ncalling put_user_page() instead of put_page().  Therefore, in order to\nconvert from the get_user_pages()/put_page() model, to the\npin_user_pages()/put_user_page() model, the only change required here is\nto change get_user_pages() to pin_user_pages().\n\nLink: http://lkml.kernel.org/r/20200107224558.2362728-17-jhubbard@nvidia.com\nSigned-off-by: John Hubbard <jhubbard@nvidia.com>\nReviewed-by: Jens Axboe <axboe@kernel.dk>\nReviewed-by: Jan Kara <jack@suse.cz>\nCc: Alex Williamson <alex.williamson@redhat.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Bj\u00f6rn T\u00f6pel <bjorn.topel@intel.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Daniel Vetter <daniel.vetter@ffwll.ch>\nCc: Dan Williams <dan.j.williams@intel.com>\nCc: Hans Verkuil <hverkuil-cisco@xs4all.nl>\nCc: Ira Weiny <ira.weiny@intel.com>\nCc: Jason Gunthorpe <jgg@mellanox.com>\nCc: Jason Gunthorpe <jgg@ziepe.ca>\nCc: Jerome Glisse <jglisse@redhat.com>\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: Kirill A. Shutemov <kirill@shutemov.name>\nCc: Leon Romanovsky <leonro@mellanox.com>\nCc: Mauro Carvalho Chehab <mchehab@kernel.org>\nCc: Mike Rapoport <rppt@linux.ibm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-31 10:30:37 -0800 fs/io_uring: set FOLL_PIN via pin_user_pages()"
    },
    {
        "commit": "a43e982082c24c2f5c0b139daac9657ac352eed3",
        "message": "Patch series \"mm/gup: prereqs to track dma-pinned pages: FOLL_PIN\", v12.\n\nOverview:\n\nThis is a prerequisite to solving the problem of proper interactions\nbetween file-backed pages, and [R]DMA activities, as discussed in [1],\n[2], [3], and in a remarkable number of email threads since about\n2017.  :)\n\nA new internal gup flag, FOLL_PIN is introduced, and thoroughly\ndocumented in the last patch's Documentation/vm/pin_user_pages.rst.\n\nI believe that this will provide a good starting point for doing the\nlayout lease work that Ira Weiny has been working on.  That's because\nthese new wrapper functions provide a clean, constrained, systematically\nnamed set of functionality that, again, is required in order to even\nknow if a page is \"dma-pinned\".\n\nIn contrast to earlier approaches, the page tracking can be\nincrementally applied to the kernel call sites that, until now, have\nbeen simply calling get_user_pages() (\"gup\").  In other words, opt-in by\nchanging from this:\n\n    get_user_pages() (sets FOLL_GET)\n    put_page()\n\nto this:\n    pin_user_pages() (sets FOLL_PIN)\n    unpin_user_page()\n\nTesting:\n\n* I've done some overall kernel testing (LTP, and a few other goodies),\n  and some directed testing to exercise some of the changes. And as you\n  can see, gup_benchmark is enhanced to exercise this. Basically, I've\n  been able to runtime test the core get_user_pages() and\n  pin_user_pages() and related routines, but not so much on several of\n  the call sites--but those are generally just a couple of lines\n  changed, each.\n\n  Not much of the kernel is actually using this, which on one hand\n  reduces risk quite a lot. But on the other hand, testing coverage\n  is low. So I'd love it if, in particular, the Infiniband and PowerPC\n  folks could do a smoke test of this series for me.\n\n  Runtime testing for the call sites so far is pretty light:\n\n    * io_uring: Some directed tests from liburing exercise this, and\n                they pass.\n    * process_vm_access.c: A small directed test passes.\n    * gup_benchmark: the enhanced version hits the new gup.c code, and\n                     passes.\n    * infiniband: Ran rdma-core tests: rdma-core/build/bin/run_tests.py\n    * VFIO: compiles (I'm vowing to set up a run time test soon, but it's\n                      not ready just yet)\n    * powerpc: it compiles...\n    * drm/via: compiles...\n    * goldfish: compiles...\n    * net/xdp: compiles...\n    * media/v4l2: compiles...\n\n[1] Some slow progress on get_user_pages() (Apr 2, 2019): https://lwn.net/Articles/784574/\n[2] DMA and get_user_pages() (LPC: Dec 12, 2018): https://lwn.net/Articles/774411/\n[3] The trouble with get_user_pages() (Apr 30, 2018): https://lwn.net/Articles/753027/\n\nThis patch (of 22):\n\nThere are four locations in gup.c that have a fair amount of code\nduplication.  This means that changing one requires making the same\nchanges in four places, not to mention reading the same code four times,\nand wondering if there are subtle differences.\n\nFactor out the common code into static functions, thus reducing the\noverall line count and the code's complexity.\n\nAlso, take the opportunity to slightly improve the efficiency of the\nerror cases, by doing a mass subtraction of the refcount, surrounded by\nget_page()/put_page().\n\nAlso, further simplify (slightly), by waiting until the the successful\nend of each routine, to increment *nr.\n\nLink: http://lkml.kernel.org/r/20200107224558.2362728-2-jhubbard@nvidia.com\nSigned-off-by: John Hubbard <jhubbard@nvidia.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: J\u00e9r\u00f4me Glisse <jglisse@redhat.com>\nReviewed-by: Jan Kara <jack@suse.cz>\nCc: Kirill A. Shutemov <kirill@shutemov.name>\nCc: Ira Weiny <ira.weiny@intel.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Alex Williamson <alex.williamson@redhat.com>\nCc: Bj\u00f6rn T\u00f6pel <bjorn.topel@intel.com>\nCc: Daniel Vetter <daniel.vetter@ffwll.ch>\nCc: Dan Williams <dan.j.williams@intel.com>\nCc: Hans Verkuil <hverkuil-cisco@xs4all.nl>\nCc: Jason Gunthorpe <jgg@mellanox.com>\nCc: Jason Gunthorpe <jgg@ziepe.ca>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: Leon Romanovsky <leonro@mellanox.com>\nCc: Mauro Carvalho Chehab <mchehab@kernel.org>\nCc: Mike Rapoport <rppt@linux.ibm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-31 10:30:37 -0800 mm/gup: factor out duplicate code from four routines"
    },
    {
        "commit": "d7f62e825fd19202a0749d10fb439714c51f67d2",
        "message": "With nesting of anonymous unions and structs it's hard to\nreview layout changes. It's better to ask the compiler\nfor these things.\n\nSigned-off-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-30 12:40:37 -0700 io_uring: add BUILD_BUG_ON() to assert the layout of struct io_uring_sqe"
    },
    {
        "commit": "87ce955b24c9940cb2ca7e5173fcf175578d9fe9",
        "message": "It can be hard to know exactly what is registered with the ring.\nEspecially for credentials, it'd be handy to be able to see which\nones are registered, what personalities they have, and what the ID\nof each of them is.\n\nThis adds support for showing information registered in the ring from\nthe fdinfo of the io_uring fd. Here's an example from a test case that\nregisters 4 files (two of them sparse), 4 buffers, and 2 personalities:\n\npos:\t0\nflags:\t02000002\nmnt_id:\t14\nUserFiles:\t4\n    0: file-no-1\n    1: file-no-2\n    2: <none>\n    3: <none>\nUserBufs:\t4\n    0: 0x563817c46000/128\n    1: 0x563817c47000/256\n    2: 0x563817c48000/512\n    3: 0x563817c49000/1024\nPersonalities:\n    1\n\tUid:\t0\t\t0\t\t0\t\t0\n\tGid:\t0\t\t0\t\t0\t\t0\n\tGroups:\t0\n\tCapEff:\t0000003fffffffff\n    2\n\tUid:\t0\t\t0\t\t0\t\t0\n\tGid:\t0\t\t0\t\t0\t\t0\n\tGroups:\t0\n\tCapEff:\t0000003fffffffff\n\nSuggested-by: Jann Horn <jannh@google.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-30 12:40:35 -0700 io_uring: add ->show_fdinfo() for the io_uring file descriptor"
    },
    {
        "commit": "896f8d23d0cb5889021d66eab6107e97109c5459",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - Support for various new opcodes (fallocate, openat, close, statx,\n   fadvise, madvise, openat2, non-vectored read/write, send/recv, and\n   epoll_ctl)\n\n - Faster ring quiesce for fileset updates\n\n - Optimizations for overflow condition checking\n\n - Support for max-sized clamping\n\n - Support for probing what opcodes are supported\n\n - Support for io-wq backend sharing between \"sibling\" rings\n\n - Support for registering personalities\n\n - Lots of little fixes and improvements\n\n* tag 'for-5.6/io_uring-vfs-2020-01-29' of git://git.kernel.dk/linux-block: (64 commits)\n  io_uring: add support for epoll_ctl(2)\n  eventpoll: support non-blocking do_epoll_ctl() calls\n  eventpoll: abstract out epoll_ctl() handler\n  io_uring: fix linked command file table usage\n  io_uring: support using a registered personality for commands\n  io_uring: allow registering credentials\n  io_uring: add io-wq workqueue sharing\n  io-wq: allow grabbing existing io-wq\n  io_uring/io-wq: don't use static creds/mm assignments\n  io-wq: make the io_wq ref counted\n  io_uring: fix refcounting with batched allocations at OOM\n  io_uring: add comment for drain_next\n  io_uring: don't attempt to copy iovec for READ/WRITE\n  io_uring: honor IOSQE_ASYNC for linked reqs\n  io_uring: prep req when do IOSQE_ASYNC\n  io_uring: use labeled array init in io_op_defs\n  io_uring: optimise sqe-to-req flags translation\n  io_uring: remove REQ_F_IO_DRAINED\n  io_uring: file switch work needs to get flushed on exit\n  io_uring: hide uring_fd in ctx\n  ...",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-29 18:53:37 -0800 Merge tag 'for-5.6/io_uring-vfs-2020-01-29' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "3e4827b05d2ac2d377ed136a52829ec46787bf4b",
        "message": "This adds IORING_OP_EPOLL_CTL, which can perform the same work as the\nepoll_ctl(2) system call.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-29 15:46:09 -0700 io_uring: add support for epoll_ctl(2)"
    },
    {
        "commit": "f86cd20c9454847a524ddbdcdec32c0380ed7c9b",
        "message": "We're not consistent in how the file table is grabbed and assigned if we\nhave a command linked that requires the use of it.\n\nAdd ->file_table to the io_op_defs[] array, and use that to determine\nwhen to grab the table instead of having the handlers set it if they\nneed to defer. This also means we can kill the IO_WQ_WORK_NEEDS_FILES\nflag. We always initialize work->files, so io-wq can just check for\nthat.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-29 13:46:44 -0700 io_uring: fix linked command file table usage"
    },
    {
        "commit": "75c6a03904e0dd414a4d99a3072075cb5117e5bc",
        "message": "For personalities previously registered via IORING_REGISTER_PERSONALITY,\nallow any command to select them. This is done through setting\nsqe->personality to the id returned from registration, and then flagging\nsqe->flags with IOSQE_PERSONALITY.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-28 17:45:31 -0700 io_uring: support using a registered personality for commands"
    },
    {
        "commit": "071698e13ac6ba786dfa22349a7b62deb5a9464d",
        "message": "If an application wants to use a ring with different kinds of\ncredentials, it can register them upfront. We don't lookup credentials,\nthe credentials of the task calling IORING_REGISTER_PERSONALITY is used.\n\nAn 'id' is returned for the application to use in subsequent personality\nsupport.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-28 17:44:44 -0700 io_uring: allow registering credentials"
    },
    {
        "commit": "24369c2e3bb06d8c4e71fd6ceaf4f8a01ae79b7c",
        "message": "If IORING_SETUP_ATTACH_WQ is set, it expects wq_fd in io_uring_params to\nbe a valid io_uring fd io-wq of which will be shared with the newly\ncreated io_uring instance. If the flag is set but it can't share io-wq,\nit fails.\n\nThis allows creation of \"sibling\" io_urings, where we prefer to keep the\nSQ/CQ private, but want to share the async backend to minimize the amount\nof overhead associated with having multiple rings that belong to the same\nbackend.\n\nReported-by: Jens Axboe <axboe@kernel.dk>\nReported-by: Daurnimator <quae@daurnimator.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-28 17:44:41 -0700 io_uring: add io-wq workqueue sharing"
    },
    {
        "commit": "cccf0ee834559ae0b327b40290e14f6a2a017177",
        "message": "We currently setup the io_wq with a static set of mm and creds. Even for\na single-use io-wq per io_uring, this is suboptimal as we have may have\nmultiple enters of the ring. For sharing the io-wq backend, it doesn't\nwork at all.\n\nSwitch to passing in the creds and mm when the work item is setup. This\nmeans that async work is no longer deferred to the io_uring mm and creds,\nit is done with the current mm and creds.\n\nFlag this behavior with IORING_FEAT_CUR_PERSONALITY, so applications know\nthey can rely on the current personality (mm and creds) being the same\nfor direct issue and async issue.\n\nReviewed-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-28 17:44:20 -0700 io_uring/io-wq: don't use static creds/mm assignments"
    },
    {
        "commit": "9466f43741bc08edd7b1bee642dd6f5561091634",
        "message": "In case of out of memory the second argument of percpu_ref_put_many() in\nio_submit_sqes() may evaluate into \"nr - (-EAGAIN)\", that is clearly\nwrong.\n\nFixes: 2b85edfc0c90 (\"io_uring: batch getting pcpu references\")\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-27 15:36:30 -0700 io_uring: fix refcounting with batched allocations at OOM"
    },
    {
        "commit": "8cdf2193a3335b4cfb6e023b41ac293d0843d287",
        "message": "Draining the middle of a link is tricky, so leave a comment there\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-27 15:36:30 -0700 io_uring: add comment for drain_next"
    },
    {
        "commit": "980ad26304abf11e78caaa68023411b9c088b848",
        "message": "For the non-vectored variant of READV/WRITEV, we don't need to setup an\nasync io context, and we flag that appropriately in the io_op_defs\narray. However, in fixing this for the 5.5 kernel in commit 74566df3a71c\nwe didn't have these opcodes, so the check there was added just for the\nREAD_FIXED and WRITE_FIXED opcodes. Replace that check with just a\nsingle check for needing async context, that covers all four of these\nread/write variants that don't use an iovec.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-27 15:36:29 -0700 io_uring: don't attempt to copy iovec for READ/WRITE"
    },
    {
        "commit": "5cf9ad0e6b164a90581a59609dbf5bda3f5a089c",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Fix for two regressions in this cycle, both reported by the postgresql\n  use case.\n\n  One removes the added restriction on who can submit IO, making it\n  possible for rings shared across forks to do so. The other fixes an\n  issue for the same kind of use case, where one exiting process would\n  cancel all IO\"\n\n* tag 'io_uring-5.5-2020-01-26' of git://git.kernel.dk/linux-block:\n  io_uring: don't cancel all work on process exit\n  Revert \"io_uring: only allow submit from owning task\"",
        "kernel_version": "v5.5",
        "release_date": "2020-01-26 12:23:04 -0800 Merge tag 'io_uring-5.5-2020-01-26' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "ebe10026210f9ea740b9a050ee84a166690fddde",
        "message": "If we're sharing the ring across forks, then one process exiting means\nthat we cancel ALL work and prevent future work. This is overly\nrestrictive. As long as we cancel the work associated with the files\nfrom the current task, it's safe to let others persist. Normal fd close\non exit will still wait (and cancel) pending work.\n\nFixes: fcb323cc53e2 (\"io_uring: io_uring: add support for async work inheriting files\")\nReported-by: Andres Freund <andres@anarazel.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5",
        "release_date": "2020-01-26 10:17:12 -0700 io_uring: don't cancel all work on process exit"
    },
    {
        "commit": "73e08e711d9c1d79fae01daed4b0e1fee5f8a275",
        "message": "This ends up being too restrictive for tasks that willingly fork and\nshare the ring between forks. Andres reports that this breaks his\npostgresql work. Since we're close to 5.5 release, revert this change\nfor now.\n\nCc: stable@vger.kernel.org\nFixes: 44d282796f81 (\"io_uring: only allow submit from owning task\")\nReported-by: Andres Freund <andres@anarazel.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5",
        "release_date": "2020-01-26 09:56:05 -0700 Revert \"io_uring: only allow submit from owning task\""
    },
    {
        "commit": "86a761f81ec87a96572214f5db606f60d36aaf08",
        "message": "REQ_F_FORCE_ASYNC is checked only for the head of a link. Fix it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-22 13:57:48 -0700 io_uring: honor IOSQE_ASYNC for linked reqs"
    },
    {
        "commit": "1118591ab883f46df4ab614cc976bc4c8e04a464",
        "message": "Whenever IOSQE_ASYNC is set, requests will be punted to async without\ngetting into io_issue_req() and without proper preparation done (e.g.\nio_req_defer_prep()). Hence they will be left uninitialised.\n\nPrepare them before punting.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-22 13:57:46 -0700 io_uring: prep req when do IOSQE_ASYNC"
    },
    {
        "commit": "dbab40bdb42c03ab12096d4aaf2dbef3fb55282c",
        "message": "Pull io_uring fix from Jens Axboe:\n \"This was supposed to have gone in last week, but due to a brain fart\n  on my part, I forgot that we made this struct addition in the 5.5\n  cycle. So here it is for 5.5, to prevent having a 32 vs 64-bit\n  compatability issue with the files_update command\"\n\n* tag 'io_uring-5.5-2020-01-22' of git://git.kernel.dk/linux-block:\n  io_uring: fix compat for IORING_REGISTER_FILES_UPDATE",
        "kernel_version": "v5.5",
        "release_date": "2020-01-22 08:30:09 -0800 Merge tag 'io_uring-5.5-2020-01-22' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "0463b6c58e557118d602b2f225fa3bbe9b6f3560",
        "message": "Don't rely on implicit ordering of IORING_OP_ and explicitly place them\nat a right place in io_op_defs. Now former comments are now a part of\nthe code and won't ever outdate.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:07 -0700 io_uring: use labeled array init in io_op_defs"
    },
    {
        "commit": "6b47ee6ecab142f938a40bf3b297abac74218ee2",
        "message": "For each IOSQE_* flag there is a corresponding REQ_F_* flag. And there\nis a repetitive pattern of their translation:\ne.g. if (sqe->flags & SQE_FLAG*) req->flags |= REQ_F_FLAG*\n\nUse same numeric values/bits for them and copy instead of manual\nhandling.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:07 -0700 io_uring: optimise sqe-to-req flags translation"
    },
    {
        "commit": "87987898a1dbc69b1138f7c10eb9abd655c03396",
        "message": "A request can get into the defer list only once, there is no need for\nmarking it as drained, so remove it. This probably was left after\nextracting __need_defer() for use in timeouts.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:07 -0700 io_uring: remove REQ_F_IO_DRAINED"
    },
    {
        "commit": "e46a7950d362231a4d0b078af5f4c109b8e5ac9e",
        "message": "We currently flush early, but if we have something in progress and a\nnew switch is scheduled, we need to ensure to flush after our teardown\nas well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:07 -0700 io_uring: file switch work needs to get flushed on exit"
    },
    {
        "commit": "b14cca0c84c760fbd39ad6bb7e1181e2df103d25",
        "message": "req->ring_fd and req->ring_file are used only during the prep stage\nduring submission, which is is protected by mutex. There is no need\nto store them per-request, place them in ctx.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:06 -0700 io_uring: hide uring_fd in ctx"
    },
    {
        "commit": "0791015837f1520dd72918355dcb1f1e79175255",
        "message": "__io_commit_cqring() is almost always called when there is a change in\nthe rings, so the check is rather pessimising.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:06 -0700 io_uring: remove extra check in __io_commit_cqring"
    },
    {
        "commit": "711be0312df4d350fb5bf1671c132cccae5aaf9a",
        "message": "Move setting ctx->drain_next to the only place it could be set, when it\ngot linked non-head requests. The same for checking it, it's interesting\nonly for a head of a link or a non-linked request.\n\nNo functional changes here. This removes some code from the common path\nand also removes REQ_F_DRAIN_LINK flag, as it doesn't need it anymore.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:06 -0700 io_uring: optimise use of ctx->drain_next"
    },
    {
        "commit": "66f4af93da5761d2fa05c0dc673a47003cdb9cfe",
        "message": "The application currently has no way of knowing if a given opcode is\nsupported or not without having to try and issue one and see if we get\n-EINVAL or not. And even this approach is fraught with peril, as maybe\nwe're getting -EINVAL due to some fields being missing, or maybe it's\njust not that easy to issue that particular command without doing some\nother leg work in terms of setup first.\n\nThis adds IORING_REGISTER_PROBE, which fills in a structure with info\non what it supported or not. This will work even with sparse opcode\nfields, which may happen in the future or even today if someone\nbackports specific features to older kernels.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:06 -0700 io_uring: add support for probing opcodes"
    },
    {
        "commit": "10fef4bebf979bb705feed087611293d5864adfe",
        "message": "We can't assume that the whole batch has fixed files in it. If it's a\nmix, or none at all, then we can end up doing a ref put that either\nmesses up accounting, or causes an oops if we have no fixed files at\nall.\n\nAlso ensure we free requests properly between inflight accounted and\nnormal requests.\n\nFixes: 82c721577011 (\"io_uring: extend batch freeing to cover more cases\")\nReported-by: Dmitrii Dolgov <9erthalion6@gmail.com>\nReported-by: Pavel Begunkov <asml.silence@gmail.com>\nTested-by: Dmitrii Dolgov <9erthalion6@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:06 -0700 io_uring: account fixed file references correctly in batch"
    },
    {
        "commit": "354420f705ccd0aa2d41249f3bb55b4afbed1873",
        "message": "For some test apps at least, user_data is just zeroes. So it's not a\ngood way to tell what the command actually is. Add the opcode to the\nissue trace point.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:06 -0700 io_uring: add opcode to issue trace event"
    },
    {
        "commit": "cebdb98617ae3e842c81c73758a185248b37cfd6",
        "message": "Add support for the new openat2(2) system call. It's trivial to do, as\nwe can have openat(2) just be wrapped around it.\n\nSuggested-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:04 -0700 io_uring: add support for IORING_OP_OPENAT2"
    },
    {
        "commit": "f8748881b17dc56b3faa1d30c823f071c56593e5",
        "message": "We only use it internally in the prep functions for both statx and\nopenat, so we don't need it to be persistent across the request.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:04 -0700 io_uring: remove 'fname' from io_open structure"
    },
    {
        "commit": "c12cedf24e786509de031a832e6b0e5f8b3ca37b",
        "message": "We'll need this for openat2(2) support, remove flags and mode from\nthe existing io_open struct.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:04 -0700 io_uring: add 'struct open_how' to the openat request context"
    },
    {
        "commit": "f2842ab5b72d7ee5f7f8385c2d4f32c133f5837b",
        "message": "If an application is using eventfd notifications with poll to know when\nnew SQEs can be issued, it's expecting the following read/writes to\ncomplete inline. And with that, it knows that there are events available,\nand don't want spurious wakeups on the eventfd for those requests.\n\nThis adds IORING_REGISTER_EVENTFD_ASYNC, which works just like\nIORING_REGISTER_EVENTFD, except it only triggers notifications for events\nthat happen from async completions (IRQ, or io-wq worker completions).\nAny completions inline from the submission itself will not trigger\nnotifications.\n\nSuggested-by: Mark Papadakis <markuspapadakis@icloud.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:04 -0700 io_uring: enable option to only trigger eventfd for async completions"
    },
    {
        "commit": "69b3e546139a21b3046b6bf0cb79d5e8c9a3fa75",
        "message": "In preparation for adding another one, which would make us spill into\nanother long (and hence bump the size of the ctx), change them to\nbit fields.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:04 -0700 io_uring: change io_ring_ctx bool fields into bit fields"
    },
    {
        "commit": "c150368b496837cb207712e78f903ccfd7633b93",
        "message": "If an application attempts to register a set with unbounded requests\npending, we can be stuck here forever if they don't complete. We can\nmake this wait interruptible, and just abort if we get signaled.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:04 -0700 io_uring: file set registration should use interruptible waits"
    },
    {
        "commit": "96fd84d83a778450ffae737d9efa546ac3983b1f",
        "message": "Null check kfree is redundant, so remove it.\nThis is detected by coccinelle.\n\nSigned-off-by: YueHaibing <yuehaibing@huawei.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:04 -0700 io_uring: Remove unnecessary null check"
    },
    {
        "commit": "fddafacee287b3140212c92464077e971401f860",
        "message": "This adds IORING_OP_SEND for send(2) support, and IORING_OP_RECV for\nrecv(2) support.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:04 -0700 io_uring: add support for send(2) and recv(2)"
    },
    {
        "commit": "2550878f8421f7912fdd56b38c630b797f95c749",
        "message": "io_wq workers use io_issue_sqe() to forward sqes and never\nio_queue_sqe(). Remove extra check for io_wq_current_is_worker()\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:04 -0700 io_uring: remove extra io_wq_current_is_worker()"
    },
    {
        "commit": "caf582c652feccd42c50923f0467c4f2dcef279e",
        "message": "It should be pretty rare to not submitting anything when there is\nsomething in the ring. No need to keep heuristics for this case.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:04 -0700 io_uring: optimise commit_sqring() for common case"
    },
    {
        "commit": "ee7d46d9db19ded7b7222af95add63606318a480",
        "message": "A user may ask to submit more than there is in the ring, and then\nio_uring will submit as much as it can. However, in the last iteration\nit will allocate an io_kiocb and immediately free it. It could do\nbetter and adjust @to_submit to what is in the ring.\n\nAnd since the ring's head is already checked here, there is no need to\ndo it in the loop, spamming with smp_load_acquire()'s barriers\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:02 -0700 io_uring: optimise head checks in io_get_sqring()"
    },
    {
        "commit": "9ef4f124894b7b9241a3cf5f9b40db0812783d66",
        "message": "Make io_submit_sqes() to clamp @to_submit itself. It removes duplicated\ncode and prepares for following changes.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:02 -0700 io_uring: clamp to_submit in io_submit_sqes()"
    },
    {
        "commit": "8110c1a6212e430a84edd2b83fe9043def8b743e",
        "message": "Some applications like to start small in terms of ring size, and then\nramp up as needed. This is a bit tricky to do currently, since we don't\nadvertise the max ring size.\n\nThis adds IORING_SETUP_CLAMP. If set, and the values for SQ or CQ ring\nsize exceed what we support, then clamp them at the max values instead\nof returning -EINVAL. Since we return the chosen ring sizes after setup,\nno further changes are needed on the application side. io_uring already\nchanges the ring sizes if the application doesn't ask for power-of-two\nsizes, for example.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:02 -0700 io_uring: add support for IORING_SETUP_CLAMP"
    },
    {
        "commit": "c6ca97b30c47c7ad36107d3764bb4dc37026d171",
        "message": "Currently we only batch free if fixed files are used, no links, no aux\ndata, etc. This extends the batch freeing to only exclude the linked\ncase and fallback case, and make io_free_req_many() handle the other\ncases just fine.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:02 -0700 io_uring: extend batch freeing to cover more cases"
    },
    {
        "commit": "8237e045983d82ba78eaab5f60b9300927fc6796",
        "message": "This cleans up the code a bit, and it allows us to build on top of the\nmulti-req freeing.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:02 -0700 io_uring: wrap multi-req freeing in struct req_batch"
    },
    {
        "commit": "2b85edfc0c90efc68dea3d665bb4111bf0694e05",
        "message": "percpu_ref_tryget() has its own overhead. Instead getting a reference\nfor each request, grab a bunch once per io_submit_sqes().\n\n~5% throughput boost for a \"submit and wait 128 nops\" benchmark.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\n\n__io_req_free_empty() -> __io_req_do_free()\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:02 -0700 io_uring: batch getting pcpu references"
    },
    {
        "commit": "c1ca757bd6f4632c510714631ddcc2d13030fe1e",
        "message": "This adds support for doing madvise(2) through io_uring. We assume that\nany operation can block, and hence punt everything async. This could be\nimproved, but hard to make bullet proof. The async punt ensures it's\nsafe.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:02 -0700 io_uring: add IORING_OP_MADVISE"
    },
    {
        "commit": "db08ca25253d56f1f76eb4b3fe32a7ac1fbab741",
        "message": "This is in preparation for enabling this functionality through io_uring.\nAdd a helper that is just exporting what sys_madvise() does, and have the\nsystem call use it.\n\nNo functional changes in this patch.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:02 -0700 mm: make do_madvise() available internally"
    },
    {
        "commit": "4840e418c2fc533d55ff6caa5b9313eed1d26cfd",
        "message": "This adds support for doing fadvise through io_uring. We assume that\nWILLNEED doesn't block, but that DONTNEED may block.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:04:01 -0700 io_uring: add IORING_OP_FADVISE"
    },
    {
        "commit": "ba04291eb66ed895f194ae5abd3748d72bf8aaea",
        "message": "This behaves like preadv2/pwritev2 with offset == -1, it'll use (and\nupdate) the current file position. This obviously comes with the caveat\nthat if the application has multiple read/writes in flight, then the\nend result will not be as expected. This is similar to threads sharing\na file descriptor and doing IO using the current file position.\n\nSince this feature isn't easily detectable by doing a read or write,\nadd a feature flags, IORING_FEAT_RW_CUR_POS, to allow applications to\ndetect presence of this feature.\n\nReported-by: \u674e\u901a\u6d32 <carter.li@eoitek.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:03:59 -0700 io_uring: allow use of offset == -1 to mean file position"
    },
    {
        "commit": "3a6820f2bb8a079975109c25a5d1f29f46bce5d2",
        "message": "For uses cases that don't already naturally have an iovec, it's easier\n(or more convenient) to just use a buffer address + length. This is\nparticular true if the use case is from languages that want to create\na memory safe abstraction on top of io_uring, and where introducing\nthe need for the iovec may impose an ownership issue. For those cases,\nthey currently need an indirection buffer, which means allocating data\njust for this purpose.\n\nAdd basic read/write that don't require the iovec.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:03:59 -0700 io_uring: add non-vectored read/write commands"
    },
    {
        "commit": "e94f141bd248ebdadcb7351f1e70b31cee5add53",
        "message": "For busy IORING_OP_POLL_ADD workloads, we can have enough contention\non the completion lock that we fail the inline completion path quite\noften as we fail the trylock on that lock. Add a list for deferred\ncompletions that we can use in that case. This helps reduce the number\nof async offloads we have to do, as if we get multiple completions in\na row, we'll piggy back on to the poll_llist instead of having to queue\nour own offload.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:03:59 -0700 io_uring: improve poll completion performance"
    },
    {
        "commit": "ad3eb2c89fb24d14ac81f43eff8e85fece2c934d",
        "message": "We currently check ->cq_overflow_list from both SQ and CQ context, which\ncauses some bouncing of that cache line. Add separate bits of state for\nthis instead, so that the SQ side can check using its own state, and\nlikewise for the CQ side.\n\nThis adds ->sq_check_overflow with the SQ state, and ->cq_check_overflow\nwith the CQ state. If we hit an overflow condition, both of these bits\nare set. Likewise for overflow flush clear, we clear both bits. For the\nfast path of just checking if there's an overflow condition on either\nthe SQ or CQ side, we can use our own private bit for this.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:03:59 -0700 io_uring: split overflow state into SQ and CQ side"
    },
    {
        "commit": "d3656344fea0339fb0365c8df4d2beba4e0089cd",
        "message": "We currently have various switch statements that check if an opcode needs\na file, mm, etc. These are hard to keep in sync as opcodes are added. Add\na struct io_op_def that holds all of this information, so we have just\none spot to update when opcodes are added.\n\nThis also enables us to NOT allocate req->io if a deferred command\ndoesn't need it, and corrects some mistakes we had in terms of what\ncommands need mm context.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:03:59 -0700 io_uring: add lookup table for various opcode needs"
    },
    {
        "commit": "add7b6b85a4dfa89283834d181e87ea2144b9028",
        "message": "__io_free_req() and io_double_put_req() aren't used before they are\ndefined, so we can kill these two forwards.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:03:59 -0700 io_uring: remove two unnecessary function declarations"
    },
    {
        "commit": "32fe525b6d10fec956cfe68f0db76839cd7f0ea5",
        "message": "Move io_queue_link_head() to links handling code in io_submit_sqe(),\nso it wouldn't need extra checks and would have better data locality.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:03:59 -0700 io_uring: move *queue_link_head() from common path"
    },
    {
        "commit": "9d76377f7e13c19441fdd066033345289f89b5fe",
        "message": "Calling \"prev\" a head of a link is a bit misleading. Rename it\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:03:59 -0700 io_uring: rename prev to head"
    },
    {
        "commit": "ce35a47a3a0208a77b4d31b7f2e8ed57d624093d",
        "message": "io_uring defaults to always doing inline submissions, if at all\npossible. But for larger copies, even if the data is fully cached, that\ncan take a long time. Add an IOSQE_ASYNC flag that the application can\nset on the SQE - if set, it'll ensure that we always go async for those\nkinds of requests. Use the io-wq IO_WQ_WORK_CONCURRENT flag to ensure we\nget the concurrency we desire for this case.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:03:59 -0700 io_uring: add IOSQE_ASYNC"
    },
    {
        "commit": "eddc7ef52a6b37b7ba3d1c8a8fbb63d5d9914f8a",
        "message": "This provides support for async statx(2) through io_uring.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:03:54 -0700 io_uring: add support for IORING_OP_STATX"
    },
    {
        "commit": "05f3fb3c5397524feae2e73ee8e150a9090a7da2",
        "message": "We currently fully quiesce the ring before an unregister or update of\nthe fixed fileset. This is very expensive, and we can be a bit smarter\nabout this.\n\nAdd a percpu refcount for the file tables as a whole. Grab a percpu ref\nwhen we use a registered file, and put it on completion. This is cheap\nto do. Upon removal of a file from a set, switch the ref count to atomic\nmode. When we hit zero ref on the completion side, then we know we can\ndrop the previously registered files. When the old files have been\ndropped, switch the ref back to percpu mode for normal operation.\n\nSince there's a period between doing the update and the kernel being\ndone with it, add a IORING_OP_FILES_UPDATE opcode that can perform the\nsame action. The application knows the update has completed when it gets\nthe CQE for it. Between doing the update and receiving this completion,\nthe application must continue to use the unregistered fd if submitting\nIO on this particular file.\n\nThis takes the runtime of test/file-register from liburing from 14s to\nabout 0.7s.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:03:50 -0700 io_uring: avoid ring quiesce for fixed file set unregister and update"
    },
    {
        "commit": "b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb",
        "message": "This works just like close(2), unsurprisingly. We remove the file\ndescriptor and post the completion inline, then offload the actual\n(potential) last file put to async context.\n\nMark the async part of this work as uncancellable, as we really must\nguarantee that the latter part of the close is run.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:01:53 -0700 io_uring: add support for IORING_OP_CLOSE"
    },
    {
        "commit": "15b71abe7b52df214785dde0de9f581cc0216d17",
        "message": "This works just like openat(2), except it can be performed async. For\nthe normal case of a non-blocking path lookup this will complete\ninline. If we have to do IO to perform the open, it'll be done from\nasync context.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:01:53 -0700 io_uring: add support for IORING_OP_OPENAT"
    },
    {
        "commit": "35cb6d54c1d5daf1d1ed585ef5ce4557e7ab284c",
        "message": "This is a prep patch for supporting non-blocking open from io_uring.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:01:53 -0700 fs: make build_open_flags() available internally"
    },
    {
        "commit": "d63d1b5edb7b832210bfde587ba9e7549fa064eb",
        "message": "This exposes fallocate(2) through io_uring.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:01:53 -0700 io_uring: add support for fallocate()"
    },
    {
        "commit": "4d927483732f759769613388813ff5dbb29902a7",
        "message": "Pull in compatability fix for the files_update command.\n\n* io_uring-5.5:\n  io_uring: fix compat for IORING_REGISTER_FILES_UPDATE",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-20 17:01:17 -0700 Merge branch 'io_uring-5.5' into for-5.6/io_uring-vfs"
    },
    {
        "commit": "1292e972fff2b2d81e139e0c2fe5f50249e78c58",
        "message": "fds field of struct io_uring_files_update is problematic with regards\nto compat user space, as pointer size is different in 32-bit, 32-on-64-bit,\nand 64-bit user space.  In order to avoid custom handling of compat in\nthe syscall implementation, make fds __u64 and use u64_to_user_ptr in\norder to retrieve it.  Also, align the field naturally and check that\nno garbage is passed there.\n\nFixes: c3a31e605620c279 (\"io_uring: add support for IORING_REGISTER_FILES_UPDATE\")\nSigned-off-by: Eugene Syromiatnikov <esyr@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5",
        "release_date": "2020-01-20 17:00:44 -0700 io_uring: fix compat for IORING_REGISTER_FILES_UPDATE"
    },
    {
        "commit": "fa7773deb344fbfe0405d1720d4a0cdd5970369e",
        "message": "Pull in Al's openat2 branch, since we'll need that for the openat2\nsupport.\n\n* 'work.openat2' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:\n  Documentation: path-lookup: include new LOOKUP flags\n  selftests: add openat2(2) selftests\n  open: introduce openat2(2) syscall\n  namei: LOOKUP_{IN_ROOT,BENEATH}: permit limited \"..\" resolution\n  namei: LOOKUP_IN_ROOT: chroot-like scoped resolution\n  namei: LOOKUP_BENEATH: O_BENEATH-like scoped resolution\n  namei: LOOKUP_NO_XDEV: block mountpoint crossing\n  namei: LOOKUP_NO_MAGICLINKS: block magic-link resolution\n  namei: LOOKUP_NO_SYMLINKS: block symlink resolution\n  namei: allow set_root() to produce errors\n  namei: allow nd_jump_link() to produce errors\n  nsfs: clean-up ns_get_path() signature to return int\n  namei: only return -ECHILD from follow_dotdot_rcu()",
        "kernel_version": "v5.6-rc1",
        "release_date": "2020-01-19 19:47:04 -0700 Merge branch 'work.openat2' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs into for-5.6/io_uring-vfs"
    },
    {
        "commit": "8cac89909a30807eb4aba56a0e29f55e3b6df42f",
        "message": "Pull thread fixes from Christian Brauner:\n \"Here is an urgent fix for ptrace_may_access() permission checking.\n\n  Commit 69f594a38967 (\"ptrace: do not audit capability check when\n  outputing /proc/pid/stat\") introduced the ability to opt out of audit\n  messages for accesses to various proc files since they are not\n  violations of policy.\n\n  While doing so it switched the check from ns_capable() to\n  has_ns_capability{_noaudit}(). That means it switched from checking\n  the subjective credentials (ktask->cred) of the task to using the\n  objective credentials (ktask->real_cred). This is appears to be wrong.\n  ptrace_has_cap() is currently only used in ptrace_may_access() And is\n  used to check whether the calling task (subject) has the\n  CAP_SYS_PTRACE capability in the provided user namespace to operate on\n  the target task (object). According to the cred.h comments this means\n  the subjective credentials of the calling task need to be used.\n\n  With this fix we switch ptrace_has_cap() to use security_capable() and\n  thus back to using the subjective credentials.\n\n  As one example where this might be particularly problematic, Jann\n  pointed out that in combination with the upcoming IORING_OP_OPENAT{2}\n  feature, this bug might allow unprivileged users to bypass the\n  capability checks while asynchronously opening files like /proc/*/mem,\n  because the capability checks for this would be performed against\n  kernel credentials.\n\n  To illustrate on the former point about this being exploitable: When\n  io_uring creates a new context it records the subjective credentials\n  of the caller. Later on, when it starts to do work it creates a kernel\n  thread and registers a callback. The callback runs with kernel creds\n  for ktask->real_cred and ktask->cred.\n\n  To prevent this from becoming a full-blown 0-day io_uring will call\n  override_cred() and override ktask->cred with the subjective\n  credentials of the creator of the io_uring instance. With\n  ptrace_has_cap() currently looking at ktask->real_cred this override\n  will be ineffective and the caller will be able to open arbitray proc\n  files as mentioned above.\n\n  Luckily, this is currently not exploitable but would be so once\n  IORING_OP_OPENAT{2} land in v5.6. Let's fix it now.\n\n  To minimize potential regressions I successfully ran the criu\n  testsuite. criu makes heavy use of ptrace() and extensively hits\n  ptrace_may_access() codepaths and has a good change of detecting any\n  regressions.\n\n  Additionally, I succesfully ran the ptrace and seccomp kernel tests\"\n\n* tag 'for-linus-2020-01-18' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux:\n  ptrace: reintroduce usage of subjective credentials in ptrace_has_cap()",
        "kernel_version": "v5.5-rc7",
        "release_date": "2020-01-18 12:23:31 -0800 Merge tag 'for-linus-2020-01-18' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux"
    },
    {
        "commit": "6b3ad6649a4c75504edeba242d3fd36b3096a57f",
        "message": "Commit 69f594a38967 (\"ptrace: do not audit capability check when outputing /proc/pid/stat\")\nintroduced the ability to opt out of audit messages for accesses to various\nproc files since they are not violations of policy.  While doing so it\nsomehow switched the check from ns_capable() to\nhas_ns_capability{_noaudit}(). That means it switched from checking the\nsubjective credentials of the task to using the objective credentials. This\nis wrong since. ptrace_has_cap() is currently only used in\nptrace_may_access() And is used to check whether the calling task (subject)\nhas the CAP_SYS_PTRACE capability in the provided user namespace to operate\non the target task (object). According to the cred.h comments this would\nmean the subjective credentials of the calling task need to be used.\nThis switches ptrace_has_cap() to use security_capable(). Because we only\ncall ptrace_has_cap() in ptrace_may_access() and in there we already have a\nstable reference to the calling task's creds under rcu_read_lock() there's\nno need to go through another series of dereferences and rcu locking done\nin ns_capable{_noaudit}().\n\nAs one example where this might be particularly problematic, Jann pointed\nout that in combination with the upcoming IORING_OP_OPENAT feature, this\nbug might allow unprivileged users to bypass the capability checks while\nasynchronously opening files like /proc/*/mem, because the capability\nchecks for this would be performed against kernel credentials.\n\nTo illustrate on the former point about this being exploitable: When\nio_uring creates a new context it records the subjective credentials of the\ncaller. Later on, when it starts to do work it creates a kernel thread and\nregisters a callback. The callback runs with kernel creds for\nktask->real_cred and ktask->cred. To prevent this from becoming a\nfull-blown 0-day io_uring will call override_cred() and override\nktask->cred with the subjective credentials of the creator of the io_uring\ninstance. With ptrace_has_cap() currently looking at ktask->real_cred this\noverride will be ineffective and the caller will be able to open arbitray\nproc files as mentioned above.\nLuckily, this is currently not exploitable but will turn into a 0-day once\nIORING_OP_OPENAT{2} land in v5.6. Fix it now!\n\nCc: Oleg Nesterov <oleg@redhat.com>\nCc: Eric Paris <eparis@redhat.com>\nCc: stable@vger.kernel.org\nReviewed-by: Kees Cook <keescook@chromium.org>\nReviewed-by: Serge Hallyn <serge@hallyn.com>\nReviewed-by: Jann Horn <jannh@google.com>\nFixes: 69f594a38967 (\"ptrace: do not audit capability check when outputing /proc/pid/stat\")\nSigned-off-by: Christian Brauner <christian.brauner@ubuntu.com>",
        "kernel_version": "v5.5-rc7",
        "release_date": "2020-01-18 13:51:39 +0100 ptrace: reintroduce usage of subjective credentials in ptrace_has_cap()"
    },
    {
        "commit": "25e73aadf297d78cf528841795cd37bad8320642",
        "message": "Pull io_uring fixes form Jens Axboe:\n\n - Ensure ->result is always set when IO is retried (Bijan)\n\n - In conjunction with the above, fix a regression in polled IO issue\n   when retried (me/Bijan)\n\n - Don't setup async context for read/write fixed, otherwise we may\n   wrongly map the iovec on retry (me)\n\n - Cancel io-wq work if we fail getting mm reference (me)\n\n - Ensure dependent work is always initialized correctly (me)\n\n - Only allow original task to submit IO, don't allow it from a passed\n   ring fd (me)\n\n* tag 'io_uring-5.5-2020-01-16' of git://git.kernel.dk/linux-block:\n  io_uring: only allow submit from owning task\n  io_uring: ensure workqueue offload grabs ring mutex for poll list\n  io_uring: clear req->result always before issuing a read/write request\n  io_uring: be consistent in assigning next work from handler\n  io-wq: cancel work if we fail getting a mm reference\n  io_uring: don't setup async context for read/write fixed",
        "kernel_version": "v5.5-rc7",
        "release_date": "2020-01-17 11:25:45 -0800 Merge tag 'io_uring-5.5-2020-01-16' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "44d282796f81eb1debc1d7cb53245b4cb3214cb5",
        "message": "If the credentials or the mm doesn't match, don't allow the task to\nsubmit anything on behalf of this ring. The task that owns the ring can\npass the file descriptor to another task, but we don't want to allow\nthat task to submit an SQE that then assumes the ring mm and creds if\nit needs to go async.\n\nCc: stable@vger.kernel.org\nSuggested-by: Stefan Metzmacher <metze@samba.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc7",
        "release_date": "2020-01-16 21:43:24 -0700 io_uring: only allow submit from owning task"
    },
    {
        "commit": "11ba820bf163e224bf5dd44e545a66a44a5b1d7a",
        "message": "A previous commit moved the locking for the async sqthread, but didn't\ntake into account that the io-wq workers still need it. We can't use\nreq->in_async for this anymore as both the sqthread and io-wq workers\nset it, gate the need for locking on io_wq_current_is_worker() instead.\n\nFixes: 8a4955ff1cca (\"io_uring: sqthread should grab ctx->uring_lock for submissions\")\nReported-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc7",
        "release_date": "2020-01-15 21:51:17 -0700 io_uring: ensure workqueue offload grabs ring mutex for poll list"
    },
    {
        "commit": "797f3f535d59f05ad12c629338beef6cb801d19e",
        "message": "req->result is cleared when io_issue_sqe() calls io_read/write_pre()\nroutines.  Those routines however are not called when the sqe\nargument is NULL, which is the case when io_issue_sqe() is called from\nio_wq_submit_work().  io_issue_sqe() may then examine a stale result if\na polled request had previously failed with -EAGAIN:\n\n        if (ctx->flags & IORING_SETUP_IOPOLL) {\n                if (req->result == -EAGAIN)\n                        return -EAGAIN;\n\n                io_iopoll_req_issued(req);\n        }\n\nand in turn cause a subsequently completed request to be re-issued in\nio_wq_submit_work().\n\nSigned-off-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc7",
        "release_date": "2020-01-15 21:36:13 -0700 io_uring: clear req->result always before issuing a read/write request"
    },
    {
        "commit": "78912934f4f7dd7a424159c69bf9bdd46e823781",
        "message": "If we pass back dependent work in case of links, we need to always\nensure that we call the link setup and work prep handler. If not, we\nmight be missing some setup for the next work item.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc7",
        "release_date": "2020-01-14 22:09:06 -0700 io_uring: be consistent in assigning next work from handler"
    },
    {
        "commit": "74566df3a71c1b92da608868cca787557d8be7b2",
        "message": "We don't need it, and if we have it, then the retry handler will attempt\nto copy the non-existent iovec with the inline iovec, with a segment\ncount that doesn't make sense.\n\nFixes: f67676d160c6 (\"io_uring: ensure async punted read/write requests copy iovec\")\nReported-by: Jonathan Lemon <jonathan.lemon@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc7",
        "release_date": "2020-01-13 19:25:29 -0700 io_uring: don't setup async context for read/write fixed"
    },
    {
        "commit": "30b6487d151981e850de19b2b3feefd9b9791957",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Single fix for this series, fixing a regression with the short read\n  handling.\n\n  This just removes it, as it cannot safely be done for all cases\"\n\n* tag 'io_uring-5.5-2020-01-10' of git://git.kernel.dk/linux-block:\n  io_uring: remove punt of short reads to async context",
        "kernel_version": "v5.5-rc6",
        "release_date": "2020-01-10 12:03:12 -0800 Merge tag 'io_uring-5.5-2020-01-10' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "eacc6dfaea963ef61540abb31ad7829be5eff284",
        "message": "We currently punt any short read on a regular file to async context,\nbut this fails if the short read is due to running into EOF. This is\nespecially problematic since we only do the single prep for commands\nnow, as we don't reset kiocb->ki_pos. This can result in a 4k read on\na 1k file returning zero, as we detect the short read and then retry\nfrom async context. At the time of retry, the position is now 1k, and\nwe end up reading nothing, and hence return 0.\n\nInstead of trying to patch around the fact that short reads can be\nlegitimate and won't succeed in case of retry, remove the logic to punt\na short read to async context. Simply return it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc6",
        "release_date": "2020-01-07 13:08:56 -0700 io_uring: remove punt of short reads to async context"
    },
    {
        "commit": "534121d289e06827ee84c9262267ca5ebf1d9fd9",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - Removal of now unused busy wqe list (Hillf)\n\n - Add cond_resched() to io-wq work processing (Hillf)\n\n - And then the series that I hinted at from last week, which removes\n   the sqe from the io_kiocb and keeps all sqe handling on the prep\n   side. This guarantees that an opcode can't do the wrong thing and\n   read the sqe more than once. This is unchanged from last week, no\n   issues have been observed with this in testing. Hence I really think\n   we should fold this into 5.5.\n\n* tag 'io_uring-5.5-20191226' of git://git.kernel.dk/linux-block:\n  io-wq: add cond_resched() to worker thread\n  io-wq: remove unused busy list from io_sqe\n  io_uring: pass in 'sqe' to the prep handlers\n  io_uring: standardize the prep methods\n  io_uring: read 'count' for IORING_OP_TIMEOUT in prep handler\n  io_uring: move all prep state for IORING_OP_{SEND,RECV}_MGS to prep handler\n  io_uring: move all prep state for IORING_OP_CONNECT to prep handler\n  io_uring: add and use struct io_rw for read/writes\n  io_uring: use u64_to_user_ptr() consistently",
        "kernel_version": "v5.5-rc4",
        "release_date": "2019-12-27 11:17:08 -0800 Merge tag 'io_uring-5.5-20191226' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "f8f04d085974ae37782c317abd75f770a25e7713",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"Here's a set of fixes that should go into 5.5-rc3 for io_uring.\n\n  This is bigger than I'd like it to be, mainly because we're fixing the\n  case where an application reuses sqe data right after issue. This\n  really must work, or it's confusing. With 5.5 we're flagging us as\n  submit stable for the actual data, this must also be the case for\n  SQEs.\n\n  Honestly, I'd really like to add another series on top of this, since\n  it cleans it up considerable and prevents any SQE reuse by design. I\n  posted that here:\n\n    https://lore.kernel.org/io-uring/20191220174742.7449-1-axboe@kernel.dk/T/#u\n\n  and may still send it your way early next week once it's been looked\n  at and had some more soak time (does pass all regression tests). With\n  that series, we've unified the prep+issue handling, and only the prep\n  phase even has access to the SQE.\n\n  Anyway, outside of that, fixes in here for a few other issues that\n  have been hit in testing or production\"\n\n* tag 'io_uring-5.5-20191220' of git://git.kernel.dk/linux-block:\n  io_uring: io_wq_submit_work() should not touch req->rw\n  io_uring: don't wait when under-submitting\n  io_uring: warn about unhandled opcode\n  io_uring: read opcode and user_data from SQE exactly once\n  io_uring: make IORING_OP_TIMEOUT_REMOVE deferrable\n  io_uring: make IORING_OP_CANCEL_ASYNC deferrable\n  io_uring: make IORING_POLL_ADD and IORING_POLL_REMOVE deferrable\n  io_uring: make HARDLINK imply LINK\n  io_uring: any deferred command must have stable sqe data\n  io_uring: remove 'sqe' parameter to the OP helpers that take it\n  io_uring: fix pre-prepped issue with force_nonblock == true\n  io-wq: re-add io_wq_current_is_worker()\n  io_uring: fix sporadic -EFAULT from IORING_OP_RECVMSG\n  io_uring: fix stale comment and a few typos",
        "kernel_version": "v5.5-rc3",
        "release_date": "2019-12-20 13:30:49 -0800 Merge tag 'io_uring-5.5-20191220' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "3529d8c2b353e6e446277ae96a36e7471cb070fc",
        "message": "This moves the prep handlers outside of the opcode handlers, and allows\nus to pass in the sqe directly. If the sqe is non-NULL, it means that\nthe request should be prepared for the first time.\n\nWith the opcode handlers not having access to the sqe at all, we are\nguaranteed that the prep handler has setup the request fully by the\ntime we get there. As before, for opcodes that need to copy in more\ndata then the io_kiocb allows for, the io_async_ctx holds that info. If\na prep handler is invoked with req->io set, it must use that to retain\ninformation for later.\n\nFinally, we can remove io_kiocb->sqe as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc4",
        "release_date": "2019-12-20 10:04:50 -0700 io_uring: pass in 'sqe' to the prep handlers"
    },
    {
        "commit": "06b76d44ba25e52711dc7cc4fc75b50907bc6b8e",
        "message": "We currently have a mix of use cases. Most of the newer ones are pretty\nuniform, but we have some older ones that use different calling\ncalling conventions. This is confusing.\n\nFor the opcodes that currently rely on the req->io->sqe copy saving\nthem from reuse, add a request type struct in the io_kiocb command\nunion to store the data they need.\n\nPrepare for all opcodes having a standard prep method, so we can call\nit in a uniform fashion and outside of the opcode handler. This is in\npreparation for passing in the 'sqe' pointer, rather than storing it\nin the io_kiocb. Once we have uniform prep handlers, we can leave all\nthe prep work to that part, and not even pass in the sqe to the opcode\nhandler. This ensures that we don't reuse sqe data inadvertently.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc4",
        "release_date": "2019-12-20 10:04:22 -0700 io_uring: standardize the prep methods"
    },
    {
        "commit": "26a61679f10c6f041726411964b172565021c2eb",
        "message": "Add the count field to struct io_timeout, and ensure the prep handler\nhas read it. Timeout also needs an async context always, set it up\nin the prep handler if we don't have one.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc4",
        "release_date": "2019-12-20 09:55:33 -0700 io_uring: read 'count' for IORING_OP_TIMEOUT in prep handler"
    },
    {
        "commit": "e47293fdf98998292a89d516c8f7b8b9eb5c5213",
        "message": "Add struct io_sr_msg in our io_kiocb per-command union, and ensure that\nthe send/recvmsg prep handlers have grabbed what they need from the SQE\nby the time prep is done.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc4",
        "release_date": "2019-12-20 09:55:23 -0700 io_uring: move all prep state for IORING_OP_{SEND,RECV}_MGS to prep handler"
    },
    {
        "commit": "3fbb51c18f5c15a23db74c4da79d3d035176c480",
        "message": "Add struct io_connect in our io_kiocb per-command union, and ensure\nthat io_connect_prep() has grabbed what it needs from the SQE.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc4",
        "release_date": "2019-12-20 09:52:48 -0700 io_uring: move all prep state for IORING_OP_CONNECT to prep handler"
    },
    {
        "commit": "9adbd45d6d32ffc1a03f3c51d72cfc69ebfc2ddb",
        "message": "Put the kiocb in struct io_rw, and add the addr/len for the request as\nwell. Use the kiocb->private field for the buffer index for fixed reads\nand writes.\n\nAny use of kiocb->ki_filp is flipped to req->file. It's the same thing,\nand less confusing.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc4",
        "release_date": "2019-12-20 09:52:45 -0700 io_uring: add and use struct io_rw for read/writes"
    },
    {
        "commit": "d55e5f5b70dd6214ef81fb2313121b72a7dd2200",
        "message": "We use it in some spots, but not consistently. Convert the rest over,\nmakes it easier to read as well.\n\nNo functional changes in this patch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc4",
        "release_date": "2019-12-20 08:36:50 -0700 io_uring: use u64_to_user_ptr() consistently"
    },
    {
        "commit": "fd6c2e4c063d64511657ad0031a1677b6a914859",
        "message": "I've been chasing a weird and obscure crash that was userspace stack\ncorruption, and finally narrowed it down to a bit flip that made a\nstack address invalid. io_wq_submit_work() unconditionally flips\nthe req->rw.ki_flags IOCB_NOWAIT bit, but since it's a generic work\nhandler, this isn't valid. Normal read/write operations own that\npart of the request, on other types it could be something else.\n\nMove the IOCB_NOWAIT clear to the read/write handlers where it belongs.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc3",
        "release_date": "2019-12-18 12:19:41 -0700 io_uring: io_wq_submit_work() should not touch req->rw"
    },
    {
        "commit": "7c504e65206a4379ff38fe41d21b32b6c2c3e53e",
        "message": "There is no reliable way to submit and wait in a single syscall, as\nio_submit_sqes() may under-consume sqes (in case of an early error).\nThen it will wait for not-yet-submitted requests, deadlocking the user\nin most cases.\n\nDon't wait/poll if can't submit all sqes\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc3",
        "release_date": "2019-12-18 10:01:49 -0700 io_uring: don't wait when under-submitting"
    },
    {
        "commit": "e781573e2fb1b75acdba61dcb9bcbfc16f288442",
        "message": "Now that we have all the opcodes handled in terms of command prep and\nSQE reuse, add a printk_once() to warn about any potentially new and\nunhandled ones.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc3",
        "release_date": "2019-12-17 19:57:27 -0700 io_uring: warn about unhandled opcode"
    },
    {
        "commit": "d625c6ee4975000140c57da7e1ff244efefde274",
        "message": "If we defer a request, we can't be reading the opcode again. Ensure that\nthe user_data and opcode fields are stable. For the user_data we already\nhave a place for it, for the opcode we can fill a one byte hold and store\nthat as well. For both of them, assign them when we originally read the\nSQE in io_get_sqring(). Any code that uses sqe->opcode or sqe->user_data\nis switched to req->opcode and req->user_data.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc3",
        "release_date": "2019-12-17 19:57:27 -0700 io_uring: read opcode and user_data from SQE exactly once"
    },
    {
        "commit": "b29472ee7b53784f44011069fad15e539fd25bcf",
        "message": "If we defer this command as part of a link, we have to make sure that\nthe SQE data has been read upfront. Integrate the timeout remove op into\nthe prep handling to make it safe for SQE reuse.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc3",
        "release_date": "2019-12-17 19:57:27 -0700 io_uring: make IORING_OP_TIMEOUT_REMOVE deferrable"
    },
    {
        "commit": "fbf23849b1724d3ea362e346d0877a8d87978fe6",
        "message": "If we defer this command as part of a link, we have to make sure that\nthe SQE data has been read upfront. Integrate the async cancel op into\nthe prep handling to make it safe for SQE reuse.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc3",
        "release_date": "2019-12-17 19:57:27 -0700 io_uring: make IORING_OP_CANCEL_ASYNC deferrable"
    },
    {
        "commit": "0969e783e3a8913f79df27286501a6c21e961524",
        "message": "If we defer these commands as part of a link, we have to make sure that\nthe SQE data has been read upfront. Integrate the poll add/remove into\nthe prep handling to make it safe for SQE reuse.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc3",
        "release_date": "2019-12-17 19:57:27 -0700 io_uring: make IORING_POLL_ADD and IORING_POLL_REMOVE deferrable"
    },
    {
        "commit": "ffbb8d6b76910d4f3a2bafeaf68c419011e98d05",
        "message": "The rules are as follows, if IOSQE_IO_HARDLINK is specified, then it's a\nlink and there is no need to set IOSQE_IO_LINK separately, though it\ncould be there. Add proper check and ensure that IOSQE_IO_HARDLINK\nimplies IOSQE_IO_LINK.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc3",
        "release_date": "2019-12-17 19:57:27 -0700 io_uring: make HARDLINK imply LINK"
    },
    {
        "commit": "8ed8d3c3bc32bf5b442c9f54013b4a47d5cae740",
        "message": "We're currently not retaining sqe data for accept, fsync, and\nsync_file_range. None of these commands need data outside of what\nis directly provided, hence it can't go stale when the request is\ndeferred. However, it can get reused, if an application reuses\nSQE entries.\n\nEnsure that we retain the information we need and only read the sqe\ncontents once, off the submission path. Most of this is just moving\ncode into a prep and finish function.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc3",
        "release_date": "2019-12-17 19:57:20 -0700 io_uring: any deferred command must have stable sqe data"
    },
    {
        "commit": "fc4df999e24fc3006441acd4ce6250e6a76ac851",
        "message": "We pass in req->sqe for all of them, no need to pass it in as the\nrequest is always passed in. This is a necessary prep patch to be\nable to cleanup/fix the request prep path.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc3",
        "release_date": "2019-12-17 19:57:20 -0700 io_uring: remove 'sqe' parameter to the OP helpers that take it"
    },
    {
        "commit": "b7bb4f7da0a1a92f142697f1c9ce335e7a44f4b1",
        "message": "Some of these code paths assume that any force_nonblock == true issue\nis not prepped, but that's not true if we did prep as part of link setup\nearlier. Check if we already have an async context allocate before\nsetting up a new one.\n\nCleanup the async context setup in general, we have a lot of duplicated\ncode there.\n\nFixes: 03b1230ca12a (\"io_uring: ensure async punted sendmsg/recvmsg requests copy data\")\nFixes: f67676d160c6 (\"io_uring: ensure async punted read/write requests copy iovec\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc3",
        "release_date": "2019-12-17 19:57:20 -0700 io_uring: fix pre-prepped issue with force_nonblock == true"
    },
    {
        "commit": "c58c1f83436b501d45d4050fd1296d71a9760bcb",
        "message": "Non-mq devs do not honor REQ_NOWAIT so give a chance to the caller to repeat\nrequest gracefully on -EAGAIN error.\n\nThe problem is well reproduced using io_uring:\n\n   mkfs.ext4 /dev/ram0\n   mount /dev/ram0 /mnt\n\n   # Preallocate a file\n   dd if=/dev/zero of=/mnt/file bs=1M count=1\n\n   # Start fio with io_uring and get -EIO\n   fio --rw=write --ioengine=io_uring --size=1M --direct=1 --name=job --filename=/mnt/file\n\nSigned-off-by: Roman Penyaev <rpenyaev@suse.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc3",
        "release_date": "2019-12-17 09:01:43 -0700 block: end bio with BLK_STS_AGAIN in case of non-mq devs and REQ_NOWAIT"
    },
    {
        "commit": "0b416c3e1345fd696db4c422643468d844410877",
        "message": "If we have to punt the recvmsg to async context, we copy all the\ncontext.  But since the iovec used can be either on-stack (if small) or\ndynamically allocated, if it's on-stack, then we need to ensure we reset\nthe iov pointer. If we don't, then we're reusing old stack data, and\nthat can lead to -EFAULTs if things get overwritten.\n\nEnsure we retain the right pointers for the iov, and free it as well if\nwe end up having to go beyond UIO_FASTIOV number of vectors.\n\nFixes: 03b1230ca12a (\"io_uring: ensure async punted sendmsg/recvmsg requests copy data\")\nReported-by: \u674e\u901a\u6d32 <carter.li@eoitek.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc3",
        "release_date": "2019-12-15 22:12:47 -0700 io_uring: fix sporadic -EFAULT from IORING_OP_RECVMSG"
    },
    {
        "commit": "d195a66e367b3d24fdd3c3565f37ab7c6882b9d2",
        "message": "- Fix a few typos found while reading the code.\n\n- Fix stale io_get_sqring comment referencing s->sqe, the 's' parameter\n  was renamed to 'req', but the comment still holds.\n\nSigned-off-by: Brian Gianforcaro <b.gianfo@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc3",
        "release_date": "2019-12-15 14:49:30 -0700 io_uring: fix stale comment and a few typos"
    },
    {
        "commit": "5bd831a469e0c8368fe02b21157ea5c731f84967",
        "message": "Pull io_uring fixes from Jens Axboe:\n\n - A tweak to IOSQE_IO_LINK (also marked for stable) to allow links that\n   don't sever if the result is < 0.\n\n   This is mostly for linked timeouts, where if we ask for a pure\n   timeout we always get -ETIME. This makes links useless for that case,\n   hence allow a case where it works.\n\n - Five minor optimizations to fix and improve cases that regressed\n   since v5.4.\n\n - An SQTHREAD locking fix.\n\n - A sendmsg/recvmsg iov assignment fix.\n\n - Net fix where read_iter/write_iter don't honor IOCB_NOWAIT, and\n   subsequently ensuring that works for io_uring.\n\n - Fix a case where for an invalid opcode we might return -EBADF instead\n   of -EINVAL, if the ->fd of that sqe was set to an invalid fd value.\n\n* tag 'io_uring-5.5-20191212' of git://git.kernel.dk/linux-block:\n  io_uring: ensure we return -EINVAL on unknown opcode\n  io_uring: add sockets to list of files that support non-blocking issue\n  net: make socket read/write_iter() honor IOCB_NOWAIT\n  io_uring: only hash regular files for async work execution\n  io_uring: run next sqe inline if possible\n  io_uring: don't dynamically allocate poll data\n  io_uring: deferred send/recvmsg should assign iov\n  io_uring: sqthread should grab ctx->uring_lock for submissions\n  io-wq: briefly spin for new work after finishing work\n  io-wq: remove worker->wait waitqueue\n  io_uring: allow unbreakable links",
        "kernel_version": "v5.5-rc2",
        "release_date": "2019-12-13 14:24:54 -0800 Merge tag 'io_uring-5.5-20191212' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "9e3aa61ae3e01ce1ce6361a41ef725e1f4d1d2bf",
        "message": "If we submit an unknown opcode and have fd == -1, io_op_needs_file()\nwill return true as we default to needing a file. Then when we go and\nassign the file, we find the 'fd' invalid and return -EBADF. We really\nshould be returning -EINVAL for that case, as we normally do for\nunsupported opcodes.\n\nChange io_op_needs_file() to have the following return values:\n\n0   - does not need a file\n1   - does need a file\n< 0 - error value\n\nand use this to pass back the right value for this invalid case.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc2",
        "release_date": "2019-12-11 16:02:32 -0700 io_uring: ensure we return -EINVAL on unknown opcode"
    },
    {
        "commit": "10d59345578a116042c1a5d737a18234aaf3e0e6",
        "message": "In chasing a performance issue between using IORING_OP_RECVMSG and\nIORING_OP_READV on sockets, tracing showed that we always punt the\nsocket reads to async offload. This is due to io_file_supports_async()\nnot checking for S_ISSOCK on the inode. Since sockets supports the\nO_NONBLOCK (or MSG_DONTWAIT) flag just fine, add sockets to the list\nof file types that we can do a non-blocking issue to.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc2",
        "release_date": "2019-12-10 16:33:23 -0700 io_uring: add sockets to list of files that support non-blocking issue"
    },
    {
        "commit": "ebfcd8955c0b52eb793bcbc9e71140e3d0cdb228",
        "message": "The socket read/write helpers only look at the file O_NONBLOCK. not\nthe iocb IOCB_NOWAIT flag. This breaks users like preadv2/pwritev2\nand io_uring that rely on not having the file itself marked nonblocking,\nbut rather the iocb itself.\n\nCc: netdev@vger.kernel.org\nAcked-by: David Miller <davem@davemloft.net>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc2",
        "release_date": "2019-12-10 16:33:23 -0700 net: make socket read/write_iter() honor IOCB_NOWAIT"
    },
    {
        "commit": "53108d476a105ab2597d7a4e6040b127829391b5",
        "message": "We hash regular files to avoid having multiple threads hammer on the\ninode mutex, but it should not be needed on other types of files\n(like sockets).\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc2",
        "release_date": "2019-12-10 16:33:23 -0700 io_uring: only hash regular files for async work execution"
    },
    {
        "commit": "4a0a7a187453e65bdd24b9ede045b4c36b958868",
        "message": "One major use case of linked commands is the ability to run the next\nlink inline, if at all possible. This is done correctly for async\noffload, but somewhere along the line we lost the ability to do so when\nwe were able to complete a request without having to punt it. Ensure\nthat we do so correctly.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc2",
        "release_date": "2019-12-10 16:33:23 -0700 io_uring: run next sqe inline if possible"
    },
    {
        "commit": "392edb45b24337eaa0bc1ecd4e3cf897e662ec61",
        "message": "This essentially reverts commit e944475e6984. For high poll ops\nworkloads, like TAO, the dynamic allocation of the wait_queue\nentry for IORING_OP_POLL_ADD adds considerable extra overhead.\nGo back to embedding the wait_queue_entry, but keep the usage of\nwait->private for the pointer stashing.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc2",
        "release_date": "2019-12-10 16:33:23 -0700 io_uring: don't dynamically allocate poll data"
    },
    {
        "commit": "d96885658d9971fc2c752b8699f17a42ef745db6",
        "message": "Don't just assign it from the main call path, that can miss the case\nwhen we're called from issue deferral.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc2",
        "release_date": "2019-12-10 16:33:23 -0700 io_uring: deferred send/recvmsg should assign iov"
    },
    {
        "commit": "8a4955ff1cca7d4da480774034a16e7c28bafec8",
        "message": "We use the mutex to guard against registered file updates, for instance.\nEnsure we're safe in accessing that state against concurrent updates.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc2",
        "release_date": "2019-12-10 16:33:23 -0700 io_uring: sqthread should grab ctx->uring_lock for submissions"
    },
    {
        "commit": "4e88d6e7793f2f445f43bd608828541d7f43b608",
        "message": "Some commands will invariably end in a failure in the sense that the\ncompletion result will be less than zero. One such example is timeouts\nthat don't have a completion count set, they will always complete with\n-ETIME unless cancelled.\n\nFor linked commands, we sever links and fail the rest of the chain if\nthe result is less than zero. Since we have commands where we know that\nwill happen, add IOSQE_IO_HARDLINK as a stronger link that doesn't sever\nregardless of the completion result. Note that the link will still sever\nif we fail submitting the parent request, hard links are only resilient\nin the presence of completion results for requests that did submit\ncorrectly.\n\nCc: stable@vger.kernel.org # v5.4\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nReported-by: \u674e\u901a\u6d32 <carter.li@eoitek.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc2",
        "release_date": "2019-12-10 16:33:06 -0700 io_uring: allow unbreakable links"
    },
    {
        "commit": "9feb1af97e7366b512ecb9e4dd61d3252074cda3",
        "message": "Pull more block and io_uring updates from Jens Axboe:\n \"I wasn't expecting this to be so big, and if I was, I would have used\n  separate branches for this. Going forward I'll be doing separate\n  branches for the current tree, just like for the next kernel version\n  tree. In any case, this contains:\n\n   - Series from Christoph that fixes an inherent race condition with\n     zoned devices and revalidation.\n\n   - null_blk zone size fix (Damien)\n\n   - Fix for a regression in this merge window that caused busy spins by\n     sending empty disk uevents (Eric)\n\n   - Fix for a regression in this merge window for bfq stats (Hou)\n\n   - Fix for io_uring creds allocation failure handling (me)\n\n   - io_uring -ERESTARTSYS send/recvmsg fix (me)\n\n   - Series that fixes the need for applications to retain state across\n     async request punts for io_uring. This one is a bit larger than I\n     would have hoped, but I think it's important we get this fixed for\n     5.5.\n\n   - connect(2) improvement for io_uring, handling EINPROGRESS instead\n     of having applications needing to poll for it (me)\n\n   - Have io_uring use a hash for poll requests instead of an rbtree.\n     This turned out to work much better in practice, so I think we\n     should make the switch now. For some workloads, even with a fair\n     amount of cancellations, the insertion sort is just too expensive.\n     (me)\n\n   - Various little io_uring fixes (me, Jackie, Pavel, LimingWu)\n\n   - Fix for brd unaligned IO, and a warning for the future (Ming)\n\n   - Fix for a bio integrity data leak (Justin)\n\n   - bvec_iter_advance() improvement (Pavel)\n\n   - Xen blkback page unmap fix (SeongJae)\n\n  The major items in here are all well tested, and on the liburing side\n  we continue to add regression and feature test cases. We're up to 50\n  topic cases now, each with anywhere from 1 to more than 10 cases in\n  each\"\n\n* tag 'for-linus-20191205' of git://git.kernel.dk/linux-block: (33 commits)\n  block: fix memleak of bio integrity data\n  io_uring: fix a typo in a comment\n  bfq-iosched: Ensure bio->bi_blkg is valid before using it\n  io_uring: hook all linked requests via link_list\n  io_uring: fix error handling in io_queue_link_head\n  io_uring: use hash table for poll command lookups\n  io-wq: clear node->next on list deletion\n  io_uring: ensure deferred timeouts copy necessary data\n  io_uring: allow IO_SQE_* flags on IORING_OP_TIMEOUT\n  null_blk: remove unused variable warning on !CONFIG_BLK_DEV_ZONED\n  brd: warn on un-aligned buffer\n  brd: remove max_hw_sectors queue limit\n  xen/blkback: Avoid unmapping unmapped grant pages\n  io_uring: handle connect -EINPROGRESS like -EAGAIN\n  block: set the zone size in blk_revalidate_disk_zones atomically\n  block: don't handle bio based drivers in blk_revalidate_disk_zones\n  block: allocate the zone bitmaps lazily\n  block: replace seq_zones_bitmap with conv_zones_bitmap\n  block: simplify blkdev_nr_zones\n  block: remove the empty line at the end of blk-zoned.c\n  ...",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-06 10:08:59 -0800 Merge tag 'for-linus-20191205' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "8539429917c48c994d2e2cafa02ab06587b3b42c",
        "message": "* io_uring-5.5:\n  io_uring: fix a typo in a comment\n  io_uring: hook all linked requests via link_list\n  io_uring: fix error handling in io_queue_link_head\n  io_uring: use hash table for poll command lookups",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-05 19:09:26 -0700 Merge branch 'io_uring-5.5' into for-linus"
    },
    {
        "commit": "0b4295b5e2b9b42f3f3096496fe4775b656c9ba6",
        "message": "thatn -> than.\n\nSigned-off-by: Liming Wu <19092205@suning.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-05 07:59:37 -0700 io_uring: fix a typo in a comment"
    },
    {
        "commit": "4493233edcfc0ad0a7f76f1c83f95b1bcf280547",
        "message": "Links are created by chaining requests through req->list with an\nexception that head uses req->link_list. (e.g. link_list->list->list)\nBecause of that, io_req_link_next() needs complex splicing to advance.\n\nLink them all through list_list. Also, it seems to be simpler and more\nconsistent IMHO.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-05 06:54:52 -0700 io_uring: hook all linked requests via link_list"
    },
    {
        "commit": "2e6e1fde32d7d41cf076c21060c329d3fdbce25c",
        "message": "In case of an error io_submit_sqe() drops a request and continues\nwithout it, even if the request was a part of a link. Not only it\ndoesn't cancel links, but also may execute wrong sequence of actions.\n\nStop consuming sqes, and let the user handle errors.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-05 06:54:51 -0700 io_uring: fix error handling in io_queue_link_head"
    },
    {
        "commit": "78076bb64aa8ba5b7207c38b2660a9e10ffa8cc7",
        "message": "We recently changed this from a single list to an rbtree, but for some\nreal life workloads, the rbtree slows down the submission/insertion\ncase enough so that it's the top cycle consumer on the io_uring side.\nIn testing, using a hash table is a more well rounded compromise. It\nis fast for insertion, and as long as it's sized appropriately, it\nworks well for the cancellation case as well. Running TAO with a lot\nof network sockets, this removes io_poll_req_insert() from spending\n2% of the CPU cycles.\n\nReported-by: Dan Melnic <dmm@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-04 20:12:58 -0700 io_uring: use hash table for poll command lookups"
    },
    {
        "commit": "2d28390aff879238f00e209e38c2a0b78717360e",
        "message": "If we defer a timeout, we should ensure that we copy the timespec\nwhen we have consumed the sqe. This is similar to commit f67676d160c6\nfor read/write requests. We already did this correctly for timeouts\ndeferred as links, but do it generally and use the infrastructure added\nby commit 1a6b74fc8702 instead of having the timeout deferral use its\nown.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-04 11:12:08 -0700 io_uring: ensure deferred timeouts copy necessary data"
    },
    {
        "commit": "901e59bba9ddad4bc6994ecb8598ea60a993da4c",
        "message": "There's really no reason why we forbid things like link/drain etc on\nregular timeout commands. Enable the usual SQE flags on timeouts.\n\nReported-by: \u674e\u901a\u6d32 <carter.li@eoitek.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-04 10:34:03 -0700 io_uring: allow IO_SQE_* flags on IORING_OP_TIMEOUT"
    },
    {
        "commit": "87f80d623c6c93c721b2aaead8a45e848bc8ffbf",
        "message": "Right now we return it to userspace, which means the application has\nto poll for the socket to be writeable. Let's just treat it like\n-EAGAIN and have io_uring handle it internally, this makes it much\neasier to use.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-03 11:23:54 -0700 io_uring: handle connect -EINPROGRESS like -EAGAIN"
    },
    {
        "commit": "8cdda87a4414092cd210e766189cf0353a844861",
        "message": "Since commit b18fdf71e01f (\"io_uring: simplify io_req_link_next()\"),\nthe io_wq_current_is_worker function is no longer needed, clean it\nup.\n\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-03 07:04:32 -0700 io_uring: remove io_wq_current_is_worker"
    },
    {
        "commit": "22efde5998657f6d1f31592c659aa3a9c7ad65f1",
        "message": "Parameter ctx we have never used, clean it up.\n\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-03 07:04:32 -0700 io_uring: remove parameter ctx of io_submit_state_start"
    },
    {
        "commit": "da8c96906990f1108cb626ee7865e69267a3263b",
        "message": "If this flag is set, applications can be certain that any data for\nasync offload has been consumed when the kernel has consumed the\nSQE.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-03 07:04:32 -0700 io_uring: mark us with IORING_FEAT_SUBMIT_STABLE"
    },
    {
        "commit": "f499a021ea8c9f70321fce3d674d8eca5bbeee2c",
        "message": "Just like commit f67676d160c6 for read/write requests, this one ensures\nthat the sockaddr data has been copied for IORING_OP_CONNECT if we need\nto punt the request to async context.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-03 07:04:30 -0700 io_uring: ensure async punted connect requests copy data"
    },
    {
        "commit": "03b1230ca12a12e045d83b0357792075bf94a1e0",
        "message": "Just like commit f67676d160c6 for read/write requests, this one ensures\nthat the msghdr data is fully copied if we need to punt a recvmsg or\nsendmsg system call to async context.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-03 07:03:35 -0700 io_uring: ensure async punted sendmsg/recvmsg requests copy data"
    },
    {
        "commit": "f67676d160c6ee2ed82917fadfed6d29cab8237c",
        "message": "Currently we don't copy the iovecs when we punt to async context. This\ncan be problematic for applications that store the iovec on the stack,\nas they often assume that it's safe to let the iovec go out of scope\nas soon as IO submission has been called. This isn't always safe, as we\nwill re-copy the iovec once we're in async context.\n\nMake this 100% safe by copying the iovec just once. With this change,\napplications may safely store the iovec on the stack for all cases.\n\nReported-by: \u674e\u901a\u6d32 <carter.li@eoitek.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-02 21:33:25 -0700 io_uring: ensure async punted read/write requests copy iovec"
    },
    {
        "commit": "1a6b74fc87024db59d41cd7346bd437f20fb3e2d",
        "message": "Right now we just copy the sqe for async offload, but we want to store\nmore context across an async punt. In preparation for doing so, put the\nsqe copy inside a structure that we can expand. With this pointer added,\nwe can get rid of REQ_F_FREE_SQE, as that is now indicated by whether\nreq->io is NULL or not.\n\nNo functional changes in this patch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-02 18:49:33 -0700 io_uring: add general async offload context"
    },
    {
        "commit": "441cdbd5449b4923cd413d3ba748124f91388be9",
        "message": "We should never return -ERESTARTSYS to userspace, transform it into\n-EINTR.\n\nCc: stable@vger.kernel.org # v5.3+\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-02 18:49:10 -0700 io_uring: transform send/recvmsg() -ERESTARTSYS to -EINTR"
    },
    {
        "commit": "0b8c0ec7eedcd8f9f1a1f238d87f9b512b09e71a",
        "message": "syzbot reports:\n\nkasan: CONFIG_KASAN_INLINE enabled\nkasan: GPF could be caused by NULL-ptr deref or user memory access\ngeneral protection fault: 0000 [#1] PREEMPT SMP KASAN\nCPU: 0 PID: 9217 Comm: io_uring-sq Not tainted 5.4.0-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS\nGoogle 01/01/2011\nRIP: 0010:creds_are_invalid kernel/cred.c:792 [inline]\nRIP: 0010:__validate_creds include/linux/cred.h:187 [inline]\nRIP: 0010:override_creds+0x9f/0x170 kernel/cred.c:550\nCode: ac 25 00 81 fb 64 65 73 43 0f 85 a3 37 00 00 e8 17 ab 25 00 49 8d 7c\n24 10 48 b8 00 00 00 00 00 fc ff df 48 89 fa 48 c1 ea 03 <0f> b6 04 02 84\nc0 74 08 3c 03 0f 8e 96 00 00 00 41 8b 5c 24 10 bf\nRSP: 0018:ffff88809c45fda0 EFLAGS: 00010202\nRAX: dffffc0000000000 RBX: 0000000043736564 RCX: ffffffff814f3318\nRDX: 0000000000000002 RSI: ffffffff814f3329 RDI: 0000000000000010\nRBP: ffff88809c45fdb8 R08: ffff8880a3aac240 R09: ffffed1014755849\nR10: ffffed1014755848 R11: ffff8880a3aac247 R12: 0000000000000000\nR13: ffff888098ab1600 R14: 0000000000000000 R15: 0000000000000000\nFS:  0000000000000000(0000) GS:ffff8880ae800000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00007ffd51c40664 CR3: 0000000092641000 CR4: 00000000001406f0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\nCall Trace:\n  io_sq_thread+0x1c7/0xa20 fs/io_uring.c:3274\n  kthread+0x361/0x430 kernel/kthread.c:255\n  ret_from_fork+0x24/0x30 arch/x86/entry/entry_64.S:352\nModules linked in:\n---[ end trace f2e1a4307fbe2245 ]---\nRIP: 0010:creds_are_invalid kernel/cred.c:792 [inline]\nRIP: 0010:__validate_creds include/linux/cred.h:187 [inline]\nRIP: 0010:override_creds+0x9f/0x170 kernel/cred.c:550\nCode: ac 25 00 81 fb 64 65 73 43 0f 85 a3 37 00 00 e8 17 ab 25 00 49 8d 7c\n24 10 48 b8 00 00 00 00 00 fc ff df 48 89 fa 48 c1 ea 03 <0f> b6 04 02 84\nc0 74 08 3c 03 0f 8e 96 00 00 00 41 8b 5c 24 10 bf\nRSP: 0018:ffff88809c45fda0 EFLAGS: 00010202\nRAX: dffffc0000000000 RBX: 0000000043736564 RCX: ffffffff814f3318\nRDX: 0000000000000002 RSI: ffffffff814f3329 RDI: 0000000000000010\nRBP: ffff88809c45fdb8 R08: ffff8880a3aac240 R09: ffffed1014755849\nR10: ffffed1014755848 R11: ffff8880a3aac247 R12: 0000000000000000\nR13: ffff888098ab1600 R14: 0000000000000000 R15: 0000000000000000\nFS:  0000000000000000(0000) GS:ffff8880ae800000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00007ffd51c40664 CR3: 0000000092641000 CR4: 00000000001406f0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n\nwhich is caused by slab fault injection triggering a failure in\nprepare_creds(). We don't actually need to create a copy of the creds\nas we're not modifying it, we just need a reference on the current task\ncreds. This avoids the failure case as well, and propagates the const\nthroughout the stack.\n\nFixes: 181e448d8709 (\"io_uring: async workers should inherit the user creds\")\nReported-by: syzbot+5320383e16029ba057ff@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-02 08:50:00 -0700 io_uring: use current task creds instead of allocating a new one"
    },
    {
        "commit": "31764f1b6d93e126ed341fb600d2765d630e8bf6",
        "message": "Pull block fixes from Jens Axboe:\n \"I wasn't going to send this one off so soon, but unfortunately one of\n  the fixes from the previous pull broke the build on some archs. So I'm\n  sending this sooner rather than later. This contains:\n\n   - Add highmem.h include for io_uring, because of the kmap() additions\n     from last round. For some reason the build bot didn't spot this\n     even though it sat for days.\n\n   - Three minor ';' removals\n\n   - Add support for the Beurer CD-on-a-chip device\n\n   - Make io_uring work on MMU-less archs\"\n\n* tag 'for-linus-20191129' of git://git.kernel.dk/linux-block:\n  io_uring: fix missing kmap() declaration on powerpc\n  ataflop: Remove unneeded semicolon\n  block: sunvdc: Remove unneeded semicolon\n  drbd: Remove unneeded semicolon\n  io_uring: add mapping support for NOMMU archs\n  sr_vendor: support Beurer GL50 evo CD-on-a-chip devices.\n  cdrom: respect device capabilities during opening action",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-12-01 18:26:56 -0800 Merge tag 'for-linus-20191129' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "aa4c3967756c6c576a38a23ac511be211462a6b7",
        "message": "Christophe reports that current master fails building on powerpc with\nthis error:\n\n   CC      fs/io_uring.o\nfs/io_uring.c: In function \u2018loop_rw_iter\u2019:\nfs/io_uring.c:1628:21: error: implicit declaration of function \u2018kmap\u2019\n[-Werror=implicit-function-declaration]\n     iovec.iov_base = kmap(iter->bvec->bv_page)\n                      ^\nfs/io_uring.c:1628:19: warning: assignment makes pointer from integer\nwithout a cast [-Wint-conversion]\n     iovec.iov_base = kmap(iter->bvec->bv_page)\n                    ^\nfs/io_uring.c:1643:4: error: implicit declaration of function \u2018kunmap\u2019\n[-Werror=implicit-function-declaration]\n     kunmap(iter->bvec->bv_page);\n     ^\n\nwhich is caused by a missing highmem.h include. Fix it by including\nit.\n\nFixes: 311ae9e159d8 (\"io_uring: fix dead-hung for non-iter fixed rw\")\nReported-by: Christophe Leroy <christophe.leroy@c-s.fr>\nTested-by: Christophe Leroy <christophe.leroy@c-s.fr>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-29 10:14:00 -0700 io_uring: fix missing kmap() declaration on powerpc"
    },
    {
        "commit": "05bd375b6bdede3748023e130990c9b6214fd46a",
        "message": "Pull more io_uring updates from Jens Axboe:\n \"As mentioned in the first pull request, there was a later batch as\n  well. This contains fixes to the stuff that already went in, cleanups,\n  and a few later additions. In particular, this contains:\n\n   - Cleanups/fixes/unification of the submission and completion path\n     (Pavel,me)\n\n   - Linked timeouts improvements (Pavel,me)\n\n   - Error path fixes (me)\n\n   - Fix lookup window where cancellations wouldn't work (me)\n\n   - Improve DRAIN support (Pavel)\n\n   - Fix backlog flushing -EBUSY on submit (me)\n\n   - Add support for connect(2) (me)\n\n   - Fix for non-iter based fixed IO (Pavel)\n\n   - creds inheritance for async workers (me)\n\n   - Disable cmsg/ancillary data for sendmsg/recvmsg (me)\n\n   - Shrink io_kiocb to 3 cachelines (me)\n\n   - NUMA fix for io-wq (Jann)\"\n\n* tag 'for-5.5/io_uring-post-20191128' of git://git.kernel.dk/linux-block: (42 commits)\n  io_uring: make poll->wait dynamically allocated\n  io-wq: shrink io_wq_work a bit\n  io-wq: fix handling of NUMA node IDs\n  io_uring: use kzalloc instead of kcalloc for single-element allocations\n  io_uring: cleanup io_import_fixed()\n  io_uring: inline struct sqe_submit\n  io_uring: store timeout's sqe->off in proper place\n  net: disallow ancillary data for __sys_{send,recv}msg_file()\n  net: separate out the msghdr copy from ___sys_{send,recv}msg()\n  io_uring: remove superfluous check for sqe->off in io_accept()\n  io_uring: async workers should inherit the user creds\n  io-wq: have io_wq_create() take a 'data' argument\n  io_uring: fix dead-hung for non-iter fixed rw\n  io_uring: add support for IORING_OP_CONNECT\n  net: add __sys_connect_file() helper\n  io_uring: only return -EBUSY for submit on non-flushed backlog\n  io_uring: only !null ptr to io_issue_sqe()\n  io_uring: simplify io_req_link_next()\n  io_uring: pass only !null to io_req_find_next()\n  io_uring: remove io_free_req_find_next()\n  ...",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-28 10:43:39 -0800 Merge tag 'for-5.5/io_uring-post-20191128' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "6c5c240e412682f97aecd233c1e706822704aa28",
        "message": "That is a bit weird scenario but I find it interesting to run fio loads\nusing LKL linux, where MMU is disabled.  Probably other real archs which\nrun uClinux can also benefit from this patch.\n\nSigned-off-by: Roman Penyaev <rpenyaev@suse.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-28 10:08:02 -0700 io_uring: add mapping support for NOMMU archs"
    },
    {
        "commit": "e944475e69849273ca8f1fe04a3ce81b5901d165",
        "message": "In the quest to bring io_kiocb down to 3 cachelines, this one does\nthe trick. Make the wait_queue_entry for the poll command come out\nof kmalloc instead of embedding it in struct io_poll_iocb, as the\nlatter is the largest member of io_kiocb. Once we trim this down a\nbit, we're back at a healthy 192 bytes for struct io_kiocb.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-26 15:02:56 -0700 io_uring: make poll->wait dynamically allocated"
    },
    {
        "commit": "6206f0e180d4eddc0a178f57120ab1b913701f6e",
        "message": "Currently we're using 40 bytes for the io_wq_work structure, and 16 of\nthose is the doubly link list node. We don't need doubly linked lists,\nwe always add to tail to keep things ordered, and any other use case\nis list traversal with deletion. For the deletion case, we can easily\nsupport any node deletion by keeping track of the previous entry.\n\nThis shrinks io_wq_work to 32 bytes, and subsequently io_kiock from\nio_uring to 216 to 208 bytes.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-26 15:02:56 -0700 io-wq: shrink io_wq_work a bit"
    },
    {
        "commit": "ad6e005ca68de7af76f9ed3e4c9b6f0aa2f842e3",
        "message": "These allocations are single-element allocations, so don't use the array\nallocation wrapper for them.\n\nSigned-off-by: Jann Horn <jannh@google.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-26 15:02:56 -0700 io_uring: use kzalloc instead of kcalloc for single-element allocations"
    },
    {
        "commit": "7d009165550adc64e3561c65ecce564125052e00",
        "message": "Clean io_import_fixed() call site and make it return proper type.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-26 15:02:56 -0700 io_uring: cleanup io_import_fixed()"
    },
    {
        "commit": "cf6fd4bd559ee61a4454b161863c8de6f30f8dca",
        "message": "There is no point left in keeping struct sqe_submit. Inline it\ninto struct io_kiocb, so any req->submit.field is now just req->field\n\n- moves initialisation of ring_file into io_get_req()\n- removes duplicated req->sequence.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-26 15:02:56 -0700 io_uring: inline struct sqe_submit"
    },
    {
        "commit": "cc42e0ac17d3664a70e020dfe7897f14e7aa7453",
        "message": "Timeouts' sequence offset (i.e. sqe->off) is stored in\nreq->submit.sequence under a false name. Keep it in timeout.data\ninstead. The unused space for sequence will be reclaimed in the\nfollowing patches.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-26 15:02:56 -0700 io_uring: store timeout's sqe->off in proper place"
    },
    {
        "commit": "d69e07793f891524c6bbf1e75b9ae69db4450953",
        "message": "Only io_uring uses (and added) these, and we want to disallow the\nuse of sendmsg/recvmsg for anything but regular data transfers.\nUse the newly added prep helper to split the msghdr copy out from\nthe core function, to check for msg_control and msg_controllen\nsettings. If either is set, we return -EINVAL.\n\nAcked-by: David S. Miller <davem@davemloft.net>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-26 15:02:53 -0700 net: disallow ancillary data for __sys_{send,recv}msg_file()"
    },
    {
        "commit": "4257c8ca13b084550574b8c9a667d9c90ff746eb",
        "message": "This is in preparation for enabling the io_uring helpers for sendmsg\nand recvmsg to first copy the header for validation before continuing\nwith the operation.\n\nThere should be no functional changes in this patch.\n\nAcked-by: David S. Miller <davem@davemloft.net>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-26 15:02:41 -0700 net: separate out the msghdr copy from ___sys_{send,recv}msg()"
    },
    {
        "commit": "8042d6ce8c40df0abb0d91662a754d074a3d3f16",
        "message": "This field contains a pointer to addrlen and checking to see if it's set\nreturns -EINVAL if the caller sets addr & addrlen pointers.\n\nFixes: 17f2fe35d080 (\"io_uring: add support for IORING_OP_ACCEPT\")\nSigned-off-by: Hrvoje Zeba <zeba.hrvoje@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:11 -0700 io_uring: remove superfluous check for sqe->off in io_accept()"
    },
    {
        "commit": "181e448d8709e517c9c7b523fcd209f24eb38ca7",
        "message": "If we don't inherit the original task creds, then we can confuse users\nlike fuse that pass creds in the request header. See link below on\nidentical aio issue.\n\nLink: https://lore.kernel.org/linux-fsdevel/26f0d78e-99ca-2f1b-78b9-433088053a61@scylladb.com/T/#u\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:11 -0700 io_uring: async workers should inherit the user creds"
    },
    {
        "commit": "311ae9e159d81a1ec1cf645daf40b39ae5a0bd84",
        "message": "Read/write requests to devices without implemented read/write_iter\nusing fixed buffers can cause general protection fault, which totally\nhangs a machine.\n\nio_import_fixed() initialises iov_iter with bvec, but loop_rw_iter()\naccesses it as iovec, dereferencing random address.\n\nkmap() page by page in this case\n\nCc: stable@vger.kernel.org\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:11 -0700 io_uring: fix dead-hung for non-iter fixed rw"
    },
    {
        "commit": "f8e85cf255ad57d65eeb9a9d0e59e3dec55bdd9e",
        "message": "This allows an application to call connect() in an async fashion. Like\nother opcodes, we first try a non-blocking connect, then punt to async\ncontext if we have to.\n\nNote that we can still return -EINPROGRESS, and in that case the caller\nshould use IORING_OP_POLL_ADD to do an async wait for completion of the\nconnect request (just like for regular connect(2), except we can do it\nasync here too).\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:11 -0700 io_uring: add support for IORING_OP_CONNECT"
    },
    {
        "commit": "c4a2ed72c9a61594b6afc23e1fbc78878d32b5a3",
        "message": "We return -EBUSY on submit when we have a CQ ring overflow backlog, but\nthat can be a bit problematic if the application is using pure userspace\npoll of the CQ ring. For that case, if the ring briefly overflowed and\nwe have pending entries in the backlog, the submit flushes the backlog\nsuccessfully but still returns -EBUSY. If we're able to fully flush the\nCQ ring backlog, let the submission proceed.\n\nReported-by: Dan Melnic <dmm@fb.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:11 -0700 io_uring: only return -EBUSY for submit on non-flushed backlog"
    },
    {
        "commit": "f9bd67f69af56d712bfd498f5ad9cf7bb177d600",
        "message": "Pass only non-null @nxt to io_issue_sqe() and handle it at the caller's\nside. And propagate it.\n\n- kiocb_done() is only called from io_read() and io_write(), which are\nonly called from io_issue_sqe(), so it's @nxt != NULL\n\n- io_put_req_find_next() is called either with explicitly non-null local\nnxt, or from one of the functions in io_issue_sqe() switch (or their\ncallees).\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:11 -0700 io_uring: only !null ptr to io_issue_sqe()"
    },
    {
        "commit": "b18fdf71e01fba29a804d63f8c1e2ed61011170d",
        "message": "\"if (nxt)\" is always true, as it was checked in the while's condition.\nio_wq_current_is_worker() is unnecessary, as non-async callers don't\npass nxt, so io_queue_async_work() will be called for them anyway.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:11 -0700 io_uring: simplify io_req_link_next()"
    },
    {
        "commit": "944e58bfeda0e9b97cd611adafc823c78e0bc464",
        "message": "Make io_req_find_next() and io_req_link_next() to accept only non-null\nnxt, and handle it in callers.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:11 -0700 io_uring: pass only !null to io_req_find_next()"
    },
    {
        "commit": "70cf9f3270a5c5148e93a526dc1e51965259e70c",
        "message": "There is only one one-liner user of io_free_req_find_next(). Inline it.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:11 -0700 io_uring: remove io_free_req_find_next()"
    },
    {
        "commit": "9835d6fafba58e6d9386a6d5af800789bdb52e5b",
        "message": "The number of SQEs to submit is specified by a user, so io_get_sqring()\nin most of the cases succeeds. Hint compilers about that.\n\nChecking ASM genereted by gcc 9.2.0 for x64, there is one branch\nmisprediction.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:10 -0700 io_uring: add likely/unlikely in io_get_sqring()"
    },
    {
        "commit": "d732447fed7d6b4c22907f630cd25d574bae5276",
        "message": "__io_submit_sqe() is issuing requests, so call it as\nsuch. Moreover, it ends by calling io_iopoll_req_issued().\n\nRename it and make terminology clearer.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:10 -0700 io_uring: rename __io_submit_sqe()"
    },
    {
        "commit": "915967f69c591b34c5a18d6618af021a81ffd700",
        "message": "We don't have shadow requests anymore, so get rid of the shadow\nargument. Add the user_data argument, as that's often useful to easily\nmatch up requests, instead of having to look at request pointers.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:10 -0700 io_uring: improve trace_io_uring_defer() trace point"
    },
    {
        "commit": "1b4a51b6d03d21f55effbcf609ba5526d87d9e9d",
        "message": "There's an issue with the shadow drain logic in that we drop the\ncompletion lock after deciding to defer a request, then re-grab it later\nand assume that the state is still the same. In the mean time, someone\nelse completing a request could have found and issued it. This can cause\na stall in the queue, by having a shadow request inserted that nobody is\ngoing to drain.\n\nAdditionally, if we fail allocating the shadow request, we simply ignore\nthe drain.\n\nInstead of using a shadow request, defer the next request/link instead.\nThis also has the following advantages:\n\n- removes semi-duplicated code\n- doesn't allocate memory for shadows\n- works better if only the head marked for drain\n- doesn't need complex synchronisation\n\nOn the flip side, it removes the shadow->seq ==\nlast_drain_in_in_link->seq optimization. That shouldn't be a common\ncase, and can always be added back, if needed.\n\nFixes: 4fe2c963154c (\"io_uring: add support for link with drain\")\nCc: Jackie Liu <liuyun01@kylinos.cn>\nReported-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:10 -0700 io_uring: drain next sqe instead of shadowing"
    },
    {
        "commit": "b76da70fc3759df13e0991706451f1a2e06ba19e",
        "message": "When we find new work to process within the work handler, we queue the\nlinked timeout before we have issued the new work. This can be\nproblematic for very short timeouts, as we have a window where the new\nwork isn't visible.\n\nAllow the work handler to store a callback function for this in the work\nitem, and flag it with IO_WQ_WORK_CB if the caller has done so. If that\nis set, then io-wq will call the callback when it has setup the new work\nitem.\n\nReported-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:10 -0700 io_uring: close lookup gap for dependent next work"
    },
    {
        "commit": "4d7dd462971405c65bfb3821dbb6b9ce13b5e8d6",
        "message": "We currently try and start the next link when we put the request, and\nonly if we were going to free it. This means that the optimization to\ncontinue executing requests from the same context often fails, as we're\nnot putting the final reference.\n\nAdd REQ_F_LINK_NEXT to keep track of this, and allow io_uring to find the\nnext request more efficiently.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:06 -0700 io_uring: allow finding next link independent of req reference count"
    },
    {
        "commit": "eb065d301e8c83643367bdb0898becc364046bda",
        "message": "We currently rely on the ring destroy on cleaning things up in case of\nfailure, but io_allocate_scq_urings() can leave things half initialized\nif only parts of it fails.\n\nBe nice and return with either everything setup in success, or return an\nerror with things nicely cleaned up.\n\nReported-by: syzbot+0d818c0d39399188f393@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:06 -0700 io_uring: io_allocate_scq_urings() should return a sane state"
    },
    {
        "commit": "bbad27b2f622fa26d107f8a72c0cd5cc102dc56e",
        "message": "Always mark requests with allocated sqe and deallocate it in\n__io_free_req(). It's easier to follow and doesn't add edge cases.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:06 -0700 io_uring: Always REQ_F_FREE_SQE for allocated sqe"
    },
    {
        "commit": "5d960724b0cb0d12469d1c62912e4a8c09c9fd92",
        "message": "We currently clear the linked timeout field if we cancel such a timeout,\nbut we should only attempt to cancel if it's the first one we see.\nOthers should simply be freed like other requests, as they haven't\nbeen started yet.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:06 -0700 io_uring: io_fail_links() should only consider first linked timeout"
    },
    {
        "commit": "09fbb0a83ec6ab5a4037766261c031151985fff6",
        "message": "let have a dependant link: REQ -> LINK_TIMEOUT -> LINK_TIMEOUT\n\n1. submission stage: submission references for REQ and LINK_TIMEOUT\nare dropped. So, references respectively (1,1,2)\n\n2. io_put(REQ) + FAIL_LINKS stage: calls io_fail_links(), which for all\nlinked timeouts will call cancel_timeout() and drop 1 reference.\nSo, references after: (0,0,1). That's a leak.\n\nMake it treat only the first linked timeout as such, and pass others\nthrough __io_double_put_req().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:06 -0700 io_uring: Fix leaking linked timeouts"
    },
    {
        "commit": "f70193d6d8cad4cc614223fef349e6ea9d48c61f",
        "message": "Pass any IORING_OP_LINK_TIMEOUT request further, where it will\neventually fail in io_issue_sqe().\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:06 -0700 io_uring: remove redundant check"
    },
    {
        "commit": "d3b35796b1e3f118017491d621f624e0de7ff9fb",
        "message": "If io_req_defer() failed, it needs to cancel a dependant link.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:06 -0700 io_uring: break links for failed defer"
    },
    {
        "commit": "b60fda6000a99a7ccac36005ab78b14b47c06de3",
        "message": "We currently have a race where if setup is really slow, we can be\ncalling io_wq_destroy() before we're done setting up. This will cause\nthe caller to get stuck waiting for the manager to set things up, but\nthe manager already exited.\n\nFix this by doing a sync setup of the manager. This also fixes the case\nwhere if we failed creating workers, we'd also get stuck.\n\nIn practice this race window was really small, as we already wait for\nthe manager to start. Hence someone would have to call io_wq_destroy()\nafter the task has started, but before it started the first loop. The\nreported test case forked tons of these, which is why it became an\nissue.\n\nReported-by: syzbot+0f1cc17f85154f400465@syzkaller.appspotmail.com\nFixes: 771b53d033e8 (\"io-wq: small threadpool implementation for io_uring\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:05 -0700 io-wq: wait for io_wq_create() to setup necessary workers"
    },
    {
        "commit": "fba38c272a0385148935d6443cb9dc68cf1f37a7",
        "message": "We currently don't explicitly break links if a request is cancelled, but\nwe should. Add explicitly link breakage for all types of request\ncancellations that we support.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:05 -0700 io_uring: request cancellations should break links"
    },
    {
        "commit": "b0dd8a412699afe3420a08f841333f3474ad45c5",
        "message": "Currently a poll request fills a completion entry of 0, even if it got\ncancelled. This is odd, and it makes it harder to support with chains.\nEnsure that it returns -ECANCELED in the completions events if it got\ncancelled, and furthermore ensure that the linked timeout that triggered\nit completes with -ETIME if we did indeed trigger the completions\nthrough a timeout.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:05 -0700 io_uring: correct poll cancel and linked timeout expiration completion"
    },
    {
        "commit": "e0e328c4b330712e45ba799dc589bda751323110",
        "message": "With the conversion to io-wq, we no longer use that flag. Kill it.\n\nFixes: 561fb04a6a22 (\"io_uring: replace workqueue usage with io-wq\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:05 -0700 io_uring: remove dead REQ_F_SEQ_PREV flag"
    },
    {
        "commit": "94ae5e77a9150a8c6c57432e2db290c6868ddfad",
        "message": "We have an issue with timeout links that are deeper in the submit chain,\nbecause we only handle it upfront, not from later submissions. Move the\nprep + issue of the timeout link to the async work prep handler, and do\nit normally for non-async queue. If we validate and prepare the timeout\nlinks upfront when we first see them, there's nothing stopping us from\nsupporting any sort of nesting.\n\nFixes: 2665abfd757f (\"io_uring: add support for linked SQE timeouts\")\nReported-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:05 -0700 io_uring: fix sequencing issues with linked timeouts"
    },
    {
        "commit": "ad8a48acc23cb13cbf4332ebabb867b1baa81842",
        "message": "There are a few reasons for this:\n\n- As a prep to improving the linked timeout logic\n- io_timeout is the biggest member in the io_kiocb opcode union\n\nThis also enables a few cleanups, like unifying the timer setup between\nIORING_OP_TIMEOUT and IORING_OP_LINK_TIMEOUT, and not needing multiple\narguments to the link/prep helpers.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:56:01 -0700 io_uring: make req->timeout be dynamically allocated"
    },
    {
        "commit": "978db57e2c329fc612ff669cab9bf0007efd3ca3",
        "message": "If we don't use the normal completion path, we may skip killing links\nthat should be errored and freed. Add __io_double_put_req() for use\nwithin the completion path itself, other calls should just use\nio_double_put_req().\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:48:31 -0700 io_uring: make io_double_put_req() use normal completion path"
    },
    {
        "commit": "0e0702dac26b282603261f04a62711a2d9aac17b",
        "message": "__io_queue_sqe(), io_queue_sqe(), io_queue_link_head() all return 0/err,\nbut the caller doesn't care since the errors are handled inline. Clean\nthese up and just make them void.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:48:31 -0700 io_uring: cleanup return values from the queueing functions"
    },
    {
        "commit": "95a5bbae05ef1ec1cceb8c1b04a482aa0b7c177c",
        "message": "If we have a linked request, this enables us to pass it back directly\nwithout having to go through async context.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 19:48:31 -0700 io_uring: io_async_cancel() should pass in 'nxt' request pointer"
    },
    {
        "commit": "fb4b3d3fd0c7f53168b6f6d07d1d97f55c676eeb",
        "message": "Pull io_uring updates from Jens Axboe:\n \"A lot of stuff has been going on this cycle, with improving the\n  support for networked IO (and hence unbounded request completion\n  times) being one of the major themes. There's been a set of fixes done\n  this week, I'll send those out as well once we're certain we're fully\n  happy with them.\n\n  This contains:\n\n   - Unification of the \"normal\" submit path and the SQPOLL path (Pavel)\n\n   - Support for sparse (and bigger) file sets, and updating of those\n     file sets without needing to unregister/register again.\n\n   - Independently sized CQ ring, instead of just making it always 2x\n     the SQ ring size. This makes it more flexible for networked\n     applications.\n\n   - Support for overflowed CQ ring, never dropping events but providing\n     backpressure on submits.\n\n   - Add support for absolute timeouts, not just relative ones.\n\n   - Support for generic cancellations. This divorces io_uring from\n     workqueues as well, which additionally gets us one step closer to\n     generic async system call support.\n\n   - With cancellations, we can support grabbing the process file table\n     as well, just like we do mm context. This allows support for system\n     calls that create file descriptors, like accept4() support that's\n     built on top of that.\n\n   - Support for io_uring tracing (Dmitrii)\n\n   - Support for linked timeouts. These abort an operation if it isn't\n     completed by the time noted in the linke timeout.\n\n   - Speedup tracking of poll requests\n\n   - Various cleanups making the coder easier to follow (Jackie, Pavel,\n     Bob, YueHaibing, me)\n\n   - Update MAINTAINERS with new io_uring list\"\n\n* tag 'for-5.5/io_uring-20191121' of git://git.kernel.dk/linux-block: (64 commits)\n  io_uring: make POLL_ADD/POLL_REMOVE scale better\n  io-wq: remove now redundant struct io_wq_nulls_list\n  io_uring: Fix getting file for non-fd opcodes\n  io_uring: introduce req_need_defer()\n  io_uring: clean up io_uring_cancel_files()\n  io-wq: ensure free/busy list browsing see all items\n  io-wq: ensure we have a stable view of ->cur_work for cancellations\n  io_wq: add get/put_work handlers to io_wq_create()\n  io_uring: check for validity of ->rings in teardown\n  io_uring: fix potential deadlock in io_poll_wake()\n  io_uring: use correct \"is IO worker\" helper\n  io_uring: fix -ENOENT issue with linked timer with short timeout\n  io_uring: don't do flush cancel under inflight_lock\n  io_uring: flag SQPOLL busy condition to userspace\n  io_uring: make ASYNC_CANCEL work with poll and timeout\n  io_uring: provide fallback request for OOM situations\n  io_uring: convert accept4() -ERESTARTSYS into -EINTR\n  io_uring: fix error clear of ->file_table in io_sqe_files_register()\n  io_uring: separate the io_free_req and io_free_req_find_next interface\n  io_uring: keep io_put_req only responsible for release and put req\n  ...",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-25 10:40:27 -0800 Merge tag 'for-5.5/io_uring-20191121' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "b226c9e1f4cb23bf6fa6c74af361e5136cb5804c",
        "message": "Pull block fixes from Jens Axboe:\n \"A few fixes that should make it into this release. This contains:\n\n   - io_uring:\n        - The timeout command assumes sequence == 0 means that we want\n          one completion, but this kind of overloading is unfortunate as\n          it prevents users from doing a pure time based wait. Since\n          this operation was introduced in this cycle, let's correct it\n          now, while we can. (me)\n        - One-liner to fix an issue with dependent links and fixed\n          buffer reads. The actual IO completed fine, but the link got\n          severed since we stored the wrong expected value. (me)\n        - Add TIMEOUT to list of opcodes that don't need a file. (Pavel)\n\n   - rsxx missing workqueue destry calls. Old bug. (Chuhong)\n\n   - Fix blk-iocost active list check (Jiufei)\n\n   - Fix impossible-to-hit overflow merge condition, that still hit some\n     folks very rarely (Junichi)\n\n   - Fix bfq hang issue from 5.3. This didn't get marked for stable, but\n     will go into stable post this merge (Paolo)\"\n\n* tag 'for-linus-20191115' of git://git.kernel.dk/linux-block:\n  rsxx: add missed destroy_workqueue calls in remove\n  iocost: check active_list of all the ancestors in iocg_activate()\n  block, bfq: deschedule empty bfq_queues not referred by any process\n  io_uring: ensure registered buffer import returns the IO length\n  io_uring: Fix getting file for timeout\n  block: check bi_size overflow before merge\n  io_uring: make timeout sequence == 0 mean no sequence",
        "kernel_version": "v5.4-rc8",
        "release_date": "2019-11-15 13:02:34 -0800 Merge tag 'for-linus-20191115' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "eac406c61cd0ec8fe7970ca46ddf23e40a86b579",
        "message": "One of the obvious use cases for these commands is networking, where\nit's not uncommon to have tons of sockets open and polled for. The\ncurrent implementation uses a list for insertion and lookup, which works\nfine for file based use cases where the count is usually low, it breaks\ndown somewhat for higher number of files / sockets. A test case with\n30k sockets being polled for and cancelled takes:\n\nreal    0m6.968s\nuser    0m0.002s\nsys     0m6.936s\n\nwith the patch it takes:\n\nreal    0m0.233s\nuser    0m0.010s\nsys     0m0.176s\n\nIf you go to 50k sockets, it gets even more abysmal with the current\ncode:\n\nreal    0m40.602s\nuser    0m0.010s\nsys     0m40.555s\n\nwith the patch it takes:\n\nreal    0m0.398s\nuser    0m0.000s\nsys     0m0.341s\n\nChange is pretty straight forward, just replace the cancel_list with\na red/black tree instead.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-14 12:09:58 -0700 io_uring: make POLL_ADD/POLL_REMOVE scale better"
    },
    {
        "commit": "a320e9fa1e2680116d165b9369dfa41d7cc1e1d1",
        "message": "For timeout requests and bunch of others io_uring tries to grab a file\nwith specified fd, which is usually stdin/fd=0.\nUpdate io_op_needs_file()\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-13 19:41:01 -0700 io_uring: Fix getting file for non-fd opcodes"
    },
    {
        "commit": "9d858b21483981db9c0cb4b184d4cdeb4bc525c2",
        "message": "Makes the code easier to read.\n\nSigned-off-by: Bob Liu <bob.liu@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-13 19:41:01 -0700 io_uring: introduce req_need_defer()"
    },
    {
        "commit": "2f6d9b9d6357ede64a29437676884ee263039910",
        "message": "We don't use the return value anymore, drop it. Also drop the\nunecessary double cancel_req value check.\n\nSigned-off-by: Bob Liu <bob.liu@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-13 19:41:01 -0700 io_uring: clean up io_uring_cancel_files()"
    },
    {
        "commit": "5e559561a8d7e6d4adfce6aa8fbf3daa3dec1577",
        "message": "A test case was reported where two linked reads with registered buffers\nfailed the second link always. This is because we set the expected value\nof a request in req->result, and if we don't get this result, then we\nfail the dependent links. For some reason the registered buffer import\nreturned -ERROR/0, while the normal import returns -ERROR/length. This\nbroke linked commands with registered buffers.\n\nFix this by making io_import_fixed() correctly return the mapped length.\n\nCc: stable@vger.kernel.org # v5.3\nReported-by: \u674e\u901a\u6d32 <carter.li@eoitek.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc8",
        "release_date": "2019-11-13 16:15:14 -0700 io_uring: ensure registered buffer import returns the IO length"
    },
    {
        "commit": "5683e5406e94ae1bfb0d9516a18fdb281d0f8d1d",
        "message": "For timeout requests io_uring tries to grab a file with specified fd,\nwhich is usually stdin/fd=0.\nUpdate io_op_needs_file()\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc8",
        "release_date": "2019-11-13 15:25:57 -0700 io_uring: Fix getting file for timeout"
    },
    {
        "commit": "15dff286d0e0087d4dcd7049911f179e4e4cfd94",
        "message": "Normally the rings are always valid, the exception is if we failed to\nallocate the rings at setup time. syzbot reports this:\n\nRSP: 002b:00007ffd6e8aa078 EFLAGS: 00000246 ORIG_RAX: 00000000000001a9\nRAX: ffffffffffffffda RBX: 0000000000000000 RCX: 0000000000441229\nRDX: 0000000000000002 RSI: 0000000020000140 RDI: 0000000000000d0d\nRBP: 00007ffd6e8aa090 R08: 0000000000000001 R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000246 R12: ffffffffffffffff\nR13: 0000000000000003 R14: 0000000000000000 R15: 0000000000000000\nkasan: CONFIG_KASAN_INLINE enabled\nkasan: GPF could be caused by NULL-ptr deref or user memory access\ngeneral protection fault: 0000 [#1] PREEMPT SMP KASAN\nCPU: 1 PID: 8903 Comm: syz-executor410 Not tainted 5.4.0-rc7-next-20191113\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS\nGoogle 01/01/2011\nRIP: 0010:__read_once_size include/linux/compiler.h:199 [inline]\nRIP: 0010:__io_commit_cqring fs/io_uring.c:496 [inline]\nRIP: 0010:io_commit_cqring+0x1e1/0xdb0 fs/io_uring.c:592\nCode: 03 0f 8e df 09 00 00 48 8b 45 d0 4c 8d a3 c0 00 00 00 4c 89 e2 48 c1\nea 03 44 8b b8 c0 01 00 00 48 b8 00 00 00 00 00 fc ff df <0f> b6 14 02 4c\n89 e0 83 e0 07 83 c0 03 38 d0 7c 08 84 d2 0f 85 61\nRSP: 0018:ffff88808f51fc08 EFLAGS: 00010006\nRAX: dffffc0000000000 RBX: 0000000000000000 RCX: ffffffff815abe4a\nRDX: 0000000000000018 RSI: ffffffff81d168d5 RDI: ffff8880a9166100\nRBP: ffff88808f51fc70 R08: 0000000000000004 R09: ffffed1011ea3f7d\nR10: ffffed1011ea3f7c R11: 0000000000000003 R12: 00000000000000c0\nR13: ffff8880a91661c0 R14: 1ffff1101522cc10 R15: 0000000000000000\nFS:  0000000001e7a880(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 0000000020000140 CR3: 000000009a74c000 CR4: 00000000001406e0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\nCall Trace:\n  io_cqring_overflow_flush+0x6b9/0xa90 fs/io_uring.c:673\n  io_ring_ctx_wait_and_kill+0x24f/0x7c0 fs/io_uring.c:4260\n  io_uring_create fs/io_uring.c:4600 [inline]\n  io_uring_setup+0x1256/0x1cc0 fs/io_uring.c:4626\n  __do_sys_io_uring_setup fs/io_uring.c:4639 [inline]\n  __se_sys_io_uring_setup fs/io_uring.c:4636 [inline]\n  __x64_sys_io_uring_setup+0x54/0x80 fs/io_uring.c:4636\n  do_syscall_64+0xfa/0x760 arch/x86/entry/common.c:290\n  entry_SYSCALL_64_after_hwframe+0x49/0xbe\nRIP: 0033:0x441229\nCode: e8 5c ae 02 00 48 83 c4 18 c3 0f 1f 80 00 00 00 00 48 89 f8 48 89 f7\n48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff\nff 0f 83 bb 0a fc ff c3 66 2e 0f 1f 84 00 00 00 00\nRSP: 002b:00007ffd6e8aa078 EFLAGS: 00000246 ORIG_RAX: 00000000000001a9\nRAX: ffffffffffffffda RBX: 0000000000000000 RCX: 0000000000441229\nRDX: 0000000000000002 RSI: 0000000020000140 RDI: 0000000000000d0d\nRBP: 00007ffd6e8aa090 R08: 0000000000000001 R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000246 R12: ffffffffffffffff\nR13: 0000000000000003 R14: 0000000000000000 R15: 0000000000000000\nModules linked in:\n---[ end trace b0f5b127a57f623f ]---\nRIP: 0010:__read_once_size include/linux/compiler.h:199 [inline]\nRIP: 0010:__io_commit_cqring fs/io_uring.c:496 [inline]\nRIP: 0010:io_commit_cqring+0x1e1/0xdb0 fs/io_uring.c:592\nCode: 03 0f 8e df 09 00 00 48 8b 45 d0 4c 8d a3 c0 00 00 00 4c 89 e2 48 c1\nea 03 44 8b b8 c0 01 00 00 48 b8 00 00 00 00 00 fc ff df <0f> b6 14 02 4c\n89 e0 83 e0 07 83 c0 03 38 d0 7c 08 84 d2 0f 85 61\nRSP: 0018:ffff88808f51fc08 EFLAGS: 00010006\nRAX: dffffc0000000000 RBX: 0000000000000000 RCX: ffffffff815abe4a\nRDX: 0000000000000018 RSI: ffffffff81d168d5 RDI: ffff8880a9166100\nRBP: ffff88808f51fc70 R08: 0000000000000004 R09: ffffed1011ea3f7d\nR10: ffffed1011ea3f7c R11: 0000000000000003 R12: 00000000000000c0\nR13: ffff8880a91661c0 R14: 1ffff1101522cc10 R15: 0000000000000000\nFS:  0000000001e7a880(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 0000000020000140 CR3: 000000009a74c000 CR4: 00000000001406e0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n\nwhich is exactly the case of failing to allocate the SQ/CQ rings, and\nthen entering shutdown. Check if the rings are valid before trying to\naccess them at shutdown time.\n\nReported-by: syzbot+21147d79607d724bd6f3@syzkaller.appspotmail.com\nFixes: 1d7bb1d50fb4 (\"io_uring: add support for backlogged CQ ring\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-13 09:11:36 -0700 io_uring: check for validity of ->rings in teardown"
    },
    {
        "commit": "7c9e7f0fe0d8abf856a957c150c48778e75154c1",
        "message": "We attempt to run the poll completion inline, but we're using trylock to\ndo so. This avoids a deadlock since we're grabbing the locks in reverse\norder at this point, we already hold the poll wq lock and we're trying\nto grab the completion lock, while the normal rules are the reverse of\nthat order.\n\nIO completion for a timeout link will need to grab the completion lock,\nbut that's not safe from this context. Put the completion under the\ncompletion_lock in io_poll_wake(), and mark the request as entering\nthe completion with the completion_lock already held.\n\nFixes: 2665abfd757f (\"io_uring: add support for linked SQE timeouts\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-12 12:26:34 -0700 io_uring: fix potential deadlock in io_poll_wake()"
    },
    {
        "commit": "960e432dfa5927892a9b170d14de874597b84849",
        "message": "Since we switched to io-wq, the dependent link optimization for when to\npass back work inline has been broken. Fix this by providing a suitable\nio-wq helper for io_uring to use to detect when to do this.\n\nFixes: 561fb04a6a22 (\"io_uring: replace workqueue usage with io-wq\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-12 08:02:26 -0700 io_uring: use correct \"is IO worker\" helper"
    },
    {
        "commit": "93bd25bb69f46367ba8f82c578e0c05702ceb482",
        "message": "Currently we make sequence == 0 be the same as sequence == 1, but that's\nnot super useful if the intent is really to have a timeout that's just\na pure timeout.\n\nIf the user passes in sqe->off == 0, then don't apply any sequence logic\nto the request, let it purely be driven by the timeout specified.\n\nReported-by: \u674e\u901a\u6d32 <carter.li@eoitek.com>\nReviewed-by: \u674e\u901a\u6d32 <carter.li@eoitek.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc8",
        "release_date": "2019-11-12 00:18:51 -0700 io_uring: make timeout sequence == 0 mean no sequence"
    },
    {
        "commit": "76a46e066e2d93bd333599d1c84c605c2c4cc909",
        "message": "If you prep a read (for example) that needs to get punted to async\ncontext with a timer, if the timeout is sufficiently short, the timer\nrequest will get completed with -ENOENT as it could not find the read.\n\nThe issue is that we prep and start the timer before we start the read.\nHence the timer can trigger before the read is even started, and the end\nresult is then that the timer completes with -ENOENT, while the read\nstarts instead of being cancelled by the timer.\n\nFix this by splitting the linked timer into two parts:\n\n1) Prep and validate the linked timer\n2) Start timer\n\nThe read is then started between steps 1 and 2, so we know that the\ntimer will always have a consistent view of the read request state.\n\nReported-by: Hrvoje Zeba <zeba.hrvoje@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-11 16:33:22 -0700 io_uring: fix -ENOENT issue with linked timer with short timeout"
    },
    {
        "commit": "768134d4f48109b90f4248feecbeeb7d684e410c",
        "message": "We can't safely cancel under the inflight lock. If the work hasn't been\nstarted yet, then io_wq_cancel_work() simply marks the work as cancelled\nand invokes the work handler. But if the work completion needs to grab\nthe inflight lock because it's grabbing user files, then we'll deadlock\ntrying to finish the work as we already hold that lock.\n\nInstead grab a reference to the request, if it isn't already zero. If\nit's zero, then we know it's going through completion anyway, and we\ncan safely ignore it. If it's not zero, then we can drop the lock and\nattempt to cancel from there.\n\nThis also fixes a missing finish_wait() at the end of\nio_uring_cancel_files().\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-11 16:33:17 -0700 io_uring: don't do flush cancel under inflight_lock"
    },
    {
        "commit": "c1edbf5f081be9fbbea68c1d564b773e59c1acf3",
        "message": "Now that we have backpressure, for SQPOLL, we have one more condition\nthat warrants flagging that the application needs to enter the kernel:\nwe failed to submit IO due to backpressure. Make sure we catch that\nand flag it appropriately.\n\nIf we run into backpressure issues with the SQPOLL thread, flag it\nas such to the application by setting IORING_SQ_NEED_WAKEUP. This will\ncause the application to enter the kernel, and that will flush the\nbacklog and clear the condition.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-11 16:33:11 -0700 io_uring: flag SQPOLL busy condition to userspace"
    },
    {
        "commit": "47f467686ec02fc07fd5c6bb34b6f6736e2884b0",
        "message": "It's a little confusing that we have multiple types of command\ncancellation opcodes now that we have a generic one. Make the generic\none work with POLL_ADD and TIMEOUT commands as well, that makes for an\neasier to use API for the application. The fact that they currently\ndon't is a bit confusing.\n\nAdd a helper that takes care of it, so we can user it from both\nIORING_OP_ASYNC_CANCEL and from the linked timeout cancellation.\n\nReported-by: Hrvoje Zeba <zeba.hrvoje@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-11 16:33:05 -0700 io_uring: make ASYNC_CANCEL work with poll and timeout"
    },
    {
        "commit": "0ddf92e848ab7abf216f218ee363eb9b9650e98f",
        "message": "One thing that really sucks for userspace APIs is if the kernel passes\nback -ENOMEM/-EAGAIN for resource shortages. The application really has\nno idea of what to do in those cases. Should it try and reap\ncompletions? Probably a good idea. Will it solve the issue? Who knows.\n\nThis patch adds a simple fallback mechanism if we fail to allocate\nmemory for a request. If we fail allocating memory from the slab for a\nrequest, we punt to a pre-allocated request. There's just one of these\nper io_ring_ctx, but the important part is if we ever return -EBUSY to\nthe application, the applications knows that it can wait for events and\nmake forward progress when events have completed. This is the important\npart.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-11 16:32:55 -0700 io_uring: provide fallback request for OOM situations"
    },
    {
        "commit": "8e3cca12706231daf8daf90dbde59f1665135e48",
        "message": "If we cancel a pending accept operating with a signal, we get\n-ERESTARTSYS returned. Turn that into -EINTR for userspace, we should\nnot be return -ERESTARTSYS.\n\nFixes: 17f2fe35d080 (\"io_uring: add support for IORING_OP_ACCEPT\")\nReported-by: Hrvoje Zeba <zeba.hrvoje@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-10 20:29:49 -0700 io_uring: convert accept4() -ERESTARTSYS into -EINTR"
    },
    {
        "commit": "46568e9be70ff8211d986685f08d919376c32998",
        "message": "syzbot reports that when using failslab and friends, we can get a double\nfree in io_sqe_files_unregister():\n\nBUG: KASAN: double-free or invalid-free in\nio_sqe_files_unregister+0x20b/0x300 fs/io_uring.c:3185\n\nCPU: 1 PID: 8819 Comm: syz-executor452 Not tainted 5.4.0-rc6-next-20191108\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS\nGoogle 01/01/2011\nCall Trace:\n  __dump_stack lib/dump_stack.c:77 [inline]\n  dump_stack+0x197/0x210 lib/dump_stack.c:118\n  print_address_description.constprop.0.cold+0xd4/0x30b mm/kasan/report.c:374\n  kasan_report_invalid_free+0x65/0xa0 mm/kasan/report.c:468\n  __kasan_slab_free+0x13a/0x150 mm/kasan/common.c:450\n  kasan_slab_free+0xe/0x10 mm/kasan/common.c:480\n  __cache_free mm/slab.c:3426 [inline]\n  kfree+0x10a/0x2c0 mm/slab.c:3757\n  io_sqe_files_unregister+0x20b/0x300 fs/io_uring.c:3185\n  io_ring_ctx_free fs/io_uring.c:3998 [inline]\n  io_ring_ctx_wait_and_kill+0x348/0x700 fs/io_uring.c:4060\n  io_uring_release+0x42/0x50 fs/io_uring.c:4068\n  __fput+0x2ff/0x890 fs/file_table.c:280\n  ____fput+0x16/0x20 fs/file_table.c:313\n  task_work_run+0x145/0x1c0 kernel/task_work.c:113\n  exit_task_work include/linux/task_work.h:22 [inline]\n  do_exit+0x904/0x2e60 kernel/exit.c:817\n  do_group_exit+0x135/0x360 kernel/exit.c:921\n  __do_sys_exit_group kernel/exit.c:932 [inline]\n  __se_sys_exit_group kernel/exit.c:930 [inline]\n  __x64_sys_exit_group+0x44/0x50 kernel/exit.c:930\n  do_syscall_64+0xfa/0x760 arch/x86/entry/common.c:290\n  entry_SYSCALL_64_after_hwframe+0x49/0xbe\nRIP: 0033:0x43f2c8\nCode: 31 b8 c5 f7 ff ff 48 8b 5c 24 28 48 8b 6c 24 30 4c 8b 64 24 38 4c 8b\n6c 24 40 4c 8b 74 24 48 4c 8b 7c 24 50 48 83 c4 58 c3 66 <0f> 1f 84 00 00\n00 00 00 48 8d 35 59 ca 00 00 0f b6 d2 48 89 fb 48\nRSP: 002b:00007ffd5b976008 EFLAGS: 00000246 ORIG_RAX: 00000000000000e7\nRAX: ffffffffffffffda RBX: 0000000000000000 RCX: 000000000043f2c8\nRDX: 0000000000000000 RSI: 000000000000003c RDI: 0000000000000000\nRBP: 00000000004bf0a8 R08: 00000000000000e7 R09: ffffffffffffffd0\nR10: 0000000000000001 R11: 0000000000000246 R12: 0000000000000001\nR13: 00000000006d1180 R14: 0000000000000000 R15: 0000000000000000\n\nThis happens if we fail allocating the file tables. For that case we do\nfree the file table correctly, but we forget to set it to NULL. This\nmeans that ring teardown will see it as being non-NULL, and attempt to\nfree it again.\n\nFix this by clearing the file_table pointer if we free the table.\n\nReported-by: syzbot+3254bc44113ae1e331ee@syzkaller.appspotmail.com\nFixes: 65e19f54d29c (\"io_uring: support for larger fixed file sets\")\nReviewed-by: Bob Liu <bob.liu@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-10 20:29:49 -0700 io_uring: fix error clear of ->file_table in io_sqe_files_register()"
    },
    {
        "commit": "c69f8dbe2426cbf6150407b7e86ce85bb463c1dc",
        "message": "Similar to the distinction between io_put_req and io_put_req_find_next,\nio_free_req has been modified similarly, with no functional changes.\n\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-10 20:29:49 -0700 io_uring: separate the io_free_req and io_free_req_find_next interface"
    },
    {
        "commit": "ec9c02ad4c3808d6d9ed28ad1d0485d6e2a33ac5",
        "message": "We already have io_put_req_find_next to find the next req of the link.\nwe should not use the io_put_req function to find them. They should be\nfunctions of the same level.\n\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-10 20:29:49 -0700 io_uring: keep io_put_req only responsible for release and put req"
    },
    {
        "commit": "a197f664a0db8a6219d9ce949f5f29b89f60fb2b",
        "message": "Many times, the core of the function is req, and req has already set\nreq->ctx at initialization time, so there is no need to pass in the\nctx from the caller.\n\nCleanup, no functional change.\n\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-10 20:29:49 -0700 io_uring: remove passed in 'ctx' function parameter ctx if possible"
    },
    {
        "commit": "206aefde4f886fdeb3b6339aacab3a85fb74cb7e",
        "message": "With the recent flurry of additions and changes to io_uring, the\nlayout of io_ring_ctx has become a bit stale. We're right now at\n704 bytes in size on my x86-64 build, or 11 cachelines. This\npatch does two things:\n\n- We have to completion structs embedded, that we only use for\n  quiesce of the ctx (or shutdown) and for sqthread init cases.\n  That 2x32 bytes right there, let's dynamically allocate them.\n\n- Reorder the struct a bit with an eye on cachelines, use cases,\n  and holes.\n\nWith this patch, we're down to 512 bytes, or 8 cachelines.\n\nReviewed-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-10 20:29:49 -0700 io_uring: reduce/pack size of io_ring_ctx"
    },
    {
        "commit": "912c0a85911a6364ac127b6e1de8c2c7782db1bc",
        "message": "Pull on for-linus to resolve what otherwise would have been a conflict\nwith the cgroups rstat patchset from Tejun.\n\n* for-linus: (942 commits)\n  blkcg: make blkcg_print_stat() print stats only for online blkgs\n  nvme: change nvme_passthru_cmd64 to explicitly mark rsvd\n  nvme-multipath: fix crash in nvme_mpath_clear_ctrl_paths\n  nvme-rdma: fix a segmentation fault during module unload\n  iocost: don't nest spin_lock_irq in ioc_weight_write()\n  io_uring: ensure we clear io_kiocb->result before each issue\n  um-ubd: Entrust re-queue to the upper layers\n  nvme-multipath: remove unused groups_only mode in ana log\n  nvme-multipath: fix possible io hang after ctrl reconnect\n  io_uring: don't touch ctx in setup after ring fd install\n  io_uring: Fix leaked shadow_req\n  Linux 5.4-rc5\n  riscv: cleanup do_trap_break\n  nbd: verify socket is supported during setup\n  ata: libahci_platform: Fix regulator_get_optional() misuse\n  nbd: handle racing with error'ed out commands\n  nbd: protect cmd->status with cmd->lock\n  io_uring: fix bad inflight accounting for SETUP_IOPOLL|SETUP_SQTHREAD\n  io_uring: used cached copies of sq->dropped and cq->overflow\n  ARM: dts: stm32: relax qspi pins slew-rate for stm32mp157\n  ...",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-07 12:27:19 -0700 Merge branch 'for-linus' into for-5.5/block"
    },
    {
        "commit": "5f8fd2d3e0a7aa7fc9d97226be24286edd289835",
        "message": "Now that io-wq supports separating the two request lifetime types, mark\nthe following IO as having unbounded runtimes:\n\n- Any read/write to a non-regular file\n- Any specific networked IO\n- Any poll command\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-07 11:57:17 -0700 io_uring: properly mark async work as bounded vs unbounded"
    },
    {
        "commit": "c5def4ab849494d3c97f6c9fc84b2ddb868fe78c",
        "message": "io_uring supports request types that basically have two different\nlifetimes:\n\n1) Bounded completion time. These are requests like disk reads or writes,\n   which we know will finish in a finite amount of time.\n2) Unbounded completion time. These are generally networked IO, where we\n   have no idea how long they will take to complete. Another example is\n   POLL commands.\n\nThis patch provides support for io-wq to handle these differently, so we\ndon't starve bounded requests by tying up workers for too long. By default\nall work is bounded, unless otherwise specified in the work item.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-07 11:41:35 -0700 io-wq: add support for bounded vs unbunded work"
    },
    {
        "commit": "1d7bb1d50fb4dc141c7431cc21fdd24ffcc83c76",
        "message": "Currently we drop completion events, if the CQ ring is full. That's fine\nfor requests with bounded completion times, but it may make it harder or\nimpossible to use io_uring with networked IO where request completion\ntimes are generally unbounded. Or with POLL, for example, which is also\nunbounded.\n\nAfter this patch, we never overflow the ring, we simply store requests\nin a backlog for later flushing. This flushing is done automatically by\nthe kernel. To prevent the backlog from growing indefinitely, if the\nbacklog is non-empty, we apply back pressure on IO submissions. Any\nattempt to submit new IO with a non-empty backlog will get an -EBUSY\nreturn from the kernel. This is a signal to the application that it has\nbacklogged CQ events, and that it must reap those before being allowed\nto submit more IO.\n\nNote that if we do return -EBUSY, we will have filled whatever\nbacklogged events into the CQ ring first, if there's room. This means\nthe application can safely reap events WITHOUT entering the kernel and\nwaiting for them, they are already available in the CQ ring.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "No tag found",
        "release_date": "2019-11-09 11:45:29 -0700 io_uring: add support for backlogged CQ ring"
    },
    {
        "commit": "78e19bbef38362cebff38aa1ca12e2c82bb72eb8",
        "message": "This is in preparation for handling CQ ring overflow a bit smarter. We\nshould not have any functional changes in this patch. Most of the\nchanges are fairly straight forward, the only ones that stick out a bit\nare the ones that change __io_free_req() to take the reference count\ninto account. If the request hasn't been submitted yet, we know it's\nsafe to simply ignore references and free it. But let's clean these up\ntoo, as later patches will depend on the caller doing the right thing if\nthe completion logging grabs a reference to the request.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-08 06:57:27 -0700 io_uring: pass in io_kiocb to fill/add CQ handlers"
    },
    {
        "commit": "84f97dc2333c626979bb547fce343a1003544dcc",
        "message": "The rings can be derived from the ctx, and we need the ctx there for\na future change.\n\nNo functional changes in this patch.\n\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-08 06:57:21 -0700 io_uring: make io_cqring_events() take 'ctx' as argument"
    },
    {
        "commit": "2665abfd757fb35a241c6f0b1ebf620e3ffb36fb",
        "message": "While we have support for generic timeouts, we don't have a way to tie\na timeout to a specific SQE. The generic timeouts simply trigger wakeups\non the CQ ring.\n\nThis adds support for IORING_OP_LINK_TIMEOUT. This command is only valid\nas a link to a previous command. The timeout specific can be either\nrelative or absolute, following the same rules as IORING_OP_TIMEOUT. If\nthe timeout triggers before the dependent command completes, it will\nattempt to cancel that command. Likewise, if the dependent command\ncompletes before the timeout triggers, it will cancel the timeout.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-07 19:12:40 -0700 io_uring: add support for linked SQE timeouts"
    },
    {
        "commit": "e977d6d34f0c08e3c3b132c9e73b98d0db50abc1",
        "message": "We're going to need this helper in a future patch, so move it out\nof io_async_cancel() and into its own separate function.\n\nNo functional changes in this patch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-07 12:31:31 -0700 io_uring: abstract out io_async_cancel_one() helper"
    },
    {
        "commit": "267bc90442aa47002e2991f7d9dd141e168b466b",
        "message": "req->submit is always up-to-date, use it directly\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-06 20:23:02 -0700 io_uring: use inlined struct sqe_submit"
    },
    {
        "commit": "50585b9a07367b92382c1e975265344daeba78cd",
        "message": "Stack allocated struct sqe_submit is passed down to the submission path\nalong with a request (a.k.a. struct io_kiocb), and will be copied into\nreq->submit for async requests.\n\nAs space for it is already allocated, fill req->submit in the first\nplace instead of using on-stack one. As a result:\n\n1. sqe->submit is the only place for sqe_submit and is always valid,\nso we don't need to track which one to use.\n2. don't need to copy in case of async\n3. allows to simplify the code by not carrying it as an argument all\nthe way down\n4. allows to reduce number of function arguments / potentially improve\nspilling\n\nThe downside is that stack is most probably be cached, that's not true\nfor just allocated memory for a request. Another concern is cache\npollution. Though, a request would be touched and fetched along with\nreq->submit at some point anyway, so shouldn't be a problem.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-06 20:23:00 -0700 io_uring: Use submit info inlined into req"
    },
    {
        "commit": "196be95cd5572078be9deb81cbea145fab246029",
        "message": "Let io_submit_sqes() to allocate io_kiocb before fetching an sqe.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-06 20:21:37 -0700 io_uring: allocate io_kiocb upfront"
    },
    {
        "commit": "e5eb6366ac2d1df8ad5b010718ac1997ceae45be",
        "message": "After a call to io_submit_sqe(), it's already known whether it needs\nto queue a link or not. Do it there, as it's simplier and doesn't keep\nan extra variable across the loop.\n\nReviewed-by\uff1aBob Liu <bob.liu@oracle.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-06 11:20:11 -0700 io_uring: io_queue_link*() right after submit"
    },
    {
        "commit": "ae9428ca61271b6b7f52ebbc359676c9fdfde523",
        "message": "io_submit_sqes() and io_ring_submit() are doing the same stuff with\na little difference. Deduplicate them.\n\nReviewed-by\uff1aBob Liu <bob.liu@oracle.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-06 11:20:07 -0700 io_uring: Merge io_submit_sqes and io_ring_submit"
    },
    {
        "commit": "3aa5fa030558e2b0da284fd069aeb7178543c987",
        "message": "We had no more use for this flag after the conversion to io-wq, kill it\noff.\n\nFixes: 561fb04a6a22 (\"io_uring: replace workqueue usage with io-wq\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-05 20:34:32 -0700 io_uring: kill dead REQ_F_LINK_DONE flag"
    },
    {
        "commit": "f1f40853c01b5ccd0a1a29ce0b515c6f5405a798",
        "message": "If a request fails, we need to ensure we set REQ_F_FAIL_LINK on it if\nREQ_F_LINK is set. Any failure in the chain should break the chain.\n\nWe were missing a few spots where this should be done. It might be nice\nto generalize this somewhat at some point, as long as we factor in the\nfact that failure looks different for each request type.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-05 20:33:16 -0700 io_uring: fixup a few spots where link failure isn't flagged"
    },
    {
        "commit": "89723d0bd6c77540c01ce7db2cd6f8c3be2fd958",
        "message": "As introduced by commit:\n\nba816ad61fdf (\"io_uring: run dependent links inline if possible\")\n\nenable inline dependent link running for poll commands.\nio_poll_complete_work() is the most important change, as it allows a\nlinked sequence of { POLL, READ } (for example) to proceed inline\ninstead of needing to get punted to another async context. The\nsubmission side only potentially matters for sqthread, but may as well\ninclude that bit.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-05 15:32:58 -0700 io_uring: enable optimized link handling for IORING_OP_POLL_ADD"
    },
    {
        "commit": "1056ef940380c4e32349ccb6d956858edf70520c",
        "message": "We now have a list that's appropriate for both kernel and userspace\ndiscussions on io_uring usage and development, add that to the\nMAINTAINERS entry.\n\nAlso add the io-wq files.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-04 08:50:02 -0700 MAINTAINERS: update io_uring entry"
    },
    {
        "commit": "51c3ff62cac635ae9d75f875ce5b7bdafc97abd5",
        "message": "We currently don't have a completion event trace, add one of those. And\nto better be able to match up submissions and completions, add user_data\nto the submission trace as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-04 07:07:52 -0700 io_uring: add completion trace event"
    },
    {
        "commit": "0821de28961d58df44ed390d2460f05c9b5195a9",
        "message": "Pull block fixes from Jens Axboe:\n\n - Two small nvme fixes, one is a fabrics connection fix, the other one\n   a cleanup made possible by that fix (Anton, via Keith)\n\n - Fix requeue handling in umb ubd (Anton)\n\n - Fix spin_lock_irq() nesting in blk-iocost (Dan)\n\n - Three small io_uring fixes:\n     - Install io_uring fd after done with ctx (me)\n     - Clear ->result before every poll issue (me)\n     - Fix leak of shadow request on error (Pavel)\n\n* tag 'for-linus-20191101' of git://git.kernel.dk/linux-block:\n  iocost: don't nest spin_lock_irq in ioc_weight_write()\n  io_uring: ensure we clear io_kiocb->result before each issue\n  um-ubd: Entrust re-queue to the upper layers\n  nvme-multipath: remove unused groups_only mode in ana log\n  nvme-multipath: fix possible io hang after ctrl reconnect\n  io_uring: don't touch ctx in setup after ring fd install\n  io_uring: Fix leaked shadow_req",
        "kernel_version": "v5.4-rc6",
        "release_date": "2019-11-01 17:33:12 -0700 Merge tag 'for-linus-20191101' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "0069fc6b1cf28de3a3890ed7c87a5b8ab79ca528",
        "message": "This internal logic was killed with the conversion to io-wq, so we no\nlonger have a need for this particular trace. Kill it.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-01 12:44:40 -0600 io_uring: remove io_uring_add_to_prev() trace event"
    },
    {
        "commit": "e9ffa5c2b77edf2689f876b640318b16fc3ea2a7",
        "message": "We didn't use -ERESTARTSYS to tell the application layer to restart the\nsystem call, but instead return -EINTR. we can set -EINTR directly when\nwakeup by the signal, which can help us save an assignment operation and\ncomparison operation.\n\nReviewed-by: Bob Liu <bob.liu@oracle.com>\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-01 08:36:36 -0600 io_uring: set -EINTR directly when a signal wakes up in io_cqring_wait"
    },
    {
        "commit": "62755e35dfb2b113c52b81cd96d01c20971c8e02",
        "message": "This adds support for IORING_OP_ASYNC_CANCEL, which will attempt to\ncancel requests that have been punted to async context and are now\nin-flight. This works for regular read/write requests to files, as\nlong as they haven't been started yet. For socket based IO (or things\nlike accept4(2)), we can cancel work that is already running as well.\n\nTo cancel a request, the sqe must have ->addr set to the user_data of\nthe request it wishes to cancel. If the request is cancelled\nsuccessfully, the original request is completed with -ECANCELED\nand the cancel request is completed with a result of 0. If the\nrequest was already running, the original may or may not complete\nin error. The cancel request will complete with -EALREADY for that\ncase. And finally, if the request to cancel wasn't found, the cancel\nrequest is completed with -ENOENT.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-11-01 08:35:31 -0600 io_uring: support for generic async request cancel"
    },
    {
        "commit": "6873e0bd6a9cb14ecfadd89d9ed9698ff1761902",
        "message": "We use io_kiocb->result == -EAGAIN as a way to know if we need to\nre-submit a polled request, as -EAGAIN reporting happens out-of-line\nfor IO submission failures. This field is cleared when we originally\nallocate the request, but it isn't reset when we retry the submission\nfrom async context. This can cause issues where we think something\nneeds a re-issue, but we're really just reading stale data.\n\nReset ->result whenever we re-prep a request for polled submission.\n\nCc: stable@vger.kernel.org\nFixes: 9e645e1105ca (\"io_uring: add support for sqe links\")\nReported-by: Bijan Mottahedeh <bijan.mottahedeh@oracle.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc6",
        "release_date": "2019-10-30 14:45:22 -0600 io_uring: ensure we clear io_kiocb->result before each issue"
    },
    {
        "commit": "975c99a570967dd48e917dd7853867fee3febabd",
        "message": "syzbot reported an issue where we crash at setup time if failslab is\nused. The issue is that io_wq_create() returns an error pointer on\nfailure, not NULL. Hence io_uring thought the io-wq was setup just\nfine, but in reality it's a garbage error pointer.\n\nUse IS_ERR() instead of a NULL check, and assign ret appropriately.\n\nReported-by: syzbot+221cc24572a2fed23b6b@syzkaller.appspotmail.com\nFixes: 561fb04a6a22 (\"io_uring: replace workqueue usage with io-wq\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-30 08:42:56 -0600 io_uring: io_wq_create() returns an error pointer, not NULL"
    },
    {
        "commit": "842f96124c5617b060cc0f071dcfb6ab24bdd042",
        "message": "If we get -1 from hrtimer_try_to_cancel(), we know that the timer\nis running. Hence leave all completion to the timeout handler. If\nwe don't, we can corrupt the list and miss a completion.\n\nFixes: 11365043e527 (\"io_uring: add support for canceling timeout requests\")\nReported-by: Hrvoje Zeba <zeba.hrvoje@gmail.com>\nTested-by: Hrvoje Zeba <zeba.hrvoje@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 15:43:30 -0600 io_uring: fix race with canceling timeouts"
    },
    {
        "commit": "65e19f54d29cd8559ce60cfd0d751bef7afbdc5c",
        "message": "There's been a few requests for supporting more fixed files than 1024.\nThis isn't really tricky to do, we just need to split up the file table\ninto multiple tables and index appropriately. As we do so, reduce the\nmax single file table to 512. This enables us to do single page allocs\nalways for the tables, which is an improvement over the situation prior.\n\nThis patch adds support for up to 64K files, which should be enough for\neveryone.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 12:43:06 -0600 io_uring: support for larger fixed file sets"
    },
    {
        "commit": "b7620121dc04e44ce654297050f9eaf39d414a34",
        "message": "We index the file tables with a user given value. After we check\nit's within our limits, use array_index_nospec() to prevent any\nspectre attacks here.\n\nSuggested-by: Jann Horn <jannh@google.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 12:43:06 -0600 io_uring: protect fixed file indexing with array_index_nospec()"
    },
    {
        "commit": "17f2fe35d080d8f64e86a60cdcd3a97edcbc213b",
        "message": "This allows an application to call accept4() in an async fashion. Like\nother opcodes, we first try a non-blocking accept, then punt to async\ncontext if we have to.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 12:43:06 -0600 io_uring: add support for IORING_OP_ACCEPT"
    },
    {
        "commit": "fcb323cc53e29d9cc696d606bb42736b32dd9825",
        "message": "This is in preparation for adding opcodes that need to add new files\nin a process file table, system calls like open(2) or accept4(2).\n\nIf an opcode needs this, it must set IO_WQ_WORK_NEEDS_FILES in the work\nitem. If work that needs to get punted to async context have this\nset, the async worker will assume the original task file table before\nexecuting the work.\n\nNote that opcodes that need access to the current files of an\napplication cannot be done through IORING_SETUP_SQPOLL.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 12:43:06 -0600 io_uring: io_uring: add support for async work inheriting files"
    },
    {
        "commit": "561fb04a6a2257716738dac2ed812f377c2634c2",
        "message": "Drop various work-arounds we have for workqueues:\n\n- We no longer need the async_list for tracking sequential IO.\n\n- We don't have to maintain our own mm tracking/setting.\n\n- We don't need a separate workqueue for buffered writes. This didn't\n  even work that well to begin with, as it was suboptimal for multiple\n  buffered writers on multiple files.\n\n- We can properly cancel pending interruptible work. This fixes\n  deadlocks with particularly socket IO, where we cannot cancel them\n  when the io_uring is closed. Hence the ring will wait forever for\n  these requests to complete, which may never happen. This is different\n  from disk IO where we know requests will complete in a finite amount\n  of time.\n\n- Due to being able to cancel work interruptible work that is already\n  running, we can implement file table support for work. We need that\n  for supporting system calls that add to a process file table.\n\n- It gets us one step closer to adding async support for any system\n  call.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 12:43:06 -0600 io_uring: replace workqueue usage with io-wq"
    },
    {
        "commit": "771b53d033e8663abdf59704806aa856b236dcdb",
        "message": "This adds support for io-wq, a smaller and specialized thread pool\nimplementation. This is meant to replace workqueues for io_uring. Among\nthe reasons for this addition are:\n\n- We can assign memory context smarter and more persistently if we\n  manage the life time of threads.\n\n- We can drop various work-arounds we have in io_uring, like the\n  async_list.\n\n- We can implement hashed work insertion, to manage concurrency of\n  buffered writes without needing a) an extra workqueue, or b)\n  needlessly making the concurrency of said workqueue very low\n  which hurts performance of multiple buffered file writers.\n\n- We can implement cancel through signals, for cancelling\n  interruptible work like read/write (or send/recv) to/from sockets.\n\n- We need the above cancel for being able to assign and use file tables\n  from a process.\n\n- We can implement a more thorough cancel operation in general.\n\n- We need it to move towards a syslet/threadlet model for even faster\n  async execution. For that we need to take ownership of the used\n  threads.\n\nThis list is just off the top of my head. Performance should be the\nsame, or better, at least that's what I've seen in my testing. io-wq\nsupports basic NUMA functionality, setting up a pool per node.\n\nio-wq hooks up to the scheduler schedule in/out just like workqueue\nand uses that to drive the need for more/less workers.\n\nAcked-by: Peter Zijlstra (Intel) <peterz@infradead.org>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 12:43:00 -0600 io-wq: small threadpool implementation for io_uring"
    },
    {
        "commit": "95a1b3ff9a3e4ea2f26c4e802067d58831f415db",
        "message": "Commit fb5ccc98782f (\"io_uring: Fix broken links with offloading\")\nintroduced a potential performance regression with unconditionally\ntaking mm even for READ/WRITE_FIXED operations.\n\nReturn the logic handling it back. mm-faulted requests will go through\nthe generic submission path, so honoring links and drains, but will\nfail further on req->has_user check.\n\nFixes: fb5ccc98782f (\"io_uring: Fix broken links with offloading\")\nCc: stable@vger.kernel.org # v5.4\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 10:24:23 -0600 io_uring: Fix mm_fault with READ/WRITE_FIXED"
    },
    {
        "commit": "fa4562280889ad372dfb1413833a8b8675721b17",
        "message": "submit->index is used only for inbound check in submission path (i.e.\nhead < ctx->sq_entries). However, it always will be true, as\n1. it's already validated by io_get_sqring()\n2. ctx->sq_entries can't be changedd in between, because of held\nctx->uring_lock and ctx->refs.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 10:24:21 -0600 io_uring: remove index from sqe_submit"
    },
    {
        "commit": "c826bd7a743f275e2b68c16d595534063b400deb",
        "message": "To trace io_uring activity one can get an information from workqueue and\nio trace events, but looks like some parts could be hard to identify via\nthis approach. Making what happens inside io_uring more transparent is\nimportant to be able to reason about many aspects of it, hence introduce\nthe set of tracing events.\n\nAll such events could be roughly divided into two categories:\n\n* those, that are helping to understand correctness (from both kernel\n  and an application point of view). E.g. a ring creation, file\n  registration, or waiting for available CQE. Proposed approach is to\n  get a pointer to an original structure of interest (ring context, or\n  request), and then find relevant events. io_uring_queue_async_work\n  also exposes a pointer to work_struct, to be able to track down\n  corresponding workqueue events.\n\n* those, that provide performance related information. Mostly it's about\n  events that change the flow of requests, e.g. whether an async work\n  was queued, or delayed due to some dependencies. Another important\n  case is how io_uring optimizations (e.g. registered files) are\n  utilized.\n\nSigned-off-by: Dmitrii Dolgov <9erthalion6@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 10:24:18 -0600 io_uring: add set of tracing events"
    },
    {
        "commit": "11365043e5271fea4c92189a976833da477a3a44",
        "message": "We might have cases where the need for a specific timeout is gone, add\nsupport for canceling an existing timeout operation. This works like the\nPOLL_REMOVE command, where the application passes in the user_data of\nthe timeout it wishes to cancel in the sqe->addr field.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 10:22:50 -0600 io_uring: add support for canceling timeout requests"
    },
    {
        "commit": "a41525ab2e75987e809926352ebc6f1397da900e",
        "message": "This is a pretty trivial addition on top of the relative timeouts\nwe have now, but it's handy for ensuring tighter timing for those\nthat are building scheduling primitives on top of io_uring.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 10:22:48 -0600 io_uring: add support for absolute timeouts"
    },
    {
        "commit": "ba5290ccb6b57fc5e274ae46d051fba1f0ece262",
        "message": "There is no function change, just to clean up the code, use s->in_async\nto make the code know where it is.\n\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 10:22:47 -0600 io_uring: replace s->needs_lock with s->in_async"
    },
    {
        "commit": "33a107f0a1b8df0ad925e39d8afc97bb78e0cec1",
        "message": "We currently size the CQ ring as twice the SQ ring, to allow some\nflexibility in not overflowing the CQ ring. This is done because the\nSQE life time is different than that of the IO request itself, the SQE\nis consumed as soon as the kernel has seen the entry.\n\nCertain application don't need a huge SQ ring size, since they just\nsubmit IO in batches. But they may have a lot of requests pending, and\nhence need a big CQ ring to hold them all. By allowing the application\nto control the CQ ring size multiplier, we can cater to those\napplications more efficiently.\n\nIf an application wants to define its own CQ ring size, it must set\nIORING_SETUP_CQSIZE in the setup flags, and fill out\nio_uring_params->cq_entries. The value must be a power of two.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 10:22:46 -0600 io_uring: allow application controlled CQ ring size"
    },
    {
        "commit": "c3a31e605620c279163c14068a60869ea3fda203",
        "message": "Allows the application to remove/replace/add files to/from a file set.\nPasses in a struct:\n\nstruct io_uring_files_update {\n\t__u32 offset;\n\t__s32 *fds;\n};\n\nthat holds an array of fds, size of array passed in through the usual\nnr_args part of the io_uring_register() system call. The logic is as\nfollows:\n\n1) If ->fds[i] is -1, the existing file at i + ->offset is removed from\n   the set.\n2) If ->fds[i] is a valid fd, the existing file at i + ->offset is\n   replaced with ->fds[i].\n\nFor case #2, is the existing file is currently empty (fd == -1), the\nnew fd is simply added to the array.\n\nReviewed-by: Jeff Moyer <jmoyer@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 10:22:44 -0600 io_uring: add support for IORING_REGISTER_FILES_UPDATE"
    },
    {
        "commit": "08a451739a9b5783f67de51e84cb6d9559bb9dc4",
        "message": "This is in preparation for allowing updates to fixed file sets without\nrequiring a full unregister+register.\n\nReviewed-by: Jeff Moyer <jmoyer@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 10:22:43 -0600 io_uring: allow sparse fixed file sets"
    },
    {
        "commit": "ba816ad61fdf31f59f423a773b00bfa2ed38243a",
        "message": "Currently any dependent link is executed from a new workqueue context,\nwhich means that we'll be doing a context switch per link in the chain.\nIf we are running the completion of the current request from our async\nworkqueue and find that the next request is a link, then run it directly\nfrom the workqueue context instead of forcing another switch.\n\nThis improves the performance of linked SQEs, and reduces the CPU\noverhead.\n\nReviewed-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.5-rc1",
        "release_date": "2019-10-29 10:22:41 -0600 io_uring: run dependent links inline if possible"
    },
    {
        "commit": "044c1ab399afbe9f2ebef49a3204ef1509826dc7",
        "message": "syzkaller reported an issue where it looks like a malicious app can\ntrigger a use-after-free of reading the ctx ->sq_array and ->rings\nvalue right after having installed the ring fd in the process file\ntable.\n\nDefer ring fd installation until after we're done reading those\nvalues.\n\nFixes: 75b28affdd6a (\"io_uring: allocate the two rings together\")\nReported-by: syzbot+6f03d895a6cd0d06187f@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc6",
        "release_date": "2019-10-28 09:15:33 -0600 io_uring: don't touch ctx in setup after ring fd install"
    },
    {
        "commit": "7b20238d28da46f394d37d4d51cc420e1ff9414a",
        "message": "io_queue_link_head() owns shadow_req after taking it as an argument.\nBy not freeing it in case of an error, it can leak the request along\nwith taken ctx->refs.\n\nReviewed-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc6",
        "release_date": "2019-10-27 21:29:18 -0600 io_uring: Fix leaked shadow_req"
    },
    {
        "commit": "acf913b7fb89f4313806ceb317d3730067f84f20",
        "message": "Pull block and io_uring fixes from Jens Axboe:\n \"A bit bigger than usual at this point in time, mostly due to some good\n  bug hunting work by Pavel that resulted in three io_uring fixes from\n  him and two from me. Anyway, this pull request contains:\n\n   - Revert of the submit-and-wait optimization for io_uring, it can't\n     always be done safely. It depends on commands always making\n     progress on their own, which isn't necessarily the case outside of\n     strict file IO. (me)\n\n   - Series of two patches from me and three from Pavel, fixing issues\n     with shared data and sequencing for io_uring.\n\n   - Lastly, two timeout sequence fixes for io_uring (zhangyi)\n\n   - Two nbd patches fixing races (Josef)\n\n   - libahci regulator_get_optional() fix (Mark)\"\n\n* tag 'for-linus-2019-10-26' of git://git.kernel.dk/linux-block:\n  nbd: verify socket is supported during setup\n  ata: libahci_platform: Fix regulator_get_optional() misuse\n  nbd: handle racing with error'ed out commands\n  nbd: protect cmd->status with cmd->lock\n  io_uring: fix bad inflight accounting for SETUP_IOPOLL|SETUP_SQTHREAD\n  io_uring: used cached copies of sq->dropped and cq->overflow\n  io_uring: Fix race for sqes with userspace\n  io_uring: Fix broken links with offloading\n  io_uring: Fix corrupted user_data\n  io_uring: correct timeout req sequence when inserting a new entry\n  io_uring : correct timeout req sequence when waiting timeout\n  io_uring: revert \"io_uring: optimize submit_and_wait API\"",
        "kernel_version": "v5.4-rc5",
        "release_date": "2019-10-26 14:59:51 -0400 Merge tag 'for-linus-2019-10-26' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "2b2ed9750fc9d040b9f6d076afcef6f00b6f1f7c",
        "message": "We currently assume that submissions from the sqthread are successful,\nand if IO polling is enabled, we use that value for knowing how many\ncompletions to look for. But if we overflowed the CQ ring or some\nrequests simply got errored and already completed, they won't be\navailable for polling.\n\nFor the case of IO polling and SQTHREAD usage, look at the pending\npoll list. If it ever hits empty then we know that we don't have\nanymore pollable requests inflight. For that case, simply reset\nthe inflight count to zero.\n\nReported-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc5",
        "release_date": "2019-10-25 10:58:53 -0600 io_uring: fix bad inflight accounting for SETUP_IOPOLL|SETUP_SQTHREAD"
    },
    {
        "commit": "498ccd9eda49117c34e0041563d0da6ac40e52b8",
        "message": "We currently use the ring values directly, but that can lead to issues\nif the application is malicious and changes these values on our behalf.\nCreated in-kernel cached versions of them, and just overwrite the user\nside when we update them. This is similar to how we treat the sq/cq\nring tail/head updates.\n\nReported-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc5",
        "release_date": "2019-10-25 10:58:45 -0600 io_uring: used cached copies of sq->dropped and cq->overflow"
    },
    {
        "commit": "935d1e45908afb8853c497f2c2bbbb685dec51dc",
        "message": "io_ring_submit() finalises with\n1. io_commit_sqring(), which releases sqes to the userspace\n2. Then calls to io_queue_link_head(), accessing released head's sqe\n\nReorder them.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc5",
        "release_date": "2019-10-25 09:02:01 -0600 io_uring: Fix race for sqes with userspace"
    },
    {
        "commit": "fb5ccc98782f654778cb8d96ba8a998304f9a51f",
        "message": "io_sq_thread() processes sqes by 8 without considering links. As a\nresult, links will be randomely subdivided.\n\nThe easiest way to fix it is to call io_get_sqring() inside\nio_submit_sqes() as do io_ring_submit().\n\nDownsides:\n1. This removes optimisation of not grabbing mm_struct for fixed files\n2. It submitting all sqes in one go, without finer-grained sheduling\nwith cq processing.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc5",
        "release_date": "2019-10-25 09:01:59 -0600 io_uring: Fix broken links with offloading"
    },
    {
        "commit": "84d55dc5b9e57b513a702fbc358e1b5489651590",
        "message": "There is a bug, where failed linked requests are returned not with\nspecified @user_data, but with garbage from a kernel stack.\n\nThe reason is that io_fail_links() uses req->user_data, which is\nuninitialised when called from io_queue_sqe() on fail path.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc5",
        "release_date": "2019-10-25 09:01:58 -0600 io_uring: Fix corrupted user_data"
    },
    {
        "commit": "a1f58ba46f794b1168d1107befcf3d4b9f9fd453",
        "message": "The sequence number of the timeout req (req->sequence) indicate the\nexpected completion request. Because of each timeout req consume a\nsequence number, so the sequence of each timeout req on the timeout\nlist shouldn't be the same. But now, we may get the same number (also\nincorrect) if we insert a new entry before the last one, such as submit\nsuch two timeout reqs on a new ring instance below.\n\n                    req->sequence\n req_1 (count = 2):       2\n req_2 (count = 1):       2\n\nThen, if we submit a nop req, req_2 will still timeout even the nop req\nfinished. This patch fix this problem by adjust the sequence number of\neach reordered reqs when inserting a new entry.\n\nSigned-off-by: zhangyi (F) <yi.zhang@huawei.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc5",
        "release_date": "2019-10-23 22:09:56 -0600 io_uring: correct timeout req sequence when inserting a new entry"
    },
    {
        "commit": "ef03681ae8df770745978148a7fb84796ae99cba",
        "message": "The sequence number of reqs on the timeout_list before the timeout req\nshould be adjusted in io_timeout_fn(), because the current timeout req\nwill consumes a slot in the cq_ring and cq_tail pointer will be\nincreased, otherwise other timeout reqs may return in advance without\nwaiting for enough wait_nr.\n\nSigned-off-by: zhangyi (F) <yi.zhang@huawei.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc5",
        "release_date": "2019-10-23 22:09:56 -0600 io_uring : correct timeout req sequence when waiting timeout"
    },
    {
        "commit": "bc808bced39f4e4b626c5ea8c63d5e41fce7205a",
        "message": "There are cases where it isn't always safe to block for submission,\neven if the caller asked to wait for events as well. Revert the\nprevious optimization of doing that.\n\nThis reverts two commits:\n\nbf7ec93c644cb\nc576666863b78\n\nFixes: c576666863b78 (\"io_uring: optimize submit_and_wait API\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc5",
        "release_date": "2019-10-23 22:09:56 -0600 io_uring: revert \"io_uring: optimize submit_and_wait API\""
    },
    {
        "commit": "d418d070057c45fd6f21567278f95452bfe690d1",
        "message": "Pull block fixes from Jens Axboe:\n\n - NVMe pull request from Keith that address deadlocks, double resets,\n   memory leaks, and other regression.\n\n - Fixup elv_support_iosched() for bio based devices (Damien)\n\n - Fixup for the ahci PCS quirk (Dan)\n\n - Socket O_NONBLOCK handling fix for io_uring (me)\n\n - Timeout sequence io_uring fixes (yangerkun)\n\n - MD warning fix for parameter default_layout (Song)\n\n - blkcg activation fixes (Tejun)\n\n - blk-rq-qos node deletion fix (Tejun)\n\n* tag 'for-linus-2019-10-18' of git://git.kernel.dk/linux-block:\n  nvme-pci: Set the prp2 correctly when using more than 4k page\n  io_uring: fix logic error in io_timeout\n  io_uring: fix up O_NONBLOCK handling for sockets\n  md/raid0: fix warning message for parameter default_layout\n  libata/ahci: Fix PCS quirk application\n  blk-rq-qos: fix first node deletion of rq_qos_del()\n  blkcg: Fix multiple bugs in blkcg_activate_policy()\n  io_uring: consider the overflow of sequence for timeout req\n  nvme-tcp: fix possible leakage during error flow\n  nvmet-loop: fix possible leakage during error flow\n  block: Fix elv_support_iosched()\n  nvme-tcp: Initialize sk->sk_ll_usec only with NET_RX_BUSY_POLL\n  nvme: Wait for reset state when required\n  nvme: Prevent resets during paused controller state\n  nvme: Restart request timers in resetting state\n  nvme: Remove ADMIN_ONLY state\n  nvme-pci: Free tagset if no IO queues\n  nvme: retain split access workaround for capability reads\n  nvme: fix possible deadlock when nvme_update_formats fails",
        "kernel_version": "v5.4-rc4",
        "release_date": "2019-10-18 22:29:36 -0400 Merge tag 'for-linus-2019-10-18' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "8b07a65ad30e5612d9590fb50468ff4fa314cfc7",
        "message": "If ctx->cached_sq_head < nxt_sq_head, we should add UINT_MAX to tmp, not\ntmp_nxt.\n\nFixes: 5da0fb1ab34c (\"io_uring: consider the overflow of sequence for timeout req\")\nSigned-off-by: yangerkun <yangerkun@huawei.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc4",
        "release_date": "2019-10-17 15:49:15 -0600 io_uring: fix logic error in io_timeout"
    },
    {
        "commit": "491381ce07ca57f68c49c79a8a43da5b60749e32",
        "message": "We've got two issues with the non-regular file handling for non-blocking\nIO:\n\n1) We don't want to re-do a short read in full for a non-regular file,\n   as we can't just read the data again.\n2) For non-regular files that don't support non-blocking IO attempts,\n   we need to punt to async context even if the file is opened as\n   non-blocking. Otherwise the caller always gets -EAGAIN.\n\nAdd two new request flags to handle these cases. One is just a cache\nof the inode S_ISREG() status, the other tells io_uring that we always\nneed to punt this request to async context, even if REQ_F_NOWAIT is set.\n\nCc: stable@vger.kernel.org\nReported-by: Hrvoje Zeba <zeba.hrvoje@gmail.com>\nTested-by: Hrvoje Zeba <zeba.hrvoje@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc4",
        "release_date": "2019-10-17 15:49:11 -0600 io_uring: fix up O_NONBLOCK handling for sockets"
    },
    {
        "commit": "5da0fb1ab34ccfe6d49210b4f5a739c59fcbf25e",
        "message": "Now we recalculate the sequence of timeout with 'req->sequence =\nctx->cached_sq_head + count - 1', judge the right place to insert\nfor timeout_list by compare the number of request we still expected for\ncompletion. But we have not consider about the situation of overflow:\n\n1. ctx->cached_sq_head + count - 1 may overflow. And a bigger count for\nthe new timeout req can have a small req->sequence.\n\n2. cached_sq_head of now may overflow compare with before req. And it\nwill lead the timeout req with small req->sequence.\n\nThis overflow will lead to the misorder of timeout_list, which can lead\nto the wrong order of the completion of timeout_list. Fix it by reuse\nreq->submit.sequence to store the count, and change the logic of\ninserting sort in io_timeout.\n\nSigned-off-by: yangerkun <yangerkun@huawei.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc4",
        "release_date": "2019-10-15 08:55:50 -0600 io_uring: consider the overflow of sequence for timeout req"
    },
    {
        "commit": "b27528b0279a84d889d40cb1b61962c8562cebb4",
        "message": "Pull io_uring fix from Jens Axboe:\n \"Single small fix for a regression in the sequence logic for linked\n  commands\"\n\n* tag 'for-linus-20191012' of git://git.kernel.dk/linux-block:\n  io_uring: fix sequence logic for timeout requests",
        "kernel_version": "v5.4-rc3",
        "release_date": "2019-10-13 08:15:35 -0700 Merge tag 'for-linus-20191012' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "297cbcccc23d4eefbc3043cd2fa5cf577930e695",
        "message": "Pull block fixes from Jens Axboe:\n\n - Fix wbt performance regression introduced with the blk-rq-qos\n   refactoring (Harshad)\n\n - Fix io_uring fileset removal inadvertently killing the workqueue (me)\n\n - Fix io_uring typo in linked command nonblock submission (Pavel)\n\n - Remove spurious io_uring wakeups on request free (Pavel)\n\n - Fix null_blk zoned command error return (Keith)\n\n - Don't use freezable workqueues for backing_dev, also means we can\n   revert a previous libata hack (Mika)\n\n - Fix nbd sysfs mutex dropped too soon at removal time (Xiubo)\n\n* tag 'for-linus-20191010' of git://git.kernel.dk/linux-block:\n  nbd: fix possible sysfs duplicate warning\n  null_blk: Fix zoned command return code\n  io_uring: only flush workqueues on fileset removal\n  io_uring: remove wait loop spurious wakeups\n  blk-wbt: fix performance regression in wbt scale_up/scale_down\n  Revert \"libata, freezer: avoid block device removal while system is frozen\"\n  bdi: Do not use freezable workqueue\n  io_uring: fix reversed nonblock flag for link submission",
        "kernel_version": "v5.4-rc3",
        "release_date": "2019-10-11 08:45:32 -0700 Merge tag 'for-linus-20191010' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "7adf4eaf60f3d8c3584bed51fe7066d4dfc2cbe1",
        "message": "We have two ways a request can be deferred:\n\n1) It's a regular request that depends on another one\n2) It's a timeout that tracks completions\n\nWe have a shared helper to determine whether to defer, and that\nattempts to make the right decision based on the request. But we\nonly have some of this information in the caller. Un-share the\ntwo timeout/defer helpers so the caller can use the right one.\n\nFixes: 5262f567987d (\"io_uring: IORING_OP_TIMEOUT support\")\nReported-by: yangerkun <yangerkun@huawei.com>\nReviewed-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc3",
        "release_date": "2019-10-10 21:42:58 -0600 io_uring: fix sequence logic for timeout requests"
    },
    {
        "commit": "8a99734081775c012a4a6c442fdef0379fe52bdf",
        "message": "We should not remove the workqueue, we just need to ensure that the\nworkqueues are synced. The workqueues are torn down on ctx removal.\n\nCc: stable@vger.kernel.org\nFixes: 6b06314c47e1 (\"io_uring: add file set registration\")\nReported-by: Stefan Hajnoczi <stefanha@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc3",
        "release_date": "2019-10-09 15:13:47 -0600 io_uring: only flush workqueues on fileset removal"
    },
    {
        "commit": "6805b32ec2b0897eb180295385efe306e5ac3b3d",
        "message": "Any changes interesting to tasks waiting in io_cqring_wait() are\ncommited with io_cqring_ev_posted(). However, io_ring_drop_ctx_refs()\nalso tries to do that but with no reason, that means spurious wakeups\nevery io_free_req() and io_uring_enter().\n\nJust use percpu_ref_put() instead.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc3",
        "release_date": "2019-10-07 21:16:24 -0600 io_uring: remove wait loop spurious wakeups"
    },
    {
        "commit": "c4bd70e8c95b2b045ac686b4c654bf9bfbfe9f3b",
        "message": "Pull block fixes from Jens Axboe:\n\n - Mandate timespec64 for the io_uring timeout ABI (Arnd)\n\n - Set of NVMe changes via Sagi:\n     - controller removal race fix from Balbir\n     - quirk additions from Gabriel and Jian-Hong\n     - nvme-pci power state save fix from Mario\n     - Add 64bit user commands (for 64bit registers) from Marta\n     - nvme-rdma/nvme-tcp fixes from Max, Mark and Me\n     - Minor cleanups and nits from James, Dan and John\n\n - Two s390 dasd fixes (Jan, Stefan)\n\n - Have loop change block size in DIO mode (Martijn)\n\n - paride pg header ifdef guard (Masahiro)\n\n - Two blk-mq queue scheduler tweaks, fixing an ordering issue on zoned\n   devices and suboptimal performance on others (Ming)\n\n* tag 'for-linus-2019-10-03' of git://git.kernel.dk/linux-block: (22 commits)\n  block: sed-opal: fix sparse warning: convert __be64 data\n  block: sed-opal: fix sparse warning: obsolete array init.\n  block: pg: add header include guard\n  Revert \"s390/dasd: Add discard support for ESE volumes\"\n  s390/dasd: Fix error handling during online processing\n  io_uring: use __kernel_timespec in timeout ABI\n  loop: change queue block size to match when using DIO\n  blk-mq: apply normal plugging for HDD\n  blk-mq: honor IO scheduler for multiqueue devices\n  nvme-rdma: fix possible use-after-free in connect timeout\n  nvme: Move ctrl sqsize to generic space\n  nvme: Add ctrl attributes for queue_count and sqsize\n  nvme: allow 64-bit results in passthru commands\n  nvme: Add quirk for Kingston NVME SSD running FW E8FK11.T\n  nvmet-tcp: remove superflous check on request sgl\n  Added QUIRKs for ADATA XPG SX8200 Pro 512GB\n  nvme-rdma: Fix max_hw_sectors calculation\n  nvme: fix an error code in nvme_init_subsystem()\n  nvme-pci: Save PCI state before putting drive into deepest state\n  nvme-tcp: fix wrong stop condition in io_work\n  ...",
        "kernel_version": "v5.4-rc2",
        "release_date": "2019-10-04 09:56:51 -0700 Merge tag 'for-linus-2019-10-03' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "bf7ec93c644cb0064ba7d2fc40d4841c5ba382ab",
        "message": "io_queue_link_head() accepts @force_nonblock flag, but io_ring_submit()\npasses something opposite.\n\nFixes: c576666863b78 (\"io_uring: optimize submit_and_wait API\")\nReported-by: kbuild test robot <lkp@intel.com>\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc3",
        "release_date": "2019-10-04 08:31:15 -0600 io_uring: fix reversed nonblock flag for link submission"
    },
    {
        "commit": "bdf200731145f07a6127cb16753e2e8fdc159cf4",
        "message": "All system calls use struct __kernel_timespec instead of the old struct\ntimespec, but this one was just added with the old-style ABI. Change it\nnow to enforce the use of __kernel_timespec, avoiding ABI confusion and\nthe need for compat handlers on 32-bit architectures.\n\nAny user space caller will have to use __kernel_timespec now, but this\nis unambiguous and works for any C library regardless of the time_t\ndefinition. A nicer way to specify the timeout would have been a less\nambiguous 64-bit nanosecond value, but I suppose it's too late now to\nchange that as this would impact both 32-bit and 64-bit users.\n\nFixes: 5262f567987d (\"io_uring: IORING_OP_TIMEOUT support\")\nSigned-off-by: Arnd Bergmann <arnd@arndb.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc2",
        "release_date": "2019-10-01 09:53:29 -0600 io_uring: use __kernel_timespec in timeout ABI"
    },
    {
        "commit": "738f531d877ac2b228b25354dfa4da6e79a2c369",
        "message": "Pull more io_uring updates from Jens Axboe:\n \"Just two things in here:\n\n   - Improvement to the io_uring CQ ring wakeup for batched IO (me)\n\n   - Fix wrong comparison in poll handling (yangerkun)\n\n  I realize the first one is a little late in the game, but it felt\n  pointless to hold it off until the next release. Went through various\n  testing and reviews with Pavel and peterz\"\n\n* tag 'for-5.4/io_uring-2019-09-27' of git://git.kernel.dk/linux-block:\n  io_uring: make CQ ring wakeups be more efficient\n  io_uring: compare cached_cq_tail with cq.head in_io_uring_poll",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-27 12:08:24 -0700 Merge tag 'for-5.4/io_uring-2019-09-27' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "bda521624e75c665c407b3d9cece6e7a28178cd8",
        "message": "For batched IO, it's not uncommon for waiters to ask for more than 1\nIO to complete before being woken up. This is a problem with\nwait_event() since tasks will get woken for every IO that completes,\nre-check condition, then go back to sleep. For batch counts on the\norder of what you do for high IOPS, that can result in 10s of extra\nwakeups for the waiting task.\n\nAdd a private wake function that checks for the wake up count criteria\nbeing met before calling autoremove_wake_function(). Pavel reports that\none test case he has runs 40% faster with proper batching of wakeups.\n\nReported-by: Pavel Begunkov <asml.silence@gmail.com>\nTested-by: Pavel Begunkov <asml.silence@gmail.com>\nReviewed-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-26 03:55:40 -0600 io_uring: make CQ ring wakeups be more efficient"
    },
    {
        "commit": "b6cb84b4fc580098a5934078e4c8dc39e3925f07",
        "message": "Pull more io_uring updates from Jens Axboe:\n \"A collection of later fixes and additions, that weren't quite ready\n  for pushing out with the initial pull request.\n\n  This contains:\n\n   - Fix potential use-after-free of shadow requests (Jackie)\n\n   - Fix potential OOM crash in request allocation (Jackie)\n\n   - kmalloc+memcpy -> kmemdup cleanup (Jackie)\n\n   - Fix poll crash regression (me)\n\n   - Fix SQ thread not being nice and giving up CPU for !PREEMPT (me)\n\n   - Add support for timeouts, making it easier to do epoll_wait()\n     conversions, for instance (me)\n\n   - Ensure io_uring works without f_ops->read_iter() and\n     f_ops->write_iter() (me)\"\n\n* tag 'for-5.4/io_uring-2019-09-24' of git://git.kernel.dk/linux-block:\n  io_uring: correctly handle non ->{read,write}_iter() file_operations\n  io_uring: IORING_OP_TIMEOUT support\n  io_uring: use cond_resched() in sqthread\n  io_uring: fix potential crash issue due to io_get_req failure\n  io_uring: ensure poll commands clear ->sqe\n  io_uring: fix use-after-free of shadow_req\n  io_uring: use kmemdup instead of kmalloc and memcpy",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-24 16:40:21 -0700 Merge tag 'for-5.4/io_uring-2019-09-24' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "daa5de5415849b9a53056ec1e1e88fe4c5c9aa2b",
        "message": "After 75b28af(\"io_uring: allocate the two rings together\"), we compare\nsq.head with cached_cq_tail to determine does there any cq invalid.\nActually, we should use cq.head.\n\nFixes: 75b28affdd6a (\"io_uring: allocate the two rings together\")\nSigned-off-by: yangerkun <yangerkun@huawei.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-24 07:04:15 -0600 io_uring: compare cached_cq_tail with cq.head in_io_uring_poll"
    },
    {
        "commit": "32960613b7c3352ddf38c42596e28a16ae36335e",
        "message": "Currently we just -EINVAL a read or write to an fd that isn't backed\nby ->read_iter() or ->write_iter(). But we can handle them just fine,\nas long as we punt fo async context first.\n\nImplement a simple loop function for doing ->read() or ->write()\ninstead, and ensure we call it appropriately.\n\nReported-by: \u674e\u901a\u6d32 <carter.li@eoitek.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-23 11:05:34 -0600 io_uring: correctly handle non ->{read,write}_iter() file_operations"
    },
    {
        "commit": "5262f567987d3c30052b22e78c35c2313d07b230",
        "message": "There's been a few requests for functionality similar to io_getevents()\nand epoll_wait(), where the user can specify a timeout for waiting on\nevents. I deliberately did not add support for this through the system\ncall initially to avoid overloading the args, but I can see that the use\ncases for this are valid.\n\nThis adds support for IORING_OP_TIMEOUT. If a user wants to get woken\nwhen waiting for events, simply submit one of these timeout commands\nwith your wait call (or before). This ensures that the application\nsleeping on the CQ ring waiting for events will get woken. The timeout\ncommand is passed in as a pointer to a struct timespec. Timeouts are\nrelative. The timeout command also includes a way to auto-cancel after\nN events has passed.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-18 10:43:22 -0600 io_uring: IORING_OP_TIMEOUT support"
    },
    {
        "commit": "9831a90ce64362f8429e8fd23838a9db2cdf7803",
        "message": "If preempt isn't enabled in the kernel, we can run into hang issues with\nsqthread submissions. Use cond_resched() to play nice instead of\ncpu_relax(), if we end up starting the loop and not having any events\npending for submissions.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-19 09:49:26 -0600 io_uring: use cond_resched() in sqthread"
    },
    {
        "commit": "a1041c27b64ce744632147e19701c95fed14fab1",
        "message": "Sometimes io_get_req will return a NUL, then we need to do the\ncorrect error handling, otherwise it will cause the kernel null\npointer exception.\n\nFixes: 4fe2c963154c (\"io_uring: add support for link with drain\")\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-18 11:20:04 -0600 io_uring: fix potential crash issue due to io_get_req failure"
    },
    {
        "commit": "6cc47d1d2a9b631f62405f56df651975c7587a97",
        "message": "If we end up getting woken in poll (due to a signal), then we may need\nto punt the poll request to an async worker. When we do that, we look up\nthe list to queue at, deferefencing req->submit.sqe, however that is\nonly set for requests we initially decided to queue async.\n\nThis fixes a crash with poll command usage and wakeups that need to punt\nto async context.\n\nFixes: 54a91f3bb9b9 (\"io_uring: limit parallelism of buffered writes\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-18 11:19:26 -0600 io_uring: ensure poll commands clear ->sqe"
    },
    {
        "commit": "5f5ad9ced33621d353be6429c3900f8a526fcae8",
        "message": "There is a potential dangling pointer problem. we never clean\nshadow_req, if there are multiple link lists in this series of\nsqes, then the shadow_req will not reallocate, and continue to\nuse the last one. but in the previous, his memory has been\nreleased, thus forming a dangling pointer. let's clean up him\nand make sure that every new link list can reapply for a new\nshadow_req.\n\nFixes: 4fe2c963154c (\"io_uring: add support for link with drain\")\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-18 11:19:06 -0600 io_uring: fix use-after-free of shadow_req"
    },
    {
        "commit": "954dab193d19cbbff8f83b58c9360bf00ddb273c",
        "message": "Just clean up the code, no function changes.\n\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-18 11:19:06 -0600 io_uring: use kmemdup instead of kmalloc and memcpy"
    },
    {
        "commit": "1e6fa3a33e6dbe4f24bcc4690950f2888d4bed3a",
        "message": "Pull io_uring updates from Jens Axboe:\n\n - Allocate SQ/CQ ring together, more efficient. Expose this through a\n   feature flag as well, so we can reduce the number of mmaps by 1\n   (Hristo and me)\n\n - Fix for sequence logic with SQ thread (Jackie).\n\n - Add support for links with drain commands (Jackie).\n\n - Improved async merging (me)\n\n - Improved buffered async write performance (me)\n\n - Support SQ poll wakeup + event get in single io_uring_enter() (me)\n\n - Support larger SQ ring size. For epoll conversions, the 4k limit was\n   too small for some prod workloads (Daniel).\n\n - put_user_page() usage (John)\n\n* tag 'for-5.4/io_uring-2019-09-15' of git://git.kernel.dk/linux-block:\n  io_uring: increase IORING_MAX_ENTRIES to 32K\n  io_uring: make sqpoll wakeup possible with getevents\n  io_uring: extend async work merging\n  io_uring: limit parallelism of buffered writes\n  io_uring: add io_queue_async_work() helper\n  io_uring: optimize submit_and_wait API\n  io_uring: add support for link with drain\n  io_uring: fix wrong sequence setting logic\n  io_uring: expose single mmap capability\n  io_uring: allocate the two rings together\n  fs/io_uring.c: convert put_page() to put_user_page*()",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-17 16:50:19 -0700 Merge tag 'for-5.4/io_uring-2019-09-15' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "5277deaab9f98229bdfb8d1e30019b6c25052708",
        "message": "Some workloads can require far more than 4K oustanding entries. For\nexample memcached can have ~300K sockets over ~40 cores. Bumping the max\nto 32K seems to work pretty well.\n\nReported-by: Dan Melnic <dmm@fb.com>\nSigned-off-by: Daniel Xu <dxu@dxuuu.xyz>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-14 17:06:22 -0600 io_uring: increase IORING_MAX_ENTRIES to 32K"
    },
    {
        "commit": "b2a9eadab85730935f5a6fe19f3f61faaaced601",
        "message": "The way the logic is setup in io_uring_enter() means that you can't wake\nup the SQ poller thread while at the same time waiting (or polling) for\ncompletions afterwards. There's no reason for that to be the case.\n\nReported-by: Lewis Baker <lbaker@fb.com>\nReviewed-by: Jeff Moyer <jmoyer@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-12 14:19:16 -0600 io_uring: make sqpoll wakeup possible with getevents"
    },
    {
        "commit": "6d5d5ac522b20b65167dafe0656b7cad05ec48b3",
        "message": "We currently merge async work items if we see a strict sequential hit.\nThis helps avoid unnecessary workqueue switches when we don't need\nthem. We can extend this merging to cover cases where it's not a strict\nsequential hit, but the IO still fits within the same page. If an\napplication is doing multiple requests within the same page, we don't\nwant separate workers waiting on the same page to complete IO. It's much\nfaster to let the first worker bring in the page, then operate on that\npage from the same worker to complete the next request(s).\n\nReviewed-by: Jeff Moyer <jmoyer@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-12 14:18:48 -0600 io_uring: extend async work merging"
    },
    {
        "commit": "54a91f3bb9b96ed86bc12b2f7e06b3fce8e86503",
        "message": "All the popular filesystems need to grab the inode lock for buffered\nwrites. With io_uring punting buffered writes to async context, we\nobserve a lot of contention with all workers hamming this mutex.\n\nFor buffered writes, we generally don't need a lot of parallelism on\nthe submission side, as the flushing will take care of that for us.\nHence we don't need a deep queue on the write side, as long as we\ncan safely punt from the original submission context.\n\nAdd a workqueue with a limit of 2 that we can use for buffered writes.\nThis greatly improves the performance and efficiency of higher queue\ndepth buffered async writes with io_uring.\n\nReported-by: Andres Freund <andres@anarazel.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-10 09:49:35 -0600 io_uring: limit parallelism of buffered writes"
    },
    {
        "commit": "18d9be1a970c3704366df902b00871bea88d9f14",
        "message": "Add a helper for queueing a request for async execution, in preparation\nfor optimizing it.\n\nNo functional change in this patch.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-10 09:13:05 -0600 io_uring: add io_queue_async_work() helper"
    },
    {
        "commit": "c576666863b788c2d7e8ab4ef4edd0e9059cb47b",
        "message": "For some applications that end up using a submit-and-wait type of\napproach for certain batches of IO, we can make that a bit more\nefficient by allowing the application to block for the last IO\nsubmission. This prevents an async when we don't need it, as the\napplication will be blocking for the completion event(s) anyway.\n\nTypical use cases are using the liburing\nio_uring_submit_and_wait() API, or just using io_uring_enter()\ndoing both submissions and completions. As a specific example,\nRocksDB doing MultiGet() is sped up quite a bit with this\nchange.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-10 08:21:03 -0600 io_uring: optimize submit_and_wait API"
    },
    {
        "commit": "4fe2c963154c31227bec2f2d690e01f9cab383ea",
        "message": "To support the link with drain, we need to do two parts.\n\nThere is an sqes:\n\n    0     1     2     3     4     5     6\n +-----+-----+-----+-----+-----+-----+-----+\n |  N  |  L  |  L  | L+D |  N  |  N  |  N  |\n +-----+-----+-----+-----+-----+-----+-----+\n\nFirst, we need to ensure that the io before the link is completed,\nthere is a easy way is set drain flag to the link list's head, so\nall subsequent io will be inserted into the defer_list.\n\n\t+-----+\n    (0) |  N  |\n\t+-----+\n           |          (2)         (3)         (4)\n\t+-----+     +-----+     +-----+     +-----+\n    (1) | L+D | --> |  L  | --> | L+D | --> |  N  |\n\t+-----+     +-----+     +-----+     +-----+\n           |\n\t+-----+\n    (5) |  N  |\n\t+-----+\n           |\n\t+-----+\n    (6) |  N  |\n\t+-----+\n\nSecond, ensure that the following IO will not be completed first,\nan easy way is to create a mirror of drain io and insert it into\ndefer_list, in this way, as long as drain io is not processed, the\nfollowing io in the defer_list will not be actively process.\n\n\t+-----+\n    (0) |  N  |\n\t+-----+\n           |          (2)         (3)         (4)\n\t+-----+     +-----+     +-----+     +-----+\n    (1) | L+D | --> |  L  | --> | L+D | --> |  N  |\n\t+-----+     +-----+     +-----+     +-----+\n           |\n\t+-----+\n   ('3) |  D  |   <== This is a shadow of (3)\n\t+-----+\n           |\n\t+-----+\n    (5) |  N  |\n\t+-----+\n           |\n\t+-----+\n    (6) |  N  |\n\t+-----+\n\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-09 16:15:00 -0600 io_uring: add support for link with drain"
    },
    {
        "commit": "8776f3fa15a5cd213c4dfab7ddaf557983374ea6",
        "message": "Sqo_thread will get sqring in batches, which will cause\nctx->cached_sq_head to be added in batches. if one of these\nsqes is set with the DRAIN flag, then he will never get a\nchance to process, and finally sqo_thread will not exit.\n\nFixes: de0617e4671 (\"io_uring: add support for marking commands as draining\")\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-09 16:14:47 -0600 io_uring: fix wrong sequence setting logic"
    },
    {
        "commit": "ac90f249e15cd2a850daa9e36e15f81ce1ff6550",
        "message": "After commit 75b28affdd6a we can get by with just a single mmap to\nmap both the sq and cq ring. However, userspace doesn't know that.\n\nAdd a features variable to io_uring_params, and notify userspace\nthat the kernel has this ability. This can then be used in liburing\n(or in applications directly) to avoid the second mmap.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-09-06 10:26:21 -0600 io_uring: expose single mmap capability"
    },
    {
        "commit": "0a56e0603fa13af08816d673f6f71b68cda2fb2e",
        "message": "Copy over powerpc syscall.tbl to grab changes from the below commits:\n\n  commit cee3536d24a1 (\"powerpc: Wire up clone3 syscall\")\n  commit 1a271a68e030 (\"arch: mark syscall number 435 reserved for clone3\")\n  commit 7615d9e1780e (\"arch: wire-up pidfd_open()\")\n  commit d8076bdb56af (\"uapi: Wire up the mount API syscalls on non-x86 arches [ver #2]\")\n  commit 39036cd27273 (\"arch: add pidfd and io_uring syscalls everywhere\")\n  commit 48166e6ea47d (\"y2038: add 64-bit time_t syscalls to all 32-bit architectures\")\n  commit d33c577cccd0 (\"y2038: rename old time and utime syscalls\")\n  commit 00bf25d693e7 (\"y2038: use time32 syscall names on 32-bit\")\n  commit 8dabe7245bbc (\"y2038: syscalls: rename y2038 compat syscalls\")\n  commit 0d6040d46817 (\"arch: add split IPC system calls where needed\")\n\nReported-by: Nicholas Piggin <npiggin@gmail.com>\nSigned-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>\nCc: Ravi Bangoria <ravi.bangoria@linux.ibm.com>\nCc: linuxppc-dev@lists.ozlabs.org\nLink: http://lkml.kernel.org/r/20190827071458.19897-1-naveen.n.rao@linux.vnet.ibm.com\nSigned-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-08-28 10:25:38 -0300 perf arch powerpc: Sync powerpc syscall.tbl"
    },
    {
        "commit": "75b28affdd6aed1c68073ef53907c7bd822aff84",
        "message": "Both the sq and the cq rings have sizes just over a power of two, and\nthe sq ring is significantly smaller. By bundling them in a single\nalllocation, we get the sq ring for free.\n\nThis also means that IORING_OFF_SQ_RING and IORING_OFF_CQ_RING now mean\nthe same thing. If we indicate this to userspace, we can save a mmap\ncall.\n\nSigned-off-by: Hristo Venev <hristo@venev.name>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-08-27 10:42:02 -0600 io_uring: allocate the two rings together"
    },
    {
        "commit": "27c4d3a3252fa661b049b8f57a548bba331133e0",
        "message": "For pages that were retained via get_user_pages*(), release those pages\nvia the new put_user_page*() routines, instead of via put_page() or\nrelease_pages().\n\nThis is part a tree-wide conversion, as described in commit fc1d8e7cca2d\n(\"mm: introduce put_user_page*(), placeholder versions\").\n\nCc: Alexander Viro <viro@zeniv.linux.org.uk>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: linux-fsdevel@vger.kernel.org\nCc: linux-block@vger.kernel.org\nSigned-off-by: John Hubbard <jhubbard@nvidia.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-08-27 10:41:41 -0600 fs/io_uring.c: convert put_page() to put_user_page*()"
    },
    {
        "commit": "b9bd6806d014f7d3647caa66ea245329a3ee0253",
        "message": "Pull block fixes from Jens Axboe:\n \"Here's a set of fixes that should go into this release. This contains:\n\n   - Three minor fixes for NVMe.\n\n   - Three minor tweaks for the io_uring polling logic.\n\n   - Officially mark Song as the MD maintainer, after he's been filling\n     that role sucessfully for the last 6 months or so\"\n\n* tag 'for-linus-20190823' of git://git.kernel.dk/linux-block:\n  io_uring: add need_resched() check in inner poll loop\n  md: update MAINTAINERS info\n  io_uring: don't enter poll loop if we have CQEs pending\n  nvme: Add quirk for LiteON CL1 devices running FW 22301111\n  nvme: Fix cntlid validation when not using NVMEoF\n  nvme-multipath: fix possible I/O hang when paths are updated\n  io_uring: fix potential hang with polled IO",
        "kernel_version": "v5.3-rc6",
        "release_date": "2019-08-23 14:45:45 -0700 Merge tag 'for-linus-20190823' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "08f5439f1df25a6cf6cf4c72cf6c13025599ce67",
        "message": "The outer poll loop checks for whether we need to reschedule, and\nreturns to userspace if we do. However, it's possible to get stuck\nin the inner loop as well, if the CPU we are running on needs to\nreschedule to finish the IO work.\n\nAdd the need_resched() check in the inner loop as well. This fixes\na potential hang if the kernel is configured with\nCONFIG_PREEMPT_VOLUNTARY=y.\n\nReported-by: Sagi Grimberg <sagi@grimberg.me>\nReviewed-by: Sagi Grimberg <sagi@grimberg.me>\nTested-by: Sagi Grimberg <sagi@grimberg.me>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc6",
        "release_date": "2019-08-22 15:32:28 -0600 io_uring: add need_resched() check in inner poll loop"
    },
    {
        "commit": "a3a0e43fd77013819e4b6f55e37e0efe8e35d805",
        "message": "We need to check if we have CQEs pending before starting a poll loop,\nas those could be the events we will be spinning for (and hence we'll\nfind none). This can happen if a CQE triggers an error, or if it is\nfound by eg an IRQ before we get a chance to find it through polling.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc6",
        "release_date": "2019-08-20 11:03:11 -0600 io_uring: don't enter poll loop if we have CQEs pending"
    },
    {
        "commit": "500f9fbadef86466a435726192f4ca4df7d94236",
        "message": "If a request issue ends up being punted to async context to avoid\nblocking, we can get into a situation where the original application\nenters the poll loop for that very request before it has been issued.\nThis should not be an issue, except that the polling will hold the\nio_uring uring_ctx mutex for the duration of the poll. When the async\nworker has actually issued the request, it needs to acquire this mutex\nto add the request to the poll issued list. Since the application\npolling is already holding this mutex, the workqueue sleeps on the\nmutex forever, and the application thus never gets a chance to poll for\nthe very request it was interested in.\n\nFix this by ensuring that the polling drops the uring_ctx occasionally\nif it's not making any progress.\n\nReported-by: Jeffrey M. Birnbaum <jmbnyc@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc6",
        "release_date": "2019-08-20 11:01:58 -0600 io_uring: fix potential hang with polled IO"
    },
    {
        "commit": "8fde2832bd0bdc5a2b57330a9e9c3d2fa16bd1d8",
        "message": "Pull block fixes from Jens Axboe:\n \"A collection of fixes that should go into this series. This contains:\n\n   - Revert of the REQ_NOWAIT_INLINE and associated dio changes. There\n     were still corner cases there, and even though I had a solution for\n     it, it's too involved for this stage. (me)\n\n   - Set of NVMe fixes (via Sagi)\n\n   - io_uring fix for fixed buffers (Anthony)\n\n   - io_uring defer issue fix (Jackie)\n\n   - Regression fix for queue sync at exit time (zhengbin)\n\n   - xen blk-back memory leak fix (Wenwen)\"\n\n* tag 'for-linus-2019-08-17' of git://git.kernel.dk/linux-block:\n  io_uring: fix an issue when IOSQE_IO_LINK is inserted into defer list\n  block: remove REQ_NOWAIT_INLINE\n  io_uring: fix manual setup of iov_iter for fixed buffers\n  xen/blkback: fix memory leaks\n  blk-mq: move cancel of requeue_work to the front of blk_exit_queue\n  nvme-pci: Fix async probe remove race\n  nvme: fix controller removal race with scan work\n  nvme-rdma: fix possible use-after-free in connect error flow\n  nvme: fix a possible deadlock when passthru commands sent to a multipath device\n  nvme-core: Fix extra device_put() call on error path\n  nvmet-file: fix nvmet_file_flush() always returning an error\n  nvmet-loop: Flush nvme_delete_wq when removing the port\n  nvmet: Fix use-after-free bug when a port is removed\n  nvme-multipath: revalidate nvme_ns_head gendisk in nvme_validate_ns",
        "kernel_version": "v5.3-rc5",
        "release_date": "2019-08-17 19:39:54 -0700 Merge tag 'for-linus-2019-08-17' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "c8186c8064c4c234a3d2e3d60e25c1f55ec4f8b3",
        "message": "Magnus Karlsson says:\n\n====================\nThis patch set adds support for a new flag called need_wakeup in the\nAF_XDP Tx and fill rings. When this flag is set by the driver, it\nmeans that the application has to explicitly wake up the kernel Rx\n(for the bit in the fill ring) or kernel Tx (for bit in the Tx ring)\nprocessing by issuing a syscall. Poll() can wake up both and sendto()\nwill wake up Tx processing only.\n\nThe main reason for introducing this new flag is to be able to\nefficiently support the case when application and driver is executing\non the same core. Previously, the driver was just busy-spinning on the\nfill ring if it ran out of buffers in the HW and there were none to\nget from the fill ring. This approach works when the application and\ndriver is running on different cores as the application can replenish\nthe fill ring while the driver is busy-spinning. Though, this is a\nlousy approach if both of them are running on the same core as the\nprobability of the fill ring getting more entries when the driver is\nbusy-spinning is zero. With this new feature the driver now sets the\nneed_wakeup flag and returns to the application. The application can\nthen replenish the fill queue and then explicitly wake up the Rx\nprocessing in the kernel using the syscall poll(). For Tx, the flag is\nonly set to one if the driver has no outstanding Tx completion\ninterrupts. If it has some, the flag is zero as it will be woken up by\na completion interrupt anyway. This flag can also be used in other\nsituations where the driver needs to be woken up explicitly.\n\nAs a nice side effect, this new flag also improves the Tx performance\nof the case where application and driver are running on two different\ncores as it reduces the number of syscalls to the kernel. The kernel\ntells user space if it needs to be woken up by a syscall, and this\neliminates many of the syscalls. The Rx performance of the 2-core case\nis on the other hand slightly worse, since there is a need to use a\nsyscall now to wake up the driver, instead of the driver\nbusy-spinning. It does waste less CPU cycles though, which might lead\nto better overall system performance.\n\nThis new flag needs some simple driver support. If the driver does not\nsupport it, the Rx flag is always zero and the Tx flag is always\none. This makes any application relying on this feature default to the\nold behavior of not requiring any syscalls in the Rx path and always\nhaving to call sendto() in the Tx path.\n\nFor backwards compatibility reasons, this feature has to be explicitly\nturned on using a new bind flag (XDP_USE_NEED_WAKEUP). I recommend\nthat you always turn it on as it has a large positive performance\nimpact for the one core case and does not degrade 2 core performance\nand actually improves it for Tx heavy workloads.\n\nHere are some performance numbers measured on my local,\nnon-performance optimized development system. That is why you are\nseeing numbers lower than the ones from Bj\u00f6rn and Jesper. 64 byte\npackets at 40Gbit/s line rate. All results in Mpps. Cores == 1 means\nthat both application and driver is executing on the same core. Cores\n== 2 that they are on different cores.\n\n                              Applications\nneed_wakeup  cores    txpush    rxdrop      l2fwd\n---------------------------------------------------------------\n     n         1       0.07      0.06        0.03\n     y         1       21.6      8.2         6.5\n     n         2       32.3      11.7        8.7\n     y         2       33.1      11.7        8.7\n\nOverall, the need_wakeup flag provides the same or better performance\nin all the micro-benchmarks. The reduction of sendto() calls in txpush\nis large. Only a few per second is needed. For l2fwd, the drop is 50%\nfor the 1 core case and more than 99.9% for the 2 core case. Do not\nknow why I am not seeing the same drop for the 1 core case yet.\n\nThe name and inspiration of the flag has been taken from io_uring by\nJens Axboe. Details about this feature in io_uring can be found in\nhttp://kernel.dk/io_uring.pdf, section 8.3. It also addresses most of\nthe denial of service and sendto() concerns raised by Maxim\nMikityanskiy in https://www.spinics.net/lists/netdev/msg554657.html.\n\nThe typical Tx part of an application will have to change from:\n\nret = sendto(fd,....)\n\nto:\n\nif (xsk_ring_prod__needs_wakeup(&xsk->tx))\n       ret = sendto(fd,....)\n\nand th Rx part from:\n\nrcvd = xsk_ring_cons__peek(&xsk->rx, BATCH_SIZE, &idx_rx);\nif (!rcvd)\n       return;\n\nto:\n\nrcvd = xsk_ring_cons__peek(&xsk->rx, BATCH_SIZE, &idx_rx);\nif (!rcvd) {\n       if (xsk_ring_prod__needs_wakeup(&xsk->umem->fq))\n              ret = poll(fd,.....);\n       return;\n}\n\nv3 -> v4:\n* Maxim found a possible race in the Tx part of the driver. The\n  setting of the flag needs to happen before the sending, otherwise it\n  might trigger this race. Fixed in ixgbe and i40e driver.\n* Mellanox support contributed by Maxim\n* Removed the XSK_DRV_CAN_SLEEP flag as it was not used\n  anymore. Thanks to Sridhar for discovering this.\n* For consistency the feature is now always called need_wakeup. There\n  were some places where it was referred to as might_sleep, but they\n  have been removed. Thanks to Sridhar for spotting.\n* Fixed some typos in the commit messages\n\nv2 -> v3:\n* Converted the Mellanox driver to the new ndo in patch 1 as pointed\n  out by Maxim\n* Fixed the compatibility code of XDP_MMAP_OFFSETS so it now works.\n\nv1 -> v2:\n* Fixed bisectability problem pointed out by Jakub\n* Added missing initiliztion of the Tx need_wakeup flag to 1\n\nThis patch has been applied against commit b753c5a7f99f (\"Merge branch 'r8152-RX-improve'\")\n\nStructure of the patch set:\n\nPatch 1: Replaces the ndo_xsk_async_xmit with ndo_xsk_wakeup to\n         support waking up both Rx and Tx processing\nPatch 2: Implements the need_wakeup functionality in common code\nPatch 3-4: Add need_wakeup support to the i40e and ixgbe drivers\nPatch 5: Add need_wakeup support to libbpf\nPatch 6: Add need_wakeup support to the xdpsock sample application\nPatch 7-8: Add need_wakeup support to the Mellanox mlx5 driver\n====================\n\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-08-17 23:07:32 +0200 Merge branch 'bpf-af-xdp-wakeup'"
    },
    {
        "commit": "77cd0d7b3f257fd0e3096b4fdcff1a7d38e99e10",
        "message": "This commit adds support for a new flag called need_wakeup in the\nAF_XDP Tx and fill rings. When this flag is set, it means that the\napplication has to explicitly wake up the kernel Rx (for the bit in\nthe fill ring) or kernel Tx (for bit in the Tx ring) processing by\nissuing a syscall. Poll() can wake up both depending on the flags\nsubmitted and sendto() will wake up tx processing only.\n\nThe main reason for introducing this new flag is to be able to\nefficiently support the case when application and driver is executing\non the same core. Previously, the driver was just busy-spinning on the\nfill ring if it ran out of buffers in the HW and there were none on\nthe fill ring. This approach works when the application is running on\nanother core as it can replenish the fill ring while the driver is\nbusy-spinning. Though, this is a lousy approach if both of them are\nrunning on the same core as the probability of the fill ring getting\nmore entries when the driver is busy-spinning is zero. With this new\nfeature the driver now sets the need_wakeup flag and returns to the\napplication. The application can then replenish the fill queue and\nthen explicitly wake up the Rx processing in the kernel using the\nsyscall poll(). For Tx, the flag is only set to one if the driver has\nno outstanding Tx completion interrupts. If it has some, the flag is\nzero as it will be woken up by a completion interrupt anyway.\n\nAs a nice side effect, this new flag also improves the performance of\nthe case where application and driver are running on two different\ncores as it reduces the number of syscalls to the kernel. The kernel\ntells user space if it needs to be woken up by a syscall, and this\neliminates many of the syscalls.\n\nThis flag needs some simple driver support. If the driver does not\nsupport this, the Rx flag is always zero and the Tx flag is always\none. This makes any application relying on this feature default to the\nold behaviour of not requiring any syscalls in the Rx path and always\nhaving to call sendto() in the Tx path.\n\nFor backwards compatibility reasons, this feature has to be explicitly\nturned on using a new bind flag (XDP_USE_NEED_WAKEUP). I recommend\nthat you always turn it on as it so far always have had a positive\nperformance impact.\n\nThe name and inspiration of the flag has been taken from io_uring by\nJens Axboe. Details about this feature in io_uring can be found in\nhttp://kernel.dk/io_uring.pdf, section 8.3.\n\nSigned-off-by: Magnus Karlsson <magnus.karlsson@intel.com>\nAcked-by: Jonathan Lemon <jonathan.lemon@gmail.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>",
        "kernel_version": "v5.4-rc1",
        "release_date": "2019-08-17 23:07:32 +0200 xsk: add support for need_wakeup flag in AF_XDP rings"
    },
    {
        "commit": "a982eeb09b6030e567b8b815277c8c9197168040",
        "message": "This patch may fix two issues:\n\nFirst, when IOSQE_IO_DRAIN set, the next IOs need to be inserted into\ndefer list to delay execution, but link io will be actively scheduled to\nrun by calling io_queue_sqe.\n\nSecond, when multiple LINK_IOs are inserted together with defer_list,\nthe LINK_IO is no longer keep order.\n\n   |-------------|\n   |   LINK_IO   |      ----> insert to defer_list  -----------\n   |-------------|                                            |\n   |   LINK_IO   |      ----> insert to defer_list  ----------|\n   |-------------|                                            |\n   |   LINK_IO   |      ----> insert to defer_list  ----------|\n   |-------------|                                            |\n   |   NORMAL_IO |      ----> insert to defer_list  ----------|\n   |-------------|                                            |\n                                                              |\n                              queue_work at same time   <-----|\n\nFixes: 9e645e1105c (\"io_uring: add support for sqe links\")\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc5",
        "release_date": "2019-08-15 11:21:39 -0600 io_uring: fix an issue when IOSQE_IO_LINK is inserted into defer list"
    },
    {
        "commit": "99c79f6692ccdc42e04deea8a36e22bb48168a62",
        "message": "Commit bd11b3a391e3 (\"io_uring: don't use iov_iter_advance() for fixed\nbuffers\") introduced an optimization to avoid using the slow\niov_iter_advance by manually populating the iov_iter iterator in some\ncases.\n\nHowever, the computation of the iterator count field was erroneous: The\nfirst bvec was always accounted for an extent of page size even if the\nbvec length was smaller.\n\nIn consequence, some I/O operations on fixed buffers were unable to\noperate on the full extent of the buffer, consistently skipping some\nbytes at the end of it.\n\nFixes: bd11b3a391e3 (\"io_uring: don't use iov_iter_advance() for fixed buffers\")\nCc: stable@vger.kernel.org\nSigned-off-by: Aleix Roca Nonell <aleix.rocanonell@bsc.es>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc5",
        "release_date": "2019-08-15 11:03:38 -0600 io_uring: fix manual setup of iov_iter for fixed buffers"
    },
    {
        "commit": "10e5ddd71fb35cfa4eb86a980b6951d4fe9f68a9",
        "message": "Pull block fixes from Jens Axboe:\n \"Here's a small collection of fixes that should go into this series.\n  This contains:\n\n   - io_uring potential use-after-free fix (Jackie)\n\n   - loop regression fix (Jan)\n\n   - O_DIRECT fragmented bio regression fix (Damien)\n\n   - Mark Denis as the new floppy maintainer (Denis)\n\n   - ataflop switch fall-through annotation (Gustavo)\n\n   - libata zpodd overflow fix (Kees)\n\n   - libata ahci deferred probe fix (Miquel)\n\n   - nbd invalidation BUG_ON() fix (Munehisa)\n\n   - dasd endless loop fix (Stefan)\"\n\n* tag 'for-linus-20190802' of git://git.kernel.dk/linux-block:\n  s390/dasd: fix endless loop after read unit address configuration\n  block: Fix __blkdev_direct_IO() for bio fragments\n  MAINTAINERS: floppy: take over maintainership\n  nbd: replace kill_bdev() with __invalidate_device() again\n  ata: libahci: do not complain in case of deferred probe\n  io_uring: fix KASAN use after free in io_sq_wq_submit_work\n  loop: Fix mount(2) failure due to race with LOOP_SET_FD\n  libata: zpodd: Fix small read overflow in zpodd_get_mech_type()\n  ataflop: Mark expected switch fall-through",
        "kernel_version": "v5.3-rc3",
        "release_date": "2019-08-02 14:31:26 -0700 Merge tag 'for-linus-20190802' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "d0ee879187df966ef638031b5f5183078d672141",
        "message": "[root@localhost ~]# ./liburing/test/link\n\nQEMU Standard PC report that:\n\n[   29.379892] CPU: 0 PID: 84 Comm: kworker/u2:2 Not tainted 5.3.0-rc2-00051-g4010b622f1d2-dirty #86\n[   29.379902] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-1 04/01/2014\n[   29.379913] Workqueue: io_ring-wq io_sq_wq_submit_work\n[   29.379929] Call Trace:\n[   29.379953]  dump_stack+0xa9/0x10e\n[   29.379970]  ? io_sq_wq_submit_work+0xbf4/0xe90\n[   29.379986]  print_address_description.cold.6+0x9/0x317\n[   29.379999]  ? io_sq_wq_submit_work+0xbf4/0xe90\n[   29.380010]  ? io_sq_wq_submit_work+0xbf4/0xe90\n[   29.380026]  __kasan_report.cold.7+0x1a/0x34\n[   29.380044]  ? io_sq_wq_submit_work+0xbf4/0xe90\n[   29.380061]  kasan_report+0xe/0x12\n[   29.380076]  io_sq_wq_submit_work+0xbf4/0xe90\n[   29.380104]  ? io_sq_thread+0xaf0/0xaf0\n[   29.380152]  process_one_work+0xb59/0x19e0\n[   29.380184]  ? pwq_dec_nr_in_flight+0x2c0/0x2c0\n[   29.380221]  worker_thread+0x8c/0xf40\n[   29.380248]  ? __kthread_parkme+0xab/0x110\n[   29.380265]  ? process_one_work+0x19e0/0x19e0\n[   29.380278]  kthread+0x30b/0x3d0\n[   29.380292]  ? kthread_create_on_node+0xe0/0xe0\n[   29.380311]  ret_from_fork+0x3a/0x50\n\n[   29.380635] Allocated by task 209:\n[   29.381255]  save_stack+0x19/0x80\n[   29.381268]  __kasan_kmalloc.constprop.6+0xc1/0xd0\n[   29.381279]  kmem_cache_alloc+0xc0/0x240\n[   29.381289]  io_submit_sqe+0x11bc/0x1c70\n[   29.381300]  io_ring_submit+0x174/0x3c0\n[   29.381311]  __x64_sys_io_uring_enter+0x601/0x780\n[   29.381322]  do_syscall_64+0x9f/0x4d0\n[   29.381336]  entry_SYSCALL_64_after_hwframe+0x49/0xbe\n\n[   29.381633] Freed by task 84:\n[   29.382186]  save_stack+0x19/0x80\n[   29.382198]  __kasan_slab_free+0x11d/0x160\n[   29.382210]  kmem_cache_free+0x8c/0x2f0\n[   29.382220]  io_put_req+0x22/0x30\n[   29.382230]  io_sq_wq_submit_work+0x28b/0xe90\n[   29.382241]  process_one_work+0xb59/0x19e0\n[   29.382251]  worker_thread+0x8c/0xf40\n[   29.382262]  kthread+0x30b/0x3d0\n[   29.382272]  ret_from_fork+0x3a/0x50\n\n[   29.382569] The buggy address belongs to the object at ffff888067172140\n                which belongs to the cache io_kiocb of size 224\n[   29.384692] The buggy address is located 120 bytes inside of\n                224-byte region [ffff888067172140, ffff888067172220)\n[   29.386723] The buggy address belongs to the page:\n[   29.387575] page:ffffea00019c5c80 refcount:1 mapcount:0 mapping:ffff88806ace5180 index:0x0\n[   29.387587] flags: 0x100000000000200(slab)\n[   29.387603] raw: 0100000000000200 dead000000000100 dead000000000122 ffff88806ace5180\n[   29.387617] raw: 0000000000000000 00000000800c000c 00000001ffffffff 0000000000000000\n[   29.387624] page dumped because: kasan: bad access detected\n\n[   29.387920] Memory state around the buggy address:\n[   29.388771]  ffff888067172080: fb fb fb fb fb fb fb fb fb fb fb fb fc fc fc fc\n[   29.390062]  ffff888067172100: fc fc fc fc fc fc fc fc fb fb fb fb fb fb fb fb\n[   29.391325] >ffff888067172180: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb\n[   29.392578]                                         ^\n[   29.393480]  ffff888067172200: fb fb fb fb fc fc fc fc fc fc fc fc fc fc fc fc\n[   29.394744]  ffff888067172280: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc\n[   29.396003] ==================================================================\n[   29.397260] Disabling lock debugging due to kernel taint\n\nio_sq_wq_submit_work free and read req again.\n\nCc: Zhengyuan Liu <liuzhengyuan@kylinos.cn>\nCc: linux-block@vger.kernel.org\nCc: stable@vger.kernel.org\nFixes: f7b76ac9d17e (\"io_uring: fix counter inc/dec mismatch in async_list\")\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc3",
        "release_date": "2019-07-31 08:45:10 -0600 io_uring: fix KASAN use after free in io_sq_wq_submit_work"
    },
    {
        "commit": "04412819652fe30f900d11e96c67b4adfdf17f6b",
        "message": "Pull block fixes from Jens Axboe:\n\n - Several io_uring fixes/improvements:\n     - Blocking fix for O_DIRECT (me)\n     - Latter page slowness for registered buffers (me)\n     - Fix poll hang under certain conditions (me)\n     - Defer sequence check fix for wrapped rings (Zhengyuan)\n     - Mismatch in async inc/dec accounting (Zhengyuan)\n     - Memory ordering issue that could cause stall (Zhengyuan)\n      - Track sequential defer in bytes, not pages (Zhengyuan)\n\n - NVMe pull request from Christoph\n\n - Set of hang fixes for wbt (Josef)\n\n - Redundant error message kill for libahci (Ding)\n\n - Remove unused blk_mq_sched_started_request() and related ops (Marcos)\n\n - drbd dynamic alloc shash descriptor to reduce stack use (Arnd)\n\n - blkcg ->pd_stat() non-debug print (Tejun)\n\n - bcache memory leak fix (Wei)\n\n - Comment fix (Akinobu)\n\n - BFQ perf regression fix (Paolo)\n\n* tag 'for-linus-20190726' of git://git.kernel.dk/linux-block: (24 commits)\n  io_uring: ensure ->list is initialized for poll commands\n  Revert \"nvme-pci: don't create a read hctx mapping without read queues\"\n  nvme: fix multipath crash when ANA is deactivated\n  nvme: fix memory leak caused by incorrect subsystem free\n  nvme: ignore subnqn for ADATA SX6000LNP\n  drbd: dynamically allocate shash descriptor\n  block: blk-mq: Remove blk_mq_sched_started_request and started_request\n  bcache: fix possible memory leak in bch_cached_dev_run()\n  io_uring: track io length in async_list based on bytes\n  io_uring: don't use iov_iter_advance() for fixed buffers\n  block: properly handle IOCB_NOWAIT for async O_DIRECT IO\n  blk-mq: allow REQ_NOWAIT to return an error inline\n  io_uring: add a memory barrier before atomic_read\n  rq-qos: use a mb for got_token\n  rq-qos: set ourself TASK_UNINTERRUPTIBLE after we schedule\n  rq-qos: don't reset has_sleepers on spurious wakeups\n  rq-qos: fix missed wake-ups in rq_qos_throttle\n  wait: add wq_has_single_sleeper helper\n  block, bfq: check also in-flight I/O in dispatch plugging\n  block: fix sysfs module parameters directory path in comment\n  ...",
        "kernel_version": "v5.3-rc2",
        "release_date": "2019-07-26 10:32:12 -0700 Merge tag 'for-linus-20190726' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "36703247d5f52a679df9da51192b6950fe81689f",
        "message": "Daniel reports that when testing an http server that uses io_uring\nto poll for incoming connections, sometimes it hard crashes. This is\ndue to an uninitialized list member for the io_uring request. Normally\nthis doesn't trigger and none of the test cases caught it.\n\nReported-by: Daniel Kozak <kozzi11@gmail.com>\nTested-by: Daniel Kozak <kozzi11@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc2",
        "release_date": "2019-07-25 10:20:18 -0600 io_uring: ensure ->list is initialized for poll commands"
    },
    {
        "commit": "9310a7ba6de8cce6209e3e8a3cdf733f824cdd9b",
        "message": "We are using PAGE_SIZE as the unit to determine if the total len in\nasync_list has exceeded max_pages, it's not fair for smaller io sizes.\nFor example, if we are doing 1k-size io streams, we will never exceed\nmax_pages since len >>= PAGE_SHIFT always gets zero. So use original\nbytes to make it more accurate.\n\nSigned-off-by: Zhengyuan Liu <liuzhengyuan@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc2",
        "release_date": "2019-07-21 21:46:55 -0600 io_uring: track io length in async_list based on bytes"
    },
    {
        "commit": "bd11b3a391e3df6fa958facbe4b3f9f4cca9bd49",
        "message": "Hrvoje reports that when a large fixed buffer is registered and IO is\nbeing done to the latter pages of said buffer, the IO submission time\nis much worse:\n\nreading to the start of the buffer: 11238 ns\nreading to the end of the buffer:   1039879 ns\n\nIn fact, it's worse by two orders of magnitude. The reason for that is\nhow io_uring figures out how to setup the iov_iter. We point the iter\nat the first bvec, and then use iov_iter_advance() to fast-forward to\nthe offset within that buffer we need.\n\nHowever, that is abysmally slow, as it entails iterating the bvecs\nthat we setup as part of buffer registration. There's really no need\nto use this generic helper, as we know it's a BVEC type iterator, and\nwe also know that each bvec is PAGE_SIZE in size, apart from possibly\nthe first and last. Hence we can just use a shift on the offset to\nfind the right index, and then adjust the iov_iter appropriately.\nAfter this fix, the timings are:\n\nreading to the start of the buffer: 10135 ns\nreading to the end of the buffer:   1377 ns\n\nOr about an 755x improvement for the tail page.\n\nReported-by: Hrvoje Zeba <zeba.hrvoje@gmail.com>\nTested-by: Hrvoje Zeba <zeba.hrvoje@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc2",
        "release_date": "2019-07-21 21:46:36 -0600 io_uring: don't use iov_iter_advance() for fixed buffers"
    },
    {
        "commit": "c0e48f9dea9129aa11bec3ed13803bcc26e96e49",
        "message": "There is a hang issue while using fio to do some basic test. The issue\ncan be easily reproduced using the below script:\n\n        while true\n        do\n                fio  --ioengine=io_uring  -rw=write -bs=4k -numjobs=1 \\\n                     -size=1G -iodepth=64 -name=uring   --filename=/dev/zero\n        done\n\nAfter several minutes (or more), fio would block at\nio_uring_enter->io_cqring_wait in order to waiting for previously\ncommitted sqes to be completed and can't return to user anymore until\nwe send a SIGTERM to fio. After receiving SIGTERM, fio hangs at\nio_ring_ctx_wait_and_kill with a backtrace like this:\n\n        [54133.243816] Call Trace:\n        [54133.243842]  __schedule+0x3a0/0x790\n        [54133.243868]  schedule+0x38/0xa0\n        [54133.243880]  schedule_timeout+0x218/0x3b0\n        [54133.243891]  ? sched_clock+0x9/0x10\n        [54133.243903]  ? wait_for_completion+0xa3/0x130\n        [54133.243916]  ? _raw_spin_unlock_irq+0x2c/0x40\n        [54133.243930]  ? trace_hardirqs_on+0x3f/0xe0\n        [54133.243951]  wait_for_completion+0xab/0x130\n        [54133.243962]  ? wake_up_q+0x70/0x70\n        [54133.243984]  io_ring_ctx_wait_and_kill+0xa0/0x1d0\n        [54133.243998]  io_uring_release+0x20/0x30\n        [54133.244008]  __fput+0xcf/0x270\n        [54133.244029]  ____fput+0xe/0x10\n        [54133.244040]  task_work_run+0x7f/0xa0\n        [54133.244056]  do_exit+0x305/0xc40\n        [54133.244067]  ? get_signal+0x13b/0xbd0\n        [54133.244088]  do_group_exit+0x50/0xd0\n        [54133.244103]  get_signal+0x18d/0xbd0\n        [54133.244112]  ? _raw_spin_unlock_irqrestore+0x36/0x60\n        [54133.244142]  do_signal+0x34/0x720\n        [54133.244171]  ? exit_to_usermode_loop+0x7e/0x130\n        [54133.244190]  exit_to_usermode_loop+0xc0/0x130\n        [54133.244209]  do_syscall_64+0x16b/0x1d0\n        [54133.244221]  entry_SYSCALL_64_after_hwframe+0x49/0xbe\n\nThe reason is that we had added a req to ctx->pending_async at the very\nend, but it didn't get a chance to be processed. How could this happen?\n\n        fio#cpu0                                        wq#cpu1\n\n        io_add_to_prev_work                    io_sq_wq_submit_work\n\n          atomic_read() <<< 1\n\n                                                  atomic_dec_return() << 1->0\n                                                  list_empty();    <<< true;\n\n          list_add_tail()\n          atomic_read() << 0 or 1?\n\nAs atomic_ops.rst states, atomic_read does not guarantee that the\nruntime modification by any other thread is visible yet, so we must take\ncare of that with a proper implicit or explicit memory barrier.\n\nThis issue was detected with the help of Jackie's <liuyun01@kylinos.cn>\n\nFixes: 31b515106428 (\"io_uring: allow workqueue item to handle multiple buffered requests\")\nSigned-off-by: Zhengyuan Liu <liuzhengyuan@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc2",
        "release_date": "2019-07-18 11:09:09 -0600 io_uring: add a memory barrier before atomic_read"
    },
    {
        "commit": "f7b76ac9d17e16e44feebb6d2749fec92bfd6dd4",
        "message": "We could queue a work for each req in defer and link list without\nincreasing async_list->cnt, so we shouldn't decrease it while exiting\nfrom workqueue as well if we didn't process the req in async list.\n\nThanks to Jens Axboe <axboe@kernel.dk> for his guidance.\n\nFixes: 31b515106428 (\"io_uring: allow workqueue item to handle multiple buffered requests\")\nSigned-off-by: Zhengyuan Liu <liuzhengyuan@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc2",
        "release_date": "2019-07-16 09:55:14 -0600 io_uring: fix counter inc/dec mismatch in async_list"
    },
    {
        "commit": "dbd0f6d6c2a11eb9c31ca9cd454f95bb5713e92e",
        "message": "sq->cached_sq_head and cq->cached_cq_tail are both unsigned int. If\ncached_sq_head overflows before cached_cq_tail, then we may miss a\nbarrier req. As cached_cq_tail always follows cached_sq_head, the NQ\nshould be enough.\n\nCc: stable@vger.kernel.org\nFixes: de0617e46717 (\"io_uring: add support for marking commands as draining\")\nSigned-off-by: Zhengyuan Liu <liuzhengyuan@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc2",
        "release_date": "2019-07-16 08:27:09 -0600 io_uring: fix the sequence comparison in io_sequence_defer"
    },
    {
        "commit": "a1240cf74e8228f7c80d44af17914c0ffc5633fb",
        "message": "Pull percpu updates from Dennis Zhou:\n \"This includes changes to let percpu_ref release the backing percpu\n  memory earlier after it has been switched to atomic in cases where the\n  percpu ref is not revived.\n\n  This will help recycle percpu memory earlier in cases where the\n  refcounts are pinned for prolonged periods of time\"\n\n* 'for-5.3' of git://git.kernel.org/pub/scm/linux/kernel/git/dennis/percpu:\n  percpu_ref: release percpu memory early without PERCPU_REF_ALLOW_REINIT\n  md: initialize percpu refcounters using PERCU_REF_ALLOW_REINIT\n  io_uring: initialize percpu refcounters using PERCU_REF_ALLOW_REINIT\n  percpu_ref: introduce PERCPU_REF_ALLOW_REINIT flag",
        "kernel_version": "v5.3-rc1",
        "release_date": "2019-07-14 16:17:18 -0700 Merge branch 'for-5.3' of git://git.kernel.org/pub/scm/linux/kernel/git/dennis/percpu"
    },
    {
        "commit": "a2d79c7174aeb43b13020dd53d85a7aefdd9f3e5",
        "message": "Pull io_uring updates from Jens Axboe:\n \"This contains:\n\n   - Support for recvmsg/sendmsg as first class opcodes.\n\n     I don't envision going much further down this path, as there are\n     plans in progress to support potentially any system call in an\n     async fashion through io_uring. But I think it does make sense to\n     have certain core ops available directly, especially those that can\n     support a \"try this non-blocking\" flag/mode. (me)\n\n   - Handle generic short reads automatically.\n\n     This can happen fairly easily if parts of the buffered read is\n     cached. Since the application needs to issue another request for\n     the remainder, just do this internally and save kernel/user\n     roundtrip while providing a nicer more robust API. (me)\n\n   - Support for linked SQEs.\n\n     This allows SQEs to depend on each other, enabling an application\n     to eg queue a read-from-this-file,write-to-that-file pair. (me)\n\n   - Fix race in stopping SQ thread (Jackie)\"\n\n* tag 'for-5.3/io_uring-20190711' of git://git.kernel.dk/linux-block:\n  io_uring: fix io_sq_thread_stop running in front of io_sq_thread\n  io_uring: add support for recvmsg()\n  io_uring: add support for sendmsg()\n  io_uring: add support for sqe links\n  io_uring: punt short reads to async context\n  uio: make import_iovec()/compat_import_iovec() return bytes on success",
        "kernel_version": "v5.3-rc1",
        "release_date": "2019-07-13 10:36:53 -0700 Merge tag 'for-5.3/io_uring-20190711' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "a4c0b3decb33fb4a2b5ecc6234a50680f0b21e7d",
        "message": "INFO: task syz-executor.5:8634 blocked for more than 143 seconds.\n       Not tainted 5.2.0-rc5+ #3\n\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nsyz-executor.5  D25632  8634   8224 0x00004004\nCall Trace:\n  context_switch kernel/sched/core.c:2818 [inline]\n  __schedule+0x658/0x9e0 kernel/sched/core.c:3445\n  schedule+0x131/0x1d0 kernel/sched/core.c:3509\n  schedule_timeout+0x9a/0x2b0 kernel/time/timer.c:1783\n  do_wait_for_common+0x35e/0x5a0 kernel/sched/completion.c:83\n  __wait_for_common kernel/sched/completion.c:104 [inline]\n  wait_for_common kernel/sched/completion.c:115 [inline]\n  wait_for_completion+0x47/0x60 kernel/sched/completion.c:136\n  kthread_stop+0xb4/0x150 kernel/kthread.c:559\n  io_sq_thread_stop fs/io_uring.c:2252 [inline]\n  io_finish_async fs/io_uring.c:2259 [inline]\n  io_ring_ctx_free fs/io_uring.c:2770 [inline]\n  io_ring_ctx_wait_and_kill+0x268/0x880 fs/io_uring.c:2834\n  io_uring_release+0x5d/0x70 fs/io_uring.c:2842\n  __fput+0x2e4/0x740 fs/file_table.c:280\n  ____fput+0x15/0x20 fs/file_table.c:313\n  task_work_run+0x17e/0x1b0 kernel/task_work.c:113\n  tracehook_notify_resume include/linux/tracehook.h:185 [inline]\n  exit_to_usermode_loop arch/x86/entry/common.c:168 [inline]\n  prepare_exit_to_usermode+0x402/0x4f0 arch/x86/entry/common.c:199\n  syscall_return_slowpath+0x110/0x440 arch/x86/entry/common.c:279\n  do_syscall_64+0x126/0x140 arch/x86/entry/common.c:304\n  entry_SYSCALL_64_after_hwframe+0x49/0xbe\nRIP: 0033:0x412fb1\nCode: 80 3b 7c 0f 84 c7 02 00 00 c7 85 d0 00 00 00 00 00 00 00 48 8b 05 cf\na6 24 00 49 8b 14 24 41 b9 cb 2a 44 00 48 89 ee 48 89 df <48> 85 c0 4c 0f\n45 c8 45 31 c0 31 c9 e8 0e 5b 00 00 85 c0 41 89 c7\nRSP: 002b:00007ffe7ee6a180 EFLAGS: 00000293 ORIG_RAX: 0000000000000003\nRAX: 0000000000000000 RBX: 0000000000000004 RCX: 0000000000412fb1\nRDX: 0000001b2d920000 RSI: 0000000000000000 RDI: 0000000000000003\nRBP: 0000000000000001 R08: 00000000f3a3e1f8 R09: 00000000f3a3e1fc\nR10: 00007ffe7ee6a260 R11: 0000000000000293 R12: 000000000075c9a0\nR13: 000000000075c9a0 R14: 0000000000024c00 R15: 000000000075bf2c\n\n=============================================\n\nThere is an wrong logic, when kthread_park running\nin front of io_sq_thread.\n\nCPU#0\t\t\t\t\tCPU#1\n\nio_sq_thread_stop:\t\t\tint kthread(void *_create):\n\nkthread_park()\n\t\t\t\t\t__kthread_parkme(self);\t <<< Wrong\nkthread_stop()\n    << wait for self->exited\n    << clear_bit KTHREAD_SHOULD_PARK\n\n\t\t\t\t\tret = threadfn(data);\n\t\t\t\t\t   |\n\t\t\t\t\t   |- io_sq_thread\n\t\t\t\t\t       |- kthread_should_park()\t<< false\n\t\t\t\t\t       |- schedule() <<< nobody wake up\n\nstuck CPU#0\t\t\t\tstuck CPU#1\n\nSo, use a new variable sqo_thread_started to ensure that io_sq_thread\nrun first, then io_sq_thread_stop.\n\nReported-by: syzbot+94324416c485d422fe15@syzkaller.appspotmail.com\nSuggested-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc1",
        "release_date": "2019-07-09 14:32:05 -0600 io_uring: fix io_sq_thread_stop running in front of io_sq_thread"
    },
    {
        "commit": "aa1fa28fc73ea6b740ee7b62bf3b07141883dbb8",
        "message": "This is done through IORING_OP_RECVMSG. This opcode uses the same\nsqe->msg_flags that IORING_OP_SENDMSG added, and we pass in the\nmsghdr struct in the sqe->addr field as well.\n\nWe use MSG_DONTWAIT to force an inline fast path if recvmsg() doesn't\nblock, and punt to async execution if it would have.\n\nAcked-by: David S. Miller <davem@davemloft.net>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc1",
        "release_date": "2019-07-09 14:32:14 -0600 io_uring: add support for recvmsg()"
    },
    {
        "commit": "0fa03c624d8fc9932d0f27c39a9deca6a37e0e17",
        "message": "This is done through IORING_OP_SENDMSG. There's a new sqe->msg_flags\nfor the flags argument, and the msghdr struct is passed in the\nsqe->addr field.\n\nWe use MSG_DONTWAIT to force an inline fast path if sendmsg() doesn't\nblock, and punt to async execution if it would have.\n\nAcked-by: David S. Miller <davem@davemloft.net>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc1",
        "release_date": "2019-07-09 14:32:05 -0600 io_uring: add support for sendmsg()"
    },
    {
        "commit": "9dda12b6fa0eba6b1fd32399b599d05765893dae",
        "message": "Pull block fixes from Jens Axboe:\n \"Just two small fixes.\n\n  One from Paolo, fixing a silly mistake in BFQ. The other one is from\n  me, ensuring that we have ->file cleared in the io_uring request a bit\n  earlier. That avoids a use-before-free, if we encounter an error\n  before ->file is assigned\"\n\n* tag 'for-linus-20190628' of git://git.kernel.dk/linux-block:\n  block, bfq: fix operator in BFQQ_TOTALLY_SEEKY\n  io_uring: ensure req->file is cleared on allocation",
        "kernel_version": "v5.2-rc7",
        "release_date": "2019-06-29 16:58:35 +0800 Merge tag 'for-linus-20190628' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "9e645e1105ca60fbbc6bddf2fd5ef7e57ed3dca8",
        "message": "With SQE links, we can create chains of dependent SQEs. One example\nwould be queueing an SQE that's a read from one file descriptor, with\nthe linked SQE being a write to another with the same set of buffers.\n\nAn SQE link will not stall the pipeline, it'll just ensure that\ndependent SQEs aren't issued before the previous link has completed.\n\nAny error at submission or completion time will break the chain of SQEs.\nFor completions, this also includes short reads or writes, as the next\nSQE could depend on the previous one being fully completed.\n\nAny SQE in a chain that gets canceled due to any of the above errors,\nwill get an CQE fill with -ECANCELED as the error value.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc1",
        "release_date": "2019-06-24 08:00:18 -0600 io_uring: add support for sqe links"
    },
    {
        "commit": "60c112b0ada09826cc4ae6a4e55df677f76f1313",
        "message": "Stephen reports:\n\nI hit the following General Protection Fault when testing io_uring via\nthe io_uring engine in fio. This was on a VM running 5.2-rc5 and the\nlatest version of fio. The issue occurs for both null_blk and fake NVMe\ndrives. I have not tested bare metal or real NVMe SSDs. The fio script\nused is given below.\n\n[io_uring]\ntime_based=1\nruntime=60\nfilename=/dev/nvme2n1 (note /dev/nullb0 also fails)\nioengine=io_uring\nbs=4k\nrw=readwrite\ndirect=1\nfixedbufs=1\nsqthread_poll=1\nsqthread_poll_cpu=0\n\ngeneral protection fault: 0000 [#1] SMP PTI\nCPU: 0 PID: 872 Comm: io_uring-sq Not tainted 5.2.0-rc5-cpacket-io-uring #1\nHardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Ubuntu-1.8.2-1ubuntu1 04/01/2014\nRIP: 0010:fput_many+0x7/0x90\nCode: 01 48 85 ff 74 17 55 48 89 e5 53 48 8b 1f e8 a0 f9 ff ff 48 85 db 48 89 df 75 f0 5b 5d f3 c3 0f 1f 40 00 0f 1f 44 00 00 89 f6 <f0> 48 29 77 38 74 01 c3 55 48 89 e5 53 48 89 fb 65 48 \\\n\nRSP: 0018:ffffadeb817ebc50 EFLAGS: 00010246\nRAX: 0000000000000004 RBX: ffff8f46ad477480 RCX: 0000000000001805\nRDX: 0000000000000000 RSI: 0000000000000001 RDI: f18b51b9a39552b5\nRBP: ffffadeb817ebc58 R08: ffff8f46b7a318c0 R09: 000000000000015d\nR10: ffffadeb817ebce8 R11: 0000000000000020 R12: ffff8f46ad4cd000\nR13: 00000000fffffff7 R14: ffffadeb817ebe30 R15: 0000000000000004\nFS:  0000000000000000(0000) GS:ffff8f46b7a00000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 000055828f0bbbf0 CR3: 0000000232176004 CR4: 00000000003606f0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\nCall Trace:\n ? fput+0x13/0x20\n io_free_req+0x20/0x40\n io_put_req+0x1b/0x20\n io_submit_sqe+0x40a/0x680\n ? __switch_to_asm+0x34/0x70\n ? __switch_to_asm+0x40/0x70\n io_submit_sqes+0xb9/0x160\n ? io_submit_sqes+0xb9/0x160\n ? __switch_to_asm+0x40/0x70\n ? __switch_to_asm+0x34/0x70\n ? __schedule+0x3f2/0x6a0\n ? __switch_to_asm+0x34/0x70\n io_sq_thread+0x1af/0x470\n ? __switch_to_asm+0x34/0x70\n ? wait_woken+0x80/0x80\n ? __switch_to+0x85/0x410\n ? __switch_to_asm+0x40/0x70\n ? __switch_to_asm+0x34/0x70\n ? __schedule+0x3f2/0x6a0\n kthread+0x105/0x140\n ? io_submit_sqes+0x160/0x160\n ? kthread+0x105/0x140\n ? io_submit_sqes+0x160/0x160\n ? kthread_destroy_worker+0x50/0x50\n ret_from_fork+0x35/0x40\n\nwhich occurs because using a kernel side submission thread isn't valid\nwithout using fixed files (registered through io_uring_register()). This\ncauses io_uring to put the request after logging an error, but before\nthe file field is set in the request. If it happens to be non-zero, we\nattempt to fput() garbage.\n\nFix this by ensuring that req->file is initialized when the request is\nallocated.\n\nCc: stable@vger.kernel.org # 5.1+\nReported-by: Stephen Bates <sbates@raithlin.com>\nTested-by: Stephen Bates <sbates@raithlin.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc7",
        "release_date": "2019-06-21 14:16:28 -0600 io_uring: ensure req->file is cleared on allocation"
    },
    {
        "commit": "7b10315128c697b32c3a920cd79a8301c7466637",
        "message": "Pull block fixes from Jens Axboe:\n\n - Remove references to old schedulers for the scheduler switching and\n   blkio controller documentation (Andreas)\n\n - Kill duplicate check for report zone for null_blk (Chaitanya)\n\n - Two bcache fixes (Coly)\n\n - Ensure that mq-deadline is selected if zoned block device is enabled,\n   as we need that to support them (Damien)\n\n - Fix io_uring memory leak (Eric)\n\n - ps3vram fallout from LBDAF removal (Geert)\n\n - Redundant blk-mq debugfs debugfs_create return check cleanup (Greg)\n\n - Extend NOPLM quirk for ST1000LM024 drives (Hans)\n\n - Remove error path warning that can now trigger after the queue\n   removal/addition fixes (Ming)\n\n* tag 'for-linus-20190614' of git://git.kernel.dk/linux-block:\n  block/ps3vram: Use %llu to format sector_t after LBDAF removal\n  libata: Extend quirks for the ST1000LM024 drives with NOLPM quirk\n  bcache: only set BCACHE_DEV_WB_RUNNING when cached device attached\n  bcache: fix stack corruption by PRECEDING_KEY()\n  blk-mq: remove WARN_ON(!q->elevator) from blk_mq_sched_free_requests\n  blkio-controller.txt: Remove references to CFQ\n  block/switching-sched.txt: Update to blk-mq schedulers\n  null_blk: remove duplicate check for report zone\n  blk-mq: no need to check return value of debugfs_create functions\n  io_uring: fix memory leak of UNIX domain socket inode\n  block: force select mq-deadline for zoned block devices",
        "kernel_version": "v5.2-rc5",
        "release_date": "2019-06-14 15:41:18 -1000 Merge tag 'for-linus-20190614' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "355e8d26f719c207aa2e00e6f3cfab3acf21769b",
        "message": "Opening and closing an io_uring instance leaks a UNIX domain socket\ninode.  This is because the ->file of the io_uring instance's internal\nUNIX domain socket is set to point to the io_uring file, but then\nsock_release() sees the non-NULL ->file and assumes the inode reference\nis held by the file so doesn't call iput().  That's not the case here,\nsince the reference is still meant to be held by the socket; the actual\ninode of the io_uring file is different.\n\nFix this leak by NULL-ing out ->file before releasing the socket.\n\nReported-by: syzbot+111cb28d9f583693aefa@syzkaller.appspotmail.com\nFixes: 2b188cc1bb85 (\"Add io_uring IO interface\")\nCc: <stable@vger.kernel.org> # v5.1+\nSigned-off-by: Eric Biggers <ebiggers@google.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc5",
        "release_date": "2019-06-13 03:00:30 -0600 io_uring: fix memory leak of UNIX domain socket inode"
    },
    {
        "commit": "9221dced3069cc9ae2986ba1191b02dae560df28",
        "message": "Pull block fixes from Jens Axboe:\n\n - A set of patches fixing code comments / kerneldoc (Bart)\n\n - Don't allow loop file change for exclusive open (Jan)\n\n - Fix revalidate of hidden genhd (Jan)\n\n - Init queue failure memory free fix (Jes)\n\n - Improve rq limits failure print (John)\n\n - Fixup for queue removal/addition (Ming)\n\n - Missed error progagation for io_uring buffer registration (Pavel)\n\n* tag 'for-linus-20190601' of git://git.kernel.dk/linux-block:\n  block: print offending values when cloned rq limits are exceeded\n  blk-mq: Document the blk_mq_hw_queue_to_node() arguments\n  blk-mq: Fix spelling in a source code comment\n  block: Fix bsg_setup_queue() kernel-doc header\n  block: Fix rq_qos_wait() kernel-doc header\n  block: Fix blk_mq_*_map_queues() kernel-doc headers\n  block: Fix throtl_pending_timer_fn() kernel-doc header\n  block: Convert blk_invalidate_devt() header into a non-kernel-doc header\n  block/partitions/ldm: Convert a kernel-doc header into a non-kernel-doc header\n  blk-mq: Fix memory leak in error handling\n  block: don't protect generic_make_request_checks with blk_queue_enter\n  block: move blk_exit_queue into __blk_release_queue\n  block: Don't revalidate bdev of hidden gendisk\n  loop: Don't change loop device under exclusive opener\n  io_uring: Fix __io_uring_register() false success",
        "kernel_version": "v5.2-rc3",
        "release_date": "2019-06-02 09:27:44 -0700 Merge tag 'for-linus-20190601' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "9d93a3f5a0c0d0f79aebc597d47c7cedc852aeb5",
        "message": "We can encounter a short read when we're doing buffered reads and the\ndata is partially cached. Right now we just return the short read, but\nthat forces the application to read that CQE, then issue another SQE\nto finish the read. That read will not be cached, and hence will result\nin an async punt.\n\nIt's more efficient to do that async punt from within the kernel, as\nthat will the not need two round trips more to the kernel.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.3-rc1",
        "release_date": "2019-05-31 15:30:03 -0600 io_uring: punt short reads to async context"
    },
    {
        "commit": "f95d050cdc5d34f9a4417e06c392ccbf146037bb",
        "message": "When a host system has kernel headers that are newer than a compiling\nkernel, mksyscalltbl fails with errors such as:\n\n  <stdin>: In function 'main':\n  <stdin>:271:44: error: '__NR_kexec_file_load' undeclared (first use in this function)\n  <stdin>:271:44: note: each undeclared identifier is reported only once for each function it appears in\n  <stdin>:272:46: error: '__NR_pidfd_send_signal' undeclared (first use in this function)\n  <stdin>:273:43: error: '__NR_io_uring_setup' undeclared (first use in this function)\n  <stdin>:274:43: error: '__NR_io_uring_enter' undeclared (first use in this function)\n  <stdin>:275:46: error: '__NR_io_uring_register' undeclared (first use in this function)\n  tools/perf/arch/arm64/entry/syscalls//mksyscalltbl: line 48: /tmp/create-table-xvUQdD: Permission denied\n\nmksyscalltbl is compiled with default host includes, but run with\ncompiling kernel tree includes, causing some syscall numbers to being\nundeclared.\n\nCommitter testing:\n\nBefore this patch, in my cross build environment, no build problems, but\nthese new syscalls were not in the syscalls.c generated from the\nunistd.h file, which is a bug, this patch fixes it:\n\nperfbuilder@6e20056ed532:/git/perf$ tail /tmp/build/perf/arch/arm64/include/generated/asm/syscalls.c\n\t[292] = \"io_pgetevents\",\n\t[293] = \"rseq\",\n\t[294] = \"kexec_file_load\",\n\t[424] = \"pidfd_send_signal\",\n\t[425] = \"io_uring_setup\",\n\t[426] = \"io_uring_enter\",\n\t[427] = \"io_uring_register\",\n\t[428] = \"syscalls\",\n};\nperfbuilder@6e20056ed532:/git/perf$ strings /tmp/build/perf/perf | egrep '^(io_uring_|pidfd_|kexec_file)'\nkexec_file_load\npidfd_send_signal\nio_uring_setup\nio_uring_enter\nio_uring_register\nperfbuilder@6e20056ed532:/git/perf$\n$\n\nWell, there is that last \"syscalls\" thing, but that looks like some\nother bug.\n\nSigned-off-by: Vitaly Chikunov <vt@altlinux.org>\nTested-by: Arnaldo Carvalho de Melo <acme@redhat.com>\nTested-by: Michael Petlan <mpetlan@redhat.com>\nCc: Alexander Shishkin <alexander.shishkin@linux.intel.com>\nCc: Hendrik Brueckner <brueckner@linux.ibm.com>\nCc: Jiri Olsa <jolsa@redhat.com>\nCc: Kim Phillips <kim.phillips@arm.com>\nCc: Namhyung Kim <namhyung@kernel.org>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Ravi Bangoria <ravi.bangoria@linux.vnet.ibm.com>\nLink: http://lkml.kernel.org/r/20190521030203.1447-1-vt@altlinux.org\nSigned-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>",
        "kernel_version": "v5.2-rc3",
        "release_date": "2019-05-28 09:49:03 -0300 perf arm64: Fix mksyscalltbl when system kernel headers are ahead of the kernel"
    },
    {
        "commit": "a278682dad37fd2f8d2f30d8e84e376a856ab472",
        "message": "If io_copy_iov() fails, it will break the loop and report success,\nalbeit partially completed operation.\n\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc3",
        "release_date": "2019-05-26 09:25:06 -0600 io_uring: Fix __io_uring_register() false success"
    },
    {
        "commit": "7fbc78e3155a0c464bd832efc07fb3c2355fe9bd",
        "message": "Pull block fixes from Jens Axboe:\n\n - NVMe pull request from Keith, with fixes from a few folks.\n\n - bio and sbitmap before atomic barrier fixes (Andrea)\n\n - Hang fix for blk-mq freeze and unfreeze (Bob)\n\n - Single segment count regression fix (Christoph)\n\n - AoE now has a new maintainer\n\n - tools/io_uring/ Makefile fix, and sync with liburing (me)\n\n* tag 'for-linus-20190524' of git://git.kernel.dk/linux-block: (23 commits)\n  tools/io_uring: sync with liburing\n  tools/io_uring: fix Makefile for pthread library link\n  blk-mq: fix hang caused by freeze/unfreeze sequence\n  block: remove the bi_seg_{front,back}_size fields in struct bio\n  block: remove the segment size check in bio_will_gap\n  block: force an unlimited segment size on queues with a virt boundary\n  block: don't decrement nr_phys_segments for physically contigous segments\n  sbitmap: fix improper use of smp_mb__before_atomic()\n  bio: fix improper use of smp_mb__before_atomic()\n  aoe: list new maintainer for aoe driver\n  nvme-pci: use blk-mq mapping for unmanaged irqs\n  nvme: update MAINTAINERS\n  nvme: copy MTFA field from identify controller\n  nvme: fix memory leak for power latency tolerance\n  nvme: release namespace SRCU protection before performing controller ioctls\n  nvme: merge nvme_ns_ioctl into nvme_ioctl\n  nvme: remove the ifdef around nvme_nvm_ioctl\n  nvme: fix srcu locking on error return in nvme_get_ns_from_disk\n  nvme: Fix known effects\n  nvme-pci: Sync queues on reset\n  ...",
        "kernel_version": "v5.2-rc2",
        "release_date": "2019-05-24 16:02:14 -0700 Merge tag 'for-linus-20190524' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "004d564f908790efe815a6510a542ac1227ef2a2",
        "message": "Various fixes and changes have been applied to liburing since we\ncopied some select bits to the kernel testing/examples part, sync\nup with liburing to get those changes.\n\nMost notable is the change that split the CQE reading into the peek\nand seen event, instead of being just a single function. Also fixes\nan unsigned wrap issue in io_uring_submit(), leak of 'fd' in setup\nif we fail, and various other little issues.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc2",
        "release_date": "2019-05-23 10:25:26 -0600 tools/io_uring: sync with liburing"
    },
    {
        "commit": "486f069253c3c738dec62daeb16f7232b2cca065",
        "message": "Currently fails with:\n\nio_uring-bench.o: In function `main':\n/home/axboe/git/linux-block/tools/io_uring/io_uring-bench.c:560: undefined reference to `pthread_create'\n/home/axboe/git/linux-block/tools/io_uring/io_uring-bench.c:588: undefined reference to `pthread_join'\ncollect2: error: ld returned 1 exit status\nMakefile:11: recipe for target 'io_uring-bench' failed\nmake: *** [io_uring-bench] Error 1\n\nMove -lpthread to the end.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc2",
        "release_date": "2019-05-23 10:25:26 -0600 tools/io_uring: fix Makefile for pthread library link"
    },
    {
        "commit": "a6a4b66bd8f41922c543f7a820c66ed59c25995e",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"A small set of fixes for io_uring.\n\n  This contains:\n\n   - smp_rmb() cleanup for io_cqring_events() (Jackie)\n\n   - io_cqring_wait() simplification (Jackie)\n\n   - removal of dead 'ev_flags' passing (me)\n\n   - SQ poll CPU affinity verification fix (me)\n\n   - SQ poll wait fix (Roman)\n\n   - SQE command prep cleanup and fix (Stefan)\"\n\n* tag 'for-linus-20190516' of git://git.kernel.dk/linux-block:\n  io_uring: use wait_event_interruptible for cq_wait conditional wait\n  io_uring: adjust smp_rmb inside io_cqring_events\n  io_uring: fix infinite wait in khread_park() on io_finish_async()\n  io_uring: remove 'ev_flags' argument\n  io_uring: fix failure to verify SQ_AFF cpu\n  io_uring: fix race condition reading SQE data",
        "kernel_version": "v5.2-rc1",
        "release_date": "2019-05-16 19:10:37 -0700 Merge tag 'for-linus-20190516' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "1cdc415f10831c18912943017d06b2be948c67b4",
        "message": "Make the name of the anon inode fd \"[fscontext]\" instead of \"fscontext\".\nThis is minor but most core-kernel anon inode fds already carry square\nbrackets around their name:\n\n[eventfd]\n[eventpoll]\n[fanotify]\n[io_uring]\n[pidfd]\n[signalfd]\n[timerfd]\n[userfaultfd]\n\nFor the sake of consistency lets do the same for the fscontext anon inode\nfd that comes with the new mount api.\n\nSigned-off-by: Christian Brauner <christian@brauner.io>\nSigned-off-by: David Howells <dhowells@redhat.com>\nSigned-off-by: Al Viro <viro@zeniv.linux.org.uk>",
        "kernel_version": "v5.2-rc1",
        "release_date": "2019-05-16 12:23:45 -0400 uapi, fsopen: use square brackets around \"fscontext\" [ver #2]"
    },
    {
        "commit": "fdb288a679cdf6a71f3c1ae6f348ba4dae742681",
        "message": "The previous patch has ensured that io_cqring_events contain\nsmp_rmb memory barriers, Now we can use wait_event_interruptible\nto keep the code simple.\n\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc1",
        "release_date": "2019-05-16 08:10:27 -0600 io_uring: use wait_event_interruptible for cq_wait conditional wait"
    },
    {
        "commit": "dc6ce4bc2b355a47f225a0205046b3ebf29a7f72",
        "message": "Whenever smp_rmb is required to use io_cqring_events,\nkeep smp_rmb inside the function io_cqring_events.\n\nSigned-off-by: Jackie Liu <liuyun01@kylinos.cn>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc1",
        "release_date": "2019-05-16 08:10:25 -0600 io_uring: adjust smp_rmb inside io_cqring_events"
    },
    {
        "commit": "2bbcd6d3b36a75a19be4917807f54ae32dd26aba",
        "message": "This fixes couple of races which lead to infinite wait of park completion\nwith the following backtraces:\n\n  [20801.303319] Call Trace:\n  [20801.303321]  ? __schedule+0x284/0x650\n  [20801.303323]  schedule+0x33/0xc0\n  [20801.303324]  schedule_timeout+0x1bc/0x210\n  [20801.303326]  ? schedule+0x3d/0xc0\n  [20801.303327]  ? schedule_timeout+0x1bc/0x210\n  [20801.303329]  ? preempt_count_add+0x79/0xb0\n  [20801.303330]  wait_for_completion+0xa5/0x120\n  [20801.303331]  ? wake_up_q+0x70/0x70\n  [20801.303333]  kthread_park+0x48/0x80\n  [20801.303335]  io_finish_async+0x2c/0x70\n  [20801.303336]  io_ring_ctx_wait_and_kill+0x95/0x180\n  [20801.303338]  io_uring_release+0x1c/0x20\n  [20801.303339]  __fput+0xad/0x210\n  [20801.303341]  task_work_run+0x8f/0xb0\n  [20801.303342]  exit_to_usermode_loop+0xa0/0xb0\n  [20801.303343]  do_syscall_64+0xe0/0x100\n  [20801.303349]  entry_SYSCALL_64_after_hwframe+0x44/0xa9\n\n  [20801.303380] Call Trace:\n  [20801.303383]  ? __schedule+0x284/0x650\n  [20801.303384]  schedule+0x33/0xc0\n  [20801.303386]  io_sq_thread+0x38a/0x410\n  [20801.303388]  ? __switch_to_asm+0x40/0x70\n  [20801.303390]  ? wait_woken+0x80/0x80\n  [20801.303392]  ? _raw_spin_lock_irqsave+0x17/0x40\n  [20801.303394]  ? io_submit_sqes+0x120/0x120\n  [20801.303395]  kthread+0x112/0x130\n  [20801.303396]  ? kthread_create_on_node+0x60/0x60\n  [20801.303398]  ret_from_fork+0x35/0x40\n\n o kthread_park() waits for park completion, so io_sq_thread() loop\n   should check kthread_should_park() along with khread_should_stop(),\n   otherwise if kthread_park() is called before prepare_to_wait()\n   the following schedule() never returns:\n\n   CPU#0                    CPU#1\n\n   io_sq_thread_stop():     io_sq_thread():\n\n                               while(!kthread_should_stop() && !ctx->sqo_stop) {\n\n      ctx->sqo_stop = 1;\n      kthread_park()\n\n\t                            prepare_to_wait();\n                                    if (kthread_should_stop() {\n\t\t\t\t    }\n                                    schedule();   <<< nobody checks park flag,\n\t\t\t\t                  <<< so schedule and never return\n\n o if the flag ctx->sqo_stop is observed by the io_sq_thread() loop\n   it is quite possible, that kthread_should_park() check and the\n   following kthread_parkme() is never called, because kthread_park()\n   has not been yet called, but few moments later is is called and\n   waits there for park completion, which never happens, because\n   kthread has already exited:\n\n   CPU#0                    CPU#1\n\n   io_sq_thread_stop():     io_sq_thread():\n\n      ctx->sqo_stop = 1;\n                               while(!kthread_should_stop() && !ctx->sqo_stop) {\n                                   <<< observe sqo_stop and exit the loop\n\t\t\t       }\n\n\t\t\t       if (kthread_should_park())\n\t\t\t           kthread_parkme();  <<< never called, since was\n\t\t\t\t\t              <<< never parked\n\n      kthread_park()           <<< waits forever for park completion\n\nIn the current patch we quit the loop by only kthread_should_park()\ncheck (kthread_park() is synchronous, so kthread_should_stop() is\nnever observed), and we abandon ->sqo_stop flag, since it is racy.\nAt the end of the io_sq_thread() we unconditionally call parmke(),\nsince we've exited the loop by the park flag.\n\nSigned-off-by: Roman Penyaev <rpenyaev@suse.de>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: linux-block@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc1",
        "release_date": "2019-05-16 08:10:24 -0600 io_uring: fix infinite wait in khread_park() on io_finish_async()"
    },
    {
        "commit": "c71ffb673cd9bb2ddc575ede9055f265b2535690",
        "message": "We always pass in 0 for the cqe flags argument, since the support for\n\"this read hit page cache\" hint was dropped.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc1",
        "release_date": "2019-05-15 13:51:11 -0600 io_uring: remove 'ev_flags' argument"
    },
    {
        "commit": "44a9bd18a0f06bba19d155aeaa11e2edce898293",
        "message": "The test case we have is rightfully failing with the current kernel:\n\nio_uring_setup(1, 0x7ffe2cafebe0), flags: IORING_SETUP_SQPOLL|IORING_SETUP_SQ_AFF, resv: 0x00000000 0x00000000 0x00000000 0x00000000 0x00000000, sq_thread_cpu: 4\nexpected -1, got 3\n\nThis is in a vm, and CPU3 is the last valid one, hence asking for 4\nshould fail the setup with -EINVAL, not succeed. The problem is that\nwe're using array_index_nospec() with nr_cpu_ids as the index, hence we\nwrap and end up using CPU0 instead of CPU4. This makes the setup\nsucceed where it should be failing.\n\nWe don't need to use array_index_nospec() as we're not indexing any\narray with this. Instead just compare with nr_cpu_ids directly. This\nis fine as we're checking with cpu_online() afterwards.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc1",
        "release_date": "2019-05-14 20:00:30 -0600 io_uring: fix failure to verify SQ_AFF cpu"
    },
    {
        "commit": "e2033e33cb3821c26d4f9e70677910827d3b7885",
        "message": "When punting to workers the SQE gets copied after the initial try.\nThere is a race condition between reading SQE data for the initial try\nand copying it for punting it to the workers.\n\nFor example io_rw_done calls kiocb->ki_complete even if it was prepared\nfor IORING_OP_FSYNC (and would be NULL).\n\nThe easiest solution for now is to alway prepare again in the worker.\n\nreq->file is safe to prepare though as long as it is checked before use.\n\nSigned-off-by: Stefan B\u00fchler <source@stbuehler.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc1",
        "release_date": "2019-05-13 09:15:42 -0600 io_uring: fix race condition reading SQE data"
    },
    {
        "commit": "214828962dead0c698f92b60ef97ce3c5fc2c8fe",
        "message": "Percpu reference counters should now be initialized with the\nPERCPU_REF_ALLOW_REINIT in order to allow switching them to the\npercpu mode from the atomic mode. This is exactly what\npercpu_ref_reinit() called from __io_uring_register() is supposed to\ndo. So let's initialize percpu refcounters with the\nPERCU_REF_ALLOW_REINIT flag.\n\nSigned-off-by: Roman Gushchin <guro@fb.com>\nAcked-by: Tejun Heo <tj@kernel.org>\nSigned-off-by: Dennis Zhou <dennis@kernel.org>",
        "kernel_version": "v5.3-rc1",
        "release_date": "2019-05-09 10:50:30 -0700 io_uring: initialize percpu refcounters using PERCU_REF_ALLOW_REINIT"
    },
    {
        "commit": "52ae2456d6a455ef958bcf1c2d1965674076887e",
        "message": "Pull io_uring updates from Jens Axboe:\n \"Set of changes/improvements for io_uring. This contains:\n\n   - Fix of a shadowed variable (Colin)\n\n   - Add support for draining commands (me)\n\n   - Add support for sync_file_range() (me)\n\n   - Add eventfd support (me)\n\n   - cpu_online() fix (Shenghui)\n\n   - Removal of a redundant ->error assignment (Stefan)\"\n\n* tag 'for-5.2/io_uring-20190507' of git://git.kernel.dk/linux-block:\n  io_uring: use cpu_online() to check p->sq_thread_cpu instead of cpu_possible()\n  io_uring: fix shadowed variable ret return code being not checked\n  req->error only used for iopoll\n  io_uring: add support for eventfd notifications\n  io_uring: add support for IORING_OP_SYNC_FILE_RANGE\n  fs: add sync_file_range() helper\n  io_uring: add support for marking commands as draining",
        "kernel_version": "v5.2-rc1",
        "release_date": "2019-05-07 18:30:11 -0700 Merge tag 'for-5.2/io_uring-20190507' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "eac7078a0fff1e72cf2b641721e3f55ec7e5e21e",
        "message": "Pull pidfd updates from Christian Brauner:\n \"This patchset makes it possible to retrieve pidfds at process creation\n  time by introducing the new flag CLONE_PIDFD to the clone() system\n  call. Linus originally suggested to implement this as a new flag to\n  clone() instead of making it a separate system call.\n\n  After a thorough review from Oleg CLONE_PIDFD returns pidfds in the\n  parent_tidptr argument. This means we can give back the associated pid\n  and the pidfd at the same time. Access to process metadata information\n  thus becomes rather trivial.\n\n  As has been agreed, CLONE_PIDFD creates file descriptors based on\n  anonymous inodes similar to the new mount api. They are made\n  unconditional by this patchset as they are now needed by core kernel\n  code (vfs, pidfd) even more than they already were before (timerfd,\n  signalfd, io_uring, epoll etc.). The core patchset is rather small.\n  The bulky looking changelist is caused by David's very simple changes\n  to Kconfig to make anon inodes unconditional.\n\n  A pidfd comes with additional information in fdinfo if the kernel\n  supports procfs. The fdinfo file contains the pid of the process in\n  the callers pid namespace in the same format as the procfs status\n  file, i.e. \"Pid:\\t%d\".\n\n  To remove worries about missing metadata access this patchset comes\n  with a sample/test program that illustrates how a combination of\n  CLONE_PIDFD and pidfd_send_signal() can be used to gain race-free\n  access to process metadata through /proc/<pid>.\n\n  Further work based on this patchset has been done by Joel. His work\n  makes pidfds pollable. It finished too late for this merge window. I\n  would prefer to have it sitting in linux-next for a while and send it\n  for inclusion during the 5.3 merge window\"\n\n* tag 'pidfd-v5.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux:\n  samples: show race-free pidfd metadata access\n  signal: support CLONE_PIDFD with pidfd_send_signal\n  clone: add CLONE_PIDFD\n  Make anon_inodes unconditional",
        "kernel_version": "v5.2-rc1",
        "release_date": "2019-05-07 12:30:24 -0700 Merge tag 'pidfd-v5.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux"
    },
    {
        "commit": "7889f44dd9cee15aff1c3f7daf81ca4dfed48fc7",
        "message": "This issue is found by running liburing/test/io_uring_setup test.\n\nWhen test run, the testcase \"attempt to bind to invalid cpu\" would not\npass with messages like:\n   io_uring_setup(1, 0xbfc2f7c8), \\\nflags: IORING_SETUP_SQPOLL|IORING_SETUP_SQ_AFF, \\\nresv: 0x00000000 0x00000000 0x00000000 0x00000000 0x00000000, \\\nsq_thread_cpu: 2\n   expected -1, got 3\n   FAIL\n\nOn my system, there is:\n   CPU(s) possible : 0-3\n   CPU(s) online   : 0-1\n   CPU(s) offline  : 2-3\n   CPU(s) present  : 0-1\n\nThe sq_thread_cpu 2 is offline on my system, so the bind should fail.\nBut cpu_possible() will pass the check. We shouldn't be able to bind\nto an offline cpu. Use cpu_online() to do the check.\n\nAfter the change, the testcase run as expected: EINVAL will be returned\nfor cpu offlined.\n\nReviewed-by: Jeff Moyer <jmoyer@redhat.com>\nSigned-off-by: Shenghui Wang <shhuiw@foxmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc1",
        "release_date": "2019-05-07 08:41:26 -0600 io_uring: use cpu_online() to check p->sq_thread_cpu instead of cpu_possible()"
    },
    {
        "commit": "efeb862bd5bc001636e690debf6f9fbba98e5bfd",
        "message": "Currently variable ret is declared in a while-loop code block that\nshadows another variable ret. When an error occurs in the while-loop\nthe error return in ret is not being set in the outer code block and\nso the error check on ret is always going to be checking on the wrong\nret variable resulting in check that is always going to be true and\na premature return occurs.\n\nFix this by removing the declaration of the inner while-loop variable\nret so that shadowing does not occur.\n\nAddresses-Coverity: (\"'Constant' variable guards dead code\")\nFixes: 6b06314c47e1 (\"io_uring: add file set registration\")\nSigned-off-by: Colin Ian King <colin.king@canonical.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc1",
        "release_date": "2019-05-06 10:21:34 -0600 io_uring: fix shadowed variable ret return code being not checked"
    },
    {
        "commit": "e87eb301bee183d82bb3d04bd71b6660889a2588",
        "message": "Just like aio/io_uring, we need to grab 2 refcount for queuing one\nrequest, one is for submission, another is for completion.\n\nIf the request isn't queued from plug code path, the refcount grabbed\nin generic_make_request() serves for submission. In theroy, this\nrefcount should have been released after the sumission(async run queue)\nis done. blk_freeze_queue() works with blk_sync_queue() together\nfor avoiding race between cleanup queue and IO submission, given async\nrun queue activities are canceled because hctx->run_work is scheduled with\nthe refcount held, so it is fine to not hold the refcount when\nrunning the run queue work function for dispatch IO.\n\nHowever, if request is staggered into plug list, and finally queued\nfrom plug code path, the refcount in submission side is actually missed.\nAnd we may start to run queue after queue is removed because the queue's\nkobject refcount isn't guaranteed to be grabbed in flushing plug list\ncontext, then kernel oops is triggered, see the following race:\n\nblk_mq_flush_plug_list():\n        blk_mq_sched_insert_requests()\n                insert requests to sw queue or scheduler queue\n                blk_mq_run_hw_queue\n\nBecause of concurrent run queue, all requests inserted above may be\ncompleted before calling the above blk_mq_run_hw_queue. Then queue can\nbe freed during the above blk_mq_run_hw_queue().\n\nFixes the issue by grab .q_usage_counter before calling\nblk_mq_sched_insert_requests() in blk_mq_flush_plug_list(). This way is\nsafe because the queue is absolutely alive before inserting request.\n\nCc: Dongli Zhang <dongli.zhang@oracle.com>\nCc: James Smart <james.smart@broadcom.com>\nCc: linux-scsi@vger.kernel.org,\nCc: Martin K . Petersen <martin.petersen@oracle.com>,\nCc: Christoph Hellwig <hch@lst.de>,\nCc: James E . J . Bottomley <jejb@linux.vnet.ibm.com>,\nReviewed-by: Bart Van Assche <bvanassche@acm.org>\nTested-by: James Smart <james.smart@broadcom.com>\nSigned-off-by: Ming Lei <ming.lei@redhat.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc1",
        "release_date": "2019-05-04 07:24:02 -0600 blk-mq: grab .q_usage_counter when queuing request from plug code path"
    },
    {
        "commit": "9b402849e80c85eee10bbd341aab3f1a0f942d4f",
        "message": "Allow registration of an eventfd, which will trigger an event every\ntime a completion event happens for this io_uring instance.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc1",
        "release_date": "2019-05-02 14:08:54 -0600 io_uring: add support for eventfd notifications"
    },
    {
        "commit": "5d17b4a4b7fa172b205be8a05051ae705d1dc3bb",
        "message": "This behaves just like sync_file_range(2) does.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc1",
        "release_date": "2019-05-02 14:08:54 -0600 io_uring: add support for IORING_OP_SYNC_FILE_RANGE"
    },
    {
        "commit": "de0617e467171ba44c73efd1ba63f101b164a035",
        "message": "There are no ordering constraints between the submission and completion\nside of io_uring. But sometimes that would be useful to have. One common\nexample is doing an fsync, for instance, and have it ordered with\nprevious writes. Without support for that, the application must do this\ntracking itself.\n\nThis adds a general SQE flag, IOSQE_IO_DRAIN. If a command is marked\nwith this flag, then it will not be issued before previous commands have\ncompleted, and subsequent commands submitted after the drain will not be\nissued before the drain is started.. If there are no pending commands,\nsetting this flag will not change the behavior of the issue of the\ncommand.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.2-rc1",
        "release_date": "2019-05-02 14:08:53 -0600 io_uring: add support for marking commands as draining"
    },
    {
        "commit": "5ce3307b6d9d25fe3c62e4749821f5e58f9161db",
        "message": "Pull io_uring fixes from Jens Axboe:\n \"This is mostly io_uring fixes/tweaks. Most of these were actually done\n  in time for the last -rc, but I wanted to ensure that everything\n  tested out great before including them. The code delta looks larger\n  than it really is, as it's mostly just comment additions/changes.\n\n  Outside of the comment additions/changes, this is mostly removal of\n  unnecessary barriers. In all, this pull request contains:\n\n   - Tweak to how we handle errors at submission time. We now post a\n     completion event if the error occurs on behalf of an sqe, instead\n     of returning it through the system call. If the error happens\n     outside of a specific sqe, we return the error through the system\n     call. This makes it nicer to use and makes the \"normal\" use case\n     behave the same as the offload cases. (me)\n\n   - Fix for a missing req reference drop from async context (me)\n\n   - If an sqe is submitted with RWF_NOWAIT, don't punt it to async\n     context. Return -EAGAIN directly, instead of using it as a hint to\n     do async punt. (Stefan)\n\n   - Fix notes on barriers (Stefan)\n\n   - Remove unnecessary barriers (Stefan)\n\n   - Fix potential double free of memory in setup error (Mark)\n\n   - Further improve sq poll CPU validation (Mark)\n\n   - Fix page allocation warning and leak on buffer registration error\n     (Mark)\n\n   - Fix iov_iter_type() for new no-ref flag (Ming)\n\n   - Fix a case where dio doesn't honor bio no-page-ref (Ming)\"\n\n* tag 'for-linus-20190502' of git://git.kernel.dk/linux-block:\n  io_uring: avoid page allocation warnings\n  iov_iter: fix iov_iter_type\n  block: fix handling for BIO_NO_PAGE_REF\n  io_uring: drop req submit reference always in async punt\n  io_uring: free allocated io_memory once\n  io_uring: fix SQPOLL cpu validation\n  io_uring: have submission side sqe errors post a cqe\n  io_uring: remove unnecessary barrier after unsetting IORING_SQ_NEED_WAKEUP\n  io_uring: remove unnecessary barrier after incrementing dropped counter\n  io_uring: remove unnecessary barrier before reading SQ tail\n  io_uring: remove unnecessary barrier after updating SQ head\n  io_uring: remove unnecessary barrier before reading cq head\n  io_uring: remove unnecessary barrier before wq_has_sleeper\n  io_uring: fix notes on barriers\n  io_uring: fix handling SQEs requesting NOWAIT",
        "kernel_version": "v5.1",
        "release_date": "2019-05-02 09:55:04 -0700 Merge tag 'for-linus-20190502' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "d4ef647510b1200fe1c996ff1cbf5ac47eb930cc",
        "message": "In io_sqe_buffer_register() we allocate a number of arrays based on the\niov_len from the user-provided iov. While we limit iov_len to SZ_1G,\nwe can still attempt to allocate arrays exceeding MAX_ORDER.\n\nOn a 64-bit system with 4KiB pages, for an iov where iov_base = 0x10 and\niov_len = SZ_1G, we'll calculate that nr_pages = 262145. When we try to\nallocate a corresponding array of (16-byte) bio_vecs, requiring 4194320\nbytes, which is greater than 4MiB. This results in SLUB warning that\nwe're trying to allocate greater than MAX_ORDER, and failing the\nallocation.\n\nAvoid this by using kvmalloc() for allocations dependent on the\nuser-provided iov_len. At the same time, fix a leak of imu->bvec when\nregistration fails.\n\nFull splat from before this patch:\n\nWARNING: CPU: 1 PID: 2314 at mm/page_alloc.c:4595 __alloc_pages_nodemask+0x7ac/0x2938 mm/page_alloc.c:4595\nKernel panic - not syncing: panic_on_warn set ...\nCPU: 1 PID: 2314 Comm: syz-executor326 Not tainted 5.1.0-rc7-dirty #4\nHardware name: linux,dummy-virt (DT)\nCall trace:\n dump_backtrace+0x0/0x2f0 include/linux/compiler.h:193\n show_stack+0x20/0x30 arch/arm64/kernel/traps.c:158\n __dump_stack lib/dump_stack.c:77 [inline]\n dump_stack+0x110/0x190 lib/dump_stack.c:113\n panic+0x384/0x68c kernel/panic.c:214\n __warn+0x2bc/0x2c0 kernel/panic.c:571\n report_bug+0x228/0x2d8 lib/bug.c:186\n bug_handler+0xa0/0x1a0 arch/arm64/kernel/traps.c:956\n call_break_hook arch/arm64/kernel/debug-monitors.c:301 [inline]\n brk_handler+0x1d4/0x388 arch/arm64/kernel/debug-monitors.c:316\n do_debug_exception+0x1a0/0x468 arch/arm64/mm/fault.c:831\n el1_dbg+0x18/0x8c\n __alloc_pages_nodemask+0x7ac/0x2938 mm/page_alloc.c:4595\n alloc_pages_current+0x164/0x278 mm/mempolicy.c:2132\n alloc_pages include/linux/gfp.h:509 [inline]\n kmalloc_order+0x20/0x50 mm/slab_common.c:1231\n kmalloc_order_trace+0x30/0x2b0 mm/slab_common.c:1243\n kmalloc_large include/linux/slab.h:480 [inline]\n __kmalloc+0x3dc/0x4f0 mm/slub.c:3791\n kmalloc_array include/linux/slab.h:670 [inline]\n io_sqe_buffer_register fs/io_uring.c:2472 [inline]\n __io_uring_register fs/io_uring.c:2962 [inline]\n __do_sys_io_uring_register fs/io_uring.c:3008 [inline]\n __se_sys_io_uring_register fs/io_uring.c:2990 [inline]\n __arm64_sys_io_uring_register+0x9e0/0x1bc8 fs/io_uring.c:2990\n __invoke_syscall arch/arm64/kernel/syscall.c:35 [inline]\n invoke_syscall arch/arm64/kernel/syscall.c:47 [inline]\n el0_svc_common.constprop.0+0x148/0x2e0 arch/arm64/kernel/syscall.c:83\n el0_svc_handler+0xdc/0x100 arch/arm64/kernel/syscall.c:129\n el0_svc+0x8/0xc arch/arm64/kernel/entry.S:948\nSMP: stopping secondary CPUs\nDumping ftrace buffer:\n   (ftrace buffer empty)\nKernel Offset: disabled\nCPU features: 0x002,23000438\nMemory Limit: none\nRebooting in 1 seconds..\n\nFixes: edafccee56ff3167 (\"io_uring: add support for pre-mapped user IO buffers\")\nSigned-off-by: Mark Rutland <mark.rutland@arm.com>\nCc: Alexander Viro <viro@zeniv.linux.org.uk>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: linux-fsdevel@vger.kernel.org\nCc: linux-block@vger.kernel.org\nCc: linux-kernel@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1",
        "release_date": "2019-05-01 10:00:25 -0600 io_uring: avoid page allocation warnings"
    },
    {
        "commit": "817869d2519f0cb7be5b3482129dadc806dfb747",
        "message": "If we don't end up actually calling submit in io_sq_wq_submit_work(),\nwe still need to drop the submit reference to the request. If we\ndon't, then we can leak the request. This can happen if we race\nwith ring shutdown while flushing the workqueue for requests that\nrequire use of the mm_struct.\n\nFixes: e65ef56db494 (\"io_uring: use regular request ref counts\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1",
        "release_date": "2019-05-01 08:38:47 -0600 io_uring: drop req submit reference always in async punt"
    },
    {
        "commit": "52e04ef4c9d459cba3afd86ec335a411b40b7fd2",
        "message": "If io_allocate_scq_urings() fails to allocate an sq_* region, it will\ncall io_mem_free() for any previously allocated regions, but leave\ndangling pointers to these regions in the ctx. Any regions which have\nnot yet been allocated are left NULL. Note that when returning\n-EOVERFLOW, the previously allocated sq_ring is not freed, which appears\nto be an unintentional leak.\n\nWhen io_allocate_scq_urings() fails, io_uring_create() will call\nio_ring_ctx_wait_and_kill(), which calls io_mem_free() on all the sq_*\nregions, assuming the pointers are valid and not NULL.\n\nThis can result in pages being freed multiple times, which has been\nobserved to corrupt the page state, leading to subsequent fun. This can\nalso result in virt_to_page() on NULL, resulting in the use of bogus\npage addresses, and yet more subsequent fun. The latter can be detected\nwith CONFIG_DEBUG_VIRTUAL on arm64.\n\nAdding a cleanup path to io_allocate_scq_urings() complicates the logic,\nso let's leave it to io_ring_ctx_free() to consistently free these\npointers, and simplify the io_allocate_scq_urings() error paths.\n\nFull splats from before this patch below. Note that the pointer logged\nby the DEBUG_VIRTUAL \"non-linear address\" warning has been hashed, and\nis actually NULL.\n\n[   26.098129] page:ffff80000e949a00 count:0 mapcount:-128 mapping:0000000000000000 index:0x0\n[   26.102976] flags: 0x63fffc000000()\n[   26.104373] raw: 000063fffc000000 ffff80000e86c188 ffff80000ea3df08 0000000000000000\n[   26.108917] raw: 0000000000000000 0000000000000001 00000000ffffff7f 0000000000000000\n[   26.137235] page dumped because: VM_BUG_ON_PAGE(page_ref_count(page) == 0)\n[   26.143960] ------------[ cut here ]------------\n[   26.146020] kernel BUG at include/linux/mm.h:547!\n[   26.147586] Internal error: Oops - BUG: 0 [#1] PREEMPT SMP\n[   26.149163] Modules linked in:\n[   26.150287] Process syz-executor.21 (pid: 20204, stack limit = 0x000000000e9cefeb)\n[   26.153307] CPU: 2 PID: 20204 Comm: syz-executor.21 Not tainted 5.1.0-rc7-00004-g7d30b2ea43d6 #18\n[   26.156566] Hardware name: linux,dummy-virt (DT)\n[   26.158089] pstate: 40400005 (nZcv daif +PAN -UAO)\n[   26.159869] pc : io_mem_free+0x9c/0xa8\n[   26.161436] lr : io_mem_free+0x9c/0xa8\n[   26.162720] sp : ffff000013003d60\n[   26.164048] x29: ffff000013003d60 x28: ffff800025048040\n[   26.165804] x27: 0000000000000000 x26: ffff800025048040\n[   26.167352] x25: 00000000000000c0 x24: ffff0000112c2820\n[   26.169682] x23: 0000000000000000 x22: 0000000020000080\n[   26.171899] x21: ffff80002143b418 x20: ffff80002143b400\n[   26.174236] x19: ffff80002143b280 x18: 0000000000000000\n[   26.176607] x17: 0000000000000000 x16: 0000000000000000\n[   26.178997] x15: 0000000000000000 x14: 0000000000000000\n[   26.181508] x13: 00009178a5e077b2 x12: 0000000000000001\n[   26.183863] x11: 0000000000000000 x10: 0000000000000980\n[   26.186437] x9 : ffff000013003a80 x8 : ffff800025048a20\n[   26.189006] x7 : ffff8000250481c0 x6 : ffff80002ffe9118\n[   26.191359] x5 : ffff80002ffe9118 x4 : 0000000000000000\n[   26.193863] x3 : ffff80002ffefe98 x2 : 44c06ddd107d1f00\n[   26.196642] x1 : 0000000000000000 x0 : 000000000000003e\n[   26.198892] Call trace:\n[   26.199893]  io_mem_free+0x9c/0xa8\n[   26.201155]  io_ring_ctx_wait_and_kill+0xec/0x180\n[   26.202688]  io_uring_setup+0x6c4/0x6f0\n[   26.204091]  __arm64_sys_io_uring_setup+0x18/0x20\n[   26.205576]  el0_svc_common.constprop.0+0x7c/0xe8\n[   26.207186]  el0_svc_handler+0x28/0x78\n[   26.208389]  el0_svc+0x8/0xc\n[   26.209408] Code: aa0203e0 d0006861 9133a021 97fcdc3c (d4210000)\n[   26.211995] ---[ end trace bdb81cd43a21e50d ]---\n\n[   81.770626] ------------[ cut here ]------------\n[   81.825015] virt_to_phys used for non-linear address: 000000000d42f2c7 (          (null))\n[   81.827860] WARNING: CPU: 1 PID: 30171 at arch/arm64/mm/physaddr.c:15 __virt_to_phys+0x48/0x68\n[   81.831202] Modules linked in:\n[   81.832212] CPU: 1 PID: 30171 Comm: syz-executor.20 Not tainted 5.1.0-rc7-00004-g7d30b2ea43d6 #19\n[   81.835616] Hardware name: linux,dummy-virt (DT)\n[   81.836863] pstate: 60400005 (nZCv daif +PAN -UAO)\n[   81.838727] pc : __virt_to_phys+0x48/0x68\n[   81.840572] lr : __virt_to_phys+0x48/0x68\n[   81.842264] sp : ffff80002cf67c70\n[   81.843858] x29: ffff80002cf67c70 x28: ffff800014358e18\n[   81.846463] x27: 0000000000000000 x26: 0000000020000080\n[   81.849148] x25: 0000000000000000 x24: ffff80001bb01f40\n[   81.851986] x23: ffff200011db06c8 x22: ffff2000127e3c60\n[   81.854351] x21: ffff800014358cc0 x20: ffff800014358d98\n[   81.856711] x19: 0000000000000000 x18: 0000000000000000\n[   81.859132] x17: 0000000000000000 x16: 0000000000000000\n[   81.861586] x15: 0000000000000000 x14: 0000000000000000\n[   81.863905] x13: 0000000000000000 x12: ffff1000037603e9\n[   81.866226] x11: 1ffff000037603e8 x10: 0000000000000980\n[   81.868776] x9 : ffff80002cf67840 x8 : ffff80001bb02920\n[   81.873272] x7 : ffff1000037603e9 x6 : ffff80001bb01f47\n[   81.875266] x5 : ffff1000037603e9 x4 : dfff200000000000\n[   81.876875] x3 : ffff200010087528 x2 : ffff1000059ecf58\n[   81.878751] x1 : 44c06ddd107d1f00 x0 : 0000000000000000\n[   81.880453] Call trace:\n[   81.881164]  __virt_to_phys+0x48/0x68\n[   81.882919]  io_mem_free+0x18/0x110\n[   81.886585]  io_ring_ctx_wait_and_kill+0x13c/0x1f0\n[   81.891212]  io_uring_setup+0xa60/0xad0\n[   81.892881]  __arm64_sys_io_uring_setup+0x2c/0x38\n[   81.894398]  el0_svc_common.constprop.0+0xac/0x150\n[   81.896306]  el0_svc_handler+0x34/0x88\n[   81.897744]  el0_svc+0x8/0xc\n[   81.898715] ---[ end trace b4a703802243cbba ]---\n\nFixes: 2b188cc1bb857a9d (\"Add io_uring IO interface\")\nSigned-off-by: Mark Rutland <mark.rutland@arm.com>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Alexander Viro <viro@zeniv.linux.org.uk>\nCc: linux-block@vger.kernel.org\nCc: linux-fsdevel@vger.kernel.org\nCc: linux-kernel@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1",
        "release_date": "2019-05-01 08:38:47 -0600 io_uring: free allocated io_memory once"
    },
    {
        "commit": "975554b03eddc1df73bda3a764a09e18cadd5f1c",
        "message": "In io_sq_offload_start(), we call cpu_possible() on an unbounded cpu\nvalue from userspace. On v5.1-rc7 on arm64 with\nCONFIG_DEBUG_PER_CPU_MAPS, this results in a splat:\n\n  WARNING: CPU: 1 PID: 27601 at include/linux/cpumask.h:121 cpu_max_bits_warn include/linux/cpumask.h:121 [inline]\n\nThere was an attempt to fix this in commit:\n\n  917257daa0fea7a0 (\"io_uring: only test SQPOLL cpu after we've verified it\")\n\n... by adding a check after the cpu value had been limited to NR_CPU_IDS\nusing array_index_nospec(). However, this left an unbound check at the\nstart of the function, for which the warning still fires.\n\nLet's fix this correctly by checking that the cpu value is bound by\nnr_cpu_ids before passing it to cpu_possible(). Note that only\nnr_cpu_ids of a cpumask are guaranteed to exist at runtime, and\nnr_cpu_ids can be significantly smaller than NR_CPUs. For example, an\narm64 defconfig has NR_CPUS=256, while my test VM has 4 vCPUs.\n\nFollowing the intent from the commit message for 917257daa0fea7a0, the\ncheck is moved under the SQ_AFF branch, which is the only branch where\nthe cpu values is consumed. The check is performed before bounding the\nvalue with array_index_nospec() so that we don't silently accept bogus\ncpu values from userspace, where array_index_nospec() would force these\nvalues to 0.\n\nI suspect we can remove the array_index_nospec() call entirely, but I've\nconservatively left that in place, updated to use nr_cpu_ids to match\nthe prior check.\n\nTested on arm64 with the Syzkaller reproducer:\n\n  https://syzkaller.appspot.com/bug?extid=cd714a07c6de2bc34293\n  https://syzkaller.appspot.com/x/repro.syz?x=15d8b397200000\n\nFull splat from before this patch:\n\nWARNING: CPU: 1 PID: 27601 at include/linux/cpumask.h:121 cpu_max_bits_warn include/linux/cpumask.h:121 [inline]\nWARNING: CPU: 1 PID: 27601 at include/linux/cpumask.h:121 cpumask_check include/linux/cpumask.h:128 [inline]\nWARNING: CPU: 1 PID: 27601 at include/linux/cpumask.h:121 cpumask_test_cpu include/linux/cpumask.h:344 [inline]\nWARNING: CPU: 1 PID: 27601 at include/linux/cpumask.h:121 io_sq_offload_start fs/io_uring.c:2244 [inline]\nWARNING: CPU: 1 PID: 27601 at include/linux/cpumask.h:121 io_uring_create fs/io_uring.c:2864 [inline]\nWARNING: CPU: 1 PID: 27601 at include/linux/cpumask.h:121 io_uring_setup+0x1108/0x15a0 fs/io_uring.c:2916\nKernel panic - not syncing: panic_on_warn set ...\nCPU: 1 PID: 27601 Comm: syz-executor.0 Not tainted 5.1.0-rc7 #3\nHardware name: linux,dummy-virt (DT)\nCall trace:\n dump_backtrace+0x0/0x2f0 include/linux/compiler.h:193\n show_stack+0x20/0x30 arch/arm64/kernel/traps.c:158\n __dump_stack lib/dump_stack.c:77 [inline]\n dump_stack+0x110/0x190 lib/dump_stack.c:113\n panic+0x384/0x68c kernel/panic.c:214\n __warn+0x2bc/0x2c0 kernel/panic.c:571\n report_bug+0x228/0x2d8 lib/bug.c:186\n bug_handler+0xa0/0x1a0 arch/arm64/kernel/traps.c:956\n call_break_hook arch/arm64/kernel/debug-monitors.c:301 [inline]\n brk_handler+0x1d4/0x388 arch/arm64/kernel/debug-monitors.c:316\n do_debug_exception+0x1a0/0x468 arch/arm64/mm/fault.c:831\n el1_dbg+0x18/0x8c\n cpu_max_bits_warn include/linux/cpumask.h:121 [inline]\n cpumask_check include/linux/cpumask.h:128 [inline]\n cpumask_test_cpu include/linux/cpumask.h:344 [inline]\n io_sq_offload_start fs/io_uring.c:2244 [inline]\n io_uring_create fs/io_uring.c:2864 [inline]\n io_uring_setup+0x1108/0x15a0 fs/io_uring.c:2916\n __do_sys_io_uring_setup fs/io_uring.c:2929 [inline]\n __se_sys_io_uring_setup fs/io_uring.c:2926 [inline]\n __arm64_sys_io_uring_setup+0x50/0x70 fs/io_uring.c:2926\n __invoke_syscall arch/arm64/kernel/syscall.c:35 [inline]\n invoke_syscall arch/arm64/kernel/syscall.c:47 [inline]\n el0_svc_common.constprop.0+0x148/0x2e0 arch/arm64/kernel/syscall.c:83\n el0_svc_handler+0xdc/0x100 arch/arm64/kernel/syscall.c:129\n el0_svc+0x8/0xc arch/arm64/kernel/entry.S:948\nSMP: stopping secondary CPUs\nDumping ftrace buffer:\n   (ftrace buffer empty)\nKernel Offset: disabled\nCPU features: 0x002,23000438\nMemory Limit: none\nRebooting in 1 seconds..\n\nFixes: 917257daa0fea7a0 (\"io_uring: only test SQPOLL cpu after we've verified it\")\nSigned-off-by: Mark Rutland <mark.rutland@arm.com>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Alexander Viro <viro@zeniv.linux.org.uk>\nCc: linux-block@vger.kernel.org\nCc: linux-fsdevel@vger.kernel.org\nCc: linux-kernel@vger.kernel.org\n\nSimplied the logic\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1",
        "release_date": "2019-05-01 08:38:37 -0600 io_uring: fix SQPOLL cpu validation"
    },
    {
        "commit": "5c8b0b54db22c54f2aec991b388f550d3a927f26",
        "message": "Currently we only post a cqe if we get an error OUTSIDE of submission.\nFor submission, we return the error directly through io_uring_enter().\nThis is a bit awkward for applications, and it makes more sense to\nalways post a cqe with an error, if the error happens on behalf of an\nsqe.\n\nThis changes submission behavior a bit. io_uring_enter() returns -ERROR\nfor an error, and > 0 for number of sqes submitted. Before this change,\nif you wanted to submit 8 entries and had an error on the 5th entry,\nio_uring_enter() would return 4 (for number of entries successfully\nsubmitted) and rewind the sqring. The application would then have to\npeek at the sqring and figure out what was wrong with the head sqe, and\nthen skip it itself. With this change, we'll return 5 since we did\nconsume 5 sqes, and the last sqe (with the error) will result in a cqe\nbeing posted with the error.\n\nThis makes the logic easier to handle in the application, and it cleans\nup the submission part.\n\nSuggested-by: Stefan B\u00fchler <source@stbuehler.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1",
        "release_date": "2019-05-01 06:37:55 -0600 io_uring: have submission side sqe errors post a cqe"
    },
    {
        "commit": "62977281a6384d3904c02272a638cc3ac3bac54d",
        "message": "There is no operation to order with afterwards, and removing the flag is\nnot critical in any way.\n\nThere will always be a \"race condition\" where the application will\ntrigger IORING_ENTER_SQ_WAKEUP when it isn't actually needed.\n\nSigned-off-by: Stefan B\u00fchler <source@stbuehler.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1",
        "release_date": "2019-04-30 09:40:02 -0600 io_uring: remove unnecessary barrier after unsetting IORING_SQ_NEED_WAKEUP"
    },
    {
        "commit": "b841f19524a16cd93a39f9306191f85c549a2bc2",
        "message": "smp_store_release in io_commit_sqring already orders the store to\ndropped before the update to SQ head.\n\nSigned-off-by: Stefan B\u00fchler <source@stbuehler.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1",
        "release_date": "2019-04-30 09:40:02 -0600 io_uring: remove unnecessary barrier after incrementing dropped counter"
    },
    {
        "commit": "82ab082c0e2f8592c2ff6b2ab99a92d8406c8c2c",
        "message": "There is no operation before to order with.\n\nSigned-off-by: Stefan B\u00fchler <source@stbuehler.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1",
        "release_date": "2019-04-30 09:40:02 -0600 io_uring: remove unnecessary barrier before reading SQ tail"
    },
    {
        "commit": "9e4c15a3939448d2ea9b9bf59561183bbe3fdc49",
        "message": "There is no operation afterwards to order with.\n\nSigned-off-by: Stefan B\u00fchler <source@stbuehler.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1",
        "release_date": "2019-04-30 09:40:02 -0600 io_uring: remove unnecessary barrier after updating SQ head"
    },
    {
        "commit": "115e12e58dbc055e98c965e3255aed7b20214f95",
        "message": "The memory operations before reading cq head are unrelated and we\ndon't care about their order.\n\nDocument that the control dependency in combination with READ_ONCE and\nWRITE_ONCE forms a barrier we need.\n\nSigned-off-by: Stefan B\u00fchler <source@stbuehler.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1",
        "release_date": "2019-04-30 09:40:02 -0600 io_uring: remove unnecessary barrier before reading cq head"
    },
    {
        "commit": "4f7067c3fb7f2974363a28c597a41949d971af02",
        "message": "wq_has_sleeper has a full barrier internally. The smp_rmb barrier in\nio_uring_poll synchronizes with it.\n\nSigned-off-by: Stefan B\u00fchler <source@stbuehler.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1",
        "release_date": "2019-04-30 09:40:02 -0600 io_uring: remove unnecessary barrier before wq_has_sleeper"
    },
    {
        "commit": "1e84b97b7377bd0198f87b49ad3e396e84bf0458",
        "message": "The application reading the CQ ring needs a barrier to pair with the\nsmp_store_release in io_commit_cqring, not the barrier after it.\n\nAlso a write barrier *after* writing something (but not *before*\nwriting anything interesting) doesn't order anything, so an smp_wmb()\nafter writing SQ tail is not needed.\n\nAdditionally consider reading SQ head and writing CQ tail in the notes.\n\nAlso add some clarifications how the various other fields in the ring\nbuffers are used.\n\nSigned-off-by: Stefan B\u00fchler <source@stbuehler.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1",
        "release_date": "2019-04-30 09:40:02 -0600 io_uring: fix notes on barriers"
    },
    {
        "commit": "8449eedaa1da6a51d67190c905b1b54243e095f6",
        "message": "Not all request types set REQ_F_FORCE_NONBLOCK when they needed async\npunting; reverse logic instead and set REQ_F_NOWAIT if request mustn't\nbe punted.\n\nSigned-off-by: Stefan B\u00fchler <source@stbuehler.de>\n\nMerged with my previous patch for this.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1",
        "release_date": "2019-04-30 09:40:02 -0600 io_uring: fix handling SQEs requesting NOWAIT"
    },
    {
        "commit": "975a0f400f2e1b5f585fec0b8b4c5942c3b05792",
        "message": "Pull block fixes from Jens Axboe:\n \"A set of io_uring fixes that should go into this release. In\n  particular, this contains:\n\n   - The mutex lock vs ctx ref count fix (me)\n\n   - Removal of a dead variable (me)\n\n   - Two race fixes (Stefan)\n\n   - Ring head/tail condition fix for poll full SQ detection (Stefan)\"\n\n* tag 'for-linus-20190428' of git://git.kernel.dk/linux-block:\n  io_uring: remove 'state' argument from io_{read,write} path\n  io_uring: fix poll full SQ detection\n  io_uring: fix race condition when sq threads goes sleeping\n  io_uring: fix race condition reading SQ entries\n  io_uring: fail io_uring_register(2) on a dying io_uring instance",
        "kernel_version": "v5.1-rc7",
        "release_date": "2019-04-28 10:06:32 -0700 Merge tag 'for-linus-20190428' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "d286e13d53f54b00bcd7443eedd067cd432cf547",
        "message": "Pull syscall numbering updates from Arnd Bergmann:\n \"arch: add pidfd and io_uring syscalls everywhere\n\n  This comes a bit late, but should be in 5.1 anyway: we want the newly\n  added system calls to be synchronized across all architectures in the\n  release.\n\n  I hope that in the future, any newly added system calls can be added\n  to all architectures at the same time, and tested there while they are\n  in linux-next, avoiding dependencies between the architecture\n  maintainer trees and the tree that contains the new system call\"\n\n* tag 'syscalls-5.1' of git://git.kernel.org/pub/scm/linux/kernel/git/arnd/asm-generic:\n  arch: add pidfd and io_uring syscalls everywhere",
        "kernel_version": "v5.1-rc7",
        "release_date": "2019-04-23 13:34:17 -0700 Merge tag 'syscalls-5.1' of git://git.kernel.org/pub/scm/linux/kernel/git/arnd/asm-generic"
    },
    {
        "commit": "8358e3a8264a228cf2dfb6f3a05c0328f4118f12",
        "message": "Since commit 09bb839434b we don't use the state argument for any sort\nof on-stack caching in the io read and write path. Remove the stale\nand unused argument from them, and bubble it up to __io_submit_sqe()\nand down to io_prep_rw().\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc7",
        "release_date": "2019-04-23 08:17:58 -0600 io_uring: remove 'state' argument from io_{read,write} path"
    },
    {
        "commit": "fb775faa9e46ff481e4ced11116c9bd45359cb43",
        "message": "io_uring_poll shouldn't signal EPOLLOUT | EPOLLWRNORM if the queue is\nfull; the old check would always signal EPOLLOUT | EPOLLWRNORM (unless\nthere were U32_MAX - 1 entries in the SQ queue).\n\nSigned-off-by: Stefan B\u00fchler <source@stbuehler.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc7",
        "release_date": "2019-04-22 11:00:58 -0600 io_uring: fix poll full SQ detection"
    },
    {
        "commit": "0d7bae69c574c5f25802f8a71252e7d66933a3ab",
        "message": "Reading the SQ tail needs to come after setting IORING_SQ_NEED_WAKEUP in\nflags; there is no cheap barrier for ordering a store before a load, a\nfull memory barrier is required.\n\nUserspace needs a full memory barrier between updating SQ tail and\nchecking for the IORING_SQ_NEED_WAKEUP too.\n\nSigned-off-by: Stefan B\u00fchler <source@stbuehler.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc7",
        "release_date": "2019-04-22 11:00:56 -0600 io_uring: fix race condition when sq threads goes sleeping"
    },
    {
        "commit": "e523a29c4f2703bdb98f68ce1bb256e259fd8d5f",
        "message": "A read memory barrier is required between reading SQ tail and reading\nthe actual data belonging to the SQ entry.\n\nUserspace needs a matching write barrier between writing SQ entries and\nupdating SQ tail (using smp_store_release to update tail will do).\n\nSigned-off-by: Stefan B\u00fchler <source@stbuehler.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc7",
        "release_date": "2019-04-22 11:00:55 -0600 io_uring: fix race condition reading SQ entries"
    },
    {
        "commit": "35fa71a030caa50458a043560d4814ea9bcd639f",
        "message": "If we have multiple threads doing io_uring_register(2) on an io_uring\nfd, then we can potentially try and kill the percpu reference while\nsomeone else has already killed it.\n\nPrevent this race by failing io_uring_register(2) if the ref is marked\ndying. This is safe since we're inside the io_uring mutex.\n\nFixes: b19062a56726 (\"io_uring: fix possible deadlock between io_uring_{enter,register}\")\nReported-by: syzbot <syzbot+10d25e23199614b7721f@syzkaller.appspotmail.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc7",
        "release_date": "2019-04-22 10:37:07 -0600 io_uring: fail io_uring_register(2) on a dying io_uring instance"
    },
    {
        "commit": "38a2ca2cac28cd202a364e30f9fd252ac66619be",
        "message": "Pull block fixes from Jens Axboe:\n \"A set of small fixes that should go into this series. This contains:\n\n   - Removal of unused queue member (Hou)\n\n   - Overflow bvec fix (Ming)\n\n   - Various little io_uring tweaks (me)\n       - kthread parking\n       - Only call cpu_possible() for verified CPU\n       - Drop unused 'file' argument to io_file_put()\n       - io_uring_enter vs io_uring_register deadlock fix\n       - CQ overflow fix\n\n   - BFQ internal depth update fix (me)\"\n\n* tag 'for-linus-20190420' of git://git.kernel.dk/linux-block:\n  block: make sure that bvec length can't be overflow\n  block: kill all_q_node in request_queue\n  io_uring: fix CQ overflow condition\n  io_uring: fix possible deadlock between io_uring_{enter,register}\n  io_uring: drop io_file_put() 'file' argument\n  bfq: update internal depth state when queue depth changes\n  io_uring: only test SQPOLL cpu after we've verified it\n  io_uring: park SQPOLL thread if it's percpu",
        "kernel_version": "v5.1-rc6",
        "release_date": "2019-04-20 12:20:58 -0700 Merge tag 'for-linus-20190420' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "74f464e97044da33b25aaed00213914b0edf1f2e",
        "message": "This is a leftover from when the rings initially were not free flowing,\nand hence a test for tail + 1 == head would indicate full. Since we now\nlet them wrap instead of mask them with the size, we need to check if\nthey drift more than the ring size from each other.\n\nThis fixes a case where we'd overwrite CQ ring entries, if the user\nfailed to reap completions. Both cases would ultimately result in lost\ncompletions as the application violated the depth it asked for. The only\ndifference is that before this fix we'd return invalid entries for the\noverflowed completions, instead of properly flagging it in the\ncq_ring->overflow variable.\n\nReported-by: Stefan B\u00fchler <source@stbuehler.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc6",
        "release_date": "2019-04-17 11:41:49 -0600 io_uring: fix CQ overflow condition"
    },
    {
        "commit": "b19062a567266ee1f10f6709325f766bbcc07d1c",
        "message": "If we have multiple threads, one doing io_uring_enter() while the other\nis doing io_uring_register(), we can run into a deadlock between the\ntwo. io_uring_register() must wait for existing users of the io_uring\ninstance to exit. But it does so while holding the io_uring mutex.\nCallers of io_uring_enter() may need this mutex to make progress (and\neventually exit). If we wait for users to exit in io_uring_register(),\nwe can't do so with the io_uring mutex held without potentially risking\na deadlock.\n\nDrop the io_uring mutex while waiting for existing callers to exit. This\nis safe and guaranteed to make forward progress, since we already killed\nthe percpu ref before doing so. Hence later callers of io_uring_enter()\nwill be rejected.\n\nReported-by: syzbot+16dc03452dee970a0c3e@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc6",
        "release_date": "2019-04-15 10:49:38 -0600 io_uring: fix possible deadlock between io_uring_{enter,register}"
    },
    {
        "commit": "39036cd2727395c3369b1051005da74059a85317",
        "message": "Add the io_uring and pidfd_send_signal system calls to all architectures.\n\nThese system calls are designed to handle both native and compat tasks,\nso all entries are the same across architectures, only arm-compat and\nthe generic tale still use an old format.\n\nAcked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)\nAcked-by: Heiko Carstens <heiko.carstens@de.ibm.com> (s390)\nAcked-by: Geert Uytterhoeven <geert@linux-m68k.org>\nSigned-off-by: Arnd Bergmann <arnd@arndb.de>",
        "kernel_version": "v5.1-rc7",
        "release_date": "2019-04-15 16:31:17 +0200 arch: add pidfd and io_uring syscalls everywhere"
    },
    {
        "commit": "3d6770fbd9353988839611bab107e4e891506aad",
        "message": "Since the fget/fput handling was reworked in commit 09bb839434bd, we\nnever call io_file_put() with state == NULL (and hence file != NULL)\nanymore. Remove that case.\n\nReported-by: Al Viro <viro@zeniv.linux.org.uk>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc6",
        "release_date": "2019-04-13 19:08:22 -0600 io_uring: drop io_file_put() 'file' argument"
    },
    {
        "commit": "917257daa0fea7a007102691c0e27d9216a96768",
        "message": "We currently call cpu_possible() even if we don't use the CPU. Move the\ntest under the SQ_AFF branch, which is the only place where we'll use\nthe value. Do the cpu_possible() test AFTER we've limited it to a max\nof NR_CPUS. This avoids triggering the following warning:\n\nWARNING: CPU: 1 PID: 7600 at include/linux/cpumask.h:121 cpu_max_bits_warn\n\nif CONFIG_DEBUG_PER_CPU_MAPS is enabled.\n\nWhile in there, also move the SQ thread idle period assignment inside\nSETUP_SQPOLL, as we don't use it otherwise either.\n\nReported-by: syzbot+cd714a07c6de2bc34293@syzkaller.appspotmail.com\nFixes: 6c271ce2f1d5 (\"io_uring: add submission polling\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc6",
        "release_date": "2019-04-13 19:08:22 -0600 io_uring: only test SQPOLL cpu after we've verified it"
    },
    {
        "commit": "06058632464845abb1af91521122fd04dd3daaec",
        "message": "kthread expects this, or we can throw a warning on exit:\n\nWARNING: CPU: 0 PID: 7822 at kernel/kthread.c:399\n__kthread_bind_mask+0x3b/0xc0 kernel/kthread.c:399\nKernel panic - not syncing: panic_on_warn set ...\nCPU: 0 PID: 7822 Comm: syz-executor030 Not tainted 5.1.0-rc4-next-20190412\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS\nGoogle 01/01/2011\nCall Trace:\n  __dump_stack lib/dump_stack.c:77 [inline]\n  dump_stack+0x172/0x1f0 lib/dump_stack.c:113\n  panic+0x2cb/0x72b kernel/panic.c:214\n  __warn.cold+0x20/0x46 kernel/panic.c:576\n  report_bug+0x263/0x2b0 lib/bug.c:186\n  fixup_bug arch/x86/kernel/traps.c:179 [inline]\n  fixup_bug arch/x86/kernel/traps.c:174 [inline]\n  do_error_trap+0x11b/0x200 arch/x86/kernel/traps.c:272\n  do_invalid_op+0x37/0x50 arch/x86/kernel/traps.c:291\n  invalid_op+0x14/0x20 arch/x86/entry/entry_64.S:973\nRIP: 0010:__kthread_bind_mask+0x3b/0xc0 kernel/kthread.c:399\nCode: 48 89 fb e8 f7 ab 24 00 4c 89 e6 48 89 df e8 ac e1 02 00 31 ff 49 89\nc4 48 89 c6 e8 7f ad 24 00 4d 85 e4 75 15 e8 d5 ab 24 00 <0f> 0b e8 ce ab\n24 00 5b 41 5c 41 5d 41 5e 5d c3 e8 c0 ab 24 00 4c\nRSP: 0018:ffff8880a89bfbb8 EFLAGS: 00010293\nRAX: ffff88808ca7a280 RBX: ffff8880a98e4380 RCX: ffffffff814bdd11\nRDX: 0000000000000000 RSI: ffffffff814bdd1b RDI: 0000000000000007\nRBP: ffff8880a89bfbd8 R08: ffff88808ca7a280 R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000000\nR13: ffffffff87691148 R14: ffff8880a98e43a0 R15: ffffffff81c91e10\n  __kthread_bind kernel/kthread.c:412 [inline]\n  kthread_unpark+0x123/0x160 kernel/kthread.c:480\n  kthread_stop+0xfa/0x6c0 kernel/kthread.c:556\n  io_sq_thread_stop fs/io_uring.c:2057 [inline]\n  io_sq_thread_stop fs/io_uring.c:2052 [inline]\n  io_finish_async+0xab/0x180 fs/io_uring.c:2064\n  io_ring_ctx_free fs/io_uring.c:2534 [inline]\n  io_ring_ctx_wait_and_kill+0x133/0x510 fs/io_uring.c:2591\n  io_uring_release+0x42/0x50 fs/io_uring.c:2599\n  __fput+0x2e5/0x8d0 fs/file_table.c:278\n  ____fput+0x16/0x20 fs/file_table.c:309\n  task_work_run+0x14a/0x1c0 kernel/task_work.c:113\n  exit_task_work include/linux/task_work.h:22 [inline]\n  do_exit+0x90a/0x2fa0 kernel/exit.c:876\n  do_group_exit+0x135/0x370 kernel/exit.c:980\n  __do_sys_exit_group kernel/exit.c:991 [inline]\n  __se_sys_exit_group kernel/exit.c:989 [inline]\n  __x64_sys_exit_group+0x44/0x50 kernel/exit.c:989\n  do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290\n  entry_SYSCALL_64_after_hwframe+0x49/0xbe\n\nReported-by: syzbot+6d4a92619eb0ad08602b@syzkaller.appspotmail.com\nFixes: 6c271ce2f1d5 (\"io_uring: add submission polling\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc6",
        "release_date": "2019-04-13 19:08:22 -0600 io_uring: park SQPOLL thread if it's percpu"
    },
    {
        "commit": "4443f8e6ac7755cd775c70d08be8042dc2f936cb",
        "message": "Pull block fixes from Jens Axboe:\n \"Set of fixes that should go into this round. This pull is larger than\n  I'd like at this time, but there's really no specific reason for that.\n  Some are fixes for issues that went into this merge window, others are\n  not. Anyway, this contains:\n\n   - Hardware queue limiting for virtio-blk/scsi (Dongli)\n\n   - Multi-page bvec fixes for lightnvm pblk\n\n   - Multi-bio dio error fix (Jason)\n\n   - Remove the cache hint from the io_uring tool side, since we didn't\n     move forward with that (me)\n\n   - Make io_uring SETUP_SQPOLL root restricted (me)\n\n   - Fix leak of page in error handling for pc requests (J\u00e9r\u00f4me)\n\n   - Fix BFQ regression introduced in this merge window (Paolo)\n\n   - Fix break logic for bio segment iteration (Ming)\n\n   - Fix NVMe cancel request error handling (Ming)\n\n   - NVMe pull request with two fixes (Christoph):\n       - fix the initial CSN for nvme-fc (James)\n       - handle log page offsets properly in the target (Keith)\"\n\n* tag 'for-linus-20190412' of git://git.kernel.dk/linux-block:\n  block: fix the return errno for direct IO\n  nvmet: fix discover log page when offsets are used\n  nvme-fc: correct csn initialization and increments on error\n  block: do not leak memory in bio_copy_user_iov()\n  lightnvm: pblk: fix crash in pblk_end_partial_read due to multipage bvecs\n  nvme: cancel request synchronously\n  blk-mq: introduce blk_mq_complete_request_sync()\n  scsi: virtio_scsi: limit number of hw queues by nr_cpu_ids\n  virtio-blk: limit number of hw queues by nr_cpu_ids\n  block, bfq: fix use after free in bfq_bfqq_expire\n  io_uring: restrict IORING_SETUP_SQPOLL to root\n  tools/io_uring: remove IOCQE_FLAG_CACHEHIT\n  block: don't use for-inside-for in bio_for_each_segment_all",
        "kernel_version": "v5.1-rc5",
        "release_date": "2019-04-13 16:23:16 -0700 Merge tag 'for-linus-20190412' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "3ec482d15cb986bf08b923f9193eeddb3b9ca69f",
        "message": "This options spawns a kernel side thread that will poll for submissions\n(and completions, if IORING_SETUP_IOPOLL is set). As this allows a user\nto potentially use more cycles outside of the normal hierarchy,\nrestrict the use of this feature to root.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc5",
        "release_date": "2019-04-08 10:51:01 -0600 io_uring: restrict IORING_SETUP_SQPOLL to root"
    },
    {
        "commit": "704236672edacf353c362bab70c3d3eda7bb4a51",
        "message": "This ended up not being included in the mainline version of io_uring,\nso drop it from the test app as well.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc5",
        "release_date": "2019-04-08 10:48:50 -0600 tools/io_uring: remove IOCQE_FLAG_CACHEHIT"
    },
    {
        "commit": "429fba106e82e2792010a825b9dbeadd00bf9e9c",
        "message": "Pull block fixes from Jens Axboe:\n\n - Fixups for the pf/pcd queue handling (YueHaibing)\n\n - Revert of the three direct issue changes as they have been proven to\n   cause an issue with dm-mpath (Bart)\n\n - Plug rq_count reset fix (Dongli)\n\n - io_uring double free in fileset registration error handling (me)\n\n - Make null_blk handle bad numa node passed in (John)\n\n - BFQ ifdef fix (Konstantin)\n\n - Flush queue leak fix (Shenghui)\n\n - Plug trace fix (Yufen)\n\n* tag 'for-linus-20190407' of git://git.kernel.dk/linux-block:\n  xsysace: Fix error handling in ace_setup\n  null_blk: prevent crash from bad home_node value\n  block: Revert v5.0 blk_mq_request_issue_directly() changes\n  paride/pcd: Fix potential NULL pointer dereference and mem leak\n  blk-mq: do not reset plug->rq_count before the list is sorted\n  paride/pf: Fix potential NULL pointer dereference\n  io_uring: fix double free in case of fileset regitration failure\n  blk-mq: add trace block plug and unplug for multiple queues\n  block: use blk_free_flush_queue() to free hctx->fq in blk_mq_init_hctx\n  block/bfq: fix ifdef for CONFIG_BFQ_GROUP_IOSCHED=y",
        "kernel_version": "v5.1-rc4",
        "release_date": "2019-04-07 13:28:36 -1000 Merge tag 'for-linus-20190407' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "25adf50fe25d506d3fc12070a5ff4be858a1ac1b",
        "message": "Will Deacon reported the following KASAN complaint:\n\n[  149.890370] ==================================================================\n[  149.891266] BUG: KASAN: double-free or invalid-free in io_sqe_files_unregister+0xa8/0x140\n[  149.892218]\n[  149.892411] CPU: 113 PID: 3974 Comm: io_uring_regist Tainted: G    B             5.1.0-rc3-00012-g40b114779944 #3\n[  149.893623] Hardware name: linux,dummy-virt (DT)\n[  149.894169] Call trace:\n[  149.894539]  dump_backtrace+0x0/0x228\n[  149.895172]  show_stack+0x14/0x20\n[  149.895747]  dump_stack+0xe8/0x124\n[  149.896335]  print_address_description+0x60/0x258\n[  149.897148]  kasan_report_invalid_free+0x78/0xb8\n[  149.897936]  __kasan_slab_free+0x1fc/0x228\n[  149.898641]  kasan_slab_free+0x10/0x18\n[  149.899283]  kfree+0x70/0x1f8\n[  149.899798]  io_sqe_files_unregister+0xa8/0x140\n[  149.900574]  io_ring_ctx_wait_and_kill+0x190/0x3c0\n[  149.901402]  io_uring_release+0x2c/0x48\n[  149.902068]  __fput+0x18c/0x510\n[  149.902612]  ____fput+0xc/0x18\n[  149.903146]  task_work_run+0xf0/0x148\n[  149.903778]  do_notify_resume+0x554/0x748\n[  149.904467]  work_pending+0x8/0x10\n[  149.905060]\n[  149.905331] Allocated by task 3974:\n[  149.905934]  __kasan_kmalloc.isra.0.part.1+0x48/0xf8\n[  149.906786]  __kasan_kmalloc.isra.0+0xb8/0xd8\n[  149.907531]  kasan_kmalloc+0xc/0x18\n[  149.908134]  __kmalloc+0x168/0x248\n[  149.908724]  __arm64_sys_io_uring_register+0x2b8/0x15a8\n[  149.909622]  el0_svc_common+0x100/0x258\n[  149.910281]  el0_svc_handler+0x48/0xc0\n[  149.910928]  el0_svc+0x8/0xc\n[  149.911425]\n[  149.911696] Freed by task 3974:\n[  149.912242]  __kasan_slab_free+0x114/0x228\n[  149.912955]  kasan_slab_free+0x10/0x18\n[  149.913602]  kfree+0x70/0x1f8\n[  149.914118]  __arm64_sys_io_uring_register+0xc2c/0x15a8\n[  149.915009]  el0_svc_common+0x100/0x258\n[  149.915670]  el0_svc_handler+0x48/0xc0\n[  149.916317]  el0_svc+0x8/0xc\n[  149.916817]\n[  149.917101] The buggy address belongs to the object at ffff8004ce07ed00\n[  149.917101]  which belongs to the cache kmalloc-128 of size 128\n[  149.919197] The buggy address is located 0 bytes inside of\n[  149.919197]  128-byte region [ffff8004ce07ed00, ffff8004ce07ed80)\n[  149.921142] The buggy address belongs to the page:\n[  149.921953] page:ffff7e0013381f00 count:1 mapcount:0 mapping:ffff800503417c00 index:0x0 compound_mapcount: 0\n[  149.923595] flags: 0x1ffff00000010200(slab|head)\n[  149.924388] raw: 1ffff00000010200 dead000000000100 dead000000000200 ffff800503417c00\n[  149.925706] raw: 0000000000000000 0000000080400040 00000001ffffffff 0000000000000000\n[  149.927011] page dumped because: kasan: bad access detected\n[  149.927956]\n[  149.928224] Memory state around the buggy address:\n[  149.929054]  ffff8004ce07ec00: 00 00 00 00 00 00 00 00 fc fc fc fc fc fc fc fc\n[  149.930274]  ffff8004ce07ec80: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc\n[  149.931494] >ffff8004ce07ed00: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb\n[  149.932712]                    ^\n[  149.933281]  ffff8004ce07ed80: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc\n[  149.934508]  ffff8004ce07ee00: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc\n[  149.935725] ==================================================================\n\nwhich is due to a failure in registrering a fileset. This frees the\nctx->user_files pointer, but doesn't clear it. When the io_uring\ninstance is later freed through the normal channels, we free this\npointer again. At this point it's invalid.\n\nEnsure we clear the pointer when we free it for the error case.\n\nReported-by: Will Deacon <will.deacon@arm.com>\nTested-by: Will Deacon <will.deacon@arm.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc4",
        "release_date": "2019-04-03 09:52:40 -0600 io_uring: fix double free in case of fileset regitration failure"
    },
    {
        "commit": "590627f755bc385bd2b2fbd87de312a462889222",
        "message": "Pull perf tooling fixes from Thomas Gleixner:\n \"Core libraries:\n   - Fix max perf_event_attr.precise_ip detection.\n   - Fix parser error for uncore event alias\n   - Fixup ordering of kernel maps after obtaining the main kernel map\n     address.\n\n  Intel PT:\n   - Fix TSC slip where A TSC packet can slip past MTC packets so that\n     the timestamp appears to go backwards.\n   - Fixes for exported-sql-viewer GUI conversion to python3.\n\n  ARM coresight:\n   - Fix the build by adding a missing case value for enumeration value\n     introduced in newer library, that now is the required one.\n\n  tool headers:\n   - Syncronize kernel headers with the kernel, getting new io_uring and\n     pidfd_send_signal syscalls so that 'perf trace' can handle them\"\n\n* 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:\n  perf pmu: Fix parser error for uncore event alias\n  perf scripts python: exported-sql-viewer.py: Fix python3 support\n  perf scripts python: exported-sql-viewer.py: Fix never-ending loop\n  perf machine: Update kernel map address and re-order properly\n  tools headers uapi: Sync powerpc's asm/kvm.h copy with the kernel sources\n  tools headers: Update x86's syscall_64.tbl and uapi/asm-generic/unistd\n  tools headers uapi: Update drm/i915_drm.h\n  tools arch x86: Sync asm/cpufeatures.h with the kernel sources\n  tools headers uapi: Sync linux/fcntl.h to get the F_SEAL_FUTURE_WRITE addition\n  tools headers uapi: Sync asm-generic/mman-common.h and linux/mman.h\n  perf evsel: Fix max perf_event_attr.precise_ip detection\n  perf intel-pt: Fix TSC slip\n  perf cs-etm: Add missing case value",
        "kernel_version": "v5.1-rc3",
        "release_date": "2019-03-31 08:37:04 -0700 Merge branch 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip"
    },
    {
        "commit": "ffb8e45cf33e14d9a565491aec7abe039bebcfce",
        "message": "Pull block fixes from Jens Axboe:\n \"Small set of fixes that should go into this series. This contains:\n\n   - compat signal mask fix for io_uring (Arnd)\n\n   - EAGAIN corner case for direct vs buffered writes for io_uring\n     (Roman)\n\n   - NVMe pull request from Christoph with various little fixes\n\n   - sbitmap ws_active fix, which caused a perf regression for shared\n     tags (me)\n\n   - sbitmap bit ordering fix (Ming)\n\n   - libata on-stack DMA fix (Raymond)\"\n\n* tag 'for-linus-20190329' of git://git.kernel.dk/linux-block:\n  nvmet: fix error flow during ns enable\n  nvmet: fix building bvec from sg list\n  nvme-multipath: relax ANA state check\n  nvme-tcp: fix an endianess miss-annotation\n  libata: fix using DMA buffers on stack\n  io_uring: offload write to async worker in case of -EAGAIN\n  sbitmap: order READ/WRITE freed instance and setting clear bit\n  blk-mq: fix sbitmap ws_active for shared tags\n  io_uring: fix big-endian compat signal mask handling\n  blk-mq: update comment for blk_mq_hctx_has_pending()\n  blk-mq: use blk_mq_put_driver_tag() to put tag",
        "kernel_version": "v5.1-rc3",
        "release_date": "2019-03-29 14:43:07 -0700 Merge tag 'for-linus-20190329' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "22261fdf68f23df3ddf359642696d6ce98e584fa",
        "message": "Pull perf/urgent fixes from Arnaldo:\n\nCore libraries:\n  Jiri Olsa:\n  - Fix max perf_event_attr.precise_ip detection.\n\n  Kan Liang:\n  - Fix parser error for uncore event alias\n\n  Wei Lin:\n  - Fixup ordering of kernel maps after obtaining the main kernel map address.\n\nIntel PT:\n  Adrian Hunter:\n  - Fix TSC slip where A TSC packet can slip past MTC packets so that the\n    timestamp appears to go backwards.\n\n  - Fixes for exported-sql-viewer GUI conversion to python3.\n\nARM coresight:\n  Solomon Tan:\n  - Fix the build by adding a missing case value for enumeration value introduced\n    in newer library, that now is the required one.\n\ntool headers:\n  Arnaldo Carvalho de Melo:\n  - Syncronize kernel headers with the kernel, getting new io_uring and\n    pidfd_send_signal syscalls so that 'perf trace' can handle them.\n\nSigned-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>",
        "kernel_version": "v5.1-rc3",
        "release_date": "2019-03-29 21:28:58 +0100 Merge tag 'perf-urgent-for-mingo-5.1-20190329' of git://git.kernel.org/pub/scm/linux/kernel/git/acme/linux into perf/urgent"
    },
    {
        "commit": "8142bd82a59e452fefea7b21113101d6a87d9fa8",
        "message": "To pick up the changes introduced in the following csets:\n\n  2b188cc1bb85 (\"Add io_uring IO interface\")\n  edafccee56ff (\"io_uring: add support for pre-mapped user IO buffers\")\n  3eb39f47934f (\"signal: add pidfd_send_signal() syscall\")\n\nThis makes 'perf trace' to become aware of these new syscalls, so that\none can use them like 'perf trace -e ui_uring*,*signal' to do a system\nwide strace-like session looking at those syscalls, for instance.\n\nFor example:\n\n  # perf trace -s io_uring-cp ~acme/isos/RHEL-x86_64-dvd1.iso ~/bla\n\n   Summary of events:\n\n   io_uring-cp (383), 1208866 events, 100.0%\n\n     syscall         calls   total    min     avg     max   stddev\n                             (msec) (msec)  (msec)  (msec)     (%)\n     -------------- ------ -------- ------ ------- -------  ------\n     io_uring_enter 605780 2955.615  0.000   0.005  33.804   1.94%\n     openat              4  459.446  0.004 114.861 459.435 100.00%\n     munmap              4    0.073  0.009   0.018   0.042  44.03%\n     mmap               10    0.054  0.002   0.005   0.026  43.24%\n     brk                28    0.038  0.001   0.001   0.003   7.51%\n     io_uring_setup      1    0.030  0.030   0.030   0.030   0.00%\n     mprotect            4    0.014  0.002   0.004   0.005  14.32%\n     close               5    0.012  0.001   0.002   0.004  28.87%\n     fstat               3    0.006  0.001   0.002   0.003  35.83%\n     read                4    0.004  0.001   0.001   0.002  13.58%\n     access              1    0.003  0.003   0.003   0.003   0.00%\n     lseek               3    0.002  0.001   0.001   0.001   9.00%\n     arch_prctl          2    0.002  0.001   0.001   0.001   0.69%\n     execve              1    0.000  0.000   0.000   0.000   0.00%\n  #\n  # perf trace -e io_uring* -s io_uring-cp ~acme/isos/RHEL-x86_64-dvd1.iso ~/bla\n\n   Summary of events:\n\n   io_uring-cp (390), 1191250 events, 100.0%\n\n     syscall         calls   total    min    avg    max  stddev\n                             (msec) (msec) (msec) (msec)    (%)\n     -------------- ------ -------- ------ ------ ------ ------\n     io_uring_enter 597093 2706.060  0.001  0.005 14.761  1.10%\n     io_uring_setup      1    0.038  0.038  0.038  0.038  0.00%\n  #\n\nMore work needed to make the tools/perf/examples/bpf/augmented_raw_syscalls.c\nBPF program to copy the 'struct io_uring_params' arguments to perf's ring\nbuffer so that 'perf trace' can use the BTF info put in place by pahole's\nconversion of the kernel DWARF and then auto-beautify those arguments.\n\nThis patch produces the expected change in the generated syscalls table\nfor x86_64:\n\n  --- /tmp/build/perf/arch/x86/include/generated/asm/syscalls_64.c.before\t2019-03-26 13:37:46.679057774 -0300\n  +++ /tmp/build/perf/arch/x86/include/generated/asm/syscalls_64.c\t2019-03-26 13:38:12.755990383 -0300\n  @@ -334,5 +334,9 @@ static const char *syscalltbl_x86_64[] =\n   \t[332] = \"statx\",\n   \t[333] = \"io_pgetevents\",\n   \t[334] = \"rseq\",\n  +\t[424] = \"pidfd_send_signal\",\n  +\t[425] = \"io_uring_setup\",\n  +\t[426] = \"io_uring_enter\",\n  +\t[427] = \"io_uring_register\",\n   };\n  -#define SYSCALLTBL_x86_64_MAX_ID 334\n  +#define SYSCALLTBL_x86_64_MAX_ID 427\n\nThis silences these perf build warnings:\n\n  Warning: Kernel ABI header at 'tools/include/uapi/asm-generic/unistd.h' differs from latest version at 'include/uapi/asm-generic/unistd.h'\n  diff -u tools/include/uapi/asm-generic/unistd.h include/uapi/asm-generic/unistd.h\n  Warning: Kernel ABI header at 'tools/perf/arch/x86/entry/syscalls/syscall_64.tbl' differs from latest version at 'arch/x86/entry/syscalls/syscall_64.tbl'\n  diff -u tools/perf/arch/x86/entry/syscalls/syscall_64.tbl arch/x86/entry/syscalls/syscall_64.tbl\n\nCc: Adrian Hunter <adrian.hunter@intel.com>\nCc: Andrii Nakryiko <andrii.nakryiko@gmail.com>\nCc: Christian Brauner <christian@brauner.io>\nCc: Daniel Borkmann <daniel@iogearbox.net>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Jiri Olsa <jolsa@kernel.org>\nCc: Martin KaFai Lau <kafai@fb.com>\nCc: Namhyung Kim <namhyung@kernel.org>\nCc: Song Liu <songliubraving@fb.com>\nCc: Yonghong Song <yhs@fb.com>\nLink: https://lkml.kernel.org/n/tip-p0ars3otuc52x5iznf21shhw@git.kernel.org\nSigned-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>",
        "kernel_version": "v5.1-rc3",
        "release_date": "2019-03-28 14:41:11 -0300 tools headers: Update x86's syscall_64.tbl and uapi/asm-generic/unistd"
    },
    {
        "commit": "9bf7933fc3f306bc4ce74ad734f690a71670178a",
        "message": "In case of direct write -EAGAIN will be returned if page cache was\npreviously populated.  To avoid immediate completion of a request\nwith -EAGAIN error write has to be offloaded to the async worker,\nlike io_read() does.\n\nSigned-off-by: Roman Penyaev <rpenyaev@suse.de>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: linux-block@vger.kernel.org\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc3",
        "release_date": "2019-03-25 13:13:21 -0600 io_uring: offload write to async worker in case of -EAGAIN"
    },
    {
        "commit": "9e75ad5d8f399a21c86271571aa630dd080223e2",
        "message": "On big-endian architectures, the signal masks are differnet\nbetween 32-bit and 64-bit tasks, so we have to use a different\nfunction for reading them from user space.\n\nio_cqring_wait() initially got this wrong, and always interprets\nthis as a native structure. This is ok on x86 and most arm64,\nbut not on s390, ppc64be, mips64be, sparc64 and parisc.\n\nSigned-off-by: Arnd Bergmann <arnd@arndb.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc3",
        "release_date": "2019-03-25 10:06:03 -0600 io_uring: fix big-endian compat signal mask handling"
    },
    {
        "commit": "1bdd3dbfff7a308643c7f9ef74e4a8ef3923e686",
        "message": "Pull io_uring fixes and improvements from Jens Axboe:\n \"The first five in this series are heavily inspired by the work Al did\n  on the aio side to fix the races there.\n\n  The last two re-introduce a feature that was in io_uring before it got\n  merged, but which I pulled since we didn't have a good way to have\n  BVEC iters that already have a stable reference. These aren't\n  necessarily related to block, it's just how io_uring pins fixed\n  buffers\"\n\n* tag 'io_uring-20190323' of git://git.kernel.dk/linux-block:\n  block: add BIO_NO_PAGE_REF flag\n  iov_iter: add ITER_BVEC_FLAG_NO_REF flag\n  io_uring: mark me as the maintainer\n  io_uring: retry bulk slab allocs as single allocs\n  io_uring: fix poll races\n  io_uring: fix fget/fput handling\n  io_uring: add prepped flag\n  io_uring: make io_read/write return an integer\n  io_uring: use regular request ref counts",
        "kernel_version": "v5.1-rc2",
        "release_date": "2019-03-23 10:25:12 -0700 Merge tag 'io_uring-20190323' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "bf33a7699e992b12d4c7d39dc3f0b61f6b26c5c2",
        "message": "And io_uring as maintained in general.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc2",
        "release_date": "2019-03-18 10:44:48 -0600 io_uring: mark me as the maintainer"
    },
    {
        "commit": "fd6fab2cb78d3b6023c26ec53e0aa6f0b477d2f7",
        "message": "I've seen cases where bulk alloc fails, since the bulk alloc API\nis all-or-nothing - either we get the number we ask for, or it\nreturns 0 as number of entries.\n\nIf we fail a batch bulk alloc, retry a \"normal\" kmem_cache_alloc()\nand just use that instead of failing with -EAGAIN.\n\nWhile in there, ensure we use GFP_KERNEL. That was an oversight in\nthe original code, when we switched away from GFP_ATOMIC.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc2",
        "release_date": "2019-03-18 10:44:44 -0600 io_uring: retry bulk slab allocs as single allocs"
    },
    {
        "commit": "8c838788775a593527803786d376393b7c28f589",
        "message": "This is a straight port of Al's fix for the aio poll implementation,\nsince the io_uring version is heavily based on that. The below\ndescription is almost straight from that patch, just modified to\nfit the io_uring situation.\n\nio_poll() has to cope with several unpleasant problems:\n\t* requests that might stay around indefinitely need to\nbe made visible for io_cancel(2); that must not be done to\na request already completed, though.\n\t* in cases when ->poll() has placed us on a waitqueue,\nwakeup might have happened (and request completed) before ->poll()\nreturns.\n\t* worse, in some early wakeup cases request might end\nup re-added into the queue later - we can't treat \"woken up and\ncurrently not in the queue\" as \"it's not going to stick around\nindefinitely\"\n\t* ... moreover, ->poll() might have decided not to\nput it on any queues to start with, and that needs to be distinguished\nfrom the previous case\n\t* ->poll() might have tried to put us on more than one queue.\nOnly the first will succeed for io poll, so we might end up missing\nwakeups.  OTOH, we might very well notice that only after the\nwakeup hits and request gets completed (all before ->poll() gets\naround to the second poll_wait()).  In that case it's too late to\ndecide that we have an error.\n\nreq->woken was an attempt to deal with that.  Unfortunately, it was\nbroken.  What we need to keep track of is not that wakeup has happened -\nthe thing might come back after that.  It's that async reference is\nalready gone and won't come back, so we can't (and needn't) put the\nrequest on the list of cancellables.\n\nThe easiest case is \"request hadn't been put on any waitqueues\"; we\ncan tell by seeing NULL apt.head, and in that case there won't be\nanything async.  We should either complete the request ourselves\n(if vfs_poll() reports anything of interest) or return an error.\n\nIn all other cases we get exclusion with wakeups by grabbing the\nqueue lock.\n\nIf request is currently on queue and we have something interesting\nfrom vfs_poll(), we can steal it and complete the request ourselves.\n\nIf it's on queue and vfs_poll() has not reported anything interesting,\nwe either put it on the cancellable list, or, if we know that it\nhadn't been put on all queues ->poll() wanted it on, we steal it and\nreturn an error.\n\nIf it's _not_ on queue, it's either been already dealt with (in which\ncase we do nothing), or there's io_poll_complete_work() about to be\nexecuted.  In that case we either put it on the cancellable list,\nor, if we know it hadn't been put on all queues ->poll() wanted it on,\nsimulate what cancel would've done.\n\nFixes: 221c5eb23382 (\"io_uring: add support for IORING_OP_POLL\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc2",
        "release_date": "2019-03-15 15:28:57 -0600 io_uring: fix poll races"
    },
    {
        "commit": "09bb839434bd845c01da3d159b0c126fe7fa90da",
        "message": "This isn't a straight port of commit 84c4e1f89fef for aio.c, since\nio_uring doesn't use files in exactly the same way. But it's pretty\nclose. See the commit message for that commit.\n\nThis essentially fixes a use-after-free with the poll command\nhandling, but it takes cue from Linus's approach to just simplifying\nthe file handling. We move the setup of the file into a higher level\nlocation, so the individual commands don't have to deal with it. And\nthen we release the reference when we free the associated io_kiocb.\n\nFixes: 221c5eb23382 (\"io_uring: add support for IORING_OP_POLL\")\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc2",
        "release_date": "2019-03-15 11:17:05 -0600 io_uring: fix fget/fput handling"
    },
    {
        "commit": "d530a402a114efcf6d2b88d7f628856dade5b90b",
        "message": "We currently use the fact that if ->ki_filp is already set, then we've\ndone the prep. In preparation for moving the file assignment earlier,\nuse a separate flag to tell whether the request has been prepped for\nIO or not.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc2",
        "release_date": "2019-03-14 22:24:00 -0600 io_uring: add prepped flag"
    },
    {
        "commit": "e0c5c576d5074b5bb7b1b4b59848c25ceb521331",
        "message": "The callers all convert to an integer, and we only return 0/-ERROR\nanyway.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc2",
        "release_date": "2019-03-14 22:23:58 -0600 io_uring: make io_read/write return an integer"
    },
    {
        "commit": "e65ef56db4945fb18a0d522e056c02ddf939e644",
        "message": "Get rid of the special casing of \"normal\" requests not having\nany references to the io_kiocb. We initialize the ref count to 2,\none for the submission side, and one or the completion side.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc2",
        "release_date": "2019-03-14 22:08:43 -0600 io_uring: use regular request ref counts"
    },
    {
        "commit": "b5420237ec817b0b5f729a674c81ace0865c3b3b",
        "message": "All users of VM_MAX_READAHEAD actually convert it to kbytes and then to\npages. Define the macro explicitly as (SZ_128K / PAGE_SIZE). This\nsimplifies the expression in every filesystem. Also rename the macro to\nVM_READAHEAD_PAGES to properly convey its meaning. Finally remove unused\nVM_MIN_READAHEAD\n\n[akpm@linux-foundation.org: fix fs/io_uring.c, per Stephen]\nLink: http://lkml.kernel.org/r/20181221144053.24318-1-nborisov@suse.com\nSigned-off-by: Nikolay Borisov <nborisov@suse.com>\nReviewed-by: Matthew Wilcox <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Eric Van Hensbergen <ericvh@gmail.com>\nCc: Latchesar Ionkov <lucho@ionkov.net>\nCc: Dominique Martinet <asmadeus@codewreck.org>\nCc: David Howells <dhowells@redhat.com>\nCc: Chris Mason <clm@fb.com>\nCc: Josef Bacik <josef@toxicpanda.com>\nCc: David Sterba <dsterba@suse.com>\nCc: Miklos Szeredi <miklos@szeredi.hu>\nCc: Stephen Rothwell <sfr@canb.auug.org.au>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-03-12 10:04:01 -0700 mm: refactor readahead defines in mm.h"
    },
    {
        "commit": "38e7571c07be01f9f19b355a9306a4e3d5cb0f5b",
        "message": "Pull io_uring IO interface from Jens Axboe:\n \"Second attempt at adding the io_uring interface.\n\n  Since the first one, we've added basic unit testing of the three\n  system calls, that resides in liburing like the other unit tests that\n  we have so far. It'll take a while to get full coverage of it, but\n  we're working towards it. I've also added two basic test programs to\n  tools/io_uring. One uses the raw interface and has support for all the\n  various features that io_uring supports outside of standard IO, like\n  fixed files, fixed IO buffers, and polled IO. The other uses the\n  liburing API, and is a simplified version of cp(1).\n\n  This adds support for a new IO interface, io_uring.\n\n  io_uring allows an application to communicate with the kernel through\n  two rings, the submission queue (SQ) and completion queue (CQ) ring.\n  This allows for very efficient handling of IOs, see the v5 posting for\n  some basic numbers:\n\n    https://lore.kernel.org/linux-block/20190116175003.17880-1-axboe@kernel.dk/\n\n  Outside of just efficiency, the interface is also flexible and\n  extendable, and allows for future use cases like the upcoming NVMe\n  key-value store API, networked IO, and so on. It also supports async\n  buffered IO, something that we've always failed to support in the\n  kernel.\n\n  Outside of basic IO features, it supports async polled IO as well.\n  This particular feature has already been tested at Facebook months ago\n  for flash storage boxes, with 25-33% improvements. It makes polled IO\n  actually useful for real world use cases, where even basic flash sees\n  a nice win in terms of efficiency, latency, and performance. These\n  boxes were IOPS bound before, now they are not.\n\n  This series adds three new system calls. One for setting up an\n  io_uring instance (io_uring_setup(2)), one for submitting/completing\n  IO (io_uring_enter(2)), and one for aux functions like registrating\n  file sets, buffers, etc (io_uring_register(2)). Through the help of\n  Arnd, I've coordinated the syscall numbers so merge on that front\n  should be painless.\n\n  Jon did a writeup of the interface a while back, which (except for\n  minor details that have been tweaked) is still accurate. Find that\n  here:\n\n    https://lwn.net/Articles/776703/\n\n  Huge thanks to Al Viro for helping getting the reference cycle code\n  correct, and to Jann Horn for his extensive reviews focused on both\n  security and bugs in general.\n\n  There's a userspace library that provides basic functionality for\n  applications that don't need or want to care about how to fiddle with\n  the rings directly. It has helpers to allow applications to easily set\n  up an io_uring instance, and submit/complete IO through it without\n  knowing about the intricacies of the rings. It also includes man pages\n  (thanks to Jeff Moyer), and will continue to grow support helper\n  functions and features as time progresses. Find it here:\n\n    git://git.kernel.dk/liburing\n\n  Fio has full support for the raw interface, both in the form of an IO\n  engine (io_uring), but also with a small test application (t/io_uring)\n  that can exercise and benchmark the interface\"\n\n* tag 'io_uring-2019-03-06' of git://git.kernel.dk/linux-block:\n  io_uring: add a few test tools\n  io_uring: allow workqueue item to handle multiple buffered requests\n  io_uring: add support for IORING_OP_POLL\n  io_uring: add io_kiocb ref count\n  io_uring: add submission polling\n  io_uring: add file set registration\n  net: split out functions related to registering inflight socket files\n  io_uring: add support for pre-mapped user IO buffers\n  block: implement bio helper to add iter bvec pages to bio\n  io_uring: batch io_kiocb allocation\n  io_uring: use fget/fput_many() for file references\n  fs: add fget_many() and fput_many()\n  io_uring: support for IO polling\n  io_uring: add fsync support\n  Add io_uring IO interface",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-03-08 14:48:40 -0800 Merge tag 'io_uring-2019-03-06' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "80201fe175cbf7f3e372f53eba0a881a702ad926",
        "message": "Pull block layer updates from Jens Axboe:\n \"Not a huge amount of changes in this round, the biggest one is that we\n  finally have Mings multi-page bvec support merged. Apart from that,\n  this pull request contains:\n\n   - Small series that avoids quiescing the queue for sysfs changes that\n     match what we currently have (Aleksei)\n\n   - Series of bcache fixes (via Coly)\n\n   - Series of lightnvm fixes (via Mathias)\n\n   - NVMe pull request from Christoph. Nothing major, just SPDX/license\n     cleanups, RR mp policy (Hannes), and little fixes (Bart,\n     Chaitanya).\n\n   - BFQ series (Paolo)\n\n   - Save blk-mq cpu -> hw queue mapping, removing a pointer indirection\n     for the fast path (Jianchao)\n\n   - fops->iopoll() added for async IO polling, this is a feature that\n     the upcoming io_uring interface will use (Christoph, me)\n\n   - Partition scan loop fixes (Dongli)\n\n   - mtip32xx conversion from managed resource API (Christoph)\n\n   - cdrom registration race fix (Guenter)\n\n   - MD pull from Song, two minor fixes.\n\n   - Various documentation fixes (Marcos)\n\n   - Multi-page bvec feature. This brings a lot of nice improvements\n     with it, like more efficient splitting, larger IOs can be supported\n     without growing the bvec table size, and so on. (Ming)\n\n   - Various little fixes to core and drivers\"\n\n* tag 'for-5.1/block-20190302' of git://git.kernel.dk/linux-block: (117 commits)\n  block: fix updating bio's front segment size\n  block: Replace function name in string with __func__\n  nbd: propagate genlmsg_reply return code\n  floppy: remove set but not used variable 'q'\n  null_blk: fix checking for REQ_FUA\n  block: fix NULL pointer dereference in register_disk\n  fs: fix guard_bio_eod to check for real EOD errors\n  blk-mq: use HCTX_TYPE_DEFAULT but not 0 to index blk_mq_tag_set->map\n  block: optimize bvec iteration in bvec_iter_advance\n  block: introduce mp_bvec_for_each_page() for iterating over page\n  block: optimize blk_bio_segment_split for single-page bvec\n  block: optimize __blk_segment_map_sg() for single-page bvec\n  block: introduce bvec_nth_page()\n  iomap: wire up the iopoll method\n  block: add bio_set_polled() helper\n  block: wire up block device iopoll method\n  fs: add an iopoll method to struct file_operations\n  loop: set GENHD_FL_NO_PART_SCAN after blkdev_reread_part()\n  loop: do not print warn message if partition scan is successful\n  block: bounce: make sure that bvec table is updated\n  ...",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-03-08 14:12:17 -0800 Merge tag 'for-5.1/block-20190302' of git://git.kernel.dk/linux-block"
    },
    {
        "commit": "21b4aa5d20fd07207e73270cadffed5c63fb4343",
        "message": "This adds two test programs in tools/io_uring/ that demonstrate both\nthe raw io_uring API (and all features) through a small benchmark\napp, io_uring-bench, and the liburing exposed API in a simplified\ncp(1) implementation through io_uring-cp.\n\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-03-06 13:00:16 -0700 io_uring: add a few test tools"
    },
    {
        "commit": "31b515106428b9717d2b6475b6f6182cf231b1e6",
        "message": "Right now we punt any buffered request that ends up triggering an\n-EAGAIN to an async workqueue. This works fine in terms of providing\nasync execution of them, but it also can create quite a lot of work\nqueue items. For sequentially buffered IO, it's advantageous to\nserialize the issue of them. For reads, the first one will trigger a\nread-ahead, and subsequent request merely end up waiting on later pages\nto complete. For writes, devices usually respond better to streamed\nsequential writes.\n\nAdd state to track the last buffered request we punted to a work queue,\nand if the next one is sequential to the previous, attempt to get the\nprevious work item to handle it. We limit the number of sequential\nadd-ons to the a multiple (8) of the max read-ahead size of the file.\nThis should be a good number for both reads and wries, as it defines the\nmax IO size the device can do directly.\n\nThis drastically cuts down on the number of context switches we need to\nhandle buffered sequential IO, and a basic test case of copying a big\nfile with io_uring sees a 5x speedup.\n\nReviewed-by: Hannes Reinecke <hare@suse.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-03-06 13:00:16 -0700 io_uring: allow workqueue item to handle multiple buffered requests"
    },
    {
        "commit": "221c5eb2338232f7340386de1c43decc32682e58",
        "message": "This is basically a direct port of bfe4037e722e, which implements a\none-shot poll command through aio. Description below is based on that\ncommit as well. However, instead of adding a POLL command and relying\non io_cancel(2) to remove it, we mimic the epoll(2) interface of\nhaving a command to add a poll notification, IORING_OP_POLL_ADD,\nand one to remove it again, IORING_OP_POLL_REMOVE.\n\nTo poll for a file descriptor the application should submit an sqe of\ntype IORING_OP_POLL. It will poll the fd for the events specified in the\npoll_events field.\n\nUnlike poll or epoll without EPOLLONESHOT this interface always works in\none shot mode, that is once the sqe is completed, it will have to be\nresubmitted.\n\nReviewed-by: Hannes Reinecke <hare@suse.com>\nBased-on-code-from: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-03-06 13:00:12 -0700 io_uring: add support for IORING_OP_POLL"
    },
    {
        "commit": "c16361c1d805b6ea50c3c1fc5c314e944c71a984",
        "message": "We'll use this for the POLL implementation. Regular requests will\nNOT be using references, so initialize it to 0. Any real use of\nthe io_kiocb ref will initialize it to at least 2.\n\nReviewed-by: Hannes Reinecke <hare@suse.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-02-28 08:24:23 -0700 io_uring: add io_kiocb ref count"
    },
    {
        "commit": "6c271ce2f1d572f7fa225700a13cfe7ced492434",
        "message": "This enables an application to do IO, without ever entering the kernel.\nBy using the SQ ring to fill in new sqes and watching for completions\non the CQ ring, we can submit and reap IOs without doing a single system\ncall. The kernel side thread will poll for new submissions, and in case\nof HIPRI/polled IO, it'll also poll for completions.\n\nBy default, we allow 1 second of active spinning. This can by changed\nby passing in a different grace period at io_uring_register(2) time.\nIf the thread exceeds this idle time without having any work to do, it\nwill set:\n\nsq_ring->flags |= IORING_SQ_NEED_WAKEUP.\n\nThe application will have to call io_uring_enter() to start things back\nup again. If IO is kept busy, that will never be needed. Basically an\napplication that has this feature enabled will guard it's\nio_uring_enter(2) call with:\n\nread_barrier();\nif (*sq_ring->flags & IORING_SQ_NEED_WAKEUP)\n\tio_uring_enter(fd, 0, 0, IORING_ENTER_SQ_WAKEUP);\n\ninstead of calling it unconditionally.\n\nIt's mandatory to use fixed files with this feature. Failure to do so\nwill result in the application getting an -EBADF CQ entry when\nsubmitting IO.\n\nReviewed-by: Hannes Reinecke <hare@suse.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-02-28 08:24:23 -0700 io_uring: add submission polling"
    },
    {
        "commit": "6b06314c47e141031be043539900d80d2c7ba10f",
        "message": "We normally have to fget/fput for each IO we do on a file. Even with\nthe batching we do, the cost of the atomic inc/dec of the file usage\ncount adds up.\n\nThis adds IORING_REGISTER_FILES, and IORING_UNREGISTER_FILES opcodes\nfor the io_uring_register(2) system call. The arguments passed in must\nbe an array of __s32 holding file descriptors, and nr_args should hold\nthe number of file descriptors the application wishes to pin for the\nduration of the io_uring instance (or until IORING_UNREGISTER_FILES is\ncalled).\n\nWhen used, the application must set IOSQE_FIXED_FILE in the sqe->flags\nmember. Then, instead of setting sqe->fd to the real fd, it sets sqe->fd\nto the index in the array passed in to IORING_REGISTER_FILES.\n\nFiles are automatically unregistered when the io_uring instance is torn\ndown. An application need only unregister if it wishes to register a new\nset of fds.\n\nReviewed-by: Hannes Reinecke <hare@suse.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-02-28 08:24:23 -0700 io_uring: add file set registration"
    },
    {
        "commit": "f4e65870e5cede5ca1ec0006b6c9803994e5f7b8",
        "message": "We need this functionality for the io_uring file registration, but\nwe cannot rely on it since CONFIG_UNIX can be modular. Move the helpers\nto a separate file, that's always builtin to the kernel if CONFIG_UNIX is\nm/y.\n\nNo functional changes in this patch, just moving code around.\n\nReviewed-by: Hannes Reinecke <hare@suse.com>\nAcked-by: David S. Miller <davem@davemloft.net>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-02-28 08:24:23 -0700 net: split out functions related to registering inflight socket files"
    },
    {
        "commit": "edafccee56ff31678a091ddb7219aba9b28bc3cb",
        "message": "If we have fixed user buffers, we can map them into the kernel when we\nsetup the io_uring. That avoids the need to do get_user_pages() for\neach and every IO.\n\nTo utilize this feature, the application must call io_uring_register()\nafter having setup an io_uring instance, passing in\nIORING_REGISTER_BUFFERS as the opcode. The argument must be a pointer to\nan iovec array, and the nr_args should contain how many iovecs the\napplication wishes to map.\n\nIf successful, these buffers are now mapped into the kernel, eligible\nfor IO. To use these fixed buffers, the application must use the\nIORING_OP_READ_FIXED and IORING_OP_WRITE_FIXED opcodes, and then\nset sqe->index to the desired buffer index. sqe->addr..sqe->addr+seq->len\nmust point to somewhere inside the indexed buffer.\n\nThe application may register buffers throughout the lifetime of the\nio_uring instance. It can call io_uring_register() with\nIORING_UNREGISTER_BUFFERS as the opcode to unregister the current set of\nbuffers, and then register a new set. The application need not\nunregister buffers explicitly before shutting down the io_uring\ninstance.\n\nIt's perfectly valid to setup a larger buffer, and then sometimes only\nuse parts of it for an IO. As long as the range is within the originally\nmapped region, it will work just fine.\n\nFor now, buffers must not be file backed. If file backed buffers are\npassed in, the registration will fail with -1/EOPNOTSUPP. This\nrestriction may be relaxed in the future.\n\nRLIMIT_MEMLOCK is used to check how much memory we can pin. A somewhat\narbitrary 1G per buffer size is also imposed.\n\nReviewed-by: Hannes Reinecke <hare@suse.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-02-28 08:24:23 -0700 io_uring: add support for pre-mapped user IO buffers"
    },
    {
        "commit": "2579f913d41a086563bb81762c519f3d62ddee37",
        "message": "Similarly to how we use the state->ios_left to know how many references\nto get to a file, we can use it to allocate the io_kiocb's we need in\nbulk.\n\nReviewed-by: Hannes Reinecke <hare@suse.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-02-28 08:24:23 -0700 io_uring: batch io_kiocb allocation"
    },
    {
        "commit": "9a56a2323dbbd8ed7f380a5af7ae3ff82caa55a6",
        "message": "Add a separate io_submit_state structure, to cache some of the things\nwe need for IO submission.\n\nOne such example is file reference batching. io_submit_state. We get as\nmany references as the number of sqes we are submitting, and drop\nunused ones if we end up switching files. The assumption here is that\nwe're usually only dealing with one fd, and if there are multiple,\nhopefuly they are at least somewhat ordered. Could trivially be extended\nto cover multiple fds, if needed.\n\nOn the completion side we do the same thing, except this is trivially\ndone just locally in io_iopoll_reap().\n\nReviewed-by: Hannes Reinecke <hare@suse.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-02-28 08:24:23 -0700 io_uring: use fget/fput_many() for file references"
    },
    {
        "commit": "def596e9557c91d9846fc4d84d26f2c564644416",
        "message": "Add support for a polled io_uring instance. When a read or write is\nsubmitted to a polled io_uring, the application must poll for\ncompletions on the CQ ring through io_uring_enter(2). Polled IO may not\ngenerate IRQ completions, hence they need to be actively found by the\napplication itself.\n\nTo use polling, io_uring_setup() must be used with the\nIORING_SETUP_IOPOLL flag being set. It is illegal to mix and match\npolled and non-polled IO on an io_uring.\n\nReviewed-by: Hannes Reinecke <hare@suse.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-02-28 08:24:23 -0700 io_uring: support for IO polling"
    },
    {
        "commit": "c992fe2925d776be066d9f6cc13f9ea11d78b657",
        "message": "Add a new fsync opcode, which either syncs a range if one is passed,\nor the whole file if the offset and length fields are both cleared\nto zero.  A flag is provided to use fdatasync semantics, that is only\nforce out metadata which is required to retrieve the file data, but\nnot others like metadata.\n\nReviewed-by: Hannes Reinecke <hare@suse.com>\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-02-28 08:24:23 -0700 io_uring: add fsync support"
    },
    {
        "commit": "2b188cc1bb857a9d4701ae59aa7768b5124e262e",
        "message": "The submission queue (SQ) and completion queue (CQ) rings are shared\nbetween the application and the kernel. This eliminates the need to\ncopy data back and forth to submit and complete IO.\n\nIO submissions use the io_uring_sqe data structure, and completions\nare generated in the form of io_uring_cqe data structures. The SQ\nring is an index into the io_uring_sqe array, which makes it possible\nto submit a batch of IOs without them being contiguous in the ring.\nThe CQ ring is always contiguous, as completion events are inherently\nunordered, and hence any io_uring_cqe entry can point back to an\narbitrary submission.\n\nTwo new system calls are added for this:\n\nio_uring_setup(entries, params)\n\tSets up an io_uring instance for doing async IO. On success,\n\treturns a file descriptor that the application can mmap to\n\tgain access to the SQ ring, CQ ring, and io_uring_sqes.\n\nio_uring_enter(fd, to_submit, min_complete, flags, sigset, sigsetsize)\n\tInitiates IO against the rings mapped to this fd, or waits for\n\tthem to complete, or both. The behavior is controlled by the\n\tparameters passed in. If 'to_submit' is non-zero, then we'll\n\ttry and submit new IO. If IORING_ENTER_GETEVENTS is set, the\n\tkernel will wait for 'min_complete' events, if they aren't\n\talready available. It's valid to set IORING_ENTER_GETEVENTS\n\tand 'min_complete' == 0 at the same time, this allows the\n\tkernel to return already completed events without waiting\n\tfor them. This is useful only for polling, as for IRQ\n\tdriven IO, the application can just check the CQ ring\n\twithout entering the kernel.\n\nWith this setup, it's possible to do async IO with a single system\ncall. Future developments will enable polled IO with this interface,\nand polled submission as well. The latter will enable an application\nto do IO without doing ANY system calls at all.\n\nFor IRQ driven IO, an application only needs to enter the kernel for\ncompletions if it wants to wait for them to occur.\n\nEach io_uring is backed by a workqueue, to support buffered async IO\nas well. We will only punt to an async context if the command would\nneed to wait for IO on the device side. Any data that can be accessed\ndirectly in the page cache is done inline. This avoids the slowness\nissue of usual threadpools, since cached data is accessed as quickly\nas a sync interface.\n\nSample application: http://git.kernel.dk/cgit/fio/plain/t/io_uring.c\n\nReviewed-by: Hannes Reinecke <hare@suse.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-02-28 08:24:23 -0700 Add io_uring IO interface"
    },
    {
        "commit": "6fb845f0e78de19eaaf6a2d351702474e44b6a9e",
        "message": "Pull in 5.0-rc6 to avoid a dumb merge conflict with fs/iomap.c.\nThis is needed since io_uring is now based on the block branch,\nto avoid a conflict between the multi-page bvecs and the bits\nof io_uring that touch the core block parts.\n\n* tag 'v5.0-rc6': (525 commits)\n  Linux 5.0-rc6\n  x86/mm: Make set_pmd_at() paravirt aware\n  MAINTAINERS: Update the ocores i2c bus driver maintainer, etc\n  blk-mq: remove duplicated definition of blk_mq_freeze_queue\n  Blk-iolatency: warn on negative inflight IO counter\n  blk-iolatency: fix IO hang due to negative inflight counter\n  MAINTAINERS: unify reference to xen-devel list\n  x86/mm/cpa: Fix set_mce_nospec()\n  futex: Handle early deadlock return correctly\n  futex: Fix barrier comment\n  net: dsa: b53: Fix for failure when irq is not defined in dt\n  blktrace: Show requests without sector\n  mips: cm: reprime error cause\n  mips: loongson64: remove unreachable(), fix loongson_poweroff().\n  sit: check if IPv6 enabled before calling ip6_err_gen_icmpv6_unreach()\n  geneve: should not call rt6_lookup() when ipv6 was disabled\n  KVM: nVMX: unconditionally cancel preemption timer in free_nested (CVE-2019-7221)\n  KVM: x86: work around leak of uninitialized stack contents (CVE-2019-7222)\n  kvm: fix kvm_ioctl_create_device() reference counting (CVE-2019-6974)\n  signal: Better detection of synchronous signals\n  ...",
        "kernel_version": "v5.1-rc1",
        "release_date": "2019-02-15 08:43:59 -0700 Merge tag 'v5.0-rc6' into for-5.1/block"
    }
]